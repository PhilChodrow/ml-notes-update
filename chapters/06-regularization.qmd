---
title: Features and Regularization
subtitle: Our first look at methods for using and controlling model complexity
---

{{< include ../assets/includes/_colab-link.qmd >}}

::: {.content-hidden}

## Boilerplate preamble for LaTeX macros and Python viz style
$$
{{< include ../assets/includes/_macros.tex >}}
$$

```{python}
import style

```
:::

Suppose that we'd like to model a distinctly *nonlinear* relationship in our data: 
```{python}
import torch 
from matplotlib import pyplot as plt
scatterplot_kwargs = dict(color = "black", label = "data", facecolors = "none", s = 40, alpha = 0.6)

sig = 0.2
X = torch.linspace(0, 2, 30).reshape(-1, 1)

signal = torch.sin(2*torch.pi*X)
noise = sig*torch.randn(X.shape)
y = signal + noise

fig, ax = plt.subplots()
ax.scatter(X.numpy(), y.numpy(), **scatterplot_kwargs)
plt.xlabel(r"$x$")
plt.ylabel(r"$y$")

```

One flexible candidate model for this kind of data is a nonlinear Gaussian model: 

::: {#def-nonlinear-gaussian}

A **nonlinear Gaussian model** with *mean function $f$* and *variance $\sigma^2$* is a probabilistic model for data $(\vx_i, y_i)_{i=1}^n$ such that

$$
\begin{aligned}
    y_i \sim \cN(f(\vx_i); \sigma^2)\;.
\end{aligned}
$$

That is, each target value $y_i$ has Gaussian distribution with a mean (or signal) given by $f(\vx_i)$ and variance (or noise level) $\sigma^2$.

:::

Here's a visualization of this model with $f(x) = \sin(2\pi x)$ and $\sigma^2 = 0.01$, which was the setting used to generate the synthetic data above: 

::: {#fig-nonlinear-gaussian}

```{python}
#| code-fold: true

def draw_gaussian_at(support, sd=1.0, height=1.0, 
        xpos=0.0, ypos=0.0, ax=None, **kwargs):
    if ax is None:
        ax = plt.gca()
    gaussian = torch.exp((-support ** 2.0) / (2 * sd ** 2.0))
    gaussian /= gaussian.max()
    gaussian *= height
    return ax.plot(gaussian + xpos, support + ypos, **kwargs)
    
support = torch.linspace(-0.3, 0.3, 1000)
fig, ax = plt.subplots()

ax.plot(X.numpy(), torch.sin(2*torch.pi*X).numpy(), color = "black", linestyle = "--", label = r"Signal: $f(x_i) = \sin(2\pi x_i)$")

for each in X:
    draw_gaussian_at(support, sd=sig, height=0.1, xpos=each, ypos=torch.sin(2*torch.pi*each), ax=ax, color='C0', alpha=0.4)

ax.scatter(X.numpy(), y.numpy(), **scatterplot_kwargs)

plt.xlabel(r"$x$")
plt.ylabel(r"$y$")
plt.title("Data generated from a nonlinear Gaussian model")
plt.legend()
plt.show()
```

Data modeled via a nonlinear Gaussian model with mean function $f(x) = \sin(2\pi x)$ and noise level $\sigma = 0.2$. The dashed line indicates the signal function, while the blue curves indicate the conditional distributions of $y$ given $x$. The black circles are the observed data points.

:::


This is all well and good as a theoretical framework, but how in the world were we supposed to know that $f(x) = \sin(2\pi x)$ was the right choice? In practice, of course, we never will. 

## Basis Function Expansion

Since we don't know the right functional form for $f$, one common approach is try to approximate 

$$
\begin{aligned}
    f(\vx) \approx w_0\phi_0(\vx) + w_1 \phi_1(\vx) + w_2 \phi_2(\vx) + \cdots + w_p \phi_p(\vx) = \sum_{j=0}^p w_j \phi_j(\vx)\;,
\end{aligned}
$$
where $\phi_j(\cdot)$ are a collection of *basis functions* that we choose ahead of time. This is called a **basis function expansion**. [Each $\phi_j$ is often also called a *feature map*.]{.aside}

Before we look at some examples of basis functions, let's take a look at the fundamental trick behind basis function expansions: when using a basis function expansion with a nonlinear Gaussian model, the model is *linear* in the parameters $w_j$, and we can therefore use our previously-developed machinery for linear regression to fit the model. To see this, note that we can write 

$$
\begin{aligned}
    \sum_{j=0}^p w_j \phi_j(\vx) = \vw^T \vphi(\vx)\;,
\end{aligned}
$$

where we've defined the vector of parameters $\vw = (w_0, w_1, \ldots, w_p)^T$ and the vector of basis functions $\vphi(\vx) = (\phi_0(\vx), \phi_1(\vx), \ldots, \phi_p(\vx))^T$. Let's give the shorthand $\hat{y}_i = \vw^T \vphi(\vx_i)$ for the prediction of the model at input $\vx_i$. Then, taken together the nonlinear Gaussian model with basis function expansion says that 

$$
\begin{aligned}
    y_i \sim \cN(\hat{y}_i; \sigma^2) = \cN(\vw^T \vphi(\vx_i); \sigma^2)\;.
\end{aligned}
$$

This is just like the linear-Gaussian model, but with the input data $\vx_i$ replaced by the transformed data $\vphi(\vx_i)$. Therefore, we can use our previous results for maximum likelihood estimation of the parameters $\vw$ in the linear-Gaussian model, simply by replacing each occurrence of $\vx_i$ with $\vphi(\vx_i)$. In particular, we can maximize the log-likelihood of the data by minimizing the mean squared error between the predictions $\hat{y}_i$ and the observed targets $y_i$:

$$
\begin{aligned}
    R(\mX, \vy; \vw) = \frac{1}{n} \sum_{i=1}^n (y_i - \vw^T \vphi(\vx_i))^2\;.
\end{aligned}
$$

If we define 

$$
\begin{aligned}
    \mPhi = \left[\begin{matrix}
        - &\vphi(\vx_1)^T & -\\
        - &\vphi(\vx_2)^T & -\\
        \vdots \\
        - &\vphi(\vx_n)^T & -    
    \end{matrix}\right]\;,
\end{aligned}
$$

we can similarly write the mean squared error in matrix form as

$$
\begin{aligned}
    R(\mPhi, \vy; \vw) = \frac{1}{n} \norm{\mPhi \vw - \vy}^2\;.
\end{aligned}
$$

So, to learn a model with basis function expansion, all we need to do is construct the feature matrix $\mPhi$ by applying the basis functions to each data point, and then run linear regression as before. 

Let's try basis function expansion on our synthetic data. For this, we'll first bring in the linear regression model that we developed previously: 

```{python}
class LinearRegression:
    def __init__(self, n_params):
        self.w = torch.zeros(n_params, 1, requires_grad=True)

    def forward(self, X):
        return X @ self.w
```

We'll also write a simple training loop, this time using some of PyTorch's built-in optimization functionality:

```{python}
def mse(y_pred, y):
    return torch.mean((y_pred - y) ** 2)

def train_model(model, X_train, y_train, lr=1e-2, n_epochs=1000, tol=1e-4, regularization = None):
    opt = torch.optim.Adam(params=[model.w], lr=lr)
    for epoch in range(n_epochs):
        y_pred = model.forward(X_train)
        loss = mse(y_pred, y_train) + (regularization(model.w) if regularization is not None else 0.0) #<1>
        opt.zero_grad()
        loss.backward()
        opt.step()
        if model.w.grad is not None:
            if model.w.grad.norm().item() < tol:
                break
```
1. This function adds a regularization term (introduced below) to the loss if provided.


It's helpful to think about the minimal linear regression model, in which we simply add a constant column to the data, as an example of a basis function expansion. We'll call this the *linear* basis function. 

```{python}
def linear_basis_function(X):
    """
    just adds the constant feature
    """
    n_samples = X.shape[0]
    Phi = torch.ones(n_samples, 2)  # intercept + linear term
    Phi[:, 1] = X.flatten()
    return Phi
```

If we try fitting this model to the sinusoidal data directly, we'll be disappointed. 

```{python}
#---
PHI = linear_basis_function(X)
LR = LinearRegression(n_params=2)  # intercept + slope
train_model(LR, PHI, y, lr=1e-2, n_epochs=1000)
#---
```

```{python}
#| code-fold: true
# utility function for model visualization
def viz_model_predictions(model, X, y, basis_fun, ax, **basis_fun_kwargs): 

    x_new = torch.linspace(X.min(), X.max(), 1000).reshape(-1, 1)
    PHI_new = basis_fun(x_new, **basis_fun_kwargs)
    y_pred = model.forward(PHI_new)
    ax.scatter(X.numpy(), y.numpy(), **scatterplot_kwargs)
    ax.plot(x_new.numpy(), y_pred.detach().numpy(), color='red', label='Prediction')
    ax.set_xlabel(r"$x$")
    ax.set_ylabel(r"$y$")
```

How did we do? 

::: {#fig-linear-regression-sin}

```{python}
fig, ax = plt.subplots()
viz_model_predictions(LR, X, y, basis_fun = linear_basis_function, ax=ax)
plt.title("Linear Regression Fit to Nonlinear Data")
plt.legend()
plt.show()
```

Example of linear regression fit to the sinusoidal data set using the linear (trivial) basis function expansion. 

:::

Now let's try some nontrivial basis function expansions. If we had reason to believe that our data was periodic, we might try using sine and cosine basis functions. For example, let's try: 

$$
\begin{aligned}
    \phi_0(x) &= 1\;,\\
    \phi_1(x) &= \sin(\pi x)\;,\\
    \phi_2(x) &= \sin(2 \pi x)\;,\\
    \phi_3(x) &= \sin(3 \pi x)\;,\\
    \vdots
\end{aligned}
$$

The following function constructs the feature matrix $\mPhi$ for this basis function expansion:

```{python}
def sinusoidal_features(X, max_freq=4):
    n_samples = X.shape[0]
    Phi = torch.ones(n_samples, max_freq + 1)  
    for i in range(1, max_freq + 1):
        Phi[:, i] = torch.sin(i * torch.pi * X).flatten()
    return Phi
```

To train models and make predictions, all we need to do is call this function to get the feature matrix, and then run linear regression as before.


```{python}
#---
# engineer features
max_freq = 15
PHI = sinusoidal_features(X, max_freq=max_freq)

# train the model as usual
LR = LinearRegression(n_params=PHI.shape[1])
train_model(LR, PHI, y, lr=1e-2, n_epochs=2000)
#---
```

::: {#fig-sinusoidal-basis-functions-example}

```{python}
#| code-fold: true
# visualize
fig, ax = plt.subplots()

viz_model_predictions(LR, X, y, basis_fun = sinusoidal_features, ax=ax, max_freq=max_freq)

plt.title(f"Sinusoidal Basis Functions with max_freq={max_freq}")
plt.legend()

```

Example fit to data using the sinusoidal basis function expansion with maximum frequency 15.

:::


Let's try this for a variety of maximum frequencies. 

::: {#fig-sinusoidal-basis-functions-varying-complexity}

```{python}
#| code-fold: true
fig, axarr = plt.subplots(2, 3, figsize=(8,5))

max_freqs = [0, 1, 2, 5, 10, 20]

for i, ax in enumerate(axarr.flatten()):
    
    PHI = sinusoidal_features(X, max_freq=max_freqs[i])
    LR = LinearRegression(n_params=PHI.shape[1])
    train_model(LR, PHI, y, lr=1e-2, n_epochs=2000)
   
    viz_model_predictions(LR, X, y, basis_fun = sinusoidal_features, ax=ax, max_freq=max_freqs[i])

    mse_val = mse(LR.forward(PHI), y).item()
    ax.set_title(f"max_freq={max_freqs[i], } | MSE={mse_val:.3f}")

    if i == 0:
        ax.legend()
plt.tight_layout()
```

Example fits to training data using the sinusoidal basis function expansion with varying maximum frequencies.

:::

As the number of basis functions increases, the model becomes more flexible and is able to better fit the training data. However, with too many basis functions, the model begins to overfit the data, capturing noise rather than the underlying signal as reflected in the seemingly random high-frequency oscillations in the predictions. 

## Kernel Basis Functions

Another common choice of basis functions are *kernel basis functions*. A kernel is simply a measure of similarity between two inputs. A common choice is the Gaussian radial-basis kernel, which is defined in one dimension by

$$
k(x, c) = \exp\left(-(x - c)^2\right)\;,
$$

The Guassian kernel measures similarity between $x$ and a center point $c$, with values close to 1 when $x$ is near $c$ and values close to 0 when $x$ is far from $c$.

Here's an implementation in one dimension, where we evenly space choices of $c$ out across the range of the data:

```{python}
def kernel_features(X, num_kernels):
    n_samples = X.shape[0]
    Phi = torch.ones(n_samples, num_kernels + 1)  
    centers = torch.linspace(X.min(), X.max(), num_kernels)
    bandwidth = (X.max() - X.min()) / num_kernels
    for j in range(num_kernels):
        Phi[:, j + 1] = torch.exp(-0.5 * ((X.flatten() - centers[j]) / bandwidth) ** 2)
    return Phi
```

We can now run the same experiment as before: 


::: {#fig-kernel-basis-functions-varying-complexity}

```{python}
#| code-fold: true

fig, axarr = plt.subplots(2, 3, figsize=(8,5))

num_kernels = [0, 3, 5, 10, 20, 30]

for i, ax in enumerate(axarr.flatten()):
    
    PHI = kernel_features(X, num_kernels=num_kernels[i])
    LR = LinearRegression(n_params=PHI.shape[1])
    train_model(LR, PHI, y, lr=1e-2, n_epochs=200000, tol = 1e-4)

    viz_model_predictions(LR, X, y, basis_fun = kernel_features, ax=ax, num_kernels=num_kernels[i])
    
    mse_val = mse(LR.forward(PHI), y).item()
    ax.set_title(f"num_kernels={num_kernels[i]} | MSE={mse_val:.3f}")

    if i == 0:
        ax.legend()
plt.tight_layout()
```

Visualization of fits to training data using the kernel basis function expansion with varying numbers of Gaussian radial basis kernels. 

:::

As before, we observe that as the number of basis functions increases, the model becomes more flexible and is able to better fit the training data, but eventually begins to overfit.

## Regularization

We now find ourselves in a bit of a dilemma -- we'd like to use flexible models with many basis functions to capture nonlinear patterns in data, but introducing flexibility raises the risk of overfitting. One approach is to simply restrict which basis functions we'll use, but this is unsatisfying: how will we know ahead of time which basis functions are best? 

An alternative approach is to use **regularization**. Regularization works by encouraging our models to maintain small entries in the parameter vector $\vw$. This effectively allows us to use many basis functions, but penalizes the model for emphasizing any one of them too heavily. 

Typical regularization schemes work by adding a penalty term to the loss function (e.g. the MSE). For example, in *ridge regression*, we add a penalty proportional to the squared $\ell_2$ norm of the parameter vector:

$$
\begin{aligned}
    \hat{\vw} = \argmin_\vw \left\{ \frac{1}{n} \norm{\mPhi \vw - \vy}^2 + \lambda \norm{\vw}^2 \right\}\;,
\end{aligned}
$$

where $\lambda$ is a hyperparameter that controls the strength of the regularization. Larger values of $\lambda$ encourage smaller parameter values, while smaller values allow the model to fit the data more closely.

We can implement ridge regression by adding an $\ell_2$ regularization term to our training loop. We'll first just implement that term itself: 

```{python}
#---
def ell_2_regularization(w):
    return torch.mean(w[0:]**2)
#---
```

The reason for excluding the first entry of $w$ from the regularization term is that this entry corresponds to the intercept term, which we typically don't want to penalize. We then pass this function in to the `regularization` argument of `train_model`, where flagged line <1> adds the regularization term to the loss. We train again and visualize the results as we vary the regularization strength, this time keeping the maximum frequency of the sinusoidal basis functions fixed at 20:

```{python}

fig, axarr = plt.subplots(2, 3, figsize=(8,5))

reg_strengths = torch.logspace(0, 1.5, 6)

for i, ax in enumerate(axarr.flatten()):
    
    PHI = sinusoidal_features(X, max_freq=20)
    
    LR = LinearRegression(n_params=PHI.shape[1])
    train_model(LR, PHI, y, lr=1e-2, n_epochs=200000, tol = 1e-4, regularization = lambda w: reg_strengths[i]*ell_2_regularization(w))

    viz_model_predictions(LR, X, y, basis_fun = sinusoidal_features, ax=ax, max_freq=20)
    ax.set_title(f"reg_strength={reg_strengths[i]:.2f}")
    if i == 0:
        ax.legend()
plt.tight_layout()
```

We observe that the tendency of $\ell_2$ regularization in this setting is to "shrink" the coefficients of the basis functions toward zero. This makes the predictions somewhat smoother, but also just makes them *smaller*, eventually leaving them systematically smaller than the data in magnitude. 


An alternative regularization is $\ell_1$ regularization, in which we penalize the absolute values of the parameters rather than their squares. This gives us a version of regression commonly called *lasso regression*:

$$
\begin{aligned}
    \hat{\vw} = \argmin_\vw \left\{ \frac{1}{n} \norm{\mPhi \vw - \vy}^2 + \lambda \sum_{j=1}^p |w_j| \right\}\;.
\end{aligned}
$$

We can implement $\ell_1$ regularization similarly to before:

```{python}
def ell_1_regularization(w):
    return torch.mean(torch.abs(w[:-1]))  # exclude intercept from regularization
```

Now we can run the same experiment: 

```{python}

fig, axarr = plt.subplots(2, 3, figsize=(8,5))

reg_strengths = torch.logspace(0, 1.5, 6)

for i, ax in enumerate(axarr.flatten()):
    
    PHI = sinusoidal_features(X, max_freq=20)
    
    LR = LinearRegression(n_params=PHI.shape[1])
    train_model(LR, PHI, y, lr=1e-2, n_epochs=20000, tol = 1e-4, regularization = lambda w: reg_strengths[i]*ell_1_regularization(w))

    viz_model_predictions(LR, X, y, basis_fun = sinusoidal_features, ax=ax, max_freq=20)
    
    ax.set_title(f"reg_strength={reg_strengths[i]:.2f}")
    if i == 0:
        ax.legend()
plt.tight_layout()
```

LASSO is somewhat more difficult to fit, resulting in longer computation times. We observe that some of the LASSO fits manage to highlight the underlying trend in the data relatively well, with somewhat less systematic underfitting when compared to the ridge regression fits.

An important and useful property of LASSO is that it tends to set many of the coefficients exactly to zero, effectively performing *feature selection*.[Features with coefficients of 0 are effectively thrown away from the model.]{.aside} This can be useful when we have a large number of basis functions, as it allows us to identify which ones are most important for modeling the data. Let's take a look at the coefficients learned by a LASSO model with moderate regularization strength:

::: {#fig-lasso-coefs}

```{python}
#| code-fold: true

LR = LinearRegression(n_params=PHI.shape[1])
train_model(LR, PHI, y, lr=1e-4, n_epochs=50000, tol = 1e-4, regularization = lambda w: 1.0*ell_1_regularization(w))

freqs = range(PHI.shape[1]-1)

zeros = torch.abs(LR.w) <= 1e-2

fig, ax = plt.subplots()
ax.scatter(freqs, LR.w.detach().numpy()[:-1], c = zeros.numpy()[:-1], cmap='Greys_r', edgecolors='black')

ax.set_xlabel("Sinusoidal frequency")
ax.set_ylabel("Coefficient value")
plt.title("Lasso Regression Coefficients (white = zeroed out)")
```

Visualization of the coefficients learned by LASSO regression. Coefficients less than $0.01$ have been highlighted in white. Given a more efficient optimizer, LASSO regression would set these coefficients to exactly 0. 

:::


## Model Selection

In one way, we've just kicked the can down the road -- in an environment in which we can't visually inspect the data or model predictions, how are we supposed to know what features or what regularization strength to use? 

This is an instance of a problem called *model selection*, which asks us to make choices between models containing different features or hyperparameters. A very common approach to model selection is to use a *validation set*. The idea is to split our data into three parts: a training set which we'll use for actually training our models, a validation set which we'll use for model selection, and a test set which we'll use for final evaluation of our chosen model. So, to make choices about the regularization strength, for example, we'll train separate models with different regularization strengths on the training set, and then evaluate their performance on the validation set. The model with the best validation performance is then selected as the final model.

### Bike Share Case Study

```{python}
import pandas as pd
from sklearn.model_selection import train_test_split
import torch
from matplotlib import pyplot as plt

df = pd.read_csv('https://raw.githubusercontent.com/PhilChodrow/ml-notes-update/refs/heads/main/data/bikeshare/hour.csv')

df.drop(columns=['instant', 'dteday'], inplace=True)

df = pd.get_dummies(df, columns=['season', 'weathersit', 'mnth', 'hr', 'weekday'], drop_first=False)
df["intercept"] = 1

X = 1.0*df.drop(columns=['cnt', 'casual', 'registered'])
y = df['cnt']

test_proportion = 0.94

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_proportion, random_state=42)

X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=1/2, random_state=42)


X_train = torch.tensor(X_train.values, dtype=torch.float32)
y_train = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32)
X_val   = torch.tensor(X_val.values, dtype=torch.float32)
y_val   = torch.tensor(y_val.values.reshape(-1, 1), dtype=torch.float32)
X_test  = torch.tensor(X_test.values, dtype=torch.float32)
y_test  = torch.tensor(y_test.values.reshape(-1, 1), dtype=torch.float32)
```

```{python}

from torch.optim import Adam

class LinearRegression:
    def __init__(self, n_params):
        self.w = torch.zeros(n_params, 1, requires_grad=True)
    
    def forward(self, X):
        if self.w is None:
            self.w = torch.zeros(X.shape[1], 1, requires_grad=True)
        return X@self.w

mse = lambda y_pred, y: torch.mean((y_pred - y)**2)
    
ell2_regularization = lambda w: torch.mean(w[:-1]**2)
ell1_regularization = lambda w: torch.mean(torch.abs(w[:-1]))

W = torch.empty(X_train.shape[1], 0)

val_losses = []

reg_strengths = torch.logspace(1, -2, 21) 

W = torch.empty(X_train.shape[1], 0)

val_losses = []

last_w = None

for reg_strength in reg_strengths:
    print(f"Training with regularization strength {reg_strength:.4f}")
    model = LinearRegression(n_params=X_train.shape[1])
    if last_w is not None:
        model.w = last_w.clone().detach().requires_grad_(True)
    opt = torch.optim.Adam(params=[model.w], lr=1e-2)
    losses = []
    for epoch in range(100000):
        y_pred = model.forward(X_train)
        loss = mse(y_pred, y_train) + reg_strength*ell2_regularization(model.w)

        opt.zero_grad()
        loss.backward()

        opt.step()

        losses.append(loss.item())

        len_compare = 1000

        if epoch % 1000 == 0:
            print(f"  Epoch {epoch}: loss={loss.item():.4f}")

        if epoch >= 2*len_compare: 
            if abs(sum(losses[-len_compare:]) - sum(losses[-len_compare*2:-len_compare]))/len_compare < 1e-3:
                print("Converged")
                break

    last_w = model.w.detach().clone()
    
    val_losses.append(mse(model.forward(X_val), y_val).item())
    W = torch.cat((W, model.w), dim=1)
```

```{python}
plt.plot(reg_strengths, val_losses)

best_reg = reg_strengths[val_losses.index(min(val_losses))]
plt.axvline(best_reg, color='red', linestyle='--', label=f'Best reg={best_reg:.4f} with loss {min(val_losses):.4f}')
plt.legend()


plt.xlabel("Regularization Strength")
plt.ylabel("Validation Loss")
plt.xscale("log")
plt.show()
```

```{python}
plt.plot(reg_strengths, W.detach().numpy().T)

plt.axvline(best_reg, color='red', linestyle='--', label=f'Best reg={best_reg:.4f}')
plt.legend()

plt.xlabel("Regularization Strength")
plt.ylabel("Weights")
plt.xscale("log")
plt.show()
```


```{python}
best_w = W[:,val_losses.index(min(val_losses))]
coefs = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': best_w.detach().numpy().flatten()
}).sort_values(by='Coefficient', key=abs, ascending=False)
coefs
```

```{python}
fig, ax = plt.subplots(1, 2, figsize=(10,6))

months = coefs[coefs["Feature"].str.contains("mnth_")]
months["month"] = months["Feature"].str.replace("mnth_", "").astype(int)

ax[0].bar(months["month"], months["Coefficient"])
ax[0].set_xlabel("Month")
ax[0].set_ylabel("Coefficient")
ax[0].set_xticks(range(1,13))

hours = coefs[coefs["Feature"].str.contains("hr_")]
hours["hour"] = hours["Feature"].str.replace("hr_", "").astype(int)
ax[1].bar(hours["hour"], hours["Coefficient"])
ax[1].set_xlabel("Hour of Day")
ax[1].set_ylabel("Coefficient")
ax[1].set_xticks(range(0,24))
fig.suptitle("Month and Hour Coefficients for Ridge Regression")

plt.show()
```


Sparsity

```{python}

reg_strengths = torch.logspace(3, 0, 21)

W = torch.empty(X_train.shape[1], 0)

val_losses = []

last_w = None

for reg_strength in reg_strengths:
    print(f"Training with regularization strength {reg_strength:.4f}")
    model = LinearRegression(n_params=X_train.shape[1])
    if last_w is not None:
        model.w = last_w.clone().detach().requires_grad_(True)
    opt = torch.optim.Adam(params=[model.w], lr=1e-2)
    losses = []
    for epoch in range(50000):
        y_pred = model.forward(X_train)
        loss = mse(y_pred, y_train) + reg_strength*ell1_regularization(model.w)

        opt.zero_grad()
        loss.backward()

        if epoch % 1000 == 0:
            print(f"  Epoch {epoch}: loss={loss.item():.4f}")

        opt.step(closure = lambda: loss)

        losses.append(loss.item())

        len_compare = 1000

        if epoch >= 2*len_compare: 
            if abs(sum(losses[-len_compare:]) - sum(losses[-len_compare*2:-len_compare]))/len_compare < 1e-3:
                print("Converged")
                break

    last_w = model.w.detach().clone()
    
    val_losses.append(mse(model.forward(X_val), y_val).item())
    W = torch.cat((W, model.w), dim=1)
```


```{python}
plt.plot(reg_strengths, val_losses)

best_reg = reg_strengths[val_losses.index(min(val_losses))]
plt.axvline(best_reg, color='red', linestyle='--', label=f'Best reg={best_reg:.4f} with loss {min(val_losses):.4f}')
plt.legend()

plt.xlabel("Regularization Strength")
plt.ylabel("Validation Loss")
plt.xscale("log")
plt.show()
```

```{python}
plt.plot(reg_strengths, W.detach().numpy().T)
plt.axvline(best_reg, color='red', linestyle='--', label=f'Best reg={best_reg:.4f} with loss {min(val_losses):.4f}')
plt.legend()

plt.xlabel("Regularization Strength")
plt.ylabel("Weights")
plt.xscale("log")
plt.show()
```

```{python}
best_w = W[:,val_losses.index(min(val_losses))] 

coefs = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': best_w.detach().numpy().flatten()
}).sort_values(by='Coefficient', key=abs, ascending=False)
coefs
```



```{python}
fig, ax = plt.subplots(1, 3, figsize=(10,6))

months = coefs[coefs["Feature"].str.contains("mnth_")]
months["month"] = months["Feature"].str.replace("mnth_", "").astype(int)

ax[0].bar(months["month"], months["Coefficient"])
ax[0].set_xlabel("Month")
ax[0].set_ylabel("Coefficient")
ax[0].set_xticks(range(1,13))

weekdays = coefs[coefs["Feature"].str.contains("weekday_")]
weekdays["weekday"] = weekdays["Feature"].str.replace("weekday_", "").astype(int)
ax[1].bar(weekdays["weekday"], weekdays["Coefficient"])
ax[1].set_xlabel("Day of Week")
ax[1].set_ylabel("Coefficient")
ax[1].set_xticks(range(0,7))


hours = coefs[coefs["Feature"].str.contains("hr_")]
hours["hour"] = hours["Feature"].str.replace("hr_", "").astype(int)
ax[2].bar(hours["hour"], hours["Coefficient"])
ax[2].set_xlabel("Hour of Day")
ax[2].set_ylabel("Coefficient")
ax[2].set_xticks(range(0,24))
fig.suptitle("Month and Hour Coefficients for LASSO Regression")

plt.show()
```



