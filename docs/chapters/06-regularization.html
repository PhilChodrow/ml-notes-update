<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>4&nbsp; Features and Regularization ‚Äì Machine Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/04-more-gradients.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-a14e345711173c227b21482e2eb4cc70.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-d8b0bb70be28f5ebcc1c8c98afdc075d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<style>
div.callout-sol.callout {
  border-left-color: pink;
}
div.callout-sol.callout-style-default > .callout-header {
  background-color: rgb(from pink r g b / 13%);
}
div.callout-sol .callout-toggle::before {  background-image: url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="rgb(33, 37, 41)" class="bi bi-chevron-down" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M1.646 4.646a.5.5 0 0 1 .708 0L8 10.293l5.646-5.647a.5.5 0 0 1 .708.708l-6 6a.5.5 0 0 1-.708 0l-6-6a.5.5 0 0 1 0-.708z"/></svg>');}
div.callout-sol.callout-style-default .callout-icon::before, div.callout-sol.callout-titled .callout-icon::before {
  content: 'üìù';
  background-image: none;
}
div.callout-question.callout {
  border-left-color: lightblue;
}
div.callout-question.callout-style-default > .callout-header {
  background-color: rgb(from lightblue r g b / 13%);
}
div.callout-question .callout-toggle::before {  background-image: url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="rgb(33, 37, 41)" class="bi bi-chevron-down" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M1.646 4.646a.5.5 0 0 1 .708 0L8 10.293l5.646-5.647a.5.5 0 0 1 .708.708l-6 6a.5.5 0 0 1-.708 0l-6-6a.5.5 0 0 1 0-.708z"/></svg>');}
div.callout-question.callout-style-default .callout-icon::before, div.callout-question.callout-titled .callout-icon::before {
  content: '‚ùì';
  background-image: none;
}
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../assets/styles.css">
</head>

<body class="nav-sidebar floating slimcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/02-signal-noise.html">Machine Learning Fundamentals</a></li><li class="breadcrumb-item"><a href="../chapters/06-regularization.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Features and Regularization</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Machine Learning</a> 
    </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Machine Learning Fundamentals</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/02-signal-noise.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Data = Signal + Noise</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/03-maximum-likelihood.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Maximum Likelihood Estimation and Gradients</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/04-more-gradients.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Higher Dimensions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/06-regularization.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Features and Regularization</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#basis-function-expansion" id="toc-basis-function-expansion" class="nav-link active" data-scroll-target="#basis-function-expansion">Basis Function Expansion</a></li>
  <li><a href="#kernel-basis-functions" id="toc-kernel-basis-functions" class="nav-link" data-scroll-target="#kernel-basis-functions">Kernel Basis Functions</a></li>
  <li><a href="#regularization" id="toc-regularization" class="nav-link" data-scroll-target="#regularization">Regularization</a></li>
  <li><a href="#model-selection" id="toc-model-selection" class="nav-link" data-scroll-target="#model-selection">Model Selection</a>
  <ul class="collapse">
  <li><a href="#features-in-the-wild" id="toc-features-in-the-wild" class="nav-link" data-scroll-target="#features-in-the-wild">Features In the Wild</a></li>
  <li><a href="#bike-share-case-study" id="toc-bike-share-case-study" class="nav-link" data-scroll-target="#bike-share-case-study">Bike Share Case Study</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/02-signal-noise.html">Machine Learning Fundamentals</a></li><li class="breadcrumb-item"><a href="../chapters/06-regularization.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Features and Regularization</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Features and Regularization</span></h1>
<p class="subtitle lead">Our first look at methods for using and controlling model complexity</p>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><em><a href="https://colab.research.google.com/github/philchodrow/ml-notes-update/blob/main/docs/live-notebooks/06-regularization.ipynb">Open the live notebook</a> in Google Colab.</em></p>
<p>Suppose that we‚Äôd like to model a distinctly <em>nonlinear</em> relationship in our data:</p>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>scatterplot_kwargs <span class="op">=</span> <span class="bu">dict</span>(color <span class="op">=</span> <span class="st">"black"</span>, label <span class="op">=</span> <span class="st">"data"</span>, facecolors <span class="op">=</span> <span class="st">"none"</span>, s <span class="op">=</span> <span class="dv">40</span>, alpha <span class="op">=</span> <span class="fl">0.6</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>sig <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.linspace(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">30</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>signal <span class="op">=</span> torch.sin(<span class="dv">2</span><span class="op">*</span>torch.pi<span class="op">*</span>X)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>noise <span class="op">=</span> sig<span class="op">*</span>torch.randn(X.shape)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> signal <span class="op">+</span> noise</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>ax.scatter(X.numpy(), y.numpy(), <span class="op">**</span>scatterplot_kwargs)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"</span><span class="dv">$</span><span class="vs">x</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"</span><span class="dv">$</span><span class="vs">y</span><span class="dv">$</span><span class="vs">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div id="fig-synthetic-sin-data" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-synthetic-sin-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="f83860db" class="cell" data-execution_count="3">
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>Text(0, 0.5, '$y$')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="06-regularization_files/figure-html/cell-4-output-2.png" class="figure-img" width="589" height="423"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-synthetic-sin-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.1: Nonlinear data generated from a sinusoidal signal plus Gaussian noise.
</figcaption>
</figure>
</div>
<p>One flexible candidate model for this kind of data is a nonlinear Gaussian model:</p>
<div id="def-nonlinear-gaussian" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.1</strong></span> A <strong>nonlinear Gaussian model</strong> with <em>mean function <span class="math inline">\(f\)</span></em> and <em>variance <span class="math inline">\(\sigma^2\)</span></em> is a probabilistic model for data <span class="math inline">\((\mathbf{x}_i, y_i)_{i=1}^n\)</span> such that</p>
<p><span class="math display">\[
\begin{aligned}
    y_i \sim \mathcal{N}(f(\mathbf{x}_i); \sigma^2)\;.
\end{aligned}
\]</span></p>
<p>That is, each target value <span class="math inline">\(y_i\)</span> has Gaussian distribution with a mean (or signal) given by <span class="math inline">\(f(\mathbf{x}_i)\)</span> and variance (or noise level) <span class="math inline">\(\sigma^2\)</span>.</p>
</div>
<p>Here‚Äôs a visualization of this model with <span class="math inline">\(f(x) = \sin(2\pi x)\)</span> and <span class="math inline">\(\sigma^2 = 0.01\)</span>, which was the setting used to generate the synthetic data above:</p>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> draw_gaussian_at(support, sd<span class="op">=</span><span class="fl">1.0</span>, height<span class="op">=</span><span class="fl">1.0</span>, </span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>        xpos<span class="op">=</span><span class="fl">0.0</span>, ypos<span class="op">=</span><span class="fl">0.0</span>, ax<span class="op">=</span><span class="va">None</span>, <span class="op">**</span>kwargs):</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> ax <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        ax <span class="op">=</span> plt.gca()</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    gaussian <span class="op">=</span> torch.exp((<span class="op">-</span>support <span class="op">**</span> <span class="fl">2.0</span>) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> sd <span class="op">**</span> <span class="fl">2.0</span>))</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    gaussian <span class="op">/=</span> gaussian.<span class="bu">max</span>()</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    gaussian <span class="op">*=</span> height</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ax.plot(gaussian <span class="op">+</span> xpos, support <span class="op">+</span> ypos, <span class="op">**</span>kwargs)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>support <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="fl">0.3</span>, <span class="fl">0.3</span>, <span class="dv">1000</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>ax.plot(X.numpy(), torch.sin(<span class="dv">2</span><span class="op">*</span>torch.pi<span class="op">*</span>X).numpy(), color <span class="op">=</span> <span class="st">"black"</span>, linestyle <span class="op">=</span> <span class="st">"--"</span>, label <span class="op">=</span> <span class="vs">r"Signal: </span><span class="dv">$</span><span class="vs">f</span><span class="kw">(</span><span class="vs">x_i</span><span class="kw">)</span><span class="vs"> = </span><span class="dv">\s</span><span class="vs">in</span><span class="kw">(</span><span class="vs">2</span><span class="er">\</span><span class="vs">pi x_i</span><span class="kw">)</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> each <span class="kw">in</span> X:</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    draw_gaussian_at(support, sd<span class="op">=</span>sig, height<span class="op">=</span><span class="fl">0.1</span>, xpos<span class="op">=</span>each, ypos<span class="op">=</span>torch.sin(<span class="dv">2</span><span class="op">*</span>torch.pi<span class="op">*</span>each), ax<span class="op">=</span>ax, color<span class="op">=</span><span class="st">'C0'</span>, alpha<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>ax.scatter(X.numpy(), y.numpy(), <span class="op">**</span>scatterplot_kwargs)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"</span><span class="dv">$</span><span class="vs">x</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"</span><span class="dv">$</span><span class="vs">y</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Data generated from a nonlinear Gaussian model"</span>)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div id="fig-nonlinear-gaussian" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-nonlinear-gaussian-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="49ec1093" class="cell" data-execution_count="4">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="06-regularization_files/figure-html/cell-5-output-1.png" class="figure-img" width="589" height="442"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-nonlinear-gaussian-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.2: Data modeled via a nonlinear Gaussian model with mean function <span class="math inline">\(f(x) = \sin(2\pi x)\)</span> and noise level <span class="math inline">\(\sigma = 0.2\)</span>. The dashed line indicates the signal function, while the blue curves indicate the conditional distributions of <span class="math inline">\(y\)</span> given <span class="math inline">\(x\)</span>. The black circles are the observed data points.
</figcaption>
</figure>
</div>
<p>This is all well and good as a theoretical framework, but how in the world were we supposed to know that <span class="math inline">\(f(x) = \sin(2\pi x)\)</span> was the right choice? In practice, of course, we never will.</p>
<section id="basis-function-expansion" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="basis-function-expansion">Basis Function Expansion</h2>
<p>Since we don‚Äôt know the right functional form for <span class="math inline">\(f\)</span>, one common approach is try to approximate</p>
<div class="page-columns page-full"><p><span class="math display">\[
\begin{aligned}
    f(\mathbf{x}) \approx w_0\phi_0(\mathbf{x}) + w_1 \phi_1(\mathbf{x}) + w_2 \phi_2(\mathbf{x}) + \cdots + w_p \phi_p(\mathbf{x}) = \sum_{j=0}^p w_j \phi_j(\mathbf{x})\;,
\end{aligned}
\]</span> where <span class="math inline">\(\phi_j(\cdot)\)</span> are a collection of <em>basis functions</em> that we choose ahead of time. This is called a <strong>basis function expansion</strong>. </p><div class="no-row-height column-margin column-container"><span class="margin-aside">Each <span class="math inline">\(\phi_j\)</span> is often also called a <em>feature map</em>.</span></div></div>
<p>Before we look at some examples of basis functions, let‚Äôs take a look at the fundamental trick behind basis function expansions: when using a basis function expansion with a nonlinear Gaussian model, the model is <em>linear</em> in the parameters <span class="math inline">\(w_j\)</span>, and we can therefore use our previously-developed machinery for linear regression to fit the model. To see this, note that we can write</p>
<p><span class="math display">\[
\begin{aligned}
    \sum_{j=0}^p w_j \phi_j(\mathbf{x}) = \mathbf{w}^T \mathbf{\phi}(\mathbf{x})\;,
\end{aligned}
\]</span></p>
<p>where we‚Äôve defined the vector of parameters <span class="math inline">\(\mathbf{w}= (w_0, w_1, \ldots, w_p)^T\)</span> and the vector of basis functions <span class="math inline">\(\mathbf{\phi}(\mathbf{x}) = (\phi_0(\mathbf{x}), \phi_1(\mathbf{x}), \ldots, \phi_p(\mathbf{x}))^T\)</span>. Let‚Äôs give the shorthand <span class="math inline">\(\hat{y}_i = \mathbf{w}^T \mathbf{\phi}(\mathbf{x}_i)\)</span> for the prediction of the model at input <span class="math inline">\(\mathbf{x}_i\)</span>. Then, taken together the nonlinear Gaussian model with basis function expansion says that</p>
<p><span class="math display">\[
\begin{aligned}
    y_i \sim \mathcal{N}(\hat{y}_i; \sigma^2) = \mathcal{N}(\mathbf{w}^T \mathbf{\phi}(\mathbf{x}_i); \sigma^2)\;.
\end{aligned}
\]</span></p>
<p>This is just like the linear-Gaussian model, but with the input data <span class="math inline">\(\mathbf{x}_i\)</span> replaced by the transformed data <span class="math inline">\(\mathbf{\phi}(\mathbf{x}_i)\)</span>. Therefore, we can use our previous results for maximum likelihood estimation of the parameters <span class="math inline">\(\mathbf{w}\)</span> in the linear-Gaussian model, simply by replacing each occurrence of <span class="math inline">\(\mathbf{x}_i\)</span> with <span class="math inline">\(\mathbf{\phi}(\mathbf{x}_i)\)</span>. In particular, we can maximize the log-likelihood of the data by minimizing the mean squared error between the predictions <span class="math inline">\(\hat{y}_i\)</span> and the observed targets <span class="math inline">\(y_i\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
    R(\mathbf{X}, \mathbf{y}; \mathbf{w}) = \frac{1}{n} \sum_{i=1}^n (y_i - \mathbf{w}^T \mathbf{\phi}(\mathbf{x}_i))^2\;.
\end{aligned}
\]</span></p>
<p>If we define</p>
<p><span class="math display">\[
\begin{aligned}
    \mathbf{\Phi}= \left[\begin{matrix}
        - &amp;\mathbf{\phi}(\mathbf{x}_1)^T &amp; -\\
        - &amp;\mathbf{\phi}(\mathbf{x}_2)^T &amp; -\\
        \vdots \\
        - &amp;\mathbf{\phi}(\mathbf{x}_n)^T &amp; -    
    \end{matrix}\right]\;,
\end{aligned}
\]</span></p>
<p>we can similarly write the mean squared error in matrix form as</p>
<p><span class="math display">\[
\begin{aligned}
    R(\mathbf{\Phi}, \mathbf{y}; \mathbf{w}) = \frac{1}{n} \lVert \mathbf{\Phi}\mathbf{w}- \mathbf{y} \rVert^2\;.
\end{aligned}
\]</span></p>
<p>So, to learn a model with basis function expansion, all we need to do is construct the feature matrix <span class="math inline">\(\mathbf{\Phi}\)</span> by applying the basis functions to each data point, and then run linear regression as before.</p>
<p>Let‚Äôs try basis function expansion on our synthetic data. For this, we‚Äôll first bring in the linear regression model that we developed previously:</p>
<div id="e84d74a4" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LinearRegression:</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_params):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w <span class="op">=</span> torch.zeros(n_params, <span class="dv">1</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> X <span class="op">@</span> <span class="va">self</span>.w</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We‚Äôll also write a simple training loop, this time using some of PyTorch‚Äôs built-in optimization functionality:</p>
<div id="28f41008" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="annotated-cell-2"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-2-1"><a href="#annotated-cell-2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse(y_pred, y):</span>
<span id="annotated-cell-2-2"><a href="#annotated-cell-2-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.mean((y_pred <span class="op">-</span> y) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="annotated-cell-2-3"><a href="#annotated-cell-2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-2-4"><a href="#annotated-cell-2-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_model(model, X_train, y_train, lr<span class="op">=</span><span class="fl">1e-2</span>, n_epochs<span class="op">=</span><span class="dv">1000</span>, tol<span class="op">=</span><span class="fl">1e-4</span>, regularization <span class="op">=</span> <span class="va">None</span>, verbose <span class="op">=</span> <span class="va">False</span>):</span>
<span id="annotated-cell-2-5"><a href="#annotated-cell-2-5" aria-hidden="true" tabindex="-1"></a>    opt <span class="op">=</span> torch.optim.Adam(params<span class="op">=</span>[model.w], lr<span class="op">=</span>lr)</span>
<span id="annotated-cell-2-6"><a href="#annotated-cell-2-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(n_epochs):</span>
<span id="annotated-cell-2-7"><a href="#annotated-cell-2-7" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model.forward(X_train)</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-2" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-2-8" class="code-annotation-target"><a href="#annotated-cell-2-8" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> mse(y_pred, y_train) <span class="op">+</span> (regularization(model.w) <span class="cf">if</span> regularization <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> <span class="fl">0.0</span>)</span>
<span id="annotated-cell-2-9"><a href="#annotated-cell-2-9" aria-hidden="true" tabindex="-1"></a>        opt.zero_grad()</span>
<span id="annotated-cell-2-10"><a href="#annotated-cell-2-10" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="annotated-cell-2-11"><a href="#annotated-cell-2-11" aria-hidden="true" tabindex="-1"></a>        opt.step()</span>
<span id="annotated-cell-2-12"><a href="#annotated-cell-2-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> model.w.grad <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="annotated-cell-2-13"><a href="#annotated-cell-2-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> model.w.grad.norm().item() <span class="op">&lt;</span> tol:</span>
<span id="annotated-cell-2-14"><a href="#annotated-cell-2-14" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> verbose: </span>
<span id="annotated-cell-2-15"><a href="#annotated-cell-2-15" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="ss">f"Converged at epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">, Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="annotated-cell-2-16"><a href="#annotated-cell-2-16" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="annotated-cell-2-17"><a href="#annotated-cell-2-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> verbose <span class="kw">and</span> epoch <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="annotated-cell-2-18"><a href="#annotated-cell-2-18" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">, Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">"</span>)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-annotation">
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-2" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-2" data-code-lines="8" data-code-annotation="1">This function adds a regularization term (introduced below) to the loss if provided.</span>
</dd>
</dl>
</div>
</div>
<p>It‚Äôs helpful to think about the minimal linear regression model, in which we simply add a constant column to the data, as an example of a basis function expansion. We‚Äôll call this the <em>linear</em> basis function.</p>
<div id="8d21d933" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> linear_basis_function(X):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co">    just adds the constant feature</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    n_samples <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    Phi <span class="op">=</span> torch.ones(n_samples, <span class="dv">2</span>)  <span class="co"># intercept + linear term</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    Phi[:, <span class="dv">1</span>] <span class="op">=</span> X.flatten()</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Phi</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>If we try fitting this model to the sinusoidal data directly, we‚Äôll be disappointed.</p>
<div id="db0fc4b7" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>PHI <span class="op">=</span> linear_basis_function(X)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>LR <span class="op">=</span> LinearRegression(n_params<span class="op">=</span><span class="dv">2</span>)  <span class="co"># intercept + slope</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>train_model(LR, PHI, y, lr<span class="op">=</span><span class="fl">1e-2</span>, n_epochs<span class="op">=</span><span class="dv">1000</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="20501f27" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># utility function for model visualization</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> viz_model_predictions(model, X, y, basis_fun, ax, <span class="op">**</span>basis_fun_kwargs): </span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    x_new <span class="op">=</span> torch.linspace(X.<span class="bu">min</span>(), X.<span class="bu">max</span>(), <span class="dv">1000</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    PHI_new <span class="op">=</span> basis_fun(x_new, <span class="op">**</span>basis_fun_kwargs)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.forward(PHI_new)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    ax.scatter(X.numpy(), y.numpy(), <span class="op">**</span>scatterplot_kwargs)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    ax.plot(x_new.numpy(), y_pred.detach().numpy(), color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="st">'Prediction'</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="vs">r"</span><span class="dv">$</span><span class="vs">x</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="vs">r"</span><span class="dv">$</span><span class="vs">y</span><span class="dv">$</span><span class="vs">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
<p>How did we do?</p>
<div id="fig-linear-regression-sin" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-linear-regression-sin-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="8284639c" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>viz_model_predictions(LR, X, y, basis_fun <span class="op">=</span> linear_basis_function, ax<span class="op">=</span>ax)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Linear Regression Fit to Nonlinear Data"</span>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="06-regularization_files/figure-html/cell-11-output-1.png" class="figure-img" width="589" height="442"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-linear-regression-sin-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.3: Example of linear regression fit to the sinusoidal data set using the linear (trivial) basis function expansion.
</figcaption>
</figure>
</div>
<p>Now let‚Äôs try some nontrivial basis function expansions. If we had reason to believe that our data was periodic, we might try using sine and cosine basis functions. For example, let‚Äôs try:</p>
<p><span class="math display">\[
\begin{aligned}
    \phi_0(x) &amp;= 1\;,\\
    \phi_1(x) &amp;= \sin(\pi x)\;,\\
    \phi_2(x) &amp;= \sin(2 \pi x)\;,\\
    \phi_3(x) &amp;= \sin(3 \pi x)\;,\\
    \vdots
\end{aligned}
\]</span></p>
<p>The following function constructs the feature matrix <span class="math inline">\(\mathbf{\Phi}\)</span> for this basis function expansion:</p>
<div id="e8a5ef3e" class="cell" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sinusoidal_features(X, max_freq<span class="op">=</span><span class="dv">4</span>):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    n_samples <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    Phi <span class="op">=</span> torch.ones(n_samples, max_freq <span class="op">+</span> <span class="dv">1</span>)  </span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, max_freq <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>        Phi[:, i] <span class="op">=</span> torch.sin(i <span class="op">*</span> torch.pi <span class="op">*</span> X).flatten()</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Phi</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>To train models and make predictions, all we need to do is call this function to get the feature matrix, and then run linear regression as before.</p>
<div id="8ccd683e" class="cell" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># engineer features</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>max_freq <span class="op">=</span> <span class="dv">15</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>PHI <span class="op">=</span> sinusoidal_features(X, max_freq<span class="op">=</span>max_freq)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co"># train the model as usual</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>LR <span class="op">=</span> LinearRegression(n_params<span class="op">=</span>PHI.shape[<span class="dv">1</span>])</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>train_model(LR, PHI, y, lr<span class="op">=</span><span class="fl">1e-2</span>, n_epochs<span class="op">=</span><span class="dv">2000</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># visualize</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>viz_model_predictions(LR, X, y, basis_fun <span class="op">=</span> sinusoidal_features, ax<span class="op">=</span>ax, max_freq<span class="op">=</span>max_freq)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f"Sinusoidal Basis Functions with max_freq=</span><span class="sc">{</span>max_freq<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div id="fig-sinusoidal-basis-functions-example" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-sinusoidal-basis-functions-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="0139fa72" class="cell" data-execution_count="13">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="06-regularization_files/figure-html/cell-14-output-1.png" class="figure-img" width="589" height="442"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-sinusoidal-basis-functions-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.4: Example fit to data using the sinusoidal basis function expansion with maximum frequency 15.
</figcaption>
</figure>
</div>
<p>Let‚Äôs try this for a variety of maximum frequencies.</p>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>fig, axarr <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">5</span>))</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>max_freqs <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">20</span>]</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, ax <span class="kw">in</span> <span class="bu">enumerate</span>(axarr.flatten()):</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    PHI <span class="op">=</span> sinusoidal_features(X, max_freq<span class="op">=</span>max_freqs[i])</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    LR <span class="op">=</span> LinearRegression(n_params<span class="op">=</span>PHI.shape[<span class="dv">1</span>])</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    train_model(LR, PHI, y, lr<span class="op">=</span><span class="fl">1e-2</span>, n_epochs<span class="op">=</span><span class="dv">2000</span>)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    viz_model_predictions(LR, X, y, basis_fun <span class="op">=</span> sinusoidal_features, ax<span class="op">=</span>ax, max_freq<span class="op">=</span>max_freqs[i])</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    mse_val <span class="op">=</span> mse(LR.forward(PHI), y).item()</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f"max_freq=</span><span class="sc">{</span>max_freqs[i]<span class="sc">,</span> <span class="sc">}</span><span class="ss"> | MSE=</span><span class="sc">{</span>mse_val<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>        ax.legend()</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div id="fig-sinusoidal-basis-functions-varying-complexity" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-sinusoidal-basis-functions-varying-complexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="4fe67a7b" class="cell" data-execution_count="14">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="06-regularization_files/figure-html/cell-15-output-1.png" class="figure-img" width="761" height="470"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-sinusoidal-basis-functions-varying-complexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.5: Example fits to training data using the sinusoidal basis function expansion with varying maximum frequencies.
</figcaption>
</figure>
</div>
<p>As the number of basis functions increases, the model becomes more flexible and is able to better fit the training data. However, with too many basis functions, the model begins to overfit the data, capturing noise rather than the underlying signal as reflected in the seemingly random high-frequency oscillations in the predictions.</p>
</section>
<section id="kernel-basis-functions" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="kernel-basis-functions">Kernel Basis Functions</h2>
<p>Another common choice of basis functions are <em>kernel basis functions</em>. A kernel is simply a measure of similarity between two inputs. A common choice is the Gaussian radial-basis kernel, which is defined in one dimension by</p>
<p><span class="math display">\[
k(x, c) = \exp\left(-(x - c)^2\right)\;,
\]</span></p>
<p>The Guassian kernel measures similarity between <span class="math inline">\(x\)</span> and a center point <span class="math inline">\(c\)</span>, with values close to 1 when <span class="math inline">\(x\)</span> is near <span class="math inline">\(c\)</span> and values close to 0 when <span class="math inline">\(x\)</span> is far from <span class="math inline">\(c\)</span>.</p>
<p>Here‚Äôs an implementation in one dimension, where we evenly space choices of <span class="math inline">\(c\)</span> out across the range of the data:</p>
<div id="ea6cc2c2" class="cell" data-execution_count="15">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> kernel_features(X, num_kernels):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    n_samples <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    Phi <span class="op">=</span> torch.ones(n_samples, num_kernels <span class="op">+</span> <span class="dv">1</span>)  </span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    centers <span class="op">=</span> torch.linspace(X.<span class="bu">min</span>(), X.<span class="bu">max</span>(), num_kernels)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    bandwidth <span class="op">=</span> (X.<span class="bu">max</span>() <span class="op">-</span> X.<span class="bu">min</span>()) <span class="op">/</span> num_kernels</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(num_kernels):</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        Phi[:, j <span class="op">+</span> <span class="dv">1</span>] <span class="op">=</span> torch.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> ((X.flatten() <span class="op">-</span> centers[j]) <span class="op">/</span> bandwidth) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Phi</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We can now run the same experiment as before:</p>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>fig, axarr <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">5</span>))</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>num_kernels <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>]</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, ax <span class="kw">in</span> <span class="bu">enumerate</span>(axarr.flatten()):</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    PHI <span class="op">=</span> kernel_features(X, num_kernels<span class="op">=</span>num_kernels[i])</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    LR <span class="op">=</span> LinearRegression(n_params<span class="op">=</span>PHI.shape[<span class="dv">1</span>])</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    train_model(LR, PHI, y, lr<span class="op">=</span><span class="fl">1e-2</span>, n_epochs<span class="op">=</span><span class="dv">200000</span>, tol <span class="op">=</span> <span class="fl">1e-4</span>)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    viz_model_predictions(LR, X, y, basis_fun <span class="op">=</span> kernel_features, ax<span class="op">=</span>ax, num_kernels<span class="op">=</span>num_kernels[i])</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    mse_val <span class="op">=</span> mse(LR.forward(PHI), y).item()</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f"num_kernels=</span><span class="sc">{</span>num_kernels[i]<span class="sc">}</span><span class="ss"> | MSE=</span><span class="sc">{</span>mse_val<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>        ax.legend()</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div id="fig-kernel-basis-functions-varying-complexity" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-kernel-basis-functions-varying-complexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="5ae0c578" class="cell" data-execution_count="16">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="06-regularization_files/figure-html/cell-17-output-1.png" class="figure-img" width="766" height="470"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-kernel-basis-functions-varying-complexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.6: Visualization of fits to training data using the kernel basis function expansion with varying numbers of Gaussian radial basis kernels.
</figcaption>
</figure>
</div>
<p>As before, we observe that as the number of basis functions increases, the model becomes more flexible and is able to better fit the training data, but eventually begins to overfit.</p>
</section>
<section id="regularization" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="regularization">Regularization</h2>
<p>We now find ourselves in a bit of a dilemma ‚Äì we‚Äôd like to use flexible models with many basis functions to capture nonlinear patterns in data, but introducing flexibility raises the risk of overfitting. One approach is to simply restrict which basis functions we‚Äôll use, but this is unsatisfying: how will we know ahead of time which basis functions are best?</p>
<p>An alternative approach is to use <strong>regularization</strong>. Regularization works by encouraging our models to maintain small entries in the parameter vector <span class="math inline">\(\mathbf{w}\)</span>. This effectively allows us to use many basis functions, but penalizes the model for emphasizing any one of them too heavily.</p>
<p>Typical regularization schemes work by adding a penalty term to the loss function (e.g.&nbsp;the MSE). For example, in <em>ridge regression</em>, we add a penalty proportional to the squared <span class="math inline">\(\ell_2\)</span> norm of the parameter vector:</p>
<p><span class="math display">\[
\begin{aligned}
    \hat{\mathbf{w}} = \mathop{\mathrm{arg\,min}}_\mathbf{w}\left\{ \frac{1}{n} \lVert \mathbf{\Phi}\mathbf{w}- \mathbf{y} \rVert^2 + \lambda \lVert \mathbf{w} \rVert^2 \right\}\;,
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> is a hyperparameter that controls the strength of the regularization. Larger values of <span class="math inline">\(\lambda\)</span> encourage smaller parameter values, while smaller values allow the model to fit the data more closely.</p>
<p>We can implement ridge regression by adding an <span class="math inline">\(\ell_2\)</span> regularization term to our training loop. We‚Äôll first just implement that term itself:</p>
<div id="0e90dc24" class="cell" data-execution_count="17">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ell_2_regularization(w):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.mean(w[<span class="dv">0</span>:]<span class="op">**</span><span class="dv">2</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The reason for excluding the first entry of <span class="math inline">\(w\)</span> from the regularization term is that this entry corresponds to the intercept term, which we typically don‚Äôt want to penalize. We then pass this function in to the <code>regularization</code> argument of <code>train_model</code>, where flagged line &lt;1&gt; adds the regularization term to the loss. We train again and visualize the results as we vary the regularization strength, this time keeping the maximum frequency of the sinusoidal basis functions fixed at 20:</p>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>fig, axarr <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">5</span>))</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>reg_strengths <span class="op">=</span> torch.logspace(<span class="dv">0</span>, <span class="fl">1.5</span>, <span class="dv">6</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, ax <span class="kw">in</span> <span class="bu">enumerate</span>(axarr.flatten()):</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    PHI <span class="op">=</span> sinusoidal_features(X, max_freq<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    LR <span class="op">=</span> LinearRegression(n_params<span class="op">=</span>PHI.shape[<span class="dv">1</span>])</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    train_model(LR, PHI, y, lr<span class="op">=</span><span class="fl">1e-2</span>, n_epochs<span class="op">=</span><span class="dv">200000</span>, tol <span class="op">=</span> <span class="fl">1e-4</span>, regularization <span class="op">=</span> <span class="kw">lambda</span> w: reg_strengths[i]<span class="op">*</span>ell_2_regularization(w))</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    viz_model_predictions(LR, X, y, basis_fun <span class="op">=</span> sinusoidal_features, ax<span class="op">=</span>ax, max_freq<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f"reg_strength=</span><span class="sc">{</span>reg_strengths[i]<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>        ax.legend()</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div id="fig-ridge-regression-varying-regularization" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-ridge-regression-varying-regularization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="e034ca4a" class="cell" data-execution_count="18">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="06-regularization_files/figure-html/cell-19-output-1.png" class="figure-img" width="757" height="470"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-ridge-regression-varying-regularization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.7: Ridge regression models using sinusoidal basis functions with maximum frequency 20, for varying regularization strengths, visualized against training data.
</figcaption>
</figure>
</div>
<p>We observe that the tendency of <span class="math inline">\(\ell_2\)</span> regularization in this setting is to ‚Äúshrink‚Äù the coefficients of the basis functions toward zero. This makes the predictions somewhat smoother, but also just makes them <em>smaller</em>, eventually leaving them systematically smaller than the data in magnitude.</p>
<p>An alternative regularization is <span class="math inline">\(\ell_1\)</span> regularization, in which we penalize the absolute values of the parameters rather than their squares. This gives us a version of regression commonly called <em>lasso regression</em>:</p>
<p><span class="math display">\[
\begin{aligned}
    \hat{\mathbf{w}} = \mathop{\mathrm{arg\,min}}_\mathbf{w}\left\{ \frac{1}{n} \lVert \mathbf{\Phi}\mathbf{w}- \mathbf{y} \rVert^2 + \lambda \sum_{j=1}^p |w_j| \right\}\;.
\end{aligned}
\]</span></p>
<p>We can implement <span class="math inline">\(\ell_1\)</span> regularization similarly to before:</p>
<div id="a53a92ce" class="cell" data-execution_count="19">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ell_1_regularization(w):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.mean(torch.<span class="bu">abs</span>(w[:<span class="op">-</span><span class="dv">1</span>]))  <span class="co"># exclude intercept from regularization</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Now we can run the same experiment:</p>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>fig, axarr <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">5</span>))</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>reg_strengths <span class="op">=</span> torch.logspace(<span class="dv">0</span>, <span class="fl">1.5</span>, <span class="dv">6</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, ax <span class="kw">in</span> <span class="bu">enumerate</span>(axarr.flatten()):</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    PHI <span class="op">=</span> sinusoidal_features(X, max_freq<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>    LR <span class="op">=</span> LinearRegression(n_params<span class="op">=</span>PHI.shape[<span class="dv">1</span>])</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>    train_model(LR, PHI, y, lr<span class="op">=</span><span class="fl">1e-2</span>, n_epochs<span class="op">=</span><span class="dv">20000</span>, tol <span class="op">=</span> <span class="fl">1e-4</span>, regularization <span class="op">=</span> <span class="kw">lambda</span> w: reg_strengths[i]<span class="op">*</span>ell_1_regularization(w))</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>    viz_model_predictions(LR, X, y, basis_fun <span class="op">=</span> sinusoidal_features, ax<span class="op">=</span>ax, max_freq<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f"reg_strength=</span><span class="sc">{</span>reg_strengths[i]<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>        ax.legend()</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div id="fig-lasso-regression-varying-regularization" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-lasso-regression-varying-regularization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="4cfb7f02" class="cell" data-execution_count="20">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="06-regularization_files/figure-html/cell-21-output-1.png" class="figure-img" width="757" height="470"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-lasso-regression-varying-regularization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.8: LASSO regression models using sinusoidal basis functions with maximum frequency 20, for varying regularization strengths, visualized against training data.
</figcaption>
</figure>
</div>
<p>LASSO is somewhat more difficult to fit in computational terms, resulting in longer computation times. We observe that some of the LASSO fits manage to highlight the underlying trend in the data relatively well, with somewhat less systematic underfitting when compared to the ridge regression fits.</p>
<div class="page-columns page-full"><p>An important and useful property of LASSO is that it tends to set many of the coefficients exactly to zero, effectively performing <em>feature selection</em>. This can be useful when we have a large number of basis functions, as it allows us to identify which ones are most important for modeling the data. Let‚Äôs take a look at the coefficients learned by a LASSO model with moderate regularization strength:</p><div class="no-row-height column-margin column-container"><span class="margin-aside">Features with coefficients of 0 are effectively thrown away from the model.</span></div></div>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>LR <span class="op">=</span> LinearRegression(n_params<span class="op">=</span>PHI.shape[<span class="dv">1</span>])</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>train_model(LR, PHI, y, lr<span class="op">=</span><span class="fl">1e-4</span>, n_epochs<span class="op">=</span><span class="dv">50000</span>, tol <span class="op">=</span> <span class="fl">1e-4</span>, regularization <span class="op">=</span> <span class="kw">lambda</span> w: <span class="fl">1.0</span><span class="op">*</span>ell_1_regularization(w))</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>freqs <span class="op">=</span> <span class="bu">range</span>(PHI.shape[<span class="dv">1</span>]<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>zeros <span class="op">=</span> torch.<span class="bu">abs</span>(LR.w) <span class="op">&lt;=</span> <span class="fl">1e-2</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>ax.scatter(freqs, LR.w.detach().numpy()[:<span class="op">-</span><span class="dv">1</span>], c <span class="op">=</span> zeros.numpy()[:<span class="op">-</span><span class="dv">1</span>], cmap<span class="op">=</span><span class="st">'Greys_r'</span>, edgecolors<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Sinusoidal frequency"</span>)</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Coefficient value"</span>)</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Lasso Regression Coefficients (white = zeroed out)"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div id="fig-lasso-coefs" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-lasso-coefs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="104f00cf" class="cell" data-execution_count="21">
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>Text(0.5, 1.0, 'Lasso Regression Coefficients (white = zeroed out)')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="06-regularization_files/figure-html/cell-22-output-2.png" class="figure-img" width="581" height="442"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-lasso-coefs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.9: Visualization of the coefficients learned by LASSO regression. Coefficients less than <span class="math inline">\(0.01\)</span> have been highlighted in white. Given a more efficient optimizer, LASSO regression would set these coefficients to exactly 0.
</figcaption>
</figure>
</div>
</section>
<section id="model-selection" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="model-selection">Model Selection</h2>
<p>In one way, we‚Äôve just kicked the can down the road ‚Äì in an environment in which we can‚Äôt visually inspect the data or model predictions, how are we supposed to know what features or what regularization strength to use?</p>
<p>This is an instance of a problem called <em>model selection</em>, which asks us to make choices between models containing different features or hyperparameters. A very common approach to model selection is to use a <em>validation set</em>. The idea is to split our data into three parts: a training set which we‚Äôll use for actually training our models, a validation set which we‚Äôll use for model selection, and a test set which we‚Äôll use for final evaluation of our chosen model. So, to make choices about the regularization strength, for example, we‚Äôll train separate models with different regularization strengths on the training set, and then evaluate their performance on the validation set. The model with the best validation performance is then selected as the final model.</p>
<section id="features-in-the-wild" class="level3">
<h3 class="anchored" data-anchor-id="features-in-the-wild">Features In the Wild</h3>
<p>Although in the previous examples we engineered our features by hand using basis-function expansions, features are also natural parts of data sets! Often the data set we want to predict already has all the features we need (or more!), and we need to make all the same choices about which features to use and how to regularize. In the case study below, we‚Äôll face some of these same questions without needing to engineer any new features by hand.</p>
</section>
<section id="bike-share-case-study" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="bike-share-case-study">Bike Share Case Study</h3>
<div id="c45975b2" class="cell" data-execution_count="22">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'https://raw.githubusercontent.com/PhilChodrow/ml-notes-update/refs/heads/main/data/bikeshare/hour.csv'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div class="page-columns page-full"><p>We‚Äôve downloaded a data set containing hourly bike rental counts in Washington, D.C., along with a variety of features that might be predictive of bike rental demand.  The data contains a variety of features including weather conditions, the year, month, week, and weekday; the hour of the day. It also includes columns for the number of casual and registered users, as well as the total count of bike rentals (<code>cnt</code>). We‚Äôll try to predict the total count of bike rentals using the other features. Let‚Äôs take a look:</p><div class="no-row-height column-margin column-container"><span class="margin-aside">This data set was collected by <span class="citation" data-cites="fanaee-tEventLabelingCombining2013">Fanaee-T and Gama (<a href="#ref-fanaee-tEventLabelingCombining2013" role="doc-biblioref">2013</a>)</span>.</span></div></div>
<div id="e22a4268" class="cell" data-execution_count="23">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>df.head()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">instant</th>
<th data-quarto-table-cell-role="th">dteday</th>
<th data-quarto-table-cell-role="th">season</th>
<th data-quarto-table-cell-role="th">yr</th>
<th data-quarto-table-cell-role="th">mnth</th>
<th data-quarto-table-cell-role="th">hr</th>
<th data-quarto-table-cell-role="th">holiday</th>
<th data-quarto-table-cell-role="th">weekday</th>
<th data-quarto-table-cell-role="th">workingday</th>
<th data-quarto-table-cell-role="th">weathersit</th>
<th data-quarto-table-cell-role="th">temp</th>
<th data-quarto-table-cell-role="th">atemp</th>
<th data-quarto-table-cell-role="th">hum</th>
<th data-quarto-table-cell-role="th">windspeed</th>
<th data-quarto-table-cell-role="th">casual</th>
<th data-quarto-table-cell-role="th">registered</th>
<th data-quarto-table-cell-role="th">cnt</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">0</th>
<td>1</td>
<td>2011-01-01</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>6</td>
<td>0</td>
<td>1</td>
<td>0.24</td>
<td>0.2879</td>
<td>0.81</td>
<td>0.0</td>
<td>3</td>
<td>13</td>
<td>16</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">1</th>
<td>2</td>
<td>2011-01-01</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>6</td>
<td>0</td>
<td>1</td>
<td>0.22</td>
<td>0.2727</td>
<td>0.80</td>
<td>0.0</td>
<td>8</td>
<td>32</td>
<td>40</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">2</th>
<td>3</td>
<td>2011-01-01</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>2</td>
<td>0</td>
<td>6</td>
<td>0</td>
<td>1</td>
<td>0.22</td>
<td>0.2727</td>
<td>0.80</td>
<td>0.0</td>
<td>5</td>
<td>27</td>
<td>32</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">3</th>
<td>4</td>
<td>2011-01-01</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>3</td>
<td>0</td>
<td>6</td>
<td>0</td>
<td>1</td>
<td>0.24</td>
<td>0.2879</td>
<td>0.75</td>
<td>0.0</td>
<td>3</td>
<td>10</td>
<td>13</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">4</th>
<td>5</td>
<td>2011-01-01</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>4</td>
<td>0</td>
<td>6</td>
<td>0</td>
<td>1</td>
<td>0.24</td>
<td>0.2879</td>
<td>0.75</td>
<td>0.0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Our aim is to predict the <code>cnt</code> column using the other features. Let‚Äôs visualize the total number of bike rentals over time:</p>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize <span class="op">=</span> (<span class="dv">6</span>, <span class="dv">4</span>))</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>ridership_by_day <span class="op">=</span> df.groupby(<span class="st">"dteday"</span>)[<span class="st">"cnt"</span>].<span class="bu">sum</span>().reset_index()</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>ridership_by_day[<span class="st">"dteday"</span>] <span class="op">=</span> pd.to_datetime(ridership_by_day[<span class="st">"dteday"</span>])</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>ax.plot(ridership_by_day[<span class="st">"dteday"</span>], ridership_by_day[<span class="st">"cnt"</span>], color<span class="op">=</span><span class="st">'steelblue'</span>)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Date"</span>)</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Total Daily Bike Rentals"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div id="fig-daily-ridership" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-daily-ridership-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="56da6ec3" class="cell" data-execution_count="24">
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>Text(0, 0.5, 'Total Daily Bike Rentals')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="06-regularization_files/figure-html/cell-25-output-2.png" class="figure-img" width="522" height="349"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-daily-ridership-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.10: Daily ridership in the bikeshare data set.
</figcaption>
</figure>
</div>
<p>Predicting the number of bike rentals on a given day is a very helpful task for bikeshare operators, as this can help them know the urgency of rebalancing (moving bikes between stations) and scheduling maintenance.</p>
<p>Let‚Äôs prepare our data for modeling by dropping some columns and transforming qualitative columns into one-hot encoded features:</p>
<div id="83f7c52b" class="cell" data-execution_count="25">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>df.drop(columns<span class="op">=</span>[<span class="st">'instant'</span>, <span class="st">'dteday'</span>, <span class="st">"workingday"</span>], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> <span class="fl">1.0</span><span class="op">*</span>pd.get_dummies(df, columns<span class="op">=</span>[<span class="st">'season'</span>, <span class="st">'weathersit'</span>, <span class="st">'mnth'</span>, <span class="st">'hr'</span>, <span class="st">'weekday'</span>], drop_first<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>df[<span class="st">"intercept"</span>] <span class="op">=</span> <span class="dv">1</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We‚Äôll now split the data into features and targets, and then into training, validation, and test sets.</p>
<div id="24c4d10f" class="cell" data-execution_count="26">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> <span class="fl">1.0</span><span class="op">*</span>df.drop(columns<span class="op">=</span>[<span class="st">'cnt'</span>, <span class="st">'casual'</span>, <span class="st">'registered'</span>])</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'cnt'</span>]</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>test_proportion <span class="op">=</span> <span class="fl">0.94</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span>test_proportion, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>X_train, X_val, y_train, y_val <span class="op">=</span> train_test_split(X_train, y_train, test_size<span class="op">=</span><span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> torch.tensor(X_train.values, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> torch.tensor(y_train.values.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>), dtype<span class="op">=</span>torch.float32)</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>X_val   <span class="op">=</span> torch.tensor(X_val.values, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>y_val   <span class="op">=</span> torch.tensor(y_val.values.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>), dtype<span class="op">=</span>torch.float32)</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>X_test  <span class="op">=</span> torch.tensor(X_test.values, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>y_test  <span class="op">=</span> torch.tensor(y_test.values.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>), dtype<span class="op">=</span>torch.float32)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We‚Äôve split the data into three pieces: a small training set (3% of the data), a validation set (3% of the data), and a test set (94% of the data). Given the size of the data set, this split is reasonable ‚Äì we have enough data to train models effectively, while still leaving a large amount of data for final evaluation.</p>
<div id="7a614454" class="cell" data-execution_count="27">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training set size: </span><span class="sc">{</span>X_train<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">}</span><span class="ss"> samples"</span>)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Validation set size: </span><span class="sc">{</span>X_val<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">}</span><span class="ss"> samples"</span>)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test set size: </span><span class="sc">{</span>X_test<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">}</span><span class="ss"> samples"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Training set size: 521 samples
Validation set size: 521 samples
Test set size: 16337 samples</code></pre>
</div>
</div>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Why so few training samples?
</div>
</div>
<div class="callout-body-container callout-body">
<p>Issues related to feature engineering and regularization most frequently arise in settings where we many <em>parameters</em> and relatively few <em>data points</em>. Therefore, to illustrate these issues clearly, we‚Äôve deliberately limited the size of the training set. In practice, of course, one would typically want to use as much data as possible for training.</p>
<p>Although this example is a bit artificial, modern neural networks are often trained with vastly more parameters than data points, making these considerations quite relevant in practice.</p>
</div>
</div>
<p>To contextualize model performance on real data, it‚Äôs helpful to compute a <em>base rate</em>.</p>
<div id="def-base-rate" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.2 (Base Rate)</strong></span> A <em>base rate</em> is a measure of performance for a model which uses <em>no features</em> in the data.</p>
<p>We typically say that a model has demonstrated success in learning from features if it outperforms the base rate on unseen data.</p>
</div>
<p>Let‚Äôs check the base rate for the bikeshare data and assess on validation data:</p>
<div id="442b136b" class="cell" data-execution_count="28">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>y_mean <span class="op">=</span> y_train.mean()</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>base_rate_mse <span class="op">=</span> mse(y_val, y_mean)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Base rate MSE on validation data: </span><span class="sc">{</span>base_rate_mse<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Base rate MSE on validation data: 33617.3125</code></pre>
</div>
</div>
<p>So, we are looking for any reasonable candidate model to achieve validation MSE less than the above.</p>
<p>In principle, we can already go ahead and fit a model:</p>
<div id="629515e3" class="cell" data-execution_count="29">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>LR <span class="op">=</span> LinearRegression(n_params<span class="op">=</span>X_train.shape[<span class="dv">1</span>])</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>train_model(LR, X_train, y_train, lr<span class="op">=</span><span class="fl">1e-2</span>, n_epochs<span class="op">=</span><span class="dv">50000</span>, tol<span class="op">=</span><span class="fl">1e-4</span>)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> LR.forward(X_val)</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>val_mse <span class="op">=</span> mse(y_pred, y_val)</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Validation MSE without regularization: </span><span class="sc">{</span>val_mse<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Fraction of base rate: </span><span class="sc">{</span>val_mse<span class="sc">.</span>item()<span class="op">/</span>base_rate_mse<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Validation MSE without regularization: 12257.3779
Fraction of base rate: 0.3646</code></pre>
</div>
</div>
<p>This simple linear regression model with no regularization already performs considerably better on validation data than the base rate. Can we do better with regularization? To find out, we‚Äôll do a systematic search in which we vary the regularization strength and evaluate performance on validation data, for each of ridge regression and LASSO.</p>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>reg_strengths <span class="op">=</span> torch.logspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">0</span>, <span class="dv">21</span>)  </span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>train_mses <span class="op">=</span> []</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>val_mses <span class="op">=</span> []</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> torch.empty(X_train.shape[<span class="dv">1</span>], <span class="dv">0</span>)</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> reg <span class="kw">in</span> reg_strengths: </span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>    LR <span class="op">=</span> LinearRegression(n_params<span class="op">=</span>X_train.shape[<span class="dv">1</span>])</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>    train_model(LR, X_train, y_train, lr<span class="op">=</span><span class="fl">1e-1</span>, n_epochs<span class="op">=</span><span class="dv">20000</span>, tol <span class="op">=</span> <span class="fl">1e-2</span>, regularization <span class="op">=</span> <span class="kw">lambda</span> w: reg<span class="op">*</span>ell_2_regularization(w))</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>    y_pred_train <span class="op">=</span> LR.forward(X_train)</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>    train_mse <span class="op">=</span> mse(y_pred_train, y_train)</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>    y_pred_val <span class="op">=</span> LR.forward(X_val)</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>    val_mse <span class="op">=</span> mse(y_pred_val, y_val)</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>    train_mses.append(train_mse.item())</span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>    val_mses.append(val_mse.item())</span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>    W <span class="op">=</span> torch.cat((W, LR.w), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>best_reg <span class="op">=</span> reg_strengths[val_mses.index(<span class="bu">min</span>(val_mses))]</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize <span class="op">=</span> (<span class="dv">8</span>,<span class="dv">3</span>))</span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(reg_strengths, train_mses, label<span class="op">=</span><span class="st">'Train MSE'</span>)</span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(reg_strengths, val_mses, label<span class="op">=</span><span class="st">'Validation MSE'</span>)</span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].axvline(best_reg, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="vs">fr'Best </span><span class="dv">$</span><span class="er">\</span><span class="vs">lambda=</span><span class="dv">$</span><span class="sc">{</span>best_reg<span class="sc">:.2f}</span><span class="vs">, val MSE </span><span class="sc">{</span><span class="bu">min</span>(val_mses)<span class="sc">:.1f}</span><span class="vs">'</span>)</span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">"Regularization Strength"</span>)</span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="st">"MSE"</span>)</span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xscale(<span class="st">"log"</span>)</span>
<span id="cb33-35"><a href="#cb33-35" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Ridge Regression: Train and Validation MSE vs Regularization Strength"</span>)</span>
<span id="cb33-36"><a href="#cb33-36" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].legend()</span>
<span id="cb33-37"><a href="#cb33-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-38"><a href="#cb33-38" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(reg_strengths, W.detach().numpy().T)</span>
<span id="cb33-39"><a href="#cb33-39" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].axvline(best_reg, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="ss">f'Best reg=</span><span class="sc">{</span>best_reg<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb33-40"><a href="#cb33-40" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">"Regularization Strength"</span>)</span>
<span id="cb33-41"><a href="#cb33-41" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="st">"Weights"</span>)</span>
<span id="cb33-42"><a href="#cb33-42" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xscale(<span class="st">"log"</span>)</span>
<span id="cb33-43"><a href="#cb33-43" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Ridge Regression: Weights vs Regularization Strength"</span>)</span>
<span id="cb33-44"><a href="#cb33-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-45"><a href="#cb33-45" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div id="fig-ridge-regression-model-selection" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-ridge-regression-model-selection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="d6d5f9d1" class="cell" data-execution_count="30">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="06-regularization_files/figure-html/cell-31-output-1.png" class="figure-img" width="798" height="275"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-ridge-regression-model-selection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.11: Model selection in ridge regression on the bikeshare data set. (Left): training and validation MSE as a function of regularization strength. (Right): learned weights as a function of regularization strength.
</figcaption>
</figure>
</div>
<p>Now let‚Äôs try the same experiment for LASSO.</p>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>reg_strengths <span class="op">=</span> torch.logspace(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">21</span>)  </span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>train_mses <span class="op">=</span> []</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>val_mses <span class="op">=</span> []</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> torch.empty(X_train.shape[<span class="dv">1</span>], <span class="dv">0</span>)</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> reg <span class="kw">in</span> reg_strengths: </span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>    LR <span class="op">=</span> LinearRegression(n_params<span class="op">=</span>X_train.shape[<span class="dv">1</span>])</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>    train_model(LR, X_train, y_train, lr<span class="op">=</span><span class="fl">1e-2</span>, n_epochs<span class="op">=</span><span class="dv">50000</span>, tol <span class="op">=</span> <span class="fl">1e-2</span>, regularization <span class="op">=</span> <span class="kw">lambda</span> w: reg<span class="op">*</span>ell_1_regularization(w), verbose <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>    y_pred_train <span class="op">=</span> LR.forward(X_train)</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>    train_mse <span class="op">=</span> mse(y_pred_train, y_train)</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>    y_pred_val <span class="op">=</span> LR.forward(X_val)</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>    val_mse <span class="op">=</span> mse(y_pred_val, y_val)</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>    train_mses.append(train_mse.item())</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>    val_mses.append(val_mse.item())</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>    W <span class="op">=</span> torch.cat((W, LR.w), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>best_reg <span class="op">=</span> reg_strengths[val_mses.index(<span class="bu">min</span>(val_mses))]</span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize <span class="op">=</span> (<span class="dv">8</span>,<span class="dv">3</span>))</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(reg_strengths, train_mses, label<span class="op">=</span><span class="st">'Train MSE'</span>)</span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(reg_strengths, val_mses, label<span class="op">=</span><span class="st">'Validation MSE'</span>)</span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].axvline(best_reg, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="vs">fr'Best </span><span class="dv">$</span><span class="er">\</span><span class="vs">lambda=</span><span class="dv">$</span><span class="sc">{</span>best_reg<span class="sc">:.2f}</span><span class="vs">, val MSE </span><span class="sc">{</span><span class="bu">min</span>(val_mses)<span class="sc">:.1f}</span><span class="vs">'</span>)</span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">"Regularization Strength"</span>)</span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="st">"MSE"</span>)</span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xscale(<span class="st">"log"</span>)</span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"LASSO Regression: Train and Validation MSE vs Regularization Strength"</span>)</span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].legend()</span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(reg_strengths, W.detach().numpy().T)</span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].axvline(best_reg, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="ss">f'Best reg=</span><span class="sc">{</span>best_reg<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb34-40"><a href="#cb34-40" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">"Regularization Strength"</span>)</span>
<span id="cb34-41"><a href="#cb34-41" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="st">"Weights"</span>)</span>
<span id="cb34-42"><a href="#cb34-42" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xscale(<span class="st">"log"</span>)</span>
<span id="cb34-43"><a href="#cb34-43" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"LASSO Regression: Weights vs Regularization Strength"</span>)</span>
<span id="cb34-44"><a href="#cb34-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-45"><a href="#cb34-45" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div id="fig-lasso-regression-model-selection" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-lasso-regression-model-selection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="13081abe" class="cell" data-execution_count="31">
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0, Loss: 63828.96875
Epoch 100, Loss: 60922.2421875
Epoch 200, Loss: 58204.4296875
Epoch 300, Loss: 55661.859375
Epoch 400, Loss: 53281.234375
Epoch 500, Loss: 51051.63671875
Epoch 600, Loss: 48963.96484375
Epoch 700, Loss: 47009.9609375
Epoch 800, Loss: 45181.90234375
Epoch 900, Loss: 43472.578125
Epoch 1000, Loss: 41875.12109375
Epoch 1100, Loss: 40382.87890625
Epoch 1200, Loss: 38989.4453125
Epoch 1300, Loss: 37688.6796875
Epoch 1400, Loss: 36474.65234375
Epoch 1500, Loss: 35341.7578125
Epoch 1600, Loss: 34284.3984375
Epoch 1700, Loss: 33297.328125
Epoch 1800, Loss: 32375.54296875
Epoch 1900, Loss: 31514.36328125
Epoch 2000, Loss: 30709.40625
Epoch 2100, Loss: 29956.591796875
Epoch 2200, Loss: 29252.201171875
Epoch 2300, Loss: 28592.66796875
Epoch 2400, Loss: 27974.79296875
Epoch 2500, Loss: 27395.48828125
Epoch 2600, Loss: 26851.85546875
Epoch 2700, Loss: 26341.259765625
Epoch 2800, Loss: 25861.25
Epoch 2900, Loss: 25409.55078125
Epoch 3000, Loss: 24984.01953125
Epoch 3100, Loss: 24582.662109375
Epoch 3200, Loss: 24203.607421875
Epoch 3300, Loss: 23845.119140625
Epoch 3400, Loss: 23505.544921875
Epoch 3500, Loss: 23183.3515625
Epoch 3600, Loss: 22877.05859375
Epoch 3700, Loss: 22585.251953125
Epoch 3800, Loss: 22306.6328125
Epoch 3900, Loss: 22040.013671875
Epoch 4000, Loss: 21784.294921875
Epoch 4100, Loss: 21538.505859375
Epoch 4200, Loss: 21301.740234375
Epoch 4300, Loss: 21073.12890625
Epoch 4400, Loss: 20851.94140625
Epoch 4500, Loss: 20637.576171875
Epoch 4600, Loss: 20429.40625
Epoch 4700, Loss: 20226.927734375
Epoch 4800, Loss: 20029.701171875
Epoch 4900, Loss: 19837.341796875
Epoch 5000, Loss: 19649.52734375
Epoch 5100, Loss: 19465.970703125
Epoch 5200, Loss: 19286.419921875
Epoch 5300, Loss: 19110.658203125
Epoch 5400, Loss: 18938.4921875
Epoch 5500, Loss: 18769.751953125
Epoch 5600, Loss: 18604.2890625
Epoch 5700, Loss: 18441.96484375
Epoch 5800, Loss: 18282.66015625
Epoch 5900, Loss: 18126.255859375
Epoch 6000, Loss: 17972.66015625
Epoch 6100, Loss: 17821.82421875
Epoch 6200, Loss: 17673.611328125
Epoch 6300, Loss: 17527.947265625
Epoch 6400, Loss: 17384.759765625
Epoch 6500, Loss: 17243.978515625
Epoch 6600, Loss: 17105.548828125
Epoch 6700, Loss: 16969.412109375
Epoch 6800, Loss: 16835.509765625
Epoch 6900, Loss: 16703.798828125
Epoch 7000, Loss: 16574.224609375
Epoch 7100, Loss: 16446.74609375
Epoch 7200, Loss: 16321.3388671875
Epoch 7300, Loss: 16197.951171875
Epoch 7400, Loss: 16076.5908203125
Epoch 7500, Loss: 15957.162109375
Epoch 7600, Loss: 15839.603515625
Epoch 7700, Loss: 15723.8837890625
Epoch 7800, Loss: 15609.9521484375
Epoch 7900, Loss: 15497.767578125
Epoch 8000, Loss: 15387.3076171875
Epoch 8100, Loss: 15278.51953125
Epoch 8200, Loss: 15171.3857421875
Epoch 8300, Loss: 15065.8623046875
Epoch 8400, Loss: 14961.9150390625
Epoch 8500, Loss: 14859.517578125
Epoch 8600, Loss: 14758.6416015625
Epoch 8700, Loss: 14659.255859375
Epoch 8800, Loss: 14561.3486328125
Epoch 8900, Loss: 14464.8798828125
Epoch 9000, Loss: 14369.8427734375
Epoch 9100, Loss: 14276.2119140625
Epoch 9200, Loss: 14183.96875
Epoch 9300, Loss: 14093.1240234375
Epoch 9400, Loss: 14003.68359375
Epoch 9500, Loss: 13915.5869140625
Epoch 9600, Loss: 13828.826171875
Epoch 9700, Loss: 13743.3935546875
Epoch 9800, Loss: 13659.275390625
Epoch 9900, Loss: 13576.46484375
Epoch 10000, Loss: 13494.9541015625
Epoch 10100, Loss: 13414.740234375
Epoch 10200, Loss: 13335.8125
Epoch 10300, Loss: 13258.1669921875
Epoch 10400, Loss: 13181.798828125
Epoch 10500, Loss: 13106.7001953125
Epoch 10600, Loss: 13032.87109375
Epoch 10700, Loss: 12960.2998046875
Epoch 10800, Loss: 12888.982421875
Epoch 10900, Loss: 12818.9052734375
Epoch 11000, Loss: 12750.0673828125
Epoch 11100, Loss: 12682.4560546875
Epoch 11200, Loss: 12616.060546875
Epoch 11300, Loss: 12550.865234375
Epoch 11400, Loss: 12486.861328125
Epoch 11500, Loss: 12424.03515625
Epoch 11600, Loss: 12362.373046875
Epoch 11700, Loss: 12301.869140625
Epoch 11800, Loss: 12242.4921875
Epoch 11900, Loss: 12184.224609375
Epoch 12000, Loss: 12127.048828125
Epoch 12100, Loss: 12070.9453125
Epoch 12200, Loss: 12015.890625
Epoch 12300, Loss: 11961.8984375
Epoch 12400, Loss: 11908.9599609375
Epoch 12500, Loss: 11857.015625
Epoch 12600, Loss: 11806.05078125
Epoch 12700, Loss: 11756.0439453125
Epoch 12800, Loss: 11706.978515625
Epoch 12900, Loss: 11658.841796875
Epoch 13000, Loss: 11611.6181640625
Epoch 13100, Loss: 11565.2919921875
Epoch 13200, Loss: 11519.8427734375
Epoch 13300, Loss: 11475.2724609375
Epoch 13400, Loss: 11431.556640625
Epoch 13500, Loss: 11388.6845703125
Epoch 13600, Loss: 11346.64453125
Epoch 13700, Loss: 11305.435546875
Epoch 13800, Loss: 11265.0322265625
Epoch 13900, Loss: 11225.4306640625
Epoch 14000, Loss: 11186.6181640625
Epoch 14100, Loss: 11148.5888671875
Epoch 14200, Loss: 11111.330078125
Epoch 14300, Loss: 11074.8603515625
Epoch 14400, Loss: 11039.201171875
Epoch 14500, Loss: 11004.291015625
Epoch 14600, Loss: 10970.119140625
Epoch 14700, Loss: 10936.6748046875
Epoch 14800, Loss: 10903.982421875
Epoch 14900, Loss: 10872.0234375
Epoch 15000, Loss: 10840.7724609375
Epoch 15100, Loss: 10810.22265625
Epoch 15200, Loss: 10780.3701171875
Epoch 15300, Loss: 10751.2080078125
Epoch 15400, Loss: 10722.7314453125
Epoch 15500, Loss: 10694.9521484375
Epoch 15600, Loss: 10667.8515625
Epoch 15700, Loss: 10641.4111328125
Epoch 15800, Loss: 10615.6279296875
Epoch 15900, Loss: 10590.490234375
Epoch 16000, Loss: 10565.998046875
Epoch 16100, Loss: 10542.1630859375
Epoch 16200, Loss: 10518.9453125
Epoch 16300, Loss: 10496.3505859375
Epoch 16400, Loss: 10474.3701171875
Epoch 16500, Loss: 10452.982421875
Epoch 16600, Loss: 10432.1748046875
Epoch 16700, Loss: 10411.9404296875
Epoch 16800, Loss: 10392.2666015625
Epoch 16900, Loss: 10373.146484375
Epoch 17000, Loss: 10354.5712890625
Epoch 17100, Loss: 10336.5263671875
Epoch 17200, Loss: 10319.00390625
Epoch 17300, Loss: 10301.99609375
Epoch 17400, Loss: 10285.4921875
Epoch 17500, Loss: 10269.486328125
Epoch 17600, Loss: 10253.96875
Epoch 17700, Loss: 10238.9326171875
Epoch 17800, Loss: 10224.3720703125
Epoch 17900, Loss: 10210.28125
Epoch 18000, Loss: 10196.65625
Epoch 18100, Loss: 10183.48828125
Epoch 18200, Loss: 10170.7744140625
Epoch 18300, Loss: 10158.5068359375
Epoch 18400, Loss: 10146.69140625
Epoch 18500, Loss: 10135.3330078125
Epoch 18600, Loss: 10124.412109375
Epoch 18700, Loss: 10113.9169921875
Epoch 18800, Loss: 10103.8427734375
Epoch 18900, Loss: 10094.1826171875
Epoch 19000, Loss: 10084.9296875
Epoch 19100, Loss: 10076.080078125
Epoch 19200, Loss: 10067.6240234375
Epoch 19300, Loss: 10059.5869140625
Epoch 19400, Loss: 10051.9345703125
Epoch 19500, Loss: 10044.658203125
Epoch 19600, Loss: 10037.748046875
Epoch 19700, Loss: 10031.197265625
Epoch 19800, Loss: 10024.99609375
Epoch 19900, Loss: 10019.134765625
Epoch 20000, Loss: 10013.6015625
Epoch 20100, Loss: 10008.400390625
Epoch 20200, Loss: 10003.521484375
Epoch 20300, Loss: 9998.9501953125
Epoch 20400, Loss: 9994.6650390625
Epoch 20500, Loss: 9990.662109375
Epoch 20600, Loss: 9986.92578125
Epoch 20700, Loss: 9983.4599609375
Epoch 20800, Loss: 9980.2392578125
Epoch 20900, Loss: 9977.2548828125
Epoch 21000, Loss: 9974.4931640625
Epoch 21100, Loss: 9971.9482421875
Epoch 21200, Loss: 9969.6015625
Epoch 21300, Loss: 9967.4453125
Epoch 21400, Loss: 9965.4658203125
Epoch 21500, Loss: 9963.65234375
Epoch 21600, Loss: 9961.9931640625
Epoch 21700, Loss: 9960.4794921875
Epoch 21800, Loss: 9959.09765625
Epoch 21900, Loss: 9957.841796875
Epoch 22000, Loss: 9956.69921875
Epoch 22100, Loss: 9955.6630859375
Epoch 22200, Loss: 9954.72265625
Epoch 22300, Loss: 9953.8720703125
Epoch 22400, Loss: 9953.103515625
Epoch 22500, Loss: 9952.4111328125
Epoch 22600, Loss: 9951.7861328125
Epoch 22700, Loss: 9951.2236328125
Epoch 22800, Loss: 9950.71875
Epoch 22900, Loss: 9950.267578125
Epoch 23000, Loss: 9949.86328125
Epoch 23100, Loss: 9949.50390625
Epoch 23200, Loss: 9949.1845703125
Epoch 23300, Loss: 9948.8994140625
Epoch 23400, Loss: 9948.6484375
Epoch 23500, Loss: 9948.431640625
Epoch 23600, Loss: 9948.2392578125
Epoch 23700, Loss: 9948.072265625
Epoch 23800, Loss: 9947.9248046875
Epoch 23900, Loss: 9947.7939453125
Epoch 24000, Loss: 9947.6796875
Epoch 24100, Loss: 9947.576171875
Epoch 24200, Loss: 9947.4833984375
Epoch 24300, Loss: 9947.3994140625
Epoch 24400, Loss: 9947.322265625
Epoch 24500, Loss: 9947.25
Epoch 24600, Loss: 9947.181640625
Epoch 24700, Loss: 9947.1171875
Epoch 24800, Loss: 9947.0517578125
Epoch 24900, Loss: 9946.9892578125
Epoch 25000, Loss: 9946.92578125
Epoch 25100, Loss: 9946.8623046875
Epoch 25200, Loss: 9946.7978515625
Epoch 25300, Loss: 9946.732421875
Epoch 25400, Loss: 9946.6650390625
Epoch 25500, Loss: 9946.59375
Epoch 25600, Loss: 9946.5244140625
Epoch 25700, Loss: 9946.4521484375
Epoch 25800, Loss: 9946.3779296875
Epoch 25900, Loss: 9946.3037109375
Epoch 26000, Loss: 9946.2255859375
Epoch 26100, Loss: 9946.142578125
Epoch 26200, Loss: 9946.05859375
Epoch 26300, Loss: 9945.970703125
Epoch 26400, Loss: 9945.880859375
Epoch 26500, Loss: 9945.78515625
Epoch 26600, Loss: 9945.6884765625
Epoch 26700, Loss: 9945.5869140625
Epoch 26800, Loss: 9945.482421875
Epoch 26900, Loss: 9945.3740234375
Epoch 27000, Loss: 9945.2626953125
Epoch 27100, Loss: 9945.146484375
Epoch 27200, Loss: 9945.0263671875
Epoch 27300, Loss: 9944.9033203125
Epoch 27400, Loss: 9944.77734375
Epoch 27500, Loss: 9944.6455078125
Epoch 27600, Loss: 9944.5107421875
Epoch 27700, Loss: 9944.37109375
Epoch 27800, Loss: 9944.228515625
Epoch 27900, Loss: 9944.08203125
Epoch 28000, Loss: 9943.931640625
Epoch 28100, Loss: 9943.7763671875
Epoch 28200, Loss: 9943.6171875
Epoch 28300, Loss: 9943.455078125
Epoch 28400, Loss: 9943.2900390625
Epoch 28500, Loss: 9943.13671875
Epoch 28600, Loss: 9942.9892578125
Epoch 28700, Loss: 9942.86328125
Epoch 28800, Loss: 9942.751953125
Epoch 28900, Loss: 9942.640625
Epoch 29000, Loss: 9942.525390625
Epoch 29100, Loss: 9942.4072265625
Epoch 29200, Loss: 9942.287109375
Epoch 29300, Loss: 9942.1611328125
Epoch 29400, Loss: 9942.03515625
Epoch 29500, Loss: 9941.9033203125
Epoch 29600, Loss: 9941.76953125
Epoch 29700, Loss: 9941.6328125
Epoch 29800, Loss: 9941.49609375
Epoch 29900, Loss: 9941.3544921875
Epoch 30000, Loss: 9941.2109375
Epoch 30100, Loss: 9941.0673828125
Epoch 30200, Loss: 9940.955078125
Epoch 30300, Loss: 9940.857421875
Epoch 30400, Loss: 9940.7568359375
Epoch 30500, Loss: 9940.677734375
Epoch 30600, Loss: 9940.6171875
Epoch 30700, Loss: 9940.5556640625
Epoch 30800, Loss: 9940.4912109375
Epoch 30900, Loss: 9940.4267578125
Epoch 31000, Loss: 9940.359375
Epoch 31100, Loss: 9940.291015625
Epoch 31200, Loss: 9940.220703125
Epoch 31300, Loss: 9940.1474609375
Epoch 31400, Loss: 9940.076171875
Epoch 31500, Loss: 9939.9990234375
Epoch 31600, Loss: 9939.9228515625
Epoch 31700, Loss: 9939.8447265625
Epoch 31800, Loss: 9939.763671875
Epoch 31900, Loss: 9939.6826171875
Epoch 32000, Loss: 9939.5986328125
Epoch 32100, Loss: 9939.529296875
Epoch 32200, Loss: 9939.4912109375
Epoch 32300, Loss: 9939.451171875
Epoch 32400, Loss: 9939.4111328125
Epoch 32500, Loss: 9939.369140625
Epoch 32600, Loss: 9939.3427734375
Epoch 32700, Loss: 9939.3310546875
Epoch 32800, Loss: 9939.3193359375
Epoch 32900, Loss: 9939.30859375
Epoch 33000, Loss: 9939.3056640625
Epoch 33100, Loss: 9939.302734375
Epoch 33200, Loss: 9939.3017578125
Epoch 33300, Loss: 9939.3017578125
Epoch 33400, Loss: 9939.3017578125
Epoch 33500, Loss: 9939.3017578125
Epoch 33600, Loss: 9939.30078125
Epoch 33700, Loss: 9939.3017578125
Epoch 33800, Loss: 9939.30078125
Epoch 33900, Loss: 9939.30078125
Epoch 34000, Loss: 9939.3017578125
Epoch 34100, Loss: 9939.3017578125
Epoch 34200, Loss: 9939.30078125
Epoch 34300, Loss: 9939.3017578125
Epoch 34400, Loss: 9939.3017578125
Epoch 34500, Loss: 9939.30078125
Epoch 34600, Loss: 9939.30078125
Epoch 34700, Loss: 9939.3017578125
Epoch 34800, Loss: 9939.3017578125
Epoch 34900, Loss: 9939.3017578125
Epoch 35000, Loss: 9939.30078125
Epoch 35100, Loss: 9939.30078125
Epoch 35200, Loss: 9939.30078125
Epoch 35300, Loss: 9939.30078125
Epoch 35400, Loss: 9939.3017578125
Epoch 35500, Loss: 9939.30078125
Epoch 35600, Loss: 9939.30078125
Epoch 35700, Loss: 9939.30078125
Epoch 35800, Loss: 9939.30078125
Epoch 35900, Loss: 9939.30078125
Epoch 36000, Loss: 9939.30078125
Epoch 36100, Loss: 9939.30078125
Epoch 36200, Loss: 9939.30078125
Epoch 36300, Loss: 9939.30078125
Epoch 36400, Loss: 9939.30078125
Epoch 36500, Loss: 9939.30078125
Epoch 36600, Loss: 9939.30078125
Epoch 36700, Loss: 9939.30078125
Epoch 36800, Loss: 9939.3017578125
Epoch 36900, Loss: 9939.30078125
Epoch 37000, Loss: 9939.30078125
Epoch 37100, Loss: 9939.30078125
Epoch 37200, Loss: 9939.3017578125
Epoch 37300, Loss: 9939.30078125
Epoch 37400, Loss: 9939.30078125
Epoch 37500, Loss: 9939.30078125
Epoch 37600, Loss: 9939.30078125
Epoch 37700, Loss: 9939.30078125
Epoch 37800, Loss: 9939.30078125
Epoch 37900, Loss: 9939.30078125
Epoch 38000, Loss: 9939.30078125
Epoch 38100, Loss: 9939.3017578125
Epoch 38200, Loss: 9939.30078125
Epoch 38300, Loss: 9939.30078125
Epoch 38400, Loss: 9939.30078125
Epoch 38500, Loss: 9939.30078125
Epoch 38600, Loss: 9939.30078125
Epoch 38700, Loss: 9939.30078125
Epoch 38800, Loss: 9939.30078125
Epoch 38900, Loss: 9939.30078125
Epoch 39000, Loss: 9939.30078125
Epoch 39100, Loss: 9939.30078125
Epoch 39200, Loss: 9939.3017578125
Epoch 39300, Loss: 9939.30078125
Epoch 39400, Loss: 9939.30078125
Epoch 39500, Loss: 9939.30078125
Epoch 39600, Loss: 9939.30078125
Epoch 39700, Loss: 9939.30078125
Epoch 39800, Loss: 9939.30078125
Epoch 39900, Loss: 9939.30078125
Epoch 40000, Loss: 9939.30078125
Epoch 40100, Loss: 9939.30078125
Epoch 40200, Loss: 9939.30078125
Epoch 40300, Loss: 9939.3017578125
Epoch 40400, Loss: 9939.30078125
Epoch 40500, Loss: 9939.30078125
Epoch 40600, Loss: 9939.30078125
Epoch 40700, Loss: 9939.30078125
Epoch 40800, Loss: 9939.3017578125
Epoch 40900, Loss: 9939.30078125
Epoch 41000, Loss: 9939.30078125
Epoch 41100, Loss: 9939.30078125
Epoch 41200, Loss: 9939.30078125
Epoch 41300, Loss: 9939.30078125
Epoch 41400, Loss: 9939.30078125
Epoch 41500, Loss: 9939.30078125
Epoch 41600, Loss: 9939.30078125
Epoch 41700, Loss: 9939.30078125
Epoch 41800, Loss: 9939.30078125
Epoch 41900, Loss: 9939.30078125
Epoch 42000, Loss: 9939.3017578125
Epoch 42100, Loss: 9939.30078125
Epoch 42200, Loss: 9939.30078125
Epoch 42300, Loss: 9939.30078125
Epoch 42400, Loss: 9939.30078125
Epoch 42500, Loss: 9939.30078125
Epoch 42600, Loss: 9939.30078125
Epoch 42700, Loss: 9939.30078125
Epoch 42800, Loss: 9939.30078125
Epoch 42900, Loss: 9939.30078125
Epoch 43000, Loss: 9939.30078125
Epoch 43100, Loss: 9939.30078125
Epoch 43200, Loss: 9939.30078125
Epoch 43300, Loss: 9939.3017578125
Epoch 43400, Loss: 9939.30078125
Epoch 43500, Loss: 9939.30078125
Epoch 43600, Loss: 9939.30078125
Epoch 43700, Loss: 9939.30078125
Epoch 43800, Loss: 9939.30078125
Epoch 43900, Loss: 9939.30078125
Epoch 44000, Loss: 9939.30078125
Epoch 44100, Loss: 9939.30078125
Epoch 44200, Loss: 9939.30078125
Epoch 44300, Loss: 9939.30078125
Epoch 44400, Loss: 9939.30078125
Epoch 44500, Loss: 9939.30078125
Epoch 44600, Loss: 9939.30078125
Epoch 44700, Loss: 9939.30078125
Epoch 44800, Loss: 9939.3017578125
Epoch 44900, Loss: 9939.30078125
Epoch 45000, Loss: 9939.3017578125
Epoch 45100, Loss: 9939.30078125
Epoch 45200, Loss: 9939.30078125
Epoch 45300, Loss: 9939.3017578125
Epoch 45400, Loss: 9939.30078125
Epoch 45500, Loss: 9939.30078125
Epoch 45600, Loss: 9939.30078125
Epoch 45700, Loss: 9939.30078125
Epoch 45800, Loss: 9939.30078125
Epoch 45900, Loss: 9939.30078125
Epoch 46000, Loss: 9939.30078125
Epoch 46100, Loss: 9939.30078125
Epoch 46200, Loss: 9939.30078125
Epoch 46300, Loss: 9939.30078125
Epoch 46400, Loss: 9939.30078125
Epoch 46500, Loss: 9939.30078125
Epoch 46600, Loss: 9939.30078125
Epoch 46700, Loss: 9939.30078125
Epoch 46800, Loss: 9939.30078125
Epoch 46900, Loss: 9939.30078125
Epoch 47000, Loss: 9939.30078125
Epoch 47100, Loss: 9939.3017578125
Epoch 47200, Loss: 9939.30078125
Epoch 47300, Loss: 9939.3017578125
Epoch 47400, Loss: 9939.2998046875
Epoch 47500, Loss: 9939.30078125
Epoch 47600, Loss: 9939.30078125
Epoch 47700, Loss: 9939.30078125
Epoch 47800, Loss: 9939.30078125
Epoch 47900, Loss: 9939.30078125
Epoch 48000, Loss: 9939.30078125
Epoch 48100, Loss: 9939.3017578125
Epoch 48200, Loss: 9939.30078125
Epoch 48300, Loss: 9939.30078125
Epoch 48400, Loss: 9939.30078125
Epoch 48500, Loss: 9939.30078125
Epoch 48600, Loss: 9939.30078125
Epoch 48700, Loss: 9939.30078125
Epoch 48800, Loss: 9939.30078125
Epoch 48900, Loss: 9939.30078125
Epoch 49000, Loss: 9939.30078125
Epoch 49100, Loss: 9939.30078125
Epoch 49200, Loss: 9939.30078125
Epoch 49300, Loss: 9939.30078125
Epoch 49400, Loss: 9939.30078125
Epoch 49500, Loss: 9939.30078125
Epoch 49600, Loss: 9939.30078125
Epoch 49700, Loss: 9939.30078125
Epoch 49800, Loss: 9939.30078125
Epoch 49900, Loss: 9939.3017578125
Epoch 0, Loss: 63828.96875
Epoch 100, Loss: 60922.49609375
Epoch 200, Loss: 58204.91015625
Epoch 300, Loss: 55662.53515625
Epoch 400, Loss: 53282.11328125
Epoch 500, Loss: 51052.69921875
Epoch 600, Loss: 48965.23046875
Epoch 700, Loss: 47011.41015625
Epoch 800, Loss: 45183.52734375
Epoch 900, Loss: 43474.3671875
Epoch 1000, Loss: 41877.0859375
Epoch 1100, Loss: 40385.0
Epoch 1200, Loss: 38991.71875
Epoch 1300, Loss: 37691.0859375
Epoch 1400, Loss: 36477.1875
Epoch 1500, Loss: 35344.421875
Epoch 1600, Loss: 34287.1953125
Epoch 1700, Loss: 33300.23046875
Epoch 1800, Loss: 32378.560546875
Epoch 1900, Loss: 31517.4765625
Epoch 2000, Loss: 30712.607421875
Epoch 2100, Loss: 29959.875
Epoch 2200, Loss: 29255.583984375
Epoch 2300, Loss: 28596.146484375
Epoch 2400, Loss: 27978.390625
Epoch 2500, Loss: 27399.212890625
Epoch 2600, Loss: 26855.701171875
Epoch 2700, Loss: 26345.21875
Epoch 2800, Loss: 25865.326171875
Epoch 2900, Loss: 25413.72265625
Epoch 3000, Loss: 24988.291015625
Epoch 3100, Loss: 24587.03515625
Epoch 3200, Loss: 24208.072265625
Epoch 3300, Loss: 23849.681640625
Epoch 3400, Loss: 23510.21484375
Epoch 3500, Loss: 23188.13671875
Epoch 3600, Loss: 22881.97265625
Epoch 3700, Loss: 22590.291015625
Epoch 3800, Loss: 22311.794921875
Epoch 3900, Loss: 22045.296875
Epoch 4000, Loss: 21789.693359375
Epoch 4100, Loss: 21544.029296875
Epoch 4200, Loss: 21307.38671875
Epoch 4300, Loss: 21078.900390625
Epoch 4400, Loss: 20857.83203125
Epoch 4500, Loss: 20643.595703125
Epoch 4600, Loss: 20435.5546875
Epoch 4700, Loss: 20233.201171875
Epoch 4800, Loss: 20036.091796875
Epoch 4900, Loss: 19843.853515625
Epoch 5000, Loss: 19656.154296875
Epoch 5100, Loss: 19472.708984375
Epoch 5200, Loss: 19293.265625
Epoch 5300, Loss: 19117.611328125
Epoch 5400, Loss: 18945.548828125
Epoch 5500, Loss: 18776.9140625
Epoch 5600, Loss: 18611.552734375
Epoch 5700, Loss: 18449.326171875
Epoch 5800, Loss: 18290.11328125
Epoch 5900, Loss: 18133.806640625
Epoch 6000, Loss: 17980.30078125
Epoch 6100, Loss: 17829.568359375
Epoch 6200, Loss: 17681.45703125
Epoch 6300, Loss: 17535.88671875
Epoch 6400, Loss: 17392.79296875
Epoch 6500, Loss: 17252.107421875
Epoch 6600, Loss: 17113.763671875
Epoch 6700, Loss: 16977.71484375
Epoch 6800, Loss: 16843.900390625
Epoch 6900, Loss: 16712.267578125
Epoch 7000, Loss: 16582.779296875
Epoch 7100, Loss: 16455.376953125
Epoch 7200, Loss: 16330.0556640625
Epoch 7300, Loss: 16206.7587890625
Epoch 7400, Loss: 16085.5
Epoch 7500, Loss: 15966.169921875
Epoch 7600, Loss: 15848.7109375
Epoch 7700, Loss: 15733.0830078125
Epoch 7800, Loss: 15619.2421875
Epoch 7900, Loss: 15507.146484375
Epoch 8000, Loss: 15396.767578125
Epoch 8100, Loss: 15288.0615234375
Epoch 8200, Loss: 15181.0068359375
Epoch 8300, Loss: 15075.560546875
Epoch 8400, Loss: 14971.6845703125
Epoch 8500, Loss: 14869.3583984375
Epoch 8600, Loss: 14768.552734375
Epoch 8700, Loss: 14669.2333984375
Epoch 8800, Loss: 14571.388671875
Epoch 8900, Loss: 14474.9833984375
Epoch 9000, Loss: 14380.0078125
Epoch 9100, Loss: 14286.4384765625
Epoch 9200, Loss: 14194.2509765625
Epoch 9300, Loss: 14103.4755859375
Epoch 9400, Loss: 14014.1064453125
Epoch 9500, Loss: 13926.0791015625
Epoch 9600, Loss: 13839.384765625
Epoch 9700, Loss: 13754.017578125
Epoch 9800, Loss: 13669.9638671875
Epoch 9900, Loss: 13587.2177734375
Epoch 10000, Loss: 13505.76953125
Epoch 10100, Loss: 13425.615234375
Epoch 10200, Loss: 13346.74609375
Epoch 10300, Loss: 13269.16015625
Epoch 10400, Loss: 13192.8505859375
Epoch 10500, Loss: 13117.814453125
Epoch 10600, Loss: 13044.0380859375
Epoch 10700, Loss: 12971.5234375
Epoch 10800, Loss: 12900.2578125
Epoch 10900, Loss: 12830.2353515625
Epoch 11000, Loss: 12761.4501953125
Epoch 11100, Loss: 12693.8876953125
Epoch 11200, Loss: 12627.541015625
Epoch 11300, Loss: 12562.396484375
Epoch 11400, Loss: 12498.439453125
Epoch 11500, Loss: 12435.658203125
Epoch 11600, Loss: 12374.0478515625
Epoch 11700, Loss: 12313.58984375
Epoch 11800, Loss: 12254.2607421875
Epoch 11900, Loss: 12196.0380859375
Epoch 12000, Loss: 12138.9072265625
Epoch 12100, Loss: 12082.8447265625
Epoch 12200, Loss: 12027.8330078125
Epoch 12300, Loss: 11973.8916015625
Epoch 12400, Loss: 11921.0068359375
Epoch 12500, Loss: 11869.11328125
Epoch 12600, Loss: 11818.1962890625
Epoch 12700, Loss: 11768.2373046875
Epoch 12800, Loss: 11719.21875
Epoch 12900, Loss: 11671.1279296875
Epoch 13000, Loss: 11623.94921875
Epoch 13100, Loss: 11577.6630859375
Epoch 13200, Loss: 11532.2578125
Epoch 13300, Loss: 11487.7265625
Epoch 13400, Loss: 11444.048828125
Epoch 13500, Loss: 11401.21484375
Epoch 13600, Loss: 11359.21484375
Epoch 13700, Loss: 11318.041015625
Epoch 13800, Loss: 11277.671875
Epoch 13900, Loss: 11238.1064453125
Epoch 14000, Loss: 11199.3271484375
Epoch 14100, Loss: 11161.330078125
Epoch 14200, Loss: 11124.1015625
Epoch 14300, Loss: 11087.6806640625
Epoch 14400, Loss: 11052.072265625
Epoch 14500, Loss: 11017.2109375
Epoch 14600, Loss: 10983.0859375
Epoch 14700, Loss: 10949.6884765625
Epoch 14800, Loss: 10917.052734375
Epoch 14900, Loss: 10885.146484375
Epoch 15000, Loss: 10853.947265625
Epoch 15100, Loss: 10823.4482421875
Epoch 15200, Loss: 10793.64453125
Epoch 15300, Loss: 10764.525390625
Epoch 15400, Loss: 10736.09375
Epoch 15500, Loss: 10708.357421875
Epoch 15600, Loss: 10681.3046875
Epoch 15700, Loss: 10654.91015625
Epoch 15800, Loss: 10629.16796875
Epoch 15900, Loss: 10604.08984375
Epoch 16000, Loss: 10579.6708984375
Epoch 16100, Loss: 10555.912109375
Epoch 16200, Loss: 10532.7734375
Epoch 16300, Loss: 10510.244140625
Epoch 16400, Loss: 10488.3134765625
Epoch 16500, Loss: 10466.970703125
Epoch 16600, Loss: 10446.2080078125
Epoch 16700, Loss: 10426.015625
Epoch 16800, Loss: 10406.384765625
Epoch 16900, Loss: 10387.3046875
Epoch 17000, Loss: 10368.765625
Epoch 17100, Loss: 10350.7568359375
Epoch 17200, Loss: 10333.26953125
Epoch 17300, Loss: 10316.29296875
Epoch 17400, Loss: 10299.8193359375
Epoch 17500, Loss: 10283.8408203125
Epoch 17600, Loss: 10268.3505859375
Epoch 17700, Loss: 10253.33984375
Epoch 17800, Loss: 10238.8046875
Epoch 17900, Loss: 10224.7353515625
Epoch 18000, Loss: 10211.1318359375
Epoch 18100, Loss: 10197.9833984375
Epoch 18200, Loss: 10185.2880859375
Epoch 18300, Loss: 10173.041015625
Epoch 18400, Loss: 10161.23828125
Epoch 18500, Loss: 10149.9013671875
Epoch 18600, Loss: 10139.001953125
Epoch 18700, Loss: 10128.5537109375
Epoch 18800, Loss: 10118.5400390625
Epoch 18900, Loss: 10108.9423828125
Epoch 19000, Loss: 10099.7529296875
Epoch 19100, Loss: 10090.96484375
Epoch 19200, Loss: 10082.5732421875
Epoch 19300, Loss: 10074.56640625
Epoch 19400, Loss: 10066.9423828125
Epoch 19500, Loss: 10059.6904296875
Epoch 19600, Loss: 10052.8046875
Epoch 19700, Loss: 10046.2763671875
Epoch 19800, Loss: 10040.0966796875
Epoch 19900, Loss: 10034.25390625
Epoch 20000, Loss: 10028.7421875
Epoch 20100, Loss: 10023.5791015625
Epoch 20200, Loss: 10018.7451171875
Epoch 20300, Loss: 10014.2138671875
Epoch 20400, Loss: 10009.9755859375
Epoch 20500, Loss: 10006.013671875
Epoch 20600, Loss: 10002.3212890625
Epoch 20700, Loss: 9998.8896484375
Epoch 20800, Loss: 9995.703125
Epoch 20900, Loss: 9992.7490234375
Epoch 21000, Loss: 9990.015625
Epoch 21100, Loss: 9987.4931640625
Epoch 21200, Loss: 9985.1669921875
Epoch 21300, Loss: 9983.0322265625
Epoch 21400, Loss: 9981.0693359375
Epoch 21500, Loss: 9979.2724609375
Epoch 21600, Loss: 9977.62890625
Epoch 21700, Loss: 9976.1279296875
Epoch 21800, Loss: 9974.7587890625
Epoch 21900, Loss: 9973.513671875
Epoch 22000, Loss: 9972.380859375
Epoch 22100, Loss: 9971.3525390625
Epoch 22200, Loss: 9970.4189453125
Epoch 22300, Loss: 9969.5751953125
Epoch 22400, Loss: 9968.810546875
Epoch 22500, Loss: 9968.1181640625
Epoch 22600, Loss: 9967.494140625
Epoch 22700, Loss: 9966.9306640625
Epoch 22800, Loss: 9966.43359375
Epoch 22900, Loss: 9965.990234375
Epoch 23000, Loss: 9965.5927734375
Epoch 23100, Loss: 9965.23828125
Epoch 23200, Loss: 9964.9228515625
Epoch 23300, Loss: 9964.640625
Epoch 23400, Loss: 9964.390625
Epoch 23500, Loss: 9964.166015625
Epoch 23600, Loss: 9963.9677734375
Epoch 23700, Loss: 9963.791015625
Epoch 23800, Loss: 9963.6318359375
Epoch 23900, Loss: 9963.490234375
Epoch 24000, Loss: 9963.3603515625
Epoch 24100, Loss: 9963.2421875
Epoch 24200, Loss: 9963.1328125
Epoch 24300, Loss: 9963.0302734375
Epoch 24400, Loss: 9962.9326171875
Epoch 24500, Loss: 9962.8388671875
Epoch 24600, Loss: 9962.7490234375
Epoch 24700, Loss: 9962.66015625
Epoch 24800, Loss: 9962.572265625
Epoch 24900, Loss: 9962.482421875
Epoch 25000, Loss: 9962.3935546875
Epoch 25100, Loss: 9962.30078125
Epoch 25200, Loss: 9962.2080078125
Epoch 25300, Loss: 9962.11328125
Epoch 25400, Loss: 9962.0146484375
Epoch 25500, Loss: 9961.916015625
Epoch 25600, Loss: 9961.8125
Epoch 25700, Loss: 9961.708984375
Epoch 25800, Loss: 9961.5986328125
Epoch 25900, Loss: 9961.4853515625
Epoch 26000, Loss: 9961.3681640625
Epoch 26100, Loss: 9961.248046875
Epoch 26200, Loss: 9961.12109375
Epoch 26300, Loss: 9960.9921875
Epoch 26400, Loss: 9960.8564453125
Epoch 26500, Loss: 9960.716796875
Epoch 26600, Loss: 9960.5732421875
Epoch 26700, Loss: 9960.423828125
Epoch 26800, Loss: 9960.2705078125
Epoch 26900, Loss: 9960.111328125
Epoch 27000, Loss: 9959.9482421875
Epoch 27100, Loss: 9959.7783203125
Epoch 27200, Loss: 9959.60546875
Epoch 27300, Loss: 9959.42578125
Epoch 27400, Loss: 9959.2421875
Epoch 27500, Loss: 9959.052734375
Epoch 27600, Loss: 9958.859375
Epoch 27700, Loss: 9958.6591796875
Epoch 27800, Loss: 9958.484375
Epoch 27900, Loss: 9958.3115234375
Epoch 28000, Loss: 9958.1357421875
Epoch 28100, Loss: 9957.955078125
Epoch 28200, Loss: 9957.7939453125
Epoch 28300, Loss: 9957.654296875
Epoch 28400, Loss: 9957.5166015625
Epoch 28500, Loss: 9957.375
Epoch 28600, Loss: 9957.228515625
Epoch 28700, Loss: 9957.078125
Epoch 28800, Loss: 9956.923828125
Epoch 28900, Loss: 9956.765625
Epoch 29000, Loss: 9956.603515625
Epoch 29100, Loss: 9956.4384765625
Epoch 29200, Loss: 9956.2685546875
Epoch 29300, Loss: 9956.095703125
Epoch 29400, Loss: 9955.9189453125
Epoch 29500, Loss: 9955.7392578125
Epoch 29600, Loss: 9955.55859375
Epoch 29700, Loss: 9955.4072265625
Epoch 29800, Loss: 9955.2822265625
Epoch 29900, Loss: 9955.1572265625
Epoch 30000, Loss: 9955.048828125
Epoch 30100, Loss: 9954.9697265625
Epoch 30200, Loss: 9954.892578125
Epoch 30300, Loss: 9954.8134765625
Epoch 30400, Loss: 9954.73046875
Epoch 30500, Loss: 9954.6484375
Epoch 30600, Loss: 9954.5615234375
Epoch 30700, Loss: 9954.474609375
Epoch 30800, Loss: 9954.3828125
Epoch 30900, Loss: 9954.2900390625
Epoch 31000, Loss: 9954.1962890625
Epoch 31100, Loss: 9954.09765625
Epoch 31200, Loss: 9953.9990234375
Epoch 31300, Loss: 9953.8974609375
Epoch 31400, Loss: 9953.794921875
Epoch 31500, Loss: 9953.6904296875
Epoch 31600, Loss: 9953.5908203125
Epoch 31700, Loss: 9953.5361328125
Epoch 31800, Loss: 9953.486328125
Epoch 31900, Loss: 9953.435546875
Epoch 32000, Loss: 9953.3818359375
Epoch 32100, Loss: 9953.33984375
Epoch 32200, Loss: 9953.3203125
Epoch 32300, Loss: 9953.3056640625
Epoch 32400, Loss: 9953.2919921875
Epoch 32500, Loss: 9953.28125
Epoch 32600, Loss: 9953.27734375
Epoch 32700, Loss: 9953.2744140625
Epoch 32800, Loss: 9953.2744140625
Epoch 32900, Loss: 9953.2744140625
Epoch 33000, Loss: 9953.2734375
Epoch 33100, Loss: 9953.2734375
Epoch 33200, Loss: 9953.2734375
Epoch 33300, Loss: 9953.2724609375
Epoch 33400, Loss: 9953.2724609375
Epoch 33500, Loss: 9953.2724609375
Epoch 33600, Loss: 9953.2724609375
Epoch 33700, Loss: 9953.2724609375
Epoch 33800, Loss: 9953.2724609375
Epoch 33900, Loss: 9953.2724609375
Epoch 34000, Loss: 9953.271484375
Epoch 34100, Loss: 9953.271484375
Epoch 34200, Loss: 9953.271484375
Epoch 34300, Loss: 9953.271484375
Epoch 34400, Loss: 9953.271484375
Epoch 34500, Loss: 9953.2724609375
Epoch 34600, Loss: 9953.271484375
Epoch 34700, Loss: 9953.2724609375
Epoch 34800, Loss: 9953.2724609375
Epoch 34900, Loss: 9953.2724609375
Epoch 35000, Loss: 9953.2724609375
Epoch 35100, Loss: 9953.2724609375
Epoch 35200, Loss: 9953.2724609375
Epoch 35300, Loss: 9953.2724609375
Epoch 35400, Loss: 9953.2724609375
Epoch 35500, Loss: 9953.2724609375
Epoch 35600, Loss: 9953.2724609375
Epoch 35700, Loss: 9953.2724609375
Epoch 35800, Loss: 9953.2724609375
Epoch 35900, Loss: 9953.2724609375
Epoch 36000, Loss: 9953.2724609375
Epoch 36100, Loss: 9953.2724609375
Epoch 36200, Loss: 9953.2724609375
Epoch 36300, Loss: 9953.2724609375
Epoch 36400, Loss: 9953.2724609375
Epoch 36500, Loss: 9953.2724609375
Epoch 36600, Loss: 9953.2724609375
Epoch 36700, Loss: 9953.2724609375
Epoch 36800, Loss: 9953.2724609375
Epoch 36900, Loss: 9953.2724609375
Epoch 37000, Loss: 9953.2724609375
Epoch 37100, Loss: 9953.2724609375
Epoch 37200, Loss: 9953.2724609375
Epoch 37300, Loss: 9953.2724609375
Epoch 37400, Loss: 9953.2724609375
Epoch 37500, Loss: 9953.2724609375
Epoch 37600, Loss: 9953.2724609375
Epoch 37700, Loss: 9953.2724609375
Epoch 37800, Loss: 9953.2724609375
Epoch 37900, Loss: 9953.2724609375
Epoch 38000, Loss: 9953.2724609375
Epoch 38100, Loss: 9953.2724609375
Epoch 38200, Loss: 9953.2724609375
Epoch 38300, Loss: 9953.2724609375
Epoch 38400, Loss: 9953.2724609375
Epoch 38500, Loss: 9953.2724609375
Epoch 38600, Loss: 9953.2724609375
Epoch 38700, Loss: 9953.2724609375
Epoch 38800, Loss: 9953.2724609375
Epoch 38900, Loss: 9953.2724609375
Epoch 39000, Loss: 9953.2724609375
Epoch 39100, Loss: 9953.2724609375
Epoch 39200, Loss: 9953.2724609375
Epoch 39300, Loss: 9953.2724609375
Epoch 39400, Loss: 9953.2724609375
Epoch 39500, Loss: 9953.2724609375
Epoch 39600, Loss: 9953.2724609375
Epoch 39700, Loss: 9953.2724609375
Epoch 39800, Loss: 9953.2724609375
Epoch 39900, Loss: 9953.2724609375
Epoch 40000, Loss: 9953.2724609375
Epoch 40100, Loss: 9953.2724609375
Epoch 40200, Loss: 9953.2724609375
Epoch 40300, Loss: 9953.2724609375
Epoch 40400, Loss: 9953.2724609375
Epoch 40500, Loss: 9953.2724609375
Epoch 40600, Loss: 9953.2724609375
Epoch 40700, Loss: 9953.2724609375
Epoch 40800, Loss: 9953.271484375
Epoch 40900, Loss: 9953.2724609375
Epoch 41000, Loss: 9953.2724609375
Epoch 41100, Loss: 9953.2724609375
Epoch 41200, Loss: 9953.271484375
Epoch 41300, Loss: 9953.2734375
Epoch 41400, Loss: 9953.2724609375
Epoch 41500, Loss: 9953.2724609375
Epoch 41600, Loss: 9953.2724609375
Epoch 41700, Loss: 9953.2724609375
Epoch 41800, Loss: 9953.2724609375
Epoch 41900, Loss: 9953.2724609375
Epoch 42000, Loss: 9953.2724609375
Epoch 42100, Loss: 9953.2724609375
Epoch 42200, Loss: 9953.2724609375
Epoch 42300, Loss: 9953.2724609375
Epoch 42400, Loss: 9953.2724609375
Epoch 42500, Loss: 9953.2724609375
Epoch 42600, Loss: 9953.2734375
Epoch 42700, Loss: 9953.2724609375
Epoch 42800, Loss: 9953.271484375
Epoch 42900, Loss: 9953.2724609375
Epoch 43000, Loss: 9953.2724609375
Epoch 43100, Loss: 9953.2724609375
Epoch 43200, Loss: 9953.2724609375
Epoch 43300, Loss: 9953.2724609375
Epoch 43400, Loss: 9953.2724609375
Epoch 43500, Loss: 9953.2724609375
Epoch 43600, Loss: 9953.2724609375
Epoch 43700, Loss: 9953.2724609375
Epoch 43800, Loss: 9953.2724609375
Epoch 43900, Loss: 9953.2724609375
Epoch 44000, Loss: 9953.2724609375
Epoch 44100, Loss: 9953.2724609375
Epoch 44200, Loss: 9953.2724609375
Epoch 44300, Loss: 9953.2724609375
Epoch 44400, Loss: 9953.2724609375
Epoch 44500, Loss: 9953.271484375
Epoch 44600, Loss: 9953.2724609375
Epoch 44700, Loss: 9953.271484375
Epoch 44800, Loss: 9953.2724609375
Epoch 44900, Loss: 9953.2724609375
Epoch 45000, Loss: 9953.2724609375
Epoch 45100, Loss: 9953.2724609375
Epoch 45200, Loss: 9953.2724609375
Epoch 45300, Loss: 9953.2734375
Epoch 45400, Loss: 9953.2724609375
Epoch 45500, Loss: 9953.2724609375
Epoch 45600, Loss: 9953.2724609375
Epoch 45700, Loss: 9953.2724609375
Epoch 45800, Loss: 9953.2724609375
Epoch 45900, Loss: 9953.2724609375
Epoch 46000, Loss: 9953.2724609375
Epoch 46100, Loss: 9953.2724609375
Epoch 46200, Loss: 9953.2724609375
Epoch 46300, Loss: 9953.2724609375
Epoch 46400, Loss: 9953.2724609375
Epoch 46500, Loss: 9953.2724609375
Epoch 46600, Loss: 9953.2724609375
Epoch 46700, Loss: 9953.2724609375
Epoch 46800, Loss: 9953.2724609375
Epoch 46900, Loss: 9953.2724609375
Epoch 47000, Loss: 9953.2724609375
Epoch 47100, Loss: 9953.271484375
Epoch 47200, Loss: 9953.2724609375
Epoch 47300, Loss: 9953.271484375
Epoch 47400, Loss: 9953.2724609375
Epoch 47500, Loss: 9953.271484375
Epoch 47600, Loss: 9953.2724609375
Epoch 47700, Loss: 9953.2724609375
Epoch 47800, Loss: 9953.2724609375
Epoch 47900, Loss: 9953.2724609375
Epoch 48000, Loss: 9953.2724609375
Epoch 48100, Loss: 9953.2724609375
Epoch 48200, Loss: 9953.2724609375
Epoch 48300, Loss: 9953.2724609375
Epoch 48400, Loss: 9953.2724609375
Epoch 48500, Loss: 9953.2724609375
Epoch 48600, Loss: 9953.2724609375
Epoch 48700, Loss: 9953.2724609375
Epoch 48800, Loss: 9953.2724609375
Epoch 48900, Loss: 9953.2724609375
Epoch 49000, Loss: 9953.2724609375
Epoch 49100, Loss: 9953.2724609375
Epoch 49200, Loss: 9953.2724609375
Epoch 49300, Loss: 9953.2724609375
Epoch 49400, Loss: 9953.2734375
Epoch 49500, Loss: 9953.2724609375
Epoch 49600, Loss: 9953.2724609375
Epoch 49700, Loss: 9953.2724609375
Epoch 49800, Loss: 9953.2724609375
Epoch 49900, Loss: 9953.2724609375
Epoch 0, Loss: 63828.96875
Epoch 100, Loss: 60922.81640625
Epoch 200, Loss: 58205.51953125
Epoch 300, Loss: 55663.38671875
Epoch 400, Loss: 53283.2109375
Epoch 500, Loss: 51054.04296875
Epoch 600, Loss: 48966.82421875
Epoch 700, Loss: 47013.23828125
Epoch 800, Loss: 45185.5703125
Epoch 900, Loss: 43476.62109375
Epoch 1000, Loss: 41879.5625
Epoch 1100, Loss: 40387.6796875
Epoch 1200, Loss: 38994.58203125
Epoch 1300, Loss: 37694.12109375
Epoch 1400, Loss: 36480.37890625
Epoch 1500, Loss: 35347.78515625
Epoch 1600, Loss: 34290.7109375
Epoch 1700, Loss: 33303.8984375
Epoch 1800, Loss: 32382.353515625
Epoch 1900, Loss: 31521.38671875
Epoch 2000, Loss: 30716.630859375
Epoch 2100, Loss: 29964.00390625
Epoch 2200, Loss: 29259.83203125
Epoch 2300, Loss: 28600.513671875
Epoch 2400, Loss: 27982.912109375
Epoch 2500, Loss: 27403.896484375
Epoch 2600, Loss: 26860.5390625
Epoch 2700, Loss: 26350.205078125
Epoch 2800, Loss: 25870.447265625
Epoch 2900, Loss: 25418.978515625
Epoch 3000, Loss: 24993.671875
Epoch 3100, Loss: 24592.53515625
Epoch 3200, Loss: 24213.6953125
Epoch 3300, Loss: 23855.423828125
Epoch 3400, Loss: 23516.087890625
Epoch 3500, Loss: 23194.16015625
Epoch 3600, Loss: 22888.16015625
Epoch 3700, Loss: 22596.63671875
Epoch 3800, Loss: 22318.294921875
Epoch 3900, Loss: 22051.943359375
Epoch 4000, Loss: 21796.484375
Epoch 4100, Loss: 21550.9765625
Epoch 4200, Loss: 21314.494140625
Epoch 4300, Loss: 21086.16015625
Epoch 4400, Loss: 20865.244140625
Epoch 4500, Loss: 20651.171875
Epoch 4600, Loss: 20443.29296875
Epoch 4700, Loss: 20241.09375
Epoch 4800, Loss: 20044.138671875
Epoch 4900, Loss: 19852.048828125
Epoch 5000, Loss: 19664.4921875
Epoch 5100, Loss: 19481.1875
Epoch 5200, Loss: 19301.8828125
Epoch 5300, Loss: 19126.361328125
Epoch 5400, Loss: 18954.431640625
Epoch 5500, Loss: 18785.921875
Epoch 5600, Loss: 18620.68359375
Epoch 5700, Loss: 18458.58203125
Epoch 5800, Loss: 18299.490234375
Epoch 5900, Loss: 18143.298828125
Epoch 6000, Loss: 17989.91796875
Epoch 6100, Loss: 17839.310546875
Epoch 6200, Loss: 17691.32421875
Epoch 6300, Loss: 17545.87890625
Epoch 6400, Loss: 17402.90234375
Epoch 6500, Loss: 17262.330078125
Epoch 6600, Loss: 17124.10546875
Epoch 6700, Loss: 16988.162109375
Epoch 6800, Loss: 16854.455078125
Epoch 6900, Loss: 16722.927734375
Epoch 7000, Loss: 16593.537109375
Epoch 7100, Loss: 16466.232421875
Epoch 7200, Loss: 16341.0205078125
Epoch 7300, Loss: 16217.837890625
Epoch 7400, Loss: 16096.705078125
Epoch 7500, Loss: 15977.5029296875
Epoch 7600, Loss: 15860.1689453125
Epoch 7700, Loss: 15744.6572265625
Epoch 7800, Loss: 15630.9296875
Epoch 7900, Loss: 15518.9443359375
Epoch 8000, Loss: 15408.66796875
Epoch 8100, Loss: 15300.0703125
Epoch 8200, Loss: 15193.1103515625
Epoch 8300, Loss: 15087.7587890625
Epoch 8400, Loss: 14983.974609375
Epoch 8500, Loss: 14881.7392578125
Epoch 8600, Loss: 14781.0166015625
Epoch 8700, Loss: 14681.783203125
Epoch 8800, Loss: 14584.017578125
Epoch 8900, Loss: 14487.6943359375
Epoch 9000, Loss: 14392.7958984375
Epoch 9100, Loss: 14299.2998046875
Epoch 9200, Loss: 14207.1865234375
Epoch 9300, Loss: 14116.4990234375
Epoch 9400, Loss: 14027.216796875
Epoch 9500, Loss: 13939.275390625
Epoch 9600, Loss: 13852.6669921875
Epoch 9700, Loss: 13767.3818359375
Epoch 9800, Loss: 13683.408203125
Epoch 9900, Loss: 13600.7412109375
Epoch 10000, Loss: 13519.3701171875
Epoch 10100, Loss: 13439.291015625
Epoch 10200, Loss: 13360.4990234375
Epoch 10300, Loss: 13282.9912109375
Epoch 10400, Loss: 13206.75390625
Epoch 10500, Loss: 13131.7880859375
Epoch 10600, Loss: 13058.08203125
Epoch 10700, Loss: 12985.6337890625
Epoch 10800, Loss: 12914.4345703125
Epoch 10900, Loss: 12844.4794921875
Epoch 11000, Loss: 12775.7578125
Epoch 11100, Loss: 12708.2587890625
Epoch 11200, Loss: 12641.97265625
Epoch 11300, Loss: 12576.88671875
Epoch 11400, Loss: 12512.990234375
Epoch 11500, Loss: 12450.2685546875
Epoch 11600, Loss: 12388.7177734375
Epoch 11700, Loss: 12328.3203125
Epoch 11800, Loss: 12269.046875
Epoch 11900, Loss: 12210.8818359375
Epoch 12000, Loss: 12153.8037109375
Epoch 12100, Loss: 12097.7919921875
Epoch 12200, Loss: 12042.833984375
Epoch 12300, Loss: 11988.9580078125
Epoch 12400, Loss: 11936.1376953125
Epoch 12500, Loss: 11884.30859375
Epoch 12600, Loss: 11833.4541015625
Epoch 12700, Loss: 11783.556640625
Epoch 12800, Loss: 11734.595703125
Epoch 12900, Loss: 11686.5595703125
Epoch 13000, Loss: 11639.43359375
Epoch 13100, Loss: 11593.19921875
Epoch 13200, Loss: 11547.8447265625
Epoch 13300, Loss: 11503.36328125
Epoch 13400, Loss: 11459.7294921875
Epoch 13500, Loss: 11416.9423828125
Epoch 13600, Loss: 11374.9921875
Epoch 13700, Loss: 11333.859375
Epoch 13800, Loss: 11293.53125
Epoch 13900, Loss: 11254.0048828125
Epoch 14000, Loss: 11215.267578125
Epoch 14100, Loss: 11177.3076171875
Epoch 14200, Loss: 11140.115234375
Epoch 14300, Loss: 11103.7548828125
Epoch 14400, Loss: 11068.2060546875
Epoch 14500, Loss: 11033.404296875
Epoch 14600, Loss: 10999.333984375
Epoch 14700, Loss: 10965.9892578125
Epoch 14800, Loss: 10933.4228515625
Epoch 14900, Loss: 10901.576171875
Epoch 15000, Loss: 10870.439453125
Epoch 15100, Loss: 10840.0009765625
Epoch 15200, Loss: 10810.2490234375
Epoch 15300, Loss: 10781.1826171875
Epoch 15400, Loss: 10752.8056640625
Epoch 15500, Loss: 10725.158203125
Epoch 15600, Loss: 10698.1982421875
Epoch 15700, Loss: 10671.896484375
Epoch 15800, Loss: 10646.2470703125
Epoch 15900, Loss: 10621.240234375
Epoch 16000, Loss: 10596.880859375
Epoch 16100, Loss: 10573.185546875
Epoch 16200, Loss: 10550.111328125
Epoch 16300, Loss: 10527.640625
Epoch 16400, Loss: 10505.7685546875
Epoch 16500, Loss: 10484.4814453125
Epoch 16600, Loss: 10463.7724609375
Epoch 16700, Loss: 10443.6298828125
Epoch 16800, Loss: 10424.046875
Epoch 16900, Loss: 10405.013671875
Epoch 17000, Loss: 10386.5185546875
Epoch 17100, Loss: 10368.548828125
Epoch 17200, Loss: 10351.099609375
Epoch 17300, Loss: 10334.1572265625
Epoch 17400, Loss: 10317.7177734375
Epoch 17500, Loss: 10301.7705078125
Epoch 17600, Loss: 10286.3095703125
Epoch 17700, Loss: 10271.3291015625
Epoch 17800, Loss: 10256.818359375
Epoch 17900, Loss: 10242.7763671875
Epoch 18000, Loss: 10229.1943359375
Epoch 18100, Loss: 10216.083984375
Epoch 18200, Loss: 10203.4580078125
Epoch 18300, Loss: 10191.2822265625
Epoch 18400, Loss: 10179.552734375
Epoch 18500, Loss: 10168.2900390625
Epoch 18600, Loss: 10157.4736328125
Epoch 18700, Loss: 10147.0849609375
Epoch 18800, Loss: 10137.1142578125
Epoch 18900, Loss: 10127.5556640625
Epoch 19000, Loss: 10118.404296875
Epoch 19100, Loss: 10109.6513671875
Epoch 19200, Loss: 10101.29296875
Epoch 19300, Loss: 10093.3212890625
Epoch 19400, Loss: 10085.7255859375
Epoch 19500, Loss: 10078.5029296875
Epoch 19600, Loss: 10071.6630859375
Epoch 19700, Loss: 10065.1865234375
Epoch 19800, Loss: 10059.0576171875
Epoch 19900, Loss: 10053.265625
Epoch 20000, Loss: 10047.8134765625
Epoch 20100, Loss: 10042.703125
Epoch 20200, Loss: 10037.9140625
Epoch 20300, Loss: 10033.4326171875
Epoch 20400, Loss: 10029.23828125
Epoch 20500, Loss: 10025.322265625
Epoch 20600, Loss: 10021.6689453125
Epoch 20700, Loss: 10018.26953125
Epoch 20800, Loss: 10015.11328125
Epoch 20900, Loss: 10012.1875
Epoch 21000, Loss: 10009.48046875
Epoch 21100, Loss: 10006.982421875
Epoch 21200, Loss: 10004.6796875
Epoch 21300, Loss: 10002.5634765625
Epoch 21400, Loss: 10000.62109375
Epoch 21500, Loss: 9998.8408203125
Epoch 21600, Loss: 9997.2119140625
Epoch 21700, Loss: 9995.724609375
Epoch 21800, Loss: 9994.3701171875
Epoch 21900, Loss: 9993.1337890625
Epoch 22000, Loss: 9992.009765625
Epoch 22100, Loss: 9990.98828125
Epoch 22200, Loss: 9990.0693359375
Epoch 22300, Loss: 9989.24609375
Epoch 22400, Loss: 9988.4990234375
Epoch 22500, Loss: 9987.8251953125
Epoch 22600, Loss: 9987.2177734375
Epoch 22700, Loss: 9986.66796875
Epoch 22800, Loss: 9986.1728515625
Epoch 22900, Loss: 9985.7275390625
Epoch 23000, Loss: 9985.3271484375
Epoch 23100, Loss: 9984.9677734375
Epoch 23200, Loss: 9984.64453125
Epoch 23300, Loss: 9984.3544921875
Epoch 23400, Loss: 9984.0908203125
Epoch 23500, Loss: 9983.857421875
Epoch 23600, Loss: 9983.64453125
Epoch 23700, Loss: 9983.44921875
Epoch 23800, Loss: 9983.271484375
Epoch 23900, Loss: 9983.1083984375
Epoch 24000, Loss: 9982.9560546875
Epoch 24100, Loss: 9982.8134765625
Epoch 24200, Loss: 9982.677734375
Epoch 24300, Loss: 9982.5478515625
Epoch 24400, Loss: 9982.421875
Epoch 24500, Loss: 9982.298828125
Epoch 24600, Loss: 9982.1748046875
Epoch 24700, Loss: 9982.0517578125
Epoch 24800, Loss: 9981.927734375
Epoch 24900, Loss: 9981.80078125
Epoch 25000, Loss: 9981.673828125
Epoch 25100, Loss: 9981.544921875
Epoch 25200, Loss: 9981.412109375
Epoch 25300, Loss: 9981.275390625
Epoch 25400, Loss: 9981.1357421875
Epoch 25500, Loss: 9980.990234375
Epoch 25600, Loss: 9980.8408203125
Epoch 25700, Loss: 9980.6845703125
Epoch 25800, Loss: 9980.5234375
Epoch 25900, Loss: 9980.3583984375
Epoch 26000, Loss: 9980.1865234375
Epoch 26100, Loss: 9980.0078125
Epoch 26200, Loss: 9979.8251953125
Epoch 26300, Loss: 9979.634765625
Epoch 26400, Loss: 9979.4384765625
Epoch 26500, Loss: 9979.2373046875
Epoch 26600, Loss: 9979.02734375
Epoch 26700, Loss: 9978.8115234375
Epoch 26800, Loss: 9978.58984375
Epoch 26900, Loss: 9978.3623046875
Epoch 27000, Loss: 9978.126953125
Epoch 27100, Loss: 9977.8857421875
Epoch 27200, Loss: 9977.64453125
Epoch 27300, Loss: 9977.4326171875
Epoch 27400, Loss: 9977.21875
Epoch 27500, Loss: 9977.001953125
Epoch 27600, Loss: 9976.77734375
Epoch 27700, Loss: 9976.5751953125
Epoch 27800, Loss: 9976.3984375
Epoch 27900, Loss: 9976.228515625
Epoch 28000, Loss: 9976.0537109375
Epoch 28100, Loss: 9975.8720703125
Epoch 28200, Loss: 9975.6865234375
Epoch 28300, Loss: 9975.494140625
Epoch 28400, Loss: 9975.2998046875
Epoch 28500, Loss: 9975.099609375
Epoch 28600, Loss: 9974.8935546875
Epoch 28700, Loss: 9974.6806640625
Epoch 28800, Loss: 9974.4658203125
Epoch 28900, Loss: 9974.24609375
Epoch 29000, Loss: 9974.0234375
Epoch 29100, Loss: 9973.7958984375
Epoch 29200, Loss: 9973.595703125
Epoch 29300, Loss: 9973.43359375
Epoch 29400, Loss: 9973.2783203125
Epoch 29500, Loss: 9973.130859375
Epoch 29600, Loss: 9973.0263671875
Epoch 29700, Loss: 9972.9306640625
Epoch 29800, Loss: 9972.8310546875
Epoch 29900, Loss: 9972.728515625
Epoch 30000, Loss: 9972.6259765625
Epoch 30100, Loss: 9972.5185546875
Epoch 30200, Loss: 9972.4072265625
Epoch 30300, Loss: 9972.2939453125
Epoch 30400, Loss: 9972.177734375
Epoch 30500, Loss: 9972.0595703125
Epoch 30600, Loss: 9971.9384765625
Epoch 30700, Loss: 9971.8134765625
Epoch 30800, Loss: 9971.6865234375
Epoch 30900, Loss: 9971.5576171875
Epoch 31000, Loss: 9971.427734375
Epoch 31100, Loss: 9971.2958984375
Epoch 31200, Loss: 9971.2060546875
Epoch 31300, Loss: 9971.142578125
Epoch 31400, Loss: 9971.078125
Epoch 31500, Loss: 9971.0126953125
Epoch 31600, Loss: 9970.9521484375
Epoch 31700, Loss: 9970.916015625
Epoch 31800, Loss: 9970.8955078125
Epoch 31900, Loss: 9970.8779296875
Epoch 32000, Loss: 9970.861328125
Epoch 32100, Loss: 9970.853515625
Epoch 32200, Loss: 9970.84765625
Epoch 32300, Loss: 9970.845703125
Epoch 32400, Loss: 9970.8427734375
Epoch 32500, Loss: 9970.841796875
Epoch 32600, Loss: 9970.841796875
Epoch 32700, Loss: 9970.8408203125
Epoch 32800, Loss: 9970.841796875
Epoch 32900, Loss: 9970.8408203125
Epoch 33000, Loss: 9970.8408203125
Epoch 33100, Loss: 9970.83984375
Epoch 33200, Loss: 9970.8388671875
Epoch 33300, Loss: 9970.8408203125
Epoch 33400, Loss: 9970.83984375
Epoch 33500, Loss: 9970.83984375
Epoch 33600, Loss: 9970.8388671875
Epoch 33700, Loss: 9970.83984375
Epoch 33800, Loss: 9970.8388671875
Epoch 33900, Loss: 9970.83984375
Epoch 34000, Loss: 9970.8388671875
Epoch 34100, Loss: 9970.83984375
Epoch 34200, Loss: 9970.8388671875
Epoch 34300, Loss: 9970.8388671875
Epoch 34400, Loss: 9970.83984375
Epoch 34500, Loss: 9970.83984375
Epoch 34600, Loss: 9970.83984375
Epoch 34700, Loss: 9970.83984375
Epoch 34800, Loss: 9970.8388671875
Epoch 34900, Loss: 9970.83984375
Epoch 35000, Loss: 9970.83984375
Epoch 35100, Loss: 9970.8388671875
Epoch 35200, Loss: 9970.83984375
Epoch 35300, Loss: 9970.8388671875
Epoch 35400, Loss: 9970.8388671875
Epoch 35500, Loss: 9970.83984375
Epoch 35600, Loss: 9970.8388671875
Epoch 35700, Loss: 9970.83984375
Epoch 35800, Loss: 9970.83984375
Epoch 35900, Loss: 9970.83984375
Epoch 36000, Loss: 9970.837890625
Epoch 36100, Loss: 9970.8388671875
Epoch 36200, Loss: 9970.8388671875
Epoch 36300, Loss: 9970.8388671875
Epoch 36400, Loss: 9970.8388671875
Epoch 36500, Loss: 9970.83984375
Epoch 36600, Loss: 9970.83984375
Epoch 36700, Loss: 9970.8388671875
Epoch 36800, Loss: 9970.8388671875
Epoch 36900, Loss: 9970.83984375
Epoch 37000, Loss: 9970.8388671875
Epoch 37100, Loss: 9970.8388671875
Epoch 37200, Loss: 9970.8388671875
Epoch 37300, Loss: 9970.83984375
Epoch 37400, Loss: 9970.83984375
Epoch 37500, Loss: 9970.83984375
Epoch 37600, Loss: 9970.8388671875
Epoch 37700, Loss: 9970.83984375
Epoch 37800, Loss: 9970.83984375
Epoch 37900, Loss: 9970.8388671875
Epoch 38000, Loss: 9970.8388671875
Epoch 38100, Loss: 9970.8388671875
Epoch 38200, Loss: 9970.83984375
Epoch 38300, Loss: 9970.8388671875
Epoch 38400, Loss: 9970.8388671875
Epoch 38500, Loss: 9970.83984375
Epoch 38600, Loss: 9970.8388671875
Epoch 38700, Loss: 9970.8388671875
Epoch 38800, Loss: 9970.83984375
Epoch 38900, Loss: 9970.8408203125
Epoch 39000, Loss: 9970.8388671875
Epoch 39100, Loss: 9970.83984375
Epoch 39200, Loss: 9970.837890625
Epoch 39300, Loss: 9970.8388671875
Epoch 39400, Loss: 9970.8388671875
Epoch 39500, Loss: 9970.8388671875
Epoch 39600, Loss: 9970.83984375
Epoch 39700, Loss: 9970.8388671875
Epoch 39800, Loss: 9970.8388671875
Epoch 39900, Loss: 9970.8388671875
Epoch 40000, Loss: 9970.8388671875
Epoch 40100, Loss: 9970.8388671875
Epoch 40200, Loss: 9970.8388671875
Epoch 40300, Loss: 9970.83984375
Epoch 40400, Loss: 9970.83984375
Epoch 40500, Loss: 9970.83984375
Epoch 40600, Loss: 9970.8388671875
Epoch 40700, Loss: 9970.8388671875
Epoch 40800, Loss: 9970.8388671875
Epoch 40900, Loss: 9970.8388671875
Epoch 41000, Loss: 9970.8388671875
Epoch 41100, Loss: 9970.83984375
Epoch 41200, Loss: 9970.83984375
Epoch 41300, Loss: 9970.83984375
Epoch 41400, Loss: 9970.8388671875
Epoch 41500, Loss: 9970.8388671875
Epoch 41600, Loss: 9970.837890625
Epoch 41700, Loss: 9970.83984375
Epoch 41800, Loss: 9970.8388671875
Epoch 41900, Loss: 9970.8388671875
Epoch 42000, Loss: 9970.8388671875
Epoch 42100, Loss: 9970.83984375
Epoch 42200, Loss: 9970.8388671875
Epoch 42300, Loss: 9970.83984375
Epoch 42400, Loss: 9970.8388671875
Epoch 42500, Loss: 9970.8388671875
Epoch 42600, Loss: 9970.8388671875
Epoch 42700, Loss: 9970.83984375
Epoch 42800, Loss: 9970.83984375
Epoch 42900, Loss: 9970.8388671875
Epoch 43000, Loss: 9970.83984375
Epoch 43100, Loss: 9970.8388671875
Epoch 43200, Loss: 9970.8388671875
Epoch 43300, Loss: 9970.837890625
Epoch 43400, Loss: 9970.837890625
Epoch 43500, Loss: 9970.83984375
Epoch 43600, Loss: 9970.8388671875
Epoch 43700, Loss: 9970.8388671875
Epoch 43800, Loss: 9970.8388671875
Epoch 43900, Loss: 9970.83984375
Epoch 44000, Loss: 9970.8388671875
Epoch 44100, Loss: 9970.83984375
Epoch 44200, Loss: 9970.8388671875
Epoch 44300, Loss: 9970.8388671875
Epoch 44400, Loss: 9970.83984375
Epoch 44500, Loss: 9970.8388671875
Epoch 44600, Loss: 9970.83984375
Epoch 44700, Loss: 9970.83984375
Epoch 44800, Loss: 9970.8388671875
Epoch 44900, Loss: 9970.8388671875
Epoch 45000, Loss: 9970.83984375
Epoch 45100, Loss: 9970.8388671875
Epoch 45200, Loss: 9970.83984375
Epoch 45300, Loss: 9970.83984375
Epoch 45400, Loss: 9970.83984375
Epoch 45500, Loss: 9970.8388671875
Epoch 45600, Loss: 9970.8388671875
Epoch 45700, Loss: 9970.8388671875
Epoch 45800, Loss: 9970.8388671875
Epoch 45900, Loss: 9970.8388671875
Epoch 46000, Loss: 9970.8388671875
Epoch 46100, Loss: 9970.83984375
Epoch 46200, Loss: 9970.83984375
Epoch 46300, Loss: 9970.83984375
Epoch 46400, Loss: 9970.83984375
Epoch 46500, Loss: 9970.83984375
Epoch 46600, Loss: 9970.83984375
Epoch 46700, Loss: 9970.83984375
Epoch 46800, Loss: 9970.83984375
Epoch 46900, Loss: 9970.8388671875
Epoch 47000, Loss: 9970.8388671875
Epoch 47100, Loss: 9970.8388671875
Epoch 47200, Loss: 9970.83984375
Epoch 47300, Loss: 9970.83984375
Epoch 47400, Loss: 9970.8388671875
Epoch 47500, Loss: 9970.8388671875
Epoch 47600, Loss: 9970.83984375
Epoch 47700, Loss: 9970.8388671875
Epoch 47800, Loss: 9970.83984375
Epoch 47900, Loss: 9970.8388671875
Epoch 48000, Loss: 9970.83984375
Epoch 48100, Loss: 9970.837890625
Epoch 48200, Loss: 9970.8388671875
Epoch 48300, Loss: 9970.8388671875
Epoch 48400, Loss: 9970.8388671875
Epoch 48500, Loss: 9970.8388671875
Epoch 48600, Loss: 9970.8388671875
Epoch 48700, Loss: 9970.83984375
Epoch 48800, Loss: 9970.8388671875
Epoch 48900, Loss: 9970.8388671875
Epoch 49000, Loss: 9970.8388671875
Epoch 49100, Loss: 9970.83984375
Epoch 49200, Loss: 9970.8388671875
Epoch 49300, Loss: 9970.8388671875
Epoch 49400, Loss: 9970.8388671875
Epoch 49500, Loss: 9970.8388671875
Epoch 49600, Loss: 9970.83984375
Epoch 49700, Loss: 9970.83984375
Epoch 49800, Loss: 9970.8388671875
Epoch 49900, Loss: 9970.8388671875
Epoch 0, Loss: 63828.96875
Epoch 100, Loss: 60923.21484375
Epoch 200, Loss: 58206.27734375
Epoch 300, Loss: 55664.4609375
Epoch 400, Loss: 53284.59765625
Epoch 500, Loss: 51055.73046875
Epoch 600, Loss: 48968.84375
Epoch 700, Loss: 47015.55078125
Epoch 800, Loss: 45188.1484375
Epoch 900, Loss: 43479.48046875
Epoch 1000, Loss: 41882.6875
Epoch 1100, Loss: 40391.0546875
Epoch 1200, Loss: 38998.1875
Epoch 1300, Loss: 37697.9453125
Epoch 1400, Loss: 36484.40234375
Epoch 1500, Loss: 35352.0234375
Epoch 1600, Loss: 34295.15234375
Epoch 1700, Loss: 33308.515625
Epoch 1800, Loss: 32387.134765625
Epoch 1900, Loss: 31526.32421875
Epoch 2000, Loss: 30721.70703125
Epoch 2100, Loss: 29969.205078125
Epoch 2200, Loss: 29265.19140625
Epoch 2300, Loss: 28606.029296875
Epoch 2400, Loss: 27988.615234375
Epoch 2500, Loss: 27409.80078125
Epoch 2600, Loss: 26866.63671875
Epoch 2700, Loss: 26356.484375
Epoch 2800, Loss: 25876.900390625
Epoch 2900, Loss: 25425.603515625
Epoch 3000, Loss: 25000.458984375
Epoch 3100, Loss: 24599.470703125
Epoch 3200, Loss: 24220.77734375
Epoch 3300, Loss: 23862.662109375
Epoch 3400, Loss: 23523.490234375
Epoch 3500, Loss: 23201.748046875
Epoch 3600, Loss: 22895.955078125
Epoch 3700, Loss: 22604.6328125
Epoch 3800, Loss: 22326.486328125
Epoch 3900, Loss: 22060.318359375
Epoch 4000, Loss: 21805.04296875
Epoch 4100, Loss: 21559.73046875
Epoch 4200, Loss: 21323.453125
Epoch 4300, Loss: 21095.3125
Epoch 4400, Loss: 20874.583984375
Epoch 4500, Loss: 20660.71875
Epoch 4600, Loss: 20453.04296875
Epoch 4700, Loss: 20251.0390625
Epoch 4800, Loss: 20054.275390625
Epoch 4900, Loss: 19862.3671875
Epoch 5000, Loss: 19674.99609375
Epoch 5100, Loss: 19491.8671875
Epoch 5200, Loss: 19312.734375
Epoch 5300, Loss: 19137.380859375
Epoch 5400, Loss: 18965.6171875
Epoch 5500, Loss: 18797.2734375
Epoch 5600, Loss: 18632.19140625
Epoch 5700, Loss: 18470.2421875
Epoch 5800, Loss: 18311.302734375
Epoch 5900, Loss: 18155.2578125
Epoch 6000, Loss: 18002.029296875
Epoch 6100, Loss: 17851.583984375
Epoch 6200, Loss: 17703.751953125
Epoch 6300, Loss: 17558.458984375
Epoch 6400, Loss: 17415.6328125
Epoch 6500, Loss: 17275.205078125
Epoch 6600, Loss: 17137.115234375
Epoch 6700, Loss: 17001.314453125
Epoch 6800, Loss: 16867.7421875
Epoch 6900, Loss: 16736.34765625
Epoch 7000, Loss: 16607.083984375
Epoch 7100, Loss: 16479.8984375
Epoch 7200, Loss: 16354.818359375
Epoch 7300, Loss: 16231.787109375
Epoch 7400, Loss: 16110.828125
Epoch 7500, Loss: 15991.7939453125
Epoch 7600, Loss: 15874.615234375
Epoch 7700, Loss: 15759.25
Epoch 7800, Loss: 15645.6591796875
Epoch 7900, Loss: 15533.8125
Epoch 8000, Loss: 15423.6669921875
Epoch 8100, Loss: 15315.201171875
Epoch 8200, Loss: 15208.3662109375
Epoch 8300, Loss: 15103.12890625
Epoch 8400, Loss: 14999.458984375
Epoch 8500, Loss: 14897.3369140625
Epoch 8600, Loss: 14796.7236328125
Epoch 8700, Loss: 14697.5947265625
Epoch 8800, Loss: 14599.9296875
Epoch 8900, Loss: 14503.7041015625
Epoch 9000, Loss: 14408.900390625
Epoch 9100, Loss: 14315.498046875
Epoch 9200, Loss: 14223.478515625
Epoch 9300, Loss: 14132.9072265625
Epoch 9400, Loss: 14043.7353515625
Epoch 9500, Loss: 13955.9033203125
Epoch 9600, Loss: 13869.3984375
Epoch 9700, Loss: 13784.2119140625
Epoch 9800, Loss: 13700.337890625
Epoch 9900, Loss: 13617.7705078125
Epoch 10000, Loss: 13536.49609375
Epoch 10100, Loss: 13456.5146484375
Epoch 10200, Loss: 13377.8173828125
Epoch 10300, Loss: 13300.3974609375
Epoch 10400, Loss: 13224.2490234375
Epoch 10500, Loss: 13149.3720703125
Epoch 10600, Loss: 13075.751953125
Epoch 10700, Loss: 13003.3876953125
Epoch 10800, Loss: 12932.2734375
Epoch 10900, Loss: 12862.3984375
Epoch 11000, Loss: 12793.75390625
Epoch 11100, Loss: 12726.33203125
Epoch 11200, Loss: 12660.123046875
Epoch 11300, Loss: 12595.1142578125
Epoch 11400, Loss: 12531.287109375
Epoch 11500, Loss: 12468.638671875
Epoch 11600, Loss: 12407.162109375
Epoch 11700, Loss: 12346.837890625
Epoch 11800, Loss: 12287.634765625
Epoch 11900, Loss: 12229.537109375
Epoch 12000, Loss: 12172.525390625
Epoch 12100, Loss: 12116.5791015625
Epoch 12200, Loss: 12061.6826171875
Epoch 12300, Loss: 12007.890625
Epoch 12400, Loss: 11955.1513671875
Epoch 12500, Loss: 11903.3974609375
Epoch 12600, Loss: 11852.6162109375
Epoch 12700, Loss: 11802.7900390625
Epoch 12800, Loss: 11753.8984375
Epoch 12900, Loss: 11705.9296875
Epoch 13000, Loss: 11658.8662109375
Epoch 13100, Loss: 11612.6923828125
Epoch 13200, Loss: 11567.4013671875
Epoch 13300, Loss: 11522.9736328125
Epoch 13400, Loss: 11479.4013671875
Epoch 13500, Loss: 11436.6708984375
Epoch 13600, Loss: 11394.771484375
Epoch 13700, Loss: 11353.6865234375
Epoch 13800, Loss: 11313.4091796875
Epoch 13900, Loss: 11273.9287109375
Epoch 14000, Loss: 11235.2333984375
Epoch 14100, Loss: 11197.3173828125
Epoch 14200, Loss: 11160.1669921875
Epoch 14300, Loss: 11123.8896484375
Epoch 14400, Loss: 11088.4091796875
Epoch 14500, Loss: 11053.6748046875
Epoch 14600, Loss: 11019.669921875
Epoch 14700, Loss: 10986.3896484375
Epoch 14800, Loss: 10953.900390625
Epoch 14900, Loss: 10922.12890625
Epoch 15000, Loss: 10891.0673828125
Epoch 15100, Loss: 10860.7421875
Epoch 15200, Loss: 10831.1083984375
Epoch 15300, Loss: 10802.158203125
Epoch 15400, Loss: 10773.8974609375
Epoch 15500, Loss: 10746.330078125
Epoch 15600, Loss: 10719.447265625
Epoch 15700, Loss: 10693.224609375
Epoch 15800, Loss: 10667.646484375
Epoch 15900, Loss: 10642.7109375
Epoch 16000, Loss: 10618.4228515625
Epoch 16100, Loss: 10594.80078125
Epoch 16200, Loss: 10571.8017578125
Epoch 16300, Loss: 10549.40234375
Epoch 16400, Loss: 10527.5986328125
Epoch 16500, Loss: 10506.3740234375
Epoch 16600, Loss: 10485.7275390625
Epoch 16700, Loss: 10465.6455078125
Epoch 16800, Loss: 10446.1171875
Epoch 16900, Loss: 10427.13671875
Epoch 17000, Loss: 10408.6875
Epoch 17100, Loss: 10390.763671875
Epoch 17200, Loss: 10373.357421875
Epoch 17300, Loss: 10356.45703125
Epoch 17400, Loss: 10340.0537109375
Epoch 17500, Loss: 10324.14453125
Epoch 17600, Loss: 10308.767578125
Epoch 17700, Loss: 10293.8818359375
Epoch 17800, Loss: 10279.46875
Epoch 17900, Loss: 10265.521484375
Epoch 18000, Loss: 10252.037109375
Epoch 18100, Loss: 10239.009765625
Epoch 18200, Loss: 10226.43359375
Epoch 18300, Loss: 10214.3017578125
Epoch 18400, Loss: 10202.615234375
Epoch 18500, Loss: 10191.392578125
Epoch 18600, Loss: 10180.62109375
Epoch 18700, Loss: 10170.2822265625
Epoch 18800, Loss: 10160.3583984375
Epoch 18900, Loss: 10150.845703125
Epoch 19000, Loss: 10141.7373046875
Epoch 19100, Loss: 10133.0458984375
Epoch 19200, Loss: 10124.7607421875
Epoch 19300, Loss: 10116.86328125
Epoch 19400, Loss: 10109.3408203125
Epoch 19500, Loss: 10102.1904296875
Epoch 19600, Loss: 10095.41015625
Epoch 19700, Loss: 10088.9873046875
Epoch 19800, Loss: 10082.912109375
Epoch 19900, Loss: 10077.1796875
Epoch 20000, Loss: 10071.7900390625
Epoch 20100, Loss: 10066.728515625
Epoch 20200, Loss: 10061.984375
Epoch 20300, Loss: 10057.54296875
Epoch 20400, Loss: 10053.392578125
Epoch 20500, Loss: 10049.515625
Epoch 20600, Loss: 10045.900390625
Epoch 20700, Loss: 10042.537109375
Epoch 20800, Loss: 10039.4140625
Epoch 20900, Loss: 10036.51953125
Epoch 21000, Loss: 10033.841796875
Epoch 21100, Loss: 10031.3681640625
Epoch 21200, Loss: 10029.0927734375
Epoch 21300, Loss: 10026.9990234375
Epoch 21400, Loss: 10025.0751953125
Epoch 21500, Loss: 10023.3134765625
Epoch 21600, Loss: 10021.7099609375
Epoch 21700, Loss: 10020.26171875
Epoch 21800, Loss: 10018.9453125
Epoch 21900, Loss: 10017.7451171875
Epoch 22000, Loss: 10016.6533203125
Epoch 22100, Loss: 10015.6630859375
Epoch 22200, Loss: 10014.76171875
Epoch 22300, Loss: 10013.9462890625
Epoch 22400, Loss: 10013.205078125
Epoch 22500, Loss: 10012.5341796875
Epoch 22600, Loss: 10011.9267578125
Epoch 22700, Loss: 10011.376953125
Epoch 22800, Loss: 10010.8779296875
Epoch 22900, Loss: 10010.4267578125
Epoch 23000, Loss: 10010.017578125
Epoch 23100, Loss: 10009.6474609375
Epoch 23200, Loss: 10009.30859375
Epoch 23300, Loss: 10009.001953125
Epoch 23400, Loss: 10008.720703125
Epoch 23500, Loss: 10008.4619140625
Epoch 23600, Loss: 10008.224609375
Epoch 23700, Loss: 10008.0029296875
Epoch 23800, Loss: 10007.794921875
Epoch 23900, Loss: 10007.5986328125
Epoch 24000, Loss: 10007.41015625
Epoch 24100, Loss: 10007.2294921875
Epoch 24200, Loss: 10007.0537109375
Epoch 24300, Loss: 10006.8818359375
Epoch 24400, Loss: 10006.7099609375
Epoch 24500, Loss: 10006.5380859375
Epoch 24600, Loss: 10006.3681640625
Epoch 24700, Loss: 10006.1953125
Epoch 24800, Loss: 10006.0205078125
Epoch 24900, Loss: 10005.841796875
Epoch 25000, Loss: 10005.6572265625
Epoch 25100, Loss: 10005.4697265625
Epoch 25200, Loss: 10005.2744140625
Epoch 25300, Loss: 10005.0732421875
Epoch 25400, Loss: 10004.8662109375
Epoch 25500, Loss: 10004.6533203125
Epoch 25600, Loss: 10004.431640625
Epoch 25700, Loss: 10004.203125
Epoch 25800, Loss: 10003.966796875
Epoch 25900, Loss: 10003.72265625
Epoch 26000, Loss: 10003.470703125
Epoch 26100, Loss: 10003.2109375
Epoch 26200, Loss: 10002.943359375
Epoch 26300, Loss: 10002.6689453125
Epoch 26400, Loss: 10002.3837890625
Epoch 26500, Loss: 10002.0908203125
Epoch 26600, Loss: 10001.7900390625
Epoch 26700, Loss: 10001.5078125
Epoch 26800, Loss: 10001.2451171875
Epoch 26900, Loss: 10000.9794921875
Epoch 27000, Loss: 10000.708984375
Epoch 27100, Loss: 10000.4365234375
Epoch 27200, Loss: 10000.185546875
Epoch 27300, Loss: 9999.9609375
Epoch 27400, Loss: 9999.75
Epoch 27500, Loss: 9999.533203125
Epoch 27600, Loss: 9999.3095703125
Epoch 27700, Loss: 9999.080078125
Epoch 27800, Loss: 9998.84375
Epoch 27900, Loss: 9998.599609375
Epoch 28000, Loss: 9998.349609375
Epoch 28100, Loss: 9998.0947265625
Epoch 28200, Loss: 9997.8330078125
Epoch 28300, Loss: 9997.564453125
Epoch 28400, Loss: 9997.291015625
Epoch 28500, Loss: 9997.01171875
Epoch 28600, Loss: 9996.7275390625
Epoch 28700, Loss: 9996.4736328125
Epoch 28800, Loss: 9996.2578125
Epoch 28900, Loss: 9996.064453125
Epoch 29000, Loss: 9995.8759765625
Epoch 29100, Loss: 9995.7314453125
Epoch 29200, Loss: 9995.6123046875
Epoch 29300, Loss: 9995.4892578125
Epoch 29400, Loss: 9995.36328125
Epoch 29500, Loss: 9995.234375
Epoch 29600, Loss: 9995.099609375
Epoch 29700, Loss: 9994.9619140625
Epoch 29800, Loss: 9994.822265625
Epoch 29900, Loss: 9994.6767578125
Epoch 30000, Loss: 9994.529296875
Epoch 30100, Loss: 9994.376953125
Epoch 30200, Loss: 9994.220703125
Epoch 30300, Loss: 9994.0625
Epoch 30400, Loss: 9993.900390625
Epoch 30500, Loss: 9993.736328125
Epoch 30600, Loss: 9993.5703125
Epoch 30700, Loss: 9993.4345703125
Epoch 30800, Loss: 9993.3486328125
Epoch 30900, Loss: 9993.26953125
Epoch 31000, Loss: 9993.1884765625
Epoch 31100, Loss: 9993.10546875
Epoch 31200, Loss: 9993.048828125
Epoch 31300, Loss: 9993.0146484375
Epoch 31400, Loss: 9992.990234375
Epoch 31500, Loss: 9992.966796875
Epoch 31600, Loss: 9992.951171875
Epoch 31700, Loss: 9992.943359375
Epoch 31800, Loss: 9992.9365234375
Epoch 31900, Loss: 9992.931640625
Epoch 32000, Loss: 9992.9306640625
Epoch 32100, Loss: 9992.927734375
Epoch 32200, Loss: 9992.927734375
Epoch 32300, Loss: 9992.92578125
Epoch 32400, Loss: 9992.923828125
Epoch 32500, Loss: 9992.9248046875
Epoch 32600, Loss: 9992.9248046875
Epoch 32700, Loss: 9992.923828125
Epoch 32800, Loss: 9992.9228515625
Epoch 32900, Loss: 9992.9228515625
Epoch 33000, Loss: 9992.9228515625
Epoch 33100, Loss: 9992.9228515625
Epoch 33200, Loss: 9992.9228515625
Epoch 33300, Loss: 9992.9228515625
Epoch 33400, Loss: 9992.921875
Epoch 33500, Loss: 9992.921875
Epoch 33600, Loss: 9992.9208984375
Epoch 33700, Loss: 9992.921875
Epoch 33800, Loss: 9992.921875
Epoch 33900, Loss: 9992.921875
Epoch 34000, Loss: 9992.9208984375
Epoch 34100, Loss: 9992.921875
Epoch 34200, Loss: 9992.921875
Epoch 34300, Loss: 9992.921875
Epoch 34400, Loss: 9992.9208984375
Epoch 34500, Loss: 9992.9208984375
Epoch 34600, Loss: 9992.921875
Epoch 34700, Loss: 9992.921875
Epoch 34800, Loss: 9992.919921875
Epoch 34900, Loss: 9992.9208984375
Epoch 35000, Loss: 9992.9208984375
Epoch 35100, Loss: 9992.9208984375
Epoch 35200, Loss: 9992.921875
Epoch 35300, Loss: 9992.921875
Epoch 35400, Loss: 9992.921875
Epoch 35500, Loss: 9992.9208984375
Epoch 35600, Loss: 9992.9208984375
Epoch 35700, Loss: 9992.9208984375
Epoch 35800, Loss: 9992.921875
Epoch 35900, Loss: 9992.9208984375
Epoch 36000, Loss: 9992.921875
Epoch 36100, Loss: 9992.9208984375
Epoch 36200, Loss: 9992.9208984375
Epoch 36300, Loss: 9992.9208984375
Epoch 36400, Loss: 9992.9228515625
Epoch 36500, Loss: 9992.9208984375
Epoch 36600, Loss: 9992.9208984375
Epoch 36700, Loss: 9992.9208984375
Epoch 36800, Loss: 9992.921875
Epoch 36900, Loss: 9992.9208984375
Epoch 37000, Loss: 9992.921875
Epoch 37100, Loss: 9992.921875
Epoch 37200, Loss: 9992.9208984375
Epoch 37300, Loss: 9992.9208984375
Epoch 37400, Loss: 9992.9208984375
Epoch 37500, Loss: 9992.9208984375
Epoch 37600, Loss: 9992.921875
Epoch 37700, Loss: 9992.921875
Epoch 37800, Loss: 9992.9208984375
Epoch 37900, Loss: 9992.921875
Epoch 38000, Loss: 9992.919921875
Epoch 38100, Loss: 9992.9208984375
Epoch 38200, Loss: 9992.9208984375
Epoch 38300, Loss: 9992.9208984375
Epoch 38400, Loss: 9992.9208984375
Epoch 38500, Loss: 9992.9208984375
Epoch 38600, Loss: 9992.921875
Epoch 38700, Loss: 9992.921875
Epoch 38800, Loss: 9992.921875
Epoch 38900, Loss: 9992.9228515625
Epoch 39000, Loss: 9992.919921875
Epoch 39100, Loss: 9992.921875
Epoch 39200, Loss: 9992.9208984375
Epoch 39300, Loss: 9992.921875
Epoch 39400, Loss: 9992.9228515625
Epoch 39500, Loss: 9992.919921875
Epoch 39600, Loss: 9992.9208984375
Epoch 39700, Loss: 9992.921875
Epoch 39800, Loss: 9992.9208984375
Epoch 39900, Loss: 9992.921875
Epoch 40000, Loss: 9992.921875
Epoch 40100, Loss: 9992.921875
Epoch 40200, Loss: 9992.921875
Epoch 40300, Loss: 9992.921875
Epoch 40400, Loss: 9992.921875
Epoch 40500, Loss: 9992.9208984375
Epoch 40600, Loss: 9992.9208984375
Epoch 40700, Loss: 9992.921875
Epoch 40800, Loss: 9992.921875
Epoch 40900, Loss: 9992.921875
Epoch 41000, Loss: 9992.921875
Epoch 41100, Loss: 9992.921875
Epoch 41200, Loss: 9992.921875
Epoch 41300, Loss: 9992.921875
Epoch 41400, Loss: 9992.9208984375
Epoch 41500, Loss: 9992.919921875
Epoch 41600, Loss: 9992.921875
Epoch 41700, Loss: 9992.9208984375
Epoch 41800, Loss: 9992.921875
Epoch 41900, Loss: 9992.9208984375
Epoch 42000, Loss: 9992.9208984375
Epoch 42100, Loss: 9992.921875
Epoch 42200, Loss: 9992.9208984375
Epoch 42300, Loss: 9992.921875
Epoch 42400, Loss: 9992.921875
Epoch 42500, Loss: 9992.921875
Epoch 42600, Loss: 9992.921875
Epoch 42700, Loss: 9992.921875
Epoch 42800, Loss: 9992.9208984375
Epoch 42900, Loss: 9992.921875
Epoch 43000, Loss: 9992.921875
Epoch 43100, Loss: 9992.921875
Epoch 43200, Loss: 9992.921875
Epoch 43300, Loss: 9992.921875
Epoch 43400, Loss: 9992.921875
Epoch 43500, Loss: 9992.9228515625
Epoch 43600, Loss: 9992.921875
Epoch 43700, Loss: 9992.921875
Epoch 43800, Loss: 9992.9208984375
Epoch 43900, Loss: 9992.921875
Epoch 44000, Loss: 9992.9208984375
Epoch 44100, Loss: 9992.9208984375
Epoch 44200, Loss: 9992.921875
Epoch 44300, Loss: 9992.921875
Epoch 44400, Loss: 9992.9208984375
Epoch 44500, Loss: 9992.921875
Epoch 44600, Loss: 9992.9208984375
Epoch 44700, Loss: 9992.9208984375
Epoch 44800, Loss: 9992.921875
Epoch 44900, Loss: 9992.919921875
Epoch 45000, Loss: 9992.921875
Epoch 45100, Loss: 9992.921875
Epoch 45200, Loss: 9992.919921875
Epoch 45300, Loss: 9992.9208984375
Epoch 45400, Loss: 9992.921875
Epoch 45500, Loss: 9992.9208984375
Epoch 45600, Loss: 9992.921875
Epoch 45700, Loss: 9992.9208984375
Epoch 45800, Loss: 9992.9228515625
Epoch 45900, Loss: 9992.921875
Epoch 46000, Loss: 9992.921875
Epoch 46100, Loss: 9992.9208984375
Epoch 46200, Loss: 9992.921875
Epoch 46300, Loss: 9992.921875
Epoch 46400, Loss: 9992.9208984375
Epoch 46500, Loss: 9992.921875
Epoch 46600, Loss: 9992.9208984375
Epoch 46700, Loss: 9992.921875
Epoch 46800, Loss: 9992.9208984375
Epoch 46900, Loss: 9992.921875
Epoch 47000, Loss: 9992.9208984375
Epoch 47100, Loss: 9992.9208984375
Epoch 47200, Loss: 9992.9208984375
Epoch 47300, Loss: 9992.919921875
Epoch 47400, Loss: 9992.921875
Epoch 47500, Loss: 9992.9208984375
Epoch 47600, Loss: 9992.9208984375
Epoch 47700, Loss: 9992.921875
Epoch 47800, Loss: 9992.921875
Epoch 47900, Loss: 9992.9228515625
Epoch 48000, Loss: 9992.9228515625
Epoch 48100, Loss: 9992.9208984375
Epoch 48200, Loss: 9992.921875
Epoch 48300, Loss: 9992.921875
Epoch 48400, Loss: 9992.921875
Epoch 48500, Loss: 9992.9208984375
Epoch 48600, Loss: 9992.921875
Epoch 48700, Loss: 9992.921875
Epoch 48800, Loss: 9992.921875
Epoch 48900, Loss: 9992.921875
Epoch 49000, Loss: 9992.921875
Epoch 49100, Loss: 9992.921875
Epoch 49200, Loss: 9992.9208984375
Epoch 49300, Loss: 9992.9208984375
Epoch 49400, Loss: 9992.9208984375
Epoch 49500, Loss: 9992.9208984375
Epoch 49600, Loss: 9992.9208984375
Epoch 49700, Loss: 9992.921875
Epoch 49800, Loss: 9992.921875
Epoch 49900, Loss: 9992.9208984375
Epoch 0, Loss: 63828.96875
Epoch 100, Loss: 60923.71875
Epoch 200, Loss: 58207.2421875
Epoch 300, Loss: 55665.81640625
Epoch 400, Loss: 53286.33984375
Epoch 500, Loss: 51057.8671875
Epoch 600, Loss: 48971.37109375
Epoch 700, Loss: 47018.453125
Epoch 800, Loss: 45191.3984375
Epoch 900, Loss: 43483.06640625
Epoch 1000, Loss: 41886.62109375
Epoch 1100, Loss: 40395.3046875
Epoch 1200, Loss: 39002.734375
Epoch 1300, Loss: 37702.75390625
Epoch 1400, Loss: 36489.46484375
Epoch 1500, Loss: 35357.3515625
Epoch 1600, Loss: 34300.73046875
Epoch 1700, Loss: 33314.32421875
Epoch 1800, Loss: 32393.15625
Epoch 1900, Loss: 31532.53515625
Epoch 2000, Loss: 30728.08984375
Epoch 2100, Loss: 29975.755859375
Epoch 2200, Loss: 29271.939453125
Epoch 2300, Loss: 28612.962890625
Epoch 2400, Loss: 27995.796875
Epoch 2500, Loss: 27417.236328125
Epoch 2600, Loss: 26874.31640625
Epoch 2700, Loss: 26364.396484375
Epoch 2800, Loss: 25885.03125
Epoch 2900, Loss: 25433.94140625
Epoch 3000, Loss: 25009.0
Epoch 3100, Loss: 24608.20703125
Epoch 3200, Loss: 24229.6953125
Epoch 3300, Loss: 23871.77734375
Epoch 3400, Loss: 23532.80859375
Epoch 3500, Loss: 23211.30859375
Epoch 3600, Loss: 22905.76953125
Epoch 3700, Loss: 22614.701171875
Epoch 3800, Loss: 22336.794921875
Epoch 3900, Loss: 22070.865234375
Epoch 4000, Loss: 21815.8203125
Epoch 4100, Loss: 21570.755859375
Epoch 4200, Loss: 21334.7265625
Epoch 4300, Loss: 21106.83203125
Epoch 4400, Loss: 20886.33984375
Epoch 4500, Loss: 20672.734375
Epoch 4600, Loss: 20465.3125
Epoch 4700, Loss: 20263.556640625
Epoch 4800, Loss: 20067.03125
Epoch 4900, Loss: 19875.359375
Epoch 5000, Loss: 19688.212890625
Epoch 5100, Loss: 19505.3046875
Epoch 5200, Loss: 19326.388671875
Epoch 5300, Loss: 19151.25
Epoch 5400, Loss: 18979.69140625
Epoch 5500, Loss: 18811.546875
Epoch 5600, Loss: 18646.666015625
Epoch 5700, Loss: 18484.91015625
Epoch 5800, Loss: 18326.158203125
Epoch 5900, Loss: 18170.296875
Epoch 6000, Loss: 18017.26953125
Epoch 6100, Loss: 17867.025390625
Epoch 6200, Loss: 17719.390625
Epoch 6300, Loss: 17574.287109375
Epoch 6400, Loss: 17431.6484375
Epoch 6500, Loss: 17291.400390625
Epoch 6600, Loss: 17153.490234375
Epoch 6700, Loss: 17017.861328125
Epoch 6800, Loss: 16884.453125
Epoch 6900, Loss: 16753.220703125
Epoch 7000, Loss: 16624.1171875
Epoch 7100, Loss: 16497.087890625
Epoch 7200, Loss: 16372.1748046875
Epoch 7300, Loss: 16249.3251953125
Epoch 7400, Loss: 16128.5625
Epoch 7500, Loss: 16009.73828125
Epoch 7600, Loss: 15892.7470703125
Epoch 7700, Loss: 15777.5654296875
Epoch 7800, Loss: 15664.1513671875
Epoch 7900, Loss: 15552.4765625
Epoch 8000, Loss: 15442.5078125
Epoch 8100, Loss: 15334.19921875
Epoch 8200, Loss: 15227.5166015625
Epoch 8300, Loss: 15122.4267578125
Epoch 8400, Loss: 15018.90625
Epoch 8500, Loss: 14916.9208984375
Epoch 8600, Loss: 14816.439453125
Epoch 8700, Loss: 14717.443359375
Epoch 8800, Loss: 14619.9052734375
Epoch 8900, Loss: 14523.8037109375
Epoch 9000, Loss: 14429.119140625
Epoch 9100, Loss: 14335.833984375
Epoch 9200, Loss: 14243.9296875
Epoch 9300, Loss: 14153.5166015625
Epoch 9400, Loss: 14064.478515625
Epoch 9500, Loss: 13976.779296875
Epoch 9600, Loss: 13890.4052734375
Epoch 9700, Loss: 13805.34765625
Epoch 9800, Loss: 13721.599609375
Epoch 9900, Loss: 13639.15234375
Epoch 10000, Loss: 13558.0029296875
Epoch 10100, Loss: 13478.138671875
Epoch 10200, Loss: 13399.556640625
Epoch 10300, Loss: 13322.2490234375
Epoch 10400, Loss: 13246.2138671875
Epoch 10500, Loss: 13171.44140625
Epoch 10600, Loss: 13097.9287109375
Epoch 10700, Loss: 13025.669921875
Epoch 10800, Loss: 12954.65625
Epoch 10900, Loss: 12884.8818359375
Epoch 11000, Loss: 12816.337890625
Epoch 11100, Loss: 12749.013671875
Epoch 11200, Loss: 12682.89453125
Epoch 11300, Loss: 12617.9775390625
Epoch 11400, Loss: 12554.2431640625
Epoch 11500, Loss: 12491.6806640625
Epoch 11600, Loss: 12430.3017578125
Epoch 11700, Loss: 12370.0654296875
Epoch 11800, Loss: 12310.94921875
Epoch 11900, Loss: 12252.9345703125
Epoch 12000, Loss: 12196.001953125
Epoch 12100, Loss: 12140.1337890625
Epoch 12200, Loss: 12085.310546875
Epoch 12300, Loss: 12031.630859375
Epoch 12400, Loss: 11978.986328125
Epoch 12500, Loss: 11927.3291015625
Epoch 12600, Loss: 11876.642578125
Epoch 12700, Loss: 11826.90234375
Epoch 12800, Loss: 11778.095703125
Epoch 12900, Loss: 11730.2060546875
Epoch 13000, Loss: 11683.216796875
Epoch 13100, Loss: 11637.1181640625
Epoch 13200, Loss: 11591.8974609375
Epoch 13300, Loss: 11547.5439453125
Epoch 13400, Loss: 11504.0390625
Epoch 13500, Loss: 11461.3720703125
Epoch 13600, Loss: 11419.5322265625
Epoch 13700, Loss: 11378.5029296875
Epoch 13800, Loss: 11338.2802734375
Epoch 13900, Loss: 11298.8544921875
Epoch 14000, Loss: 11260.2109375
Epoch 14100, Loss: 11222.3388671875
Epoch 14200, Loss: 11185.2431640625
Epoch 14300, Loss: 11149.0595703125
Epoch 14400, Loss: 11113.658203125
Epoch 14500, Loss: 11079.0009765625
Epoch 14600, Loss: 11045.080078125
Epoch 14700, Loss: 11011.953125
Epoch 14800, Loss: 10979.6181640625
Epoch 14900, Loss: 10948.0009765625
Epoch 15000, Loss: 10917.0849609375
Epoch 15100, Loss: 10886.8603515625
Epoch 15200, Loss: 10857.3232421875
Epoch 15300, Loss: 10828.466796875
Epoch 15400, Loss: 10800.30078125
Epoch 15500, Loss: 10772.8212890625
Epoch 15600, Loss: 10746.0263671875
Epoch 15700, Loss: 10719.8916015625
Epoch 15800, Loss: 10694.4033203125
Epoch 15900, Loss: 10669.548828125
Epoch 16000, Loss: 10645.345703125
Epoch 16100, Loss: 10621.8056640625
Epoch 16200, Loss: 10598.8935546875
Epoch 16300, Loss: 10576.580078125
Epoch 16400, Loss: 10554.8515625
Epoch 16500, Loss: 10533.70703125
Epoch 16600, Loss: 10513.1318359375
Epoch 16700, Loss: 10493.1171875
Epoch 16800, Loss: 10473.6533203125
Epoch 16900, Loss: 10454.73046875
Epoch 17000, Loss: 10436.3603515625
Epoch 17100, Loss: 10418.568359375
Epoch 17200, Loss: 10401.2880859375
Epoch 17300, Loss: 10384.5185546875
Epoch 17400, Loss: 10368.2451171875
Epoch 17500, Loss: 10352.4619140625
Epoch 17600, Loss: 10337.1630859375
Epoch 17700, Loss: 10322.341796875
Epoch 17800, Loss: 10307.990234375
Epoch 17900, Loss: 10294.1044921875
Epoch 18000, Loss: 10280.677734375
Epoch 18100, Loss: 10267.70703125
Epoch 18200, Loss: 10255.1826171875
Epoch 18300, Loss: 10243.1044921875
Epoch 18400, Loss: 10231.4677734375
Epoch 18500, Loss: 10220.291015625
Epoch 18600, Loss: 10209.5791015625
Epoch 18700, Loss: 10199.3369140625
Epoch 18800, Loss: 10189.5166015625
Epoch 18900, Loss: 10180.1064453125
Epoch 19000, Loss: 10171.0986328125
Epoch 19100, Loss: 10162.490234375
Epoch 19200, Loss: 10154.283203125
Epoch 19300, Loss: 10146.4609375
Epoch 19400, Loss: 10139.015625
Epoch 19500, Loss: 10131.9384765625
Epoch 19600, Loss: 10125.2197265625
Epoch 19700, Loss: 10118.8515625
Epoch 19800, Loss: 10112.83203125
Epoch 19900, Loss: 10107.162109375
Epoch 20000, Loss: 10101.828125
Epoch 20100, Loss: 10096.8193359375
Epoch 20200, Loss: 10092.123046875
Epoch 20300, Loss: 10087.7314453125
Epoch 20400, Loss: 10083.626953125
Epoch 20500, Loss: 10079.7958984375
Epoch 20600, Loss: 10076.2255859375
Epoch 20700, Loss: 10072.9052734375
Epoch 20800, Loss: 10069.8193359375
Epoch 20900, Loss: 10066.9619140625
Epoch 21000, Loss: 10064.3154296875
Epoch 21100, Loss: 10061.884765625
Epoch 21200, Loss: 10059.671875
Epoch 21300, Loss: 10057.6435546875
Epoch 21400, Loss: 10055.78125
Epoch 21500, Loss: 10054.0771484375
Epoch 21600, Loss: 10052.517578125
Epoch 21700, Loss: 10051.0947265625
Epoch 21800, Loss: 10049.794921875
Epoch 21900, Loss: 10048.6123046875
Epoch 22000, Loss: 10047.53515625
Epoch 22100, Loss: 10046.552734375
Epoch 22200, Loss: 10045.6611328125
Epoch 22300, Loss: 10044.8505859375
Epoch 22400, Loss: 10044.111328125
Epoch 22500, Loss: 10043.4384765625
Epoch 22600, Loss: 10042.8251953125
Epoch 22700, Loss: 10042.267578125
Epoch 22800, Loss: 10041.7568359375
Epoch 22900, Loss: 10041.291015625
Epoch 23000, Loss: 10040.8623046875
Epoch 23100, Loss: 10040.46875
Epoch 23200, Loss: 10040.1044921875
Epoch 23300, Loss: 10039.7666015625
Epoch 23400, Loss: 10039.451171875
Epoch 23500, Loss: 10039.1552734375
Epoch 23600, Loss: 10038.8759765625
Epoch 23700, Loss: 10038.6083984375
Epoch 23800, Loss: 10038.3525390625
Epoch 23900, Loss: 10038.10546875
Epoch 24000, Loss: 10037.8642578125
Epoch 24100, Loss: 10037.626953125
Epoch 24200, Loss: 10037.390625
Epoch 24300, Loss: 10037.15625
Epoch 24400, Loss: 10036.9189453125
Epoch 24500, Loss: 10036.6787109375
Epoch 24600, Loss: 10036.43359375
Epoch 24700, Loss: 10036.185546875
Epoch 24800, Loss: 10035.9296875
Epoch 24900, Loss: 10035.6669921875
Epoch 25000, Loss: 10035.3974609375
Epoch 25100, Loss: 10035.119140625
Epoch 25200, Loss: 10034.8330078125
Epoch 25300, Loss: 10034.53515625
Epoch 25400, Loss: 10034.2294921875
Epoch 25500, Loss: 10033.9150390625
Epoch 25600, Loss: 10033.5888671875
Epoch 25700, Loss: 10033.2548828125
Epoch 25800, Loss: 10032.91015625
Epoch 25900, Loss: 10032.5546875
Epoch 26000, Loss: 10032.1884765625
Epoch 26100, Loss: 10031.8173828125
Epoch 26200, Loss: 10031.4892578125
Epoch 26300, Loss: 10031.1611328125
Epoch 26400, Loss: 10030.830078125
Epoch 26500, Loss: 10030.4931640625
Epoch 26600, Loss: 10030.1630859375
Epoch 26700, Loss: 10029.857421875
Epoch 26800, Loss: 10029.5751953125
Epoch 26900, Loss: 10029.3095703125
Epoch 27000, Loss: 10029.0400390625
Epoch 27100, Loss: 10028.7626953125
Epoch 27200, Loss: 10028.478515625
Epoch 27300, Loss: 10028.1845703125
Epoch 27400, Loss: 10027.8818359375
Epoch 27500, Loss: 10027.5712890625
Epoch 27600, Loss: 10027.2529296875
Epoch 27700, Loss: 10026.92578125
Epoch 27800, Loss: 10026.5947265625
Epoch 27900, Loss: 10026.251953125
Epoch 28000, Loss: 10025.9033203125
Epoch 28100, Loss: 10025.55078125
Epoch 28200, Loss: 10025.2294921875
Epoch 28300, Loss: 10024.9521484375
Epoch 28400, Loss: 10024.703125
Epoch 28500, Loss: 10024.4697265625
Epoch 28600, Loss: 10024.2783203125
Epoch 28700, Loss: 10024.1220703125
Epoch 28800, Loss: 10023.970703125
Epoch 28900, Loss: 10023.814453125
Epoch 29000, Loss: 10023.6533203125
Epoch 29100, Loss: 10023.48828125
Epoch 29200, Loss: 10023.3193359375
Epoch 29300, Loss: 10023.1435546875
Epoch 29400, Loss: 10022.962890625
Epoch 29500, Loss: 10022.779296875
Epoch 29600, Loss: 10022.5888671875
Epoch 29700, Loss: 10022.396484375
Epoch 29800, Loss: 10022.1982421875
Epoch 29900, Loss: 10021.99609375
Epoch 30000, Loss: 10021.7890625
Epoch 30100, Loss: 10021.580078125
Epoch 30200, Loss: 10021.3955078125
Epoch 30300, Loss: 10021.267578125
Epoch 30400, Loss: 10021.166015625
Epoch 30500, Loss: 10021.0634765625
Epoch 30600, Loss: 10020.9599609375
Epoch 30700, Loss: 10020.8779296875
Epoch 30800, Loss: 10020.8212890625
Epoch 30900, Loss: 10020.783203125
Epoch 31000, Loss: 10020.7529296875
Epoch 31100, Loss: 10020.7294921875
Epoch 31200, Loss: 10020.712890625
Epoch 31300, Loss: 10020.7001953125
Epoch 31400, Loss: 10020.693359375
Epoch 31500, Loss: 10020.6884765625
Epoch 31600, Loss: 10020.6845703125
Epoch 31700, Loss: 10020.6796875
Epoch 31800, Loss: 10020.6787109375
Epoch 31900, Loss: 10020.67578125
Epoch 32000, Loss: 10020.67578125
Epoch 32100, Loss: 10020.673828125
Epoch 32200, Loss: 10020.6728515625
Epoch 32300, Loss: 10020.671875
Epoch 32400, Loss: 10020.671875
Epoch 32500, Loss: 10020.671875
Epoch 32600, Loss: 10020.671875
Epoch 32700, Loss: 10020.6708984375
Epoch 32800, Loss: 10020.669921875
Epoch 32900, Loss: 10020.669921875
Epoch 33000, Loss: 10020.669921875
Epoch 33100, Loss: 10020.669921875
Epoch 33200, Loss: 10020.669921875
Epoch 33300, Loss: 10020.6689453125
Epoch 33400, Loss: 10020.669921875
Epoch 33500, Loss: 10020.6689453125
Epoch 33600, Loss: 10020.6689453125
Epoch 33700, Loss: 10020.669921875
Epoch 33800, Loss: 10020.6689453125
Epoch 33900, Loss: 10020.66796875
Epoch 34000, Loss: 10020.669921875
Epoch 34100, Loss: 10020.6689453125
Epoch 34200, Loss: 10020.669921875
Epoch 34300, Loss: 10020.669921875
Epoch 34400, Loss: 10020.669921875
Epoch 34500, Loss: 10020.6689453125
Epoch 34600, Loss: 10020.66796875
Epoch 34700, Loss: 10020.6689453125
Epoch 34800, Loss: 10020.6689453125
Epoch 34900, Loss: 10020.669921875
Epoch 35000, Loss: 10020.669921875
Epoch 35100, Loss: 10020.669921875
Epoch 35200, Loss: 10020.66796875
Epoch 35300, Loss: 10020.6689453125
Epoch 35400, Loss: 10020.6689453125
Epoch 35500, Loss: 10020.6689453125
Epoch 35600, Loss: 10020.6689453125
Epoch 35700, Loss: 10020.66796875
Epoch 35800, Loss: 10020.6689453125
Epoch 35900, Loss: 10020.66796875
Epoch 36000, Loss: 10020.66796875
Epoch 36100, Loss: 10020.6689453125
Epoch 36200, Loss: 10020.6689453125
Epoch 36300, Loss: 10020.66796875
Epoch 36400, Loss: 10020.6689453125
Epoch 36500, Loss: 10020.6689453125
Epoch 36600, Loss: 10020.6689453125
Epoch 36700, Loss: 10020.6689453125
Epoch 36800, Loss: 10020.66796875
Epoch 36900, Loss: 10020.66796875
Epoch 37000, Loss: 10020.6689453125
Epoch 37100, Loss: 10020.66796875
Epoch 37200, Loss: 10020.6689453125
Epoch 37300, Loss: 10020.6689453125
Epoch 37400, Loss: 10020.6689453125
Epoch 37500, Loss: 10020.66796875
Epoch 37600, Loss: 10020.669921875
Epoch 37700, Loss: 10020.6689453125
Epoch 37800, Loss: 10020.66796875
Epoch 37900, Loss: 10020.6689453125
Epoch 38000, Loss: 10020.669921875
Epoch 38100, Loss: 10020.66796875
Epoch 38200, Loss: 10020.66796875
Epoch 38300, Loss: 10020.66796875
Epoch 38400, Loss: 10020.6689453125
Epoch 38500, Loss: 10020.669921875
Epoch 38600, Loss: 10020.66796875
Epoch 38700, Loss: 10020.6689453125
Epoch 38800, Loss: 10020.66796875
Epoch 38900, Loss: 10020.66796875
Epoch 39000, Loss: 10020.66796875
Epoch 39100, Loss: 10020.6689453125
Epoch 39200, Loss: 10020.6689453125
Epoch 39300, Loss: 10020.6689453125
Epoch 39400, Loss: 10020.66796875
Epoch 39500, Loss: 10020.66796875
Epoch 39600, Loss: 10020.66796875
Epoch 39700, Loss: 10020.669921875
Epoch 39800, Loss: 10020.6689453125
Epoch 39900, Loss: 10020.66796875
Epoch 40000, Loss: 10020.66796875
Epoch 40100, Loss: 10020.66796875
Epoch 40200, Loss: 10020.6689453125
Epoch 40300, Loss: 10020.669921875
Epoch 40400, Loss: 10020.6689453125
Epoch 40500, Loss: 10020.6689453125
Epoch 40600, Loss: 10020.6689453125
Epoch 40700, Loss: 10020.6689453125
Epoch 40800, Loss: 10020.6689453125
Epoch 40900, Loss: 10020.66796875
Epoch 41000, Loss: 10020.6689453125
Epoch 41100, Loss: 10020.66796875
Epoch 41200, Loss: 10020.66796875
Epoch 41300, Loss: 10020.66796875
Epoch 41400, Loss: 10020.6689453125
Epoch 41500, Loss: 10020.669921875
Epoch 41600, Loss: 10020.66796875
Epoch 41700, Loss: 10020.6689453125
Epoch 41800, Loss: 10020.6689453125
Epoch 41900, Loss: 10020.6689453125
Epoch 42000, Loss: 10020.669921875
Epoch 42100, Loss: 10020.66796875
Epoch 42200, Loss: 10020.66796875
Epoch 42300, Loss: 10020.6689453125
Epoch 42400, Loss: 10020.66796875
Epoch 42500, Loss: 10020.6689453125
Epoch 42600, Loss: 10020.6689453125
Epoch 42700, Loss: 10020.66796875
Epoch 42800, Loss: 10020.6689453125
Epoch 42900, Loss: 10020.6689453125
Epoch 43000, Loss: 10020.6689453125
Epoch 43100, Loss: 10020.6689453125
Epoch 43200, Loss: 10020.66796875
Epoch 43300, Loss: 10020.6689453125
Epoch 43400, Loss: 10020.6689453125
Epoch 43500, Loss: 10020.66796875
Epoch 43600, Loss: 10020.66796875
Epoch 43700, Loss: 10020.66796875
Epoch 43800, Loss: 10020.6689453125
Epoch 43900, Loss: 10020.6689453125
Epoch 44000, Loss: 10020.6689453125
Epoch 44100, Loss: 10020.6689453125
Epoch 44200, Loss: 10020.669921875
Epoch 44300, Loss: 10020.66796875
Epoch 44400, Loss: 10020.66796875
Epoch 44500, Loss: 10020.66796875
Epoch 44600, Loss: 10020.66796875
Epoch 44700, Loss: 10020.6689453125
Epoch 44800, Loss: 10020.669921875
Epoch 44900, Loss: 10020.6689453125
Epoch 45000, Loss: 10020.66796875
Epoch 45100, Loss: 10020.66796875
Epoch 45200, Loss: 10020.669921875
Epoch 45300, Loss: 10020.6689453125
Epoch 45400, Loss: 10020.669921875
Epoch 45500, Loss: 10020.66796875
Epoch 45600, Loss: 10020.66796875
Epoch 45700, Loss: 10020.6689453125
Epoch 45800, Loss: 10020.66796875
Epoch 45900, Loss: 10020.66796875
Epoch 46000, Loss: 10020.6689453125
Epoch 46100, Loss: 10020.669921875
Epoch 46200, Loss: 10020.66796875
Epoch 46300, Loss: 10020.66796875
Epoch 46400, Loss: 10020.6689453125
Epoch 46500, Loss: 10020.669921875
Epoch 46600, Loss: 10020.6689453125
Epoch 46700, Loss: 10020.66796875
Epoch 46800, Loss: 10020.66796875
Epoch 46900, Loss: 10020.6689453125
Epoch 47000, Loss: 10020.6689453125
Epoch 47100, Loss: 10020.6689453125
Epoch 47200, Loss: 10020.6689453125
Epoch 47300, Loss: 10020.66796875
Epoch 47400, Loss: 10020.66796875
Epoch 47500, Loss: 10020.6689453125
Epoch 47600, Loss: 10020.6689453125
Epoch 47700, Loss: 10020.669921875
Epoch 47800, Loss: 10020.66796875
Epoch 47900, Loss: 10020.6689453125
Epoch 48000, Loss: 10020.6689453125
Epoch 48100, Loss: 10020.6689453125
Epoch 48200, Loss: 10020.66796875
Epoch 48300, Loss: 10020.6689453125
Epoch 48400, Loss: 10020.6689453125
Epoch 48500, Loss: 10020.66796875
Epoch 48600, Loss: 10020.6689453125
Epoch 48700, Loss: 10020.669921875
Epoch 48800, Loss: 10020.6689453125
Epoch 48900, Loss: 10020.6689453125
Epoch 49000, Loss: 10020.66796875
Epoch 49100, Loss: 10020.6689453125
Epoch 49200, Loss: 10020.6689453125
Epoch 49300, Loss: 10020.6689453125
Epoch 49400, Loss: 10020.6689453125
Epoch 49500, Loss: 10020.66796875
Epoch 49600, Loss: 10020.6689453125
Epoch 49700, Loss: 10020.66796875
Epoch 49800, Loss: 10020.6689453125
Epoch 49900, Loss: 10020.6689453125
Epoch 0, Loss: 63828.96875
Epoch 100, Loss: 60924.35546875
Epoch 200, Loss: 58208.44140625
Epoch 300, Loss: 55667.51171875
Epoch 400, Loss: 53288.54296875
Epoch 500, Loss: 51060.5546875
Epoch 600, Loss: 48974.5625
Epoch 700, Loss: 47022.11328125
Epoch 800, Loss: 45195.4921875
Epoch 900, Loss: 43487.58984375
Epoch 1000, Loss: 41891.56640625
Epoch 1100, Loss: 40400.65234375
Epoch 1200, Loss: 39008.453125
Epoch 1300, Loss: 37708.8125
Epoch 1400, Loss: 36495.8359375
Epoch 1500, Loss: 35364.0703125
Epoch 1600, Loss: 34307.7578125
Epoch 1700, Loss: 33321.640625
Epoch 1800, Loss: 32400.73046875
Epoch 1900, Loss: 31540.34765625
Epoch 2000, Loss: 30736.123046875
Epoch 2100, Loss: 29983.994140625
Epoch 2200, Loss: 29280.431640625
Epoch 2300, Loss: 28621.6953125
Epoch 2400, Loss: 28004.841796875
Epoch 2500, Loss: 27426.603515625
Epoch 2600, Loss: 26883.986328125
Epoch 2700, Loss: 26374.35546875
Epoch 2800, Loss: 25895.265625
Epoch 2900, Loss: 25444.4375
Epoch 3000, Loss: 25019.748046875
Epoch 3100, Loss: 24619.197265625
Epoch 3200, Loss: 24240.921875
Epoch 3300, Loss: 23883.2421875
Epoch 3400, Loss: 23544.53515625
Epoch 3500, Loss: 23223.3359375
Epoch 3600, Loss: 22918.123046875
Epoch 3700, Loss: 22627.373046875
Epoch 3800, Loss: 22349.771484375
Epoch 3900, Loss: 22084.138671875
Epoch 4000, Loss: 21829.373046875
Epoch 4100, Loss: 21584.630859375
Epoch 4200, Loss: 21348.9140625
Epoch 4300, Loss: 21121.326171875
Epoch 4400, Loss: 20901.130859375
Epoch 4500, Loss: 20687.859375
Epoch 4600, Loss: 20480.751953125
Epoch 4700, Loss: 20279.306640625
Epoch 4800, Loss: 20083.080078125
Epoch 4900, Loss: 19891.703125
Epoch 5000, Loss: 19704.841796875
Epoch 5100, Loss: 19522.21484375
Epoch 5200, Loss: 19343.5703125
Epoch 5300, Loss: 19168.693359375
Epoch 5400, Loss: 18997.396484375
Epoch 5500, Loss: 18829.5078125
Epoch 5600, Loss: 18664.873046875
Epoch 5700, Loss: 18503.359375
Epoch 5800, Loss: 18344.845703125
Epoch 5900, Loss: 18189.220703125
Epoch 6000, Loss: 18036.4453125
Epoch 6100, Loss: 17886.455078125
Epoch 6200, Loss: 17739.064453125
Epoch 6300, Loss: 17594.201171875
Epoch 6400, Loss: 17451.79296875
Epoch 6500, Loss: 17311.775390625
Epoch 6600, Loss: 17174.0859375
Epoch 6700, Loss: 17038.669921875
Epoch 6800, Loss: 16905.47265625
Epoch 6900, Loss: 16774.447265625
Epoch 7000, Loss: 16645.541015625
Epoch 7100, Loss: 16518.7109375
Epoch 7200, Loss: 16394.0078125
Epoch 7300, Loss: 16271.39453125
Epoch 7400, Loss: 16150.8681640625
Epoch 7500, Loss: 16032.2919921875
Epoch 7600, Loss: 15915.5498046875
Epoch 7700, Loss: 15800.611328125
Epoch 7800, Loss: 15687.4345703125
Epoch 7900, Loss: 15575.9853515625
Epoch 8000, Loss: 15466.220703125
Epoch 8100, Loss: 15358.109375
Epoch 8200, Loss: 15251.6201171875
Epoch 8300, Loss: 15146.7138671875
Epoch 8400, Loss: 15043.3720703125
Epoch 8500, Loss: 14941.5595703125
Epoch 8600, Loss: 14841.2470703125
Epoch 8700, Loss: 14742.416015625
Epoch 8800, Loss: 14645.03515625
Epoch 8900, Loss: 14549.0869140625
Epoch 9000, Loss: 14454.55078125
Epoch 9100, Loss: 14361.4130859375
Epoch 9200, Loss: 14269.6796875
Epoch 9300, Loss: 14179.4521484375
Epoch 9400, Loss: 14090.5791015625
Epoch 9500, Loss: 14003.044921875
Epoch 9600, Loss: 13916.833984375
Epoch 9700, Loss: 13831.9345703125
Epoch 9800, Loss: 13748.3408203125
Epoch 9900, Loss: 13666.046875
Epoch 10000, Loss: 13585.0458984375
Epoch 10100, Loss: 13505.326171875
Epoch 10200, Loss: 13426.8857421875
Epoch 10300, Loss: 13349.7177734375
Epoch 10400, Loss: 13273.8193359375
Epoch 10500, Loss: 13199.181640625
Epoch 10600, Loss: 13125.80078125
Epoch 10700, Loss: 13053.6689453125
Epoch 10800, Loss: 12982.78125
Epoch 10900, Loss: 12913.1298828125
Epoch 11000, Loss: 12844.7041015625
Epoch 11100, Loss: 12777.4990234375
Epoch 11200, Loss: 12711.4951171875
Epoch 11300, Loss: 12646.6904296875
Epoch 11400, Loss: 12583.0634765625
Epoch 11500, Loss: 12520.6171875
Epoch 11600, Loss: 12459.349609375
Epoch 11700, Loss: 12399.2197265625
Epoch 11800, Loss: 12340.208984375
Epoch 11900, Loss: 12282.296875
Epoch 12000, Loss: 12225.462890625
Epoch 12100, Loss: 12169.6884765625
Epoch 12200, Loss: 12114.9541015625
Epoch 12300, Loss: 12061.4169921875
Epoch 12400, Loss: 12008.890625
Epoch 12500, Loss: 11957.3466796875
Epoch 12600, Loss: 11906.7666015625
Epoch 12700, Loss: 11857.126953125
Epoch 12800, Loss: 11808.4189453125
Epoch 12900, Loss: 11760.62890625
Epoch 13000, Loss: 11713.7333984375
Epoch 13100, Loss: 11667.7294921875
Epoch 13200, Loss: 11622.5947265625
Epoch 13300, Loss: 11578.314453125
Epoch 13400, Loss: 11534.8837890625
Epoch 13500, Loss: 11492.287109375
Epoch 13600, Loss: 11450.51171875
Epoch 13700, Loss: 11409.5458984375
Epoch 13800, Loss: 11369.38671875
Epoch 13900, Loss: 11330.0146484375
Epoch 14000, Loss: 11291.423828125
Epoch 14100, Loss: 11253.6025390625
Epoch 14200, Loss: 11216.6103515625
Epoch 14300, Loss: 11180.6005859375
Epoch 14400, Loss: 11145.3828125
Epoch 14500, Loss: 11110.9052734375
Epoch 14600, Loss: 11077.1533203125
Epoch 14700, Loss: 11044.158203125
Epoch 14800, Loss: 11011.9501953125
Epoch 14900, Loss: 10980.4580078125
Epoch 15000, Loss: 10949.6611328125
Epoch 15100, Loss: 10919.5517578125
Epoch 15200, Loss: 10890.1220703125
Epoch 15300, Loss: 10861.3779296875
Epoch 15400, Loss: 10833.322265625
Epoch 15500, Loss: 10805.94921875
Epoch 15600, Loss: 10779.248046875
Epoch 15700, Loss: 10753.21484375
Epoch 15800, Loss: 10727.826171875
Epoch 15900, Loss: 10703.0673828125
Epoch 16000, Loss: 10678.9638671875
Epoch 16100, Loss: 10655.5166015625
Epoch 16200, Loss: 10632.703125
Epoch 16300, Loss: 10610.486328125
Epoch 16400, Loss: 10588.8544921875
Epoch 16500, Loss: 10567.8603515625
Epoch 16600, Loss: 10547.466796875
Epoch 16700, Loss: 10527.6337890625
Epoch 16800, Loss: 10508.345703125
Epoch 16900, Loss: 10489.5986328125
Epoch 17000, Loss: 10471.3798828125
Epoch 17100, Loss: 10453.681640625
Epoch 17200, Loss: 10436.4951171875
Epoch 17300, Loss: 10419.8115234375
Epoch 17400, Loss: 10403.6220703125
Epoch 17500, Loss: 10387.9208984375
Epoch 17600, Loss: 10372.69921875
Epoch 17700, Loss: 10357.9521484375
Epoch 17800, Loss: 10343.673828125
Epoch 17900, Loss: 10329.857421875
Epoch 18000, Loss: 10316.49609375
Epoch 18100, Loss: 10303.603515625
Epoch 18200, Loss: 10291.1953125
Epoch 18300, Loss: 10279.2333984375
Epoch 18400, Loss: 10267.7138671875
Epoch 18500, Loss: 10256.6513671875
Epoch 18600, Loss: 10246.037109375
Epoch 18700, Loss: 10235.8759765625
Epoch 18800, Loss: 10226.154296875
Epoch 18900, Loss: 10216.84765625
Epoch 19000, Loss: 10207.9443359375
Epoch 19100, Loss: 10199.4326171875
Epoch 19200, Loss: 10191.3095703125
Epoch 19300, Loss: 10183.564453125
Epoch 19400, Loss: 10176.1904296875
Epoch 19500, Loss: 10169.1806640625
Epoch 19600, Loss: 10162.52734375
Epoch 19700, Loss: 10156.2265625
Epoch 19800, Loss: 10150.2841796875
Epoch 19900, Loss: 10144.6845703125
Epoch 20000, Loss: 10139.4140625
Epoch 20100, Loss: 10134.462890625
Epoch 20200, Loss: 10129.8251953125
Epoch 20300, Loss: 10125.4833984375
Epoch 20400, Loss: 10121.4306640625
Epoch 20500, Loss: 10117.6513671875
Epoch 20600, Loss: 10114.1376953125
Epoch 20700, Loss: 10110.9111328125
Epoch 20800, Loss: 10107.927734375
Epoch 20900, Loss: 10105.166015625
Epoch 21000, Loss: 10102.6142578125
Epoch 21100, Loss: 10100.259765625
Epoch 21200, Loss: 10098.091796875
Epoch 21300, Loss: 10096.09765625
Epoch 21400, Loss: 10094.267578125
Epoch 21500, Loss: 10092.5908203125
Epoch 21600, Loss: 10091.0556640625
Epoch 21700, Loss: 10089.654296875
Epoch 21800, Loss: 10088.3720703125
Epoch 21900, Loss: 10087.201171875
Epoch 22000, Loss: 10086.1337890625
Epoch 22100, Loss: 10085.158203125
Epoch 22200, Loss: 10084.2666015625
Epoch 22300, Loss: 10083.453125
Epoch 22400, Loss: 10082.70703125
Epoch 22500, Loss: 10082.0234375
Epoch 22600, Loss: 10081.3935546875
Epoch 22700, Loss: 10080.8134765625
Epoch 22800, Loss: 10080.2763671875
Epoch 22900, Loss: 10079.779296875
Epoch 23000, Loss: 10079.3154296875
Epoch 23100, Loss: 10078.8818359375
Epoch 23200, Loss: 10078.47265625
Epoch 23300, Loss: 10078.0869140625
Epoch 23400, Loss: 10077.7158203125
Epoch 23500, Loss: 10077.3603515625
Epoch 23600, Loss: 10077.0166015625
Epoch 23700, Loss: 10076.6806640625
Epoch 23800, Loss: 10076.349609375
Epoch 23900, Loss: 10076.021484375
Epoch 24000, Loss: 10075.6962890625
Epoch 24100, Loss: 10075.369140625
Epoch 24200, Loss: 10075.0390625
Epoch 24300, Loss: 10074.7041015625
Epoch 24400, Loss: 10074.36328125
Epoch 24500, Loss: 10074.015625
Epoch 24600, Loss: 10073.6591796875
Epoch 24700, Loss: 10073.29296875
Epoch 24800, Loss: 10072.91796875
Epoch 24900, Loss: 10072.5302734375
Epoch 25000, Loss: 10072.1318359375
Epoch 25100, Loss: 10071.7216796875
Epoch 25200, Loss: 10071.2998046875
Epoch 25300, Loss: 10070.8642578125
Epoch 25400, Loss: 10070.4189453125
Epoch 25500, Loss: 10069.9580078125
Epoch 25600, Loss: 10069.54296875
Epoch 25700, Loss: 10069.1376953125
Epoch 25800, Loss: 10068.728515625
Epoch 25900, Loss: 10068.314453125
Epoch 26000, Loss: 10067.8994140625
Epoch 26100, Loss: 10067.50390625
Epoch 26200, Loss: 10067.1298828125
Epoch 26300, Loss: 10066.77734375
Epoch 26400, Loss: 10066.4423828125
Epoch 26500, Loss: 10066.10546875
Epoch 26600, Loss: 10065.76171875
Epoch 26700, Loss: 10065.404296875
Epoch 26800, Loss: 10065.0390625
Epoch 26900, Loss: 10064.6611328125
Epoch 27000, Loss: 10064.2744140625
Epoch 27100, Loss: 10063.8759765625
Epoch 27200, Loss: 10063.4697265625
Epoch 27300, Loss: 10063.052734375
Epoch 27400, Loss: 10062.626953125
Epoch 27500, Loss: 10062.19140625
Epoch 27600, Loss: 10061.7578125
Epoch 27700, Loss: 10061.3603515625
Epoch 27800, Loss: 10061.00390625
Epoch 27900, Loss: 10060.6826171875
Epoch 28000, Loss: 10060.39453125
Epoch 28100, Loss: 10060.150390625
Epoch 28200, Loss: 10059.9443359375
Epoch 28300, Loss: 10059.7548828125
Epoch 28400, Loss: 10059.560546875
Epoch 28500, Loss: 10059.3623046875
Epoch 28600, Loss: 10059.1572265625
Epoch 28700, Loss: 10058.9453125
Epoch 28800, Loss: 10058.728515625
Epoch 28900, Loss: 10058.5048828125
Epoch 29000, Loss: 10058.2744140625
Epoch 29100, Loss: 10058.0390625
Epoch 29200, Loss: 10057.7978515625
Epoch 29300, Loss: 10057.5517578125
Epoch 29400, Loss: 10057.298828125
Epoch 29500, Loss: 10057.0419921875
Epoch 29600, Loss: 10056.7783203125
Epoch 29700, Loss: 10056.5390625
Epoch 29800, Loss: 10056.357421875
Epoch 29900, Loss: 10056.21875
Epoch 30000, Loss: 10056.091796875
Epoch 30100, Loss: 10055.9609375
Epoch 30200, Loss: 10055.849609375
Epoch 30300, Loss: 10055.765625
Epoch 30400, Loss: 10055.703125
Epoch 30500, Loss: 10055.6591796875
Epoch 30600, Loss: 10055.6259765625
Epoch 30700, Loss: 10055.599609375
Epoch 30800, Loss: 10055.5810546875
Epoch 30900, Loss: 10055.56640625
Epoch 31000, Loss: 10055.5556640625
Epoch 31100, Loss: 10055.546875
Epoch 31200, Loss: 10055.5419921875
Epoch 31300, Loss: 10055.5361328125
Epoch 31400, Loss: 10055.533203125
Epoch 31500, Loss: 10055.529296875
Epoch 31600, Loss: 10055.52734375
Epoch 31700, Loss: 10055.525390625
Epoch 31800, Loss: 10055.525390625
Epoch 31900, Loss: 10055.5224609375
Epoch 32000, Loss: 10055.521484375
Epoch 32100, Loss: 10055.51953125
Epoch 32200, Loss: 10055.5205078125
Epoch 32300, Loss: 10055.517578125
Epoch 32400, Loss: 10055.5185546875
Epoch 32500, Loss: 10055.5185546875
Epoch 32600, Loss: 10055.5166015625
Epoch 32700, Loss: 10055.517578125
Epoch 32800, Loss: 10055.5166015625
Epoch 32900, Loss: 10055.5166015625
Epoch 33000, Loss: 10055.515625
Epoch 33100, Loss: 10055.515625
Epoch 33200, Loss: 10055.5146484375
Epoch 33300, Loss: 10055.5146484375
Epoch 33400, Loss: 10055.515625
Epoch 33500, Loss: 10055.515625
Epoch 33600, Loss: 10055.515625
Epoch 33700, Loss: 10055.515625
Epoch 33800, Loss: 10055.5146484375
Epoch 33900, Loss: 10055.515625
Epoch 34000, Loss: 10055.5146484375
Epoch 34100, Loss: 10055.515625
Epoch 34200, Loss: 10055.515625
Epoch 34300, Loss: 10055.5146484375
Epoch 34400, Loss: 10055.515625
Epoch 34500, Loss: 10055.5146484375
Epoch 34600, Loss: 10055.515625
Epoch 34700, Loss: 10055.5146484375
Epoch 34800, Loss: 10055.515625
Epoch 34900, Loss: 10055.5146484375
Epoch 35000, Loss: 10055.515625
Epoch 35100, Loss: 10055.515625
Epoch 35200, Loss: 10055.5146484375
Epoch 35300, Loss: 10055.515625
Epoch 35400, Loss: 10055.515625
Epoch 35500, Loss: 10055.5146484375
Epoch 35600, Loss: 10055.515625
Epoch 35700, Loss: 10055.515625
Epoch 35800, Loss: 10055.515625
Epoch 35900, Loss: 10055.515625
Epoch 36000, Loss: 10055.5146484375
Epoch 36100, Loss: 10055.5146484375
Epoch 36200, Loss: 10055.5146484375
Epoch 36300, Loss: 10055.5146484375
Epoch 36400, Loss: 10055.515625
Epoch 36500, Loss: 10055.515625
Epoch 36600, Loss: 10055.515625
Epoch 36700, Loss: 10055.515625
Epoch 36800, Loss: 10055.515625
Epoch 36900, Loss: 10055.515625
Epoch 37000, Loss: 10055.515625
Epoch 37100, Loss: 10055.515625
Epoch 37200, Loss: 10055.515625
Epoch 37300, Loss: 10055.515625
Epoch 37400, Loss: 10055.515625
Epoch 37500, Loss: 10055.515625
Epoch 37600, Loss: 10055.515625
Epoch 37700, Loss: 10055.5146484375
Epoch 37800, Loss: 10055.515625
Epoch 37900, Loss: 10055.515625
Epoch 38000, Loss: 10055.515625
Epoch 38100, Loss: 10055.5146484375
Epoch 38200, Loss: 10055.515625
Epoch 38300, Loss: 10055.5146484375
Epoch 38400, Loss: 10055.515625
Epoch 38500, Loss: 10055.5146484375
Epoch 38600, Loss: 10055.515625
Epoch 38700, Loss: 10055.515625
Epoch 38800, Loss: 10055.515625
Epoch 38900, Loss: 10055.515625
Epoch 39000, Loss: 10055.515625
Epoch 39100, Loss: 10055.515625
Epoch 39200, Loss: 10055.515625
Epoch 39300, Loss: 10055.515625
Epoch 39400, Loss: 10055.515625
Epoch 39500, Loss: 10055.5146484375
Epoch 39600, Loss: 10055.515625
Epoch 39700, Loss: 10055.515625
Epoch 39800, Loss: 10055.515625
Epoch 39900, Loss: 10055.5146484375
Epoch 40000, Loss: 10055.515625
Epoch 40100, Loss: 10055.5146484375
Epoch 40200, Loss: 10055.515625
Epoch 40300, Loss: 10055.515625
Epoch 40400, Loss: 10055.5166015625
Epoch 40500, Loss: 10055.515625
Epoch 40600, Loss: 10055.515625
Epoch 40700, Loss: 10055.515625
Epoch 40800, Loss: 10055.515625
Epoch 40900, Loss: 10055.515625
Epoch 41000, Loss: 10055.515625
Epoch 41100, Loss: 10055.515625
Epoch 41200, Loss: 10055.515625
Epoch 41300, Loss: 10055.5146484375
Epoch 41400, Loss: 10055.515625
Epoch 41500, Loss: 10055.515625
Epoch 41600, Loss: 10055.515625
Epoch 41700, Loss: 10055.515625
Epoch 41800, Loss: 10055.515625
Epoch 41900, Loss: 10055.5146484375
Epoch 42000, Loss: 10055.5146484375
Epoch 42100, Loss: 10055.515625
Epoch 42200, Loss: 10055.515625
Epoch 42300, Loss: 10055.515625
Epoch 42400, Loss: 10055.515625
Epoch 42500, Loss: 10055.515625
Epoch 42600, Loss: 10055.5146484375
Epoch 42700, Loss: 10055.515625
Epoch 42800, Loss: 10055.515625
Epoch 42900, Loss: 10055.515625
Epoch 43000, Loss: 10055.515625
Epoch 43100, Loss: 10055.515625
Epoch 43200, Loss: 10055.515625
Epoch 43300, Loss: 10055.515625
Epoch 43400, Loss: 10055.515625
Epoch 43500, Loss: 10055.515625
Epoch 43600, Loss: 10055.515625
Epoch 43700, Loss: 10055.515625
Epoch 43800, Loss: 10055.515625
Epoch 43900, Loss: 10055.5146484375
Epoch 44000, Loss: 10055.5146484375
Epoch 44100, Loss: 10055.515625
Epoch 44200, Loss: 10055.515625
Epoch 44300, Loss: 10055.515625
Epoch 44400, Loss: 10055.515625
Epoch 44500, Loss: 10055.515625
Epoch 44600, Loss: 10055.515625
Epoch 44700, Loss: 10055.515625
Epoch 44800, Loss: 10055.5146484375
Epoch 44900, Loss: 10055.515625
Epoch 45000, Loss: 10055.515625
Epoch 45100, Loss: 10055.5146484375
Epoch 45200, Loss: 10055.515625
Epoch 45300, Loss: 10055.515625
Epoch 45400, Loss: 10055.5146484375
Epoch 45500, Loss: 10055.515625
Epoch 45600, Loss: 10055.515625
Epoch 45700, Loss: 10055.5146484375
Epoch 45800, Loss: 10055.515625
Epoch 45900, Loss: 10055.515625
Epoch 46000, Loss: 10055.515625
Epoch 46100, Loss: 10055.515625
Epoch 46200, Loss: 10055.515625
Epoch 46300, Loss: 10055.515625
Epoch 46400, Loss: 10055.515625
Epoch 46500, Loss: 10055.515625
Epoch 46600, Loss: 10055.515625
Epoch 46700, Loss: 10055.515625
Epoch 46800, Loss: 10055.515625
Epoch 46900, Loss: 10055.515625
Epoch 47000, Loss: 10055.515625
Epoch 47100, Loss: 10055.515625
Epoch 47200, Loss: 10055.515625
Epoch 47300, Loss: 10055.5146484375
Epoch 47400, Loss: 10055.515625
Epoch 47500, Loss: 10055.515625
Epoch 47600, Loss: 10055.515625
Epoch 47700, Loss: 10055.515625
Epoch 47800, Loss: 10055.515625
Epoch 47900, Loss: 10055.5146484375
Epoch 48000, Loss: 10055.515625
Epoch 48100, Loss: 10055.515625
Epoch 48200, Loss: 10055.515625
Epoch 48300, Loss: 10055.515625
Epoch 48400, Loss: 10055.515625
Epoch 48500, Loss: 10055.515625
Epoch 48600, Loss: 10055.515625
Epoch 48700, Loss: 10055.515625
Epoch 48800, Loss: 10055.515625
Epoch 48900, Loss: 10055.515625
Epoch 49000, Loss: 10055.515625
Epoch 49100, Loss: 10055.515625
Epoch 49200, Loss: 10055.515625
Epoch 49300, Loss: 10055.515625
Epoch 49400, Loss: 10055.515625
Epoch 49500, Loss: 10055.515625
Epoch 49600, Loss: 10055.515625
Epoch 49700, Loss: 10055.515625
Epoch 49800, Loss: 10055.515625
Epoch 49900, Loss: 10055.515625
Epoch 0, Loss: 63828.96875
Epoch 100, Loss: 60925.1484375
Epoch 200, Loss: 58209.96875
Epoch 300, Loss: 55669.66015625
Epoch 400, Loss: 53291.32421875
Epoch 500, Loss: 51063.953125
Epoch 600, Loss: 48978.60546875
Epoch 700, Loss: 47026.7421875
Epoch 800, Loss: 45200.66015625
Epoch 900, Loss: 43493.3046875
Epoch 1000, Loss: 41897.82421875
Epoch 1100, Loss: 40407.41015625
Epoch 1200, Loss: 39015.66796875
Epoch 1300, Loss: 37716.44921875
Epoch 1400, Loss: 36503.88671875
Epoch 1500, Loss: 35372.546875
Epoch 1600, Loss: 34316.62890625
Epoch 1700, Loss: 33330.86328125
Epoch 1800, Loss: 32410.28515625
Epoch 1900, Loss: 31550.205078125
Epoch 2000, Loss: 30746.2578125
Epoch 2100, Loss: 29994.384765625
Epoch 2200, Loss: 29291.146484375
Epoch 2300, Loss: 28632.7109375
Epoch 2400, Loss: 28016.265625
Epoch 2500, Loss: 27438.423828125
Epoch 2600, Loss: 26896.18359375
Epoch 2700, Loss: 26386.91796875
Epoch 2800, Loss: 25908.177734375
Epoch 2900, Loss: 25457.6796875
Epoch 3000, Loss: 25033.30078125
Epoch 3100, Loss: 24633.056640625
Epoch 3200, Loss: 24255.072265625
Epoch 3300, Loss: 23897.6953125
Epoch 3400, Loss: 23559.314453125
Epoch 3500, Loss: 23238.4921875
Epoch 3600, Loss: 22933.69921875
Epoch 3700, Loss: 22643.34375
Epoch 3800, Loss: 22366.125
Epoch 3900, Loss: 22100.86328125
Epoch 4000, Loss: 21846.458984375
Epoch 4100, Loss: 21602.1171875
Epoch 4200, Loss: 21366.796875
Epoch 4300, Loss: 21139.591796875
Epoch 4400, Loss: 20919.77734375
Epoch 4500, Loss: 20706.91796875
Epoch 4600, Loss: 20500.2109375
Epoch 4700, Loss: 20299.1484375
Epoch 4800, Loss: 20103.30078125
Epoch 4900, Loss: 19912.291015625
Epoch 5000, Loss: 19725.787109375
Epoch 5100, Loss: 19543.50390625
Epoch 5200, Loss: 19365.203125
Epoch 5300, Loss: 19190.66015625
Epoch 5400, Loss: 19019.689453125
Epoch 5500, Loss: 18852.11328125
Epoch 5600, Loss: 18687.794921875
Epoch 5700, Loss: 18526.583984375
Epoch 5800, Loss: 18368.369140625
Epoch 5900, Loss: 18213.02734375
Epoch 6000, Loss: 18060.5859375
Epoch 6100, Loss: 17910.91015625
Epoch 6200, Loss: 17763.828125
Epoch 6300, Loss: 17619.26171875
Epoch 6400, Loss: 17477.1484375
Epoch 6500, Loss: 17337.412109375
Epoch 6600, Loss: 17200.001953125
Epoch 6700, Loss: 17064.853515625
Epoch 6800, Loss: 16931.921875
Epoch 6900, Loss: 16801.150390625
Epoch 7000, Loss: 16672.494140625
Epoch 7100, Loss: 16545.912109375
Epoch 7200, Loss: 16421.478515625
Epoch 7300, Loss: 16299.169921875
Epoch 7400, Loss: 16178.9326171875
Epoch 7500, Loss: 16060.666015625
Epoch 7600, Loss: 15944.220703125
Epoch 7700, Loss: 15829.568359375
Epoch 7800, Loss: 15716.671875
Epoch 7900, Loss: 15605.482421875
Epoch 8000, Loss: 15495.9794921875
Epoch 8100, Loss: 15388.115234375
Epoch 8200, Loss: 15281.8623046875
Epoch 8300, Loss: 15177.1943359375
Epoch 8400, Loss: 15074.0791015625
Epoch 8500, Loss: 14972.4814453125
Epoch 8600, Loss: 14872.3798828125
Epoch 8700, Loss: 14773.7490234375
Epoch 8800, Loss: 14676.5654296875
Epoch 8900, Loss: 14580.8095703125
Epoch 9000, Loss: 14486.4599609375
Epoch 9100, Loss: 14393.501953125
Epoch 9200, Loss: 14302.0205078125
Epoch 9300, Loss: 14212.005859375
Epoch 9400, Loss: 14123.3408203125
Epoch 9500, Loss: 14036.0078125
Epoch 9600, Loss: 13949.9970703125
Epoch 9700, Loss: 13865.2939453125
Epoch 9800, Loss: 13781.8955078125
Epoch 9900, Loss: 13699.787109375
Epoch 10000, Loss: 13618.96875
Epoch 10100, Loss: 13539.4287109375
Epoch 10200, Loss: 13461.166015625
Epoch 10300, Loss: 13384.16796875
Epoch 10400, Loss: 13308.4375
Epoch 10500, Loss: 13233.962890625
Epoch 10600, Loss: 13160.7451171875
Epoch 10700, Loss: 13088.771484375
Epoch 10800, Loss: 13018.0380859375
Epoch 10900, Loss: 12948.5361328125
Epoch 11000, Loss: 12880.2578125
Epoch 11100, Loss: 12813.1923828125
Epoch 11200, Loss: 12747.3330078125
Epoch 11300, Loss: 12682.662109375
Epoch 11400, Loss: 12619.1669921875
Epoch 11500, Loss: 12556.8662109375
Epoch 11600, Loss: 12495.73046875
Epoch 11700, Loss: 12435.7353515625
Epoch 11800, Loss: 12376.84765625
Epoch 11900, Loss: 12319.052734375
Epoch 12000, Loss: 12262.3330078125
Epoch 12100, Loss: 12206.66796875
Epoch 12200, Loss: 12152.08203125
Epoch 12300, Loss: 12098.6962890625
Epoch 12400, Loss: 12046.30078125
Epoch 12500, Loss: 11994.8876953125
Epoch 12600, Loss: 11944.4267578125
Epoch 12700, Loss: 11894.90625
Epoch 12800, Loss: 11846.3173828125
Epoch 12900, Loss: 11798.6318359375
Epoch 13000, Loss: 11751.83984375
Epoch 13100, Loss: 11705.9306640625
Epoch 13200, Loss: 11660.8837890625
Epoch 13300, Loss: 11616.69140625
Epoch 13400, Loss: 11573.3408203125
Epoch 13500, Loss: 11530.8212890625
Epoch 13600, Loss: 11489.1142578125
Epoch 13700, Loss: 11448.2255859375
Epoch 13800, Loss: 11408.1298828125
Epoch 13900, Loss: 11368.90234375
Epoch 14000, Loss: 11330.490234375
Epoch 14100, Loss: 11292.84375
Epoch 14200, Loss: 11256.1181640625
Epoch 14300, Loss: 11220.2578125
Epoch 14400, Loss: 11185.1767578125
Epoch 14500, Loss: 11150.83203125
Epoch 14600, Loss: 11117.2109375
Epoch 14700, Loss: 11084.37890625
Epoch 14800, Loss: 11052.3193359375
Epoch 14900, Loss: 11020.9736328125
Epoch 15000, Loss: 10990.3193359375
Epoch 15100, Loss: 10960.345703125
Epoch 15200, Loss: 10931.0478515625
Epoch 15300, Loss: 10902.4345703125
Epoch 15400, Loss: 10874.5068359375
Epoch 15500, Loss: 10847.2529296875
Epoch 15600, Loss: 10820.666015625
Epoch 15700, Loss: 10794.73828125
Epoch 15800, Loss: 10769.4619140625
Epoch 15900, Loss: 10744.8193359375
Epoch 16000, Loss: 10720.921875
Epoch 16100, Loss: 10697.7099609375
Epoch 16200, Loss: 10675.1396484375
Epoch 16300, Loss: 10653.173828125
Epoch 16400, Loss: 10631.7900390625
Epoch 16500, Loss: 10610.9765625
Epoch 16600, Loss: 10590.7255859375
Epoch 16700, Loss: 10571.0244140625
Epoch 16800, Loss: 10551.865234375
Epoch 16900, Loss: 10533.2392578125
Epoch 17000, Loss: 10515.138671875
Epoch 17100, Loss: 10497.552734375
Epoch 17200, Loss: 10480.4755859375
Epoch 17300, Loss: 10463.8916015625
Epoch 17400, Loss: 10447.802734375
Epoch 17500, Loss: 10432.1953125
Epoch 17600, Loss: 10417.08203125
Epoch 17700, Loss: 10402.486328125
Epoch 17800, Loss: 10388.36328125
Epoch 17900, Loss: 10374.7001953125
Epoch 18000, Loss: 10361.494140625
Epoch 18100, Loss: 10348.7392578125
Epoch 18200, Loss: 10336.4306640625
Epoch 18300, Loss: 10324.5830078125
Epoch 18400, Loss: 10313.1787109375
Epoch 18500, Loss: 10302.2333984375
Epoch 18600, Loss: 10291.73046875
Epoch 18700, Loss: 10281.6669921875
Epoch 18800, Loss: 10272.029296875
Epoch 18900, Loss: 10262.8154296875
Epoch 19000, Loss: 10254.009765625
Epoch 19100, Loss: 10245.5986328125
Epoch 19200, Loss: 10237.5693359375
Epoch 19300, Loss: 10229.9150390625
Epoch 19400, Loss: 10222.62890625
Epoch 19500, Loss: 10215.701171875
Epoch 19600, Loss: 10209.134765625
Epoch 19700, Loss: 10202.9296875
Epoch 19800, Loss: 10197.0703125
Epoch 19900, Loss: 10191.5498046875
Epoch 20000, Loss: 10186.353515625
Epoch 20100, Loss: 10181.474609375
Epoch 20200, Loss: 10176.95703125
Epoch 20300, Loss: 10172.75390625
Epoch 20400, Loss: 10168.8369140625
Epoch 20500, Loss: 10165.1904296875
Epoch 20600, Loss: 10161.80078125
Epoch 20700, Loss: 10158.6533203125
Epoch 20800, Loss: 10155.7333984375
Epoch 20900, Loss: 10153.029296875
Epoch 21000, Loss: 10150.529296875
Epoch 21100, Loss: 10148.2216796875
Epoch 21200, Loss: 10146.0966796875
Epoch 21300, Loss: 10144.1396484375
Epoch 21400, Loss: 10142.341796875
Epoch 21500, Loss: 10140.6923828125
Epoch 21600, Loss: 10139.1796875
Epoch 21700, Loss: 10137.7939453125
Epoch 21800, Loss: 10136.5234375
Epoch 21900, Loss: 10135.3603515625
Epoch 22000, Loss: 10134.291015625
Epoch 22100, Loss: 10133.3125
Epoch 22200, Loss: 10132.412109375
Epoch 22300, Loss: 10131.58203125
Epoch 22400, Loss: 10130.8134765625
Epoch 22500, Loss: 10130.1005859375
Epoch 22600, Loss: 10129.4365234375
Epoch 22700, Loss: 10128.81640625
Epoch 22800, Loss: 10128.2314453125
Epoch 22900, Loss: 10127.6787109375
Epoch 23000, Loss: 10127.150390625
Epoch 23100, Loss: 10126.646484375
Epoch 23200, Loss: 10126.16015625
Epoch 23300, Loss: 10125.6884765625
Epoch 23400, Loss: 10125.2275390625
Epoch 23500, Loss: 10124.771484375
Epoch 23600, Loss: 10124.3212890625
Epoch 23700, Loss: 10123.873046875
Epoch 23800, Loss: 10123.423828125
Epoch 23900, Loss: 10122.970703125
Epoch 24000, Loss: 10122.5126953125
Epoch 24100, Loss: 10122.0458984375
Epoch 24200, Loss: 10121.572265625
Epoch 24300, Loss: 10121.087890625
Epoch 24400, Loss: 10120.5908203125
Epoch 24500, Loss: 10120.0830078125
Epoch 24600, Loss: 10119.560546875
Epoch 24700, Loss: 10119.0244140625
Epoch 24800, Loss: 10118.4736328125
Epoch 24900, Loss: 10117.9111328125
Epoch 25000, Loss: 10117.4013671875
Epoch 25100, Loss: 10116.89453125
Epoch 25200, Loss: 10116.38671875
Epoch 25300, Loss: 10115.87109375
Epoch 25400, Loss: 10115.3583984375
Epoch 25500, Loss: 10114.8642578125
Epoch 25600, Loss: 10114.3876953125
Epoch 25700, Loss: 10113.9306640625
Epoch 25800, Loss: 10113.490234375
Epoch 25900, Loss: 10113.0673828125
Epoch 26000, Loss: 10112.642578125
Epoch 26100, Loss: 10112.208984375
Epoch 26200, Loss: 10111.76171875
Epoch 26300, Loss: 10111.3017578125
Epoch 26400, Loss: 10110.8291015625
Epoch 26500, Loss: 10110.3427734375
Epoch 26600, Loss: 10109.8447265625
Epoch 26700, Loss: 10109.333984375
Epoch 26800, Loss: 10108.8115234375
Epoch 26900, Loss: 10108.2763671875
Epoch 27000, Loss: 10107.732421875
Epoch 27100, Loss: 10107.205078125
Epoch 27200, Loss: 10106.7177734375
Epoch 27300, Loss: 10106.2685546875
Epoch 27400, Loss: 10105.85546875
Epoch 27500, Loss: 10105.50390625
Epoch 27600, Loss: 10105.19921875
Epoch 27700, Loss: 10104.9306640625
Epoch 27800, Loss: 10104.6884765625
Epoch 27900, Loss: 10104.447265625
Epoch 28000, Loss: 10104.19921875
Epoch 28100, Loss: 10103.9443359375
Epoch 28200, Loss: 10103.681640625
Epoch 28300, Loss: 10103.412109375
Epoch 28400, Loss: 10103.134765625
Epoch 28500, Loss: 10102.8486328125
Epoch 28600, Loss: 10102.5556640625
Epoch 28700, Loss: 10102.2548828125
Epoch 28800, Loss: 10101.947265625
Epoch 28900, Loss: 10101.6328125
Epoch 29000, Loss: 10101.310546875
Epoch 29100, Loss: 10100.982421875
Epoch 29200, Loss: 10100.6796875
Epoch 29300, Loss: 10100.4365234375
Epoch 29400, Loss: 10100.2392578125
Epoch 29500, Loss: 10100.07421875
Epoch 29600, Loss: 10099.9130859375
Epoch 29700, Loss: 10099.76953125
Epoch 29800, Loss: 10099.6552734375
Epoch 29900, Loss: 10099.5625
Epoch 30000, Loss: 10099.490234375
Epoch 30100, Loss: 10099.439453125
Epoch 30200, Loss: 10099.40234375
Epoch 30300, Loss: 10099.373046875
Epoch 30400, Loss: 10099.349609375
Epoch 30500, Loss: 10099.3291015625
Epoch 30600, Loss: 10099.3154296875
Epoch 30700, Loss: 10099.3037109375
Epoch 30800, Loss: 10099.2939453125
Epoch 30900, Loss: 10099.287109375
Epoch 31000, Loss: 10099.2802734375
Epoch 31100, Loss: 10099.2763671875
Epoch 31200, Loss: 10099.271484375
Epoch 31300, Loss: 10099.26953125
Epoch 31400, Loss: 10099.265625
Epoch 31500, Loss: 10099.263671875
Epoch 31600, Loss: 10099.26171875
Epoch 31700, Loss: 10099.2607421875
Epoch 31800, Loss: 10099.2587890625
Epoch 31900, Loss: 10099.2568359375
Epoch 32000, Loss: 10099.2568359375
Epoch 32100, Loss: 10099.2548828125
Epoch 32200, Loss: 10099.2548828125
Epoch 32300, Loss: 10099.25390625
Epoch 32400, Loss: 10099.2529296875
Epoch 32500, Loss: 10099.2529296875
Epoch 32600, Loss: 10099.2529296875
Epoch 32700, Loss: 10099.251953125
Epoch 32800, Loss: 10099.251953125
Epoch 32900, Loss: 10099.251953125
Epoch 33000, Loss: 10099.2529296875
Epoch 33100, Loss: 10099.2529296875
Epoch 33200, Loss: 10099.251953125
Epoch 33300, Loss: 10099.251953125
Epoch 33400, Loss: 10099.251953125
Epoch 33500, Loss: 10099.251953125
Epoch 33600, Loss: 10099.2529296875
Epoch 33700, Loss: 10099.2529296875
Epoch 33800, Loss: 10099.251953125
Epoch 33900, Loss: 10099.251953125
Epoch 34000, Loss: 10099.251953125
Epoch 34100, Loss: 10099.2509765625
Epoch 34200, Loss: 10099.251953125
Epoch 34300, Loss: 10099.2509765625
Epoch 34400, Loss: 10099.251953125
Epoch 34500, Loss: 10099.251953125
Epoch 34600, Loss: 10099.251953125
Epoch 34700, Loss: 10099.2509765625
Epoch 34800, Loss: 10099.2509765625
Epoch 34900, Loss: 10099.251953125
Epoch 35000, Loss: 10099.2509765625
Epoch 35100, Loss: 10099.2509765625
Epoch 35200, Loss: 10099.251953125
Epoch 35300, Loss: 10099.2509765625
Epoch 35400, Loss: 10099.2509765625
Epoch 35500, Loss: 10099.251953125
Epoch 35600, Loss: 10099.2509765625
Epoch 35700, Loss: 10099.2509765625
Epoch 35800, Loss: 10099.251953125
Epoch 35900, Loss: 10099.2509765625
Epoch 36000, Loss: 10099.2509765625
Epoch 36100, Loss: 10099.2509765625
Epoch 36200, Loss: 10099.251953125
Epoch 36300, Loss: 10099.2509765625
Epoch 36400, Loss: 10099.2509765625
Epoch 36500, Loss: 10099.2529296875
Epoch 36600, Loss: 10099.2509765625
Epoch 36700, Loss: 10099.2509765625
Epoch 36800, Loss: 10099.251953125
Epoch 36900, Loss: 10099.251953125
Epoch 37000, Loss: 10099.2529296875
Epoch 37100, Loss: 10099.2509765625
Epoch 37200, Loss: 10099.2509765625
Epoch 37300, Loss: 10099.251953125
Epoch 37400, Loss: 10099.251953125
Epoch 37500, Loss: 10099.2509765625
Epoch 37600, Loss: 10099.2509765625
Epoch 37700, Loss: 10099.2509765625
Epoch 37800, Loss: 10099.2509765625
Epoch 37900, Loss: 10099.251953125
Epoch 38000, Loss: 10099.2509765625
Epoch 38100, Loss: 10099.2509765625
Epoch 38200, Loss: 10099.251953125
Epoch 38300, Loss: 10099.2509765625
Epoch 38400, Loss: 10099.251953125
Epoch 38500, Loss: 10099.251953125
Epoch 38600, Loss: 10099.251953125
Epoch 38700, Loss: 10099.251953125
Epoch 38800, Loss: 10099.251953125
Epoch 38900, Loss: 10099.2509765625
Epoch 39000, Loss: 10099.2509765625
Epoch 39100, Loss: 10099.2509765625
Epoch 39200, Loss: 10099.2509765625
Epoch 39300, Loss: 10099.251953125
Epoch 39400, Loss: 10099.2509765625
Epoch 39500, Loss: 10099.2509765625
Epoch 39600, Loss: 10099.2509765625
Epoch 39700, Loss: 10099.251953125
Epoch 39800, Loss: 10099.2509765625
Epoch 39900, Loss: 10099.251953125
Epoch 40000, Loss: 10099.251953125
Epoch 40100, Loss: 10099.2509765625
Epoch 40200, Loss: 10099.2509765625
Epoch 40300, Loss: 10099.2509765625
Epoch 40400, Loss: 10099.251953125
Epoch 40500, Loss: 10099.251953125
Epoch 40600, Loss: 10099.2509765625
Epoch 40700, Loss: 10099.2509765625
Epoch 40800, Loss: 10099.2509765625
Epoch 40900, Loss: 10099.2509765625
Epoch 41000, Loss: 10099.251953125
Epoch 41100, Loss: 10099.2509765625
Epoch 41200, Loss: 10099.251953125
Epoch 41300, Loss: 10099.2509765625
Epoch 41400, Loss: 10099.251953125
Epoch 41500, Loss: 10099.2509765625
Epoch 41600, Loss: 10099.2509765625
Epoch 41700, Loss: 10099.2509765625
Epoch 41800, Loss: 10099.2509765625
Epoch 41900, Loss: 10099.2509765625
Epoch 42000, Loss: 10099.251953125
Epoch 42100, Loss: 10099.251953125
Epoch 42200, Loss: 10099.2509765625
Epoch 42300, Loss: 10099.2509765625
Epoch 42400, Loss: 10099.2509765625
Epoch 42500, Loss: 10099.2509765625
Epoch 42600, Loss: 10099.251953125
Epoch 42700, Loss: 10099.2509765625
Epoch 42800, Loss: 10099.2509765625
Epoch 42900, Loss: 10099.2509765625
Epoch 43000, Loss: 10099.251953125
Epoch 43100, Loss: 10099.2509765625
Epoch 43200, Loss: 10099.2509765625
Epoch 43300, Loss: 10099.2509765625
Epoch 43400, Loss: 10099.251953125
Epoch 43500, Loss: 10099.251953125
Epoch 43600, Loss: 10099.251953125
Epoch 43700, Loss: 10099.251953125
Epoch 43800, Loss: 10099.2509765625
Epoch 43900, Loss: 10099.2509765625
Epoch 44000, Loss: 10099.2509765625
Epoch 44100, Loss: 10099.251953125
Epoch 44200, Loss: 10099.2509765625
Epoch 44300, Loss: 10099.251953125
Epoch 44400, Loss: 10099.2529296875
Epoch 44500, Loss: 10099.251953125
Epoch 44600, Loss: 10099.251953125
Epoch 44700, Loss: 10099.251953125
Epoch 44800, Loss: 10099.2509765625
Epoch 44900, Loss: 10099.2509765625
Epoch 45000, Loss: 10099.251953125
Epoch 45100, Loss: 10099.251953125
Epoch 45200, Loss: 10099.251953125
Epoch 45300, Loss: 10099.251953125
Epoch 45400, Loss: 10099.2509765625
Epoch 45500, Loss: 10099.251953125
Epoch 45600, Loss: 10099.251953125
Epoch 45700, Loss: 10099.2509765625
Epoch 45800, Loss: 10099.251953125
Epoch 45900, Loss: 10099.2509765625
Epoch 46000, Loss: 10099.2509765625
Epoch 46100, Loss: 10099.251953125
Epoch 46200, Loss: 10099.251953125
Epoch 46300, Loss: 10099.2509765625
Epoch 46400, Loss: 10099.251953125
Epoch 46500, Loss: 10099.251953125
Epoch 46600, Loss: 10099.2509765625
Epoch 46700, Loss: 10099.2509765625
Epoch 46800, Loss: 10099.251953125
Epoch 46900, Loss: 10099.2509765625
Epoch 47000, Loss: 10099.2529296875
Epoch 47100, Loss: 10099.2509765625
Epoch 47200, Loss: 10099.251953125
Epoch 47300, Loss: 10099.251953125
Epoch 47400, Loss: 10099.2529296875
Epoch 47500, Loss: 10099.251953125
Epoch 47600, Loss: 10099.251953125
Epoch 47700, Loss: 10099.2509765625
Epoch 47800, Loss: 10099.2509765625
Epoch 47900, Loss: 10099.2509765625
Epoch 48000, Loss: 10099.251953125
Epoch 48100, Loss: 10099.251953125
Epoch 48200, Loss: 10099.251953125
Epoch 48300, Loss: 10099.2509765625
Epoch 48400, Loss: 10099.251953125
Epoch 48500, Loss: 10099.251953125
Epoch 48600, Loss: 10099.251953125
Epoch 48700, Loss: 10099.251953125
Epoch 48800, Loss: 10099.251953125
Epoch 48900, Loss: 10099.251953125
Epoch 49000, Loss: 10099.2509765625
Epoch 49100, Loss: 10099.251953125
Epoch 49200, Loss: 10099.251953125
Epoch 49300, Loss: 10099.2509765625
Epoch 49400, Loss: 10099.251953125
Epoch 49500, Loss: 10099.2509765625
Epoch 49600, Loss: 10099.2509765625
Epoch 49700, Loss: 10099.2509765625
Epoch 49800, Loss: 10099.2529296875
Epoch 49900, Loss: 10099.251953125
Epoch 0, Loss: 63828.96875
Epoch 100, Loss: 60926.1484375
Epoch 200, Loss: 58211.88671875
Epoch 300, Loss: 55672.3671875
Epoch 400, Loss: 53294.84765625
Epoch 500, Loss: 51068.2578125
Epoch 600, Loss: 48983.70703125
Epoch 700, Loss: 47032.58203125
Epoch 800, Loss: 45207.171875
Epoch 900, Loss: 43500.515625
Epoch 1000, Loss: 41905.71484375
Epoch 1100, Loss: 40415.9296875
Epoch 1200, Loss: 39024.76953125
Epoch 1300, Loss: 37726.08203125
Epoch 1400, Loss: 36514.0390625
Epoch 1500, Loss: 35383.23828125
Epoch 1600, Loss: 34327.8203125
Epoch 1700, Loss: 33342.5078125
Epoch 1800, Loss: 32422.3359375
Epoch 1900, Loss: 31562.625
Epoch 2000, Loss: 30759.02734375
Epoch 2100, Loss: 30007.494140625
Epoch 2200, Loss: 29304.654296875
Epoch 2300, Loss: 28646.591796875
Epoch 2400, Loss: 28030.66796875
Epoch 2500, Loss: 27453.330078125
Epoch 2600, Loss: 26911.5703125
Epoch 2700, Loss: 26402.76171875
Epoch 2800, Loss: 25924.451171875
Epoch 2900, Loss: 25474.365234375
Epoch 3000, Loss: 25050.384765625
Epoch 3100, Loss: 24650.51953125
Epoch 3200, Loss: 24272.900390625
Epoch 3300, Loss: 23915.908203125
Epoch 3400, Loss: 23577.94140625
Epoch 3500, Loss: 23257.599609375
Epoch 3600, Loss: 22953.33203125
Epoch 3700, Loss: 22663.470703125
Epoch 3800, Loss: 22386.732421875
Epoch 3900, Loss: 22121.9296875
Epoch 4000, Loss: 21867.9765625
Epoch 4100, Loss: 21624.158203125
Epoch 4200, Loss: 21389.330078125
Epoch 4300, Loss: 21162.603515625
Epoch 4400, Loss: 20943.2734375
Epoch 4500, Loss: 20730.9296875
Epoch 4600, Loss: 20524.716796875
Epoch 4700, Loss: 20324.142578125
Epoch 4800, Loss: 20128.765625
Epoch 4900, Loss: 19938.2109375
Epoch 5000, Loss: 19752.15625
Epoch 5100, Loss: 19570.314453125
Epoch 5200, Loss: 19392.4375
Epoch 5300, Loss: 19218.3125
Epoch 5400, Loss: 19047.748046875
Epoch 5500, Loss: 18880.57421875
Epoch 5600, Loss: 18716.63671875
Epoch 5700, Loss: 18555.80859375
Epoch 5800, Loss: 18397.962890625
Epoch 5900, Loss: 18242.986328125
Epoch 6000, Loss: 18090.978515625
Epoch 6100, Loss: 17941.693359375
Epoch 6200, Loss: 17794.9921875
Epoch 6300, Loss: 17650.80078125
Epoch 6400, Loss: 17509.048828125
Epoch 6500, Loss: 17369.66796875
Epoch 6600, Loss: 17232.6015625
Epoch 6700, Loss: 17097.791015625
Epoch 6800, Loss: 16965.185546875
Epoch 6900, Loss: 16834.73046875
Epoch 7000, Loss: 16706.384765625
Epoch 7100, Loss: 16580.1171875
Epoch 7200, Loss: 16456.041015625
Epoch 7300, Loss: 16334.1064453125
Epoch 7400, Loss: 16214.23046875
Epoch 7500, Loss: 16096.341796875
Epoch 7600, Loss: 15980.26953125
Epoch 7700, Loss: 15865.978515625
Epoch 7800, Loss: 15753.419921875
Epoch 7900, Loss: 15642.56640625
Epoch 8000, Loss: 15533.376953125
Epoch 8100, Loss: 15425.8232421875
Epoch 8200, Loss: 15319.869140625
Epoch 8300, Loss: 15215.4912109375
Epoch 8400, Loss: 15112.65234375
Epoch 8500, Loss: 15011.322265625
Epoch 8600, Loss: 14911.4853515625
Epoch 8700, Loss: 14813.1044921875
Epoch 8800, Loss: 14716.166015625
Epoch 8900, Loss: 14620.6474609375
Epoch 9000, Loss: 14526.5322265625
Epoch 9100, Loss: 14433.796875
Epoch 9200, Loss: 14342.662109375
Epoch 9300, Loss: 14252.9091796875
Epoch 9400, Loss: 14164.4990234375
Epoch 9500, Loss: 14077.4150390625
Epoch 9600, Loss: 13991.6474609375
Epoch 9700, Loss: 13907.185546875
Epoch 9800, Loss: 13824.0234375
Epoch 9900, Loss: 13742.1474609375
Epoch 10000, Loss: 13661.552734375
Epoch 10100, Loss: 13582.232421875
Epoch 10200, Loss: 13504.1845703125
Epoch 10300, Loss: 13427.3984375
Epoch 10400, Loss: 13351.8720703125
Epoch 10500, Loss: 13277.59765625
Epoch 10600, Loss: 13204.57421875
Epoch 10700, Loss: 13132.7919921875
Epoch 10800, Loss: 13062.2451171875
Epoch 10900, Loss: 12992.9248046875
Epoch 11000, Loss: 12924.8251953125
Epoch 11100, Loss: 12857.93359375
Epoch 11200, Loss: 12792.240234375
Epoch 11300, Loss: 12727.73046875
Epoch 11400, Loss: 12664.40234375
Epoch 11500, Loss: 12602.271484375
Epoch 11600, Loss: 12541.294921875
Epoch 11700, Loss: 12481.447265625
Epoch 11800, Loss: 12422.705078125
Epoch 11900, Loss: 12365.048828125
Epoch 12000, Loss: 12308.4580078125
Epoch 12100, Loss: 12252.916015625
Epoch 12200, Loss: 12198.5341796875
Epoch 12300, Loss: 12145.3046875
Epoch 12400, Loss: 12093.0703125
Epoch 12500, Loss: 12041.8056640625
Epoch 12600, Loss: 11991.48828125
Epoch 12700, Loss: 11942.1083984375
Epoch 12800, Loss: 11893.640625
Epoch 12900, Loss: 11846.0732421875
Epoch 13000, Loss: 11799.396484375
Epoch 13100, Loss: 11753.587890625
Epoch 13200, Loss: 11708.640625
Epoch 13300, Loss: 11664.5419921875
Epoch 13400, Loss: 11621.2783203125
Epoch 13500, Loss: 11578.9033203125
Epoch 13600, Loss: 11537.4306640625
Epoch 13700, Loss: 11496.7724609375
Epoch 13800, Loss: 11456.9072265625
Epoch 13900, Loss: 11417.8271484375
Epoch 14000, Loss: 11379.517578125
Epoch 14100, Loss: 11342.0322265625
Epoch 14200, Loss: 11305.494140625
Epoch 14300, Loss: 11269.791015625
Epoch 14400, Loss: 11234.8681640625
Epoch 14500, Loss: 11200.6826171875
Epoch 14600, Loss: 11167.22265625
Epoch 14700, Loss: 11134.5751953125
Epoch 14800, Loss: 11102.6865234375
Epoch 14900, Loss: 11071.509765625
Epoch 15000, Loss: 11041.0205078125
Epoch 15100, Loss: 11011.203125
Epoch 15200, Loss: 10982.0654296875
Epoch 15300, Loss: 10953.6103515625
Epoch 15400, Loss: 10925.8310546875
Epoch 15500, Loss: 10898.8203125
Epoch 15600, Loss: 10872.52734375
Epoch 15700, Loss: 10846.892578125
Epoch 15800, Loss: 10821.9072265625
Epoch 15900, Loss: 10797.5712890625
Epoch 16000, Loss: 10773.8955078125
Epoch 16100, Loss: 10750.8642578125
Epoch 16200, Loss: 10728.4658203125
Epoch 16300, Loss: 10706.685546875
Epoch 16400, Loss: 10685.4833984375
Epoch 16500, Loss: 10664.8447265625
Epoch 16600, Loss: 10644.7568359375
Epoch 16700, Loss: 10625.2158203125
Epoch 16800, Loss: 10606.20703125
Epoch 16900, Loss: 10587.728515625
Epoch 17000, Loss: 10569.7646484375
Epoch 17100, Loss: 10552.3251953125
Epoch 17200, Loss: 10535.4462890625
Epoch 17300, Loss: 10519.0751953125
Epoch 17400, Loss: 10503.1953125
Epoch 17500, Loss: 10487.79296875
Epoch 17600, Loss: 10472.869140625
Epoch 17700, Loss: 10458.41015625
Epoch 17800, Loss: 10444.4130859375
Epoch 17900, Loss: 10430.8994140625
Epoch 18000, Loss: 10417.8486328125
Epoch 18100, Loss: 10405.2470703125
Epoch 18200, Loss: 10393.0908203125
Epoch 18300, Loss: 10381.3720703125
Epoch 18400, Loss: 10370.0869140625
Epoch 18500, Loss: 10359.2509765625
Epoch 18600, Loss: 10348.853515625
Epoch 18700, Loss: 10338.890625
Epoch 18800, Loss: 10329.3505859375
Epoch 18900, Loss: 10320.2275390625
Epoch 19000, Loss: 10311.513671875
Epoch 19100, Loss: 10303.201171875
Epoch 19200, Loss: 10295.27734375
Epoch 19300, Loss: 10287.73046875
Epoch 19400, Loss: 10280.5458984375
Epoch 19500, Loss: 10273.73828125
Epoch 19600, Loss: 10267.291015625
Epoch 19700, Loss: 10261.26171875
Epoch 19800, Loss: 10255.6044921875
Epoch 19900, Loss: 10250.28125
Epoch 20000, Loss: 10245.2802734375
Epoch 20100, Loss: 10240.5849609375
Epoch 20200, Loss: 10236.1884765625
Epoch 20300, Loss: 10232.076171875
Epoch 20400, Loss: 10228.2392578125
Epoch 20500, Loss: 10224.666015625
Epoch 20600, Loss: 10221.34375
Epoch 20700, Loss: 10218.2626953125
Epoch 20800, Loss: 10215.4072265625
Epoch 20900, Loss: 10212.7646484375
Epoch 21000, Loss: 10210.3193359375
Epoch 21100, Loss: 10208.0615234375
Epoch 21200, Loss: 10205.974609375
Epoch 21300, Loss: 10204.0556640625
Epoch 21400, Loss: 10202.2890625
Epoch 21500, Loss: 10200.6650390625
Epoch 21600, Loss: 10199.169921875
Epoch 21700, Loss: 10197.7919921875
Epoch 21800, Loss: 10196.521484375
Epoch 21900, Loss: 10195.349609375
Epoch 22000, Loss: 10194.2646484375
Epoch 22100, Loss: 10193.259765625
Epoch 22200, Loss: 10192.322265625
Epoch 22300, Loss: 10191.447265625
Epoch 22400, Loss: 10190.625
Epoch 22500, Loss: 10189.849609375
Epoch 22600, Loss: 10189.11328125
Epoch 22700, Loss: 10188.408203125
Epoch 22800, Loss: 10187.732421875
Epoch 22900, Loss: 10187.078125
Epoch 23000, Loss: 10186.4404296875
Epoch 23100, Loss: 10185.81640625
Epoch 23200, Loss: 10185.19921875
Epoch 23300, Loss: 10184.5888671875
Epoch 23400, Loss: 10183.9775390625
Epoch 23500, Loss: 10183.3642578125
Epoch 23600, Loss: 10182.7490234375
Epoch 23700, Loss: 10182.125
Epoch 23800, Loss: 10181.490234375
Epoch 23900, Loss: 10180.8466796875
Epoch 24000, Loss: 10180.1884765625
Epoch 24100, Loss: 10179.515625
Epoch 24200, Loss: 10178.8291015625
Epoch 24300, Loss: 10178.1806640625
Epoch 24400, Loss: 10177.5517578125
Epoch 24500, Loss: 10176.919921875
Epoch 24600, Loss: 10176.28125
Epoch 24700, Loss: 10175.638671875
Epoch 24800, Loss: 10175.0087890625
Epoch 24900, Loss: 10174.3984375
Epoch 25000, Loss: 10173.802734375
Epoch 25100, Loss: 10173.2197265625
Epoch 25200, Loss: 10172.6572265625
Epoch 25300, Loss: 10172.1064453125
Epoch 25400, Loss: 10171.568359375
Epoch 25500, Loss: 10171.029296875
Epoch 25600, Loss: 10170.4775390625
Epoch 25700, Loss: 10169.91015625
Epoch 25800, Loss: 10169.328125
Epoch 25900, Loss: 10168.7314453125
Epoch 26000, Loss: 10168.1181640625
Epoch 26100, Loss: 10167.48828125
Epoch 26200, Loss: 10166.84375
Epoch 26300, Loss: 10166.18359375
Epoch 26400, Loss: 10165.5107421875
Epoch 26500, Loss: 10164.841796875
Epoch 26600, Loss: 10164.2119140625
Epoch 26700, Loss: 10163.6123046875
Epoch 26800, Loss: 10163.05078125
Epoch 26900, Loss: 10162.5439453125
Epoch 27000, Loss: 10162.107421875
Epoch 27100, Loss: 10161.728515625
Epoch 27200, Loss: 10161.384765625
Epoch 27300, Loss: 10161.0703125
Epoch 27400, Loss: 10160.7666015625
Epoch 27500, Loss: 10160.45703125
Epoch 27600, Loss: 10160.1376953125
Epoch 27700, Loss: 10159.810546875
Epoch 27800, Loss: 10159.474609375
Epoch 27900, Loss: 10159.1279296875
Epoch 28000, Loss: 10158.7724609375
Epoch 28100, Loss: 10158.4072265625
Epoch 28200, Loss: 10158.0322265625
Epoch 28300, Loss: 10157.646484375
Epoch 28400, Loss: 10157.2548828125
Epoch 28500, Loss: 10156.8515625
Epoch 28600, Loss: 10156.443359375
Epoch 28700, Loss: 10156.0693359375
Epoch 28800, Loss: 10155.7529296875
Epoch 28900, Loss: 10155.48828125
Epoch 29000, Loss: 10155.26171875
Epoch 29100, Loss: 10155.060546875
Epoch 29200, Loss: 10154.8818359375
Epoch 29300, Loss: 10154.732421875
Epoch 29400, Loss: 10154.603515625
Epoch 29500, Loss: 10154.4970703125
Epoch 29600, Loss: 10154.416015625
Epoch 29700, Loss: 10154.3564453125
Epoch 29800, Loss: 10154.3115234375
Epoch 29900, Loss: 10154.2763671875
Epoch 30000, Loss: 10154.2470703125
Epoch 30100, Loss: 10154.220703125
Epoch 30200, Loss: 10154.201171875
Epoch 30300, Loss: 10154.18359375
Epoch 30400, Loss: 10154.169921875
Epoch 30500, Loss: 10154.158203125
Epoch 30600, Loss: 10154.1474609375
Epoch 30700, Loss: 10154.1396484375
Epoch 30800, Loss: 10154.1337890625
Epoch 30900, Loss: 10154.1279296875
Epoch 31000, Loss: 10154.125
Epoch 31100, Loss: 10154.119140625
Epoch 31200, Loss: 10154.1171875
Epoch 31300, Loss: 10154.115234375
Epoch 31400, Loss: 10154.1123046875
Epoch 31500, Loss: 10154.1103515625
Epoch 31600, Loss: 10154.1083984375
Epoch 31700, Loss: 10154.107421875
Epoch 31800, Loss: 10154.10546875
Epoch 31900, Loss: 10154.10546875
Epoch 32000, Loss: 10154.1044921875
Epoch 32100, Loss: 10154.103515625
Epoch 32200, Loss: 10154.103515625
Epoch 32300, Loss: 10154.1025390625
Epoch 32400, Loss: 10154.1025390625
Epoch 32500, Loss: 10154.1015625
Epoch 32600, Loss: 10154.1025390625
Epoch 32700, Loss: 10154.1015625
Epoch 32800, Loss: 10154.1005859375
Epoch 32900, Loss: 10154.1005859375
Epoch 33000, Loss: 10154.1005859375
Epoch 33100, Loss: 10154.1005859375
Epoch 33200, Loss: 10154.099609375
Epoch 33300, Loss: 10154.1005859375
Epoch 33400, Loss: 10154.0986328125
Epoch 33500, Loss: 10154.1005859375
Epoch 33600, Loss: 10154.1005859375
Epoch 33700, Loss: 10154.1005859375
Epoch 33800, Loss: 10154.1005859375
Epoch 33900, Loss: 10154.1005859375
Epoch 34000, Loss: 10154.099609375
Epoch 34100, Loss: 10154.099609375
Epoch 34200, Loss: 10154.1005859375
Epoch 34300, Loss: 10154.099609375
Epoch 34400, Loss: 10154.1005859375
Epoch 34500, Loss: 10154.1005859375
Epoch 34600, Loss: 10154.1005859375
Epoch 34700, Loss: 10154.1005859375
Epoch 34800, Loss: 10154.1005859375
Epoch 34900, Loss: 10154.1005859375
Epoch 35000, Loss: 10154.1015625
Epoch 35100, Loss: 10154.1005859375
Epoch 35200, Loss: 10154.099609375
Epoch 35300, Loss: 10154.1005859375
Epoch 35400, Loss: 10154.1005859375
Epoch 35500, Loss: 10154.1005859375
Epoch 35600, Loss: 10154.1005859375
Epoch 35700, Loss: 10154.1005859375
Epoch 35800, Loss: 10154.099609375
Epoch 35900, Loss: 10154.0986328125
Epoch 36000, Loss: 10154.099609375
Epoch 36100, Loss: 10154.099609375
Epoch 36200, Loss: 10154.1005859375
Epoch 36300, Loss: 10154.1005859375
Epoch 36400, Loss: 10154.099609375
Epoch 36500, Loss: 10154.1005859375
Epoch 36600, Loss: 10154.099609375
Epoch 36700, Loss: 10154.1005859375
Epoch 36800, Loss: 10154.099609375
Epoch 36900, Loss: 10154.099609375
Epoch 37000, Loss: 10154.1005859375
Epoch 37100, Loss: 10154.0986328125
Epoch 37200, Loss: 10154.099609375
Epoch 37300, Loss: 10154.099609375
Epoch 37400, Loss: 10154.1005859375
Epoch 37500, Loss: 10154.0986328125
Epoch 37600, Loss: 10154.099609375
Epoch 37700, Loss: 10154.099609375
Epoch 37800, Loss: 10154.099609375
Epoch 37900, Loss: 10154.1005859375
Epoch 38000, Loss: 10154.099609375
Epoch 38100, Loss: 10154.1005859375
Epoch 38200, Loss: 10154.1005859375
Epoch 38300, Loss: 10154.1005859375
Epoch 38400, Loss: 10154.1005859375
Epoch 38500, Loss: 10154.1005859375
Epoch 38600, Loss: 10154.099609375
Epoch 38700, Loss: 10154.1005859375
Epoch 38800, Loss: 10154.1005859375
Epoch 38900, Loss: 10154.099609375
Epoch 39000, Loss: 10154.1005859375
Epoch 39100, Loss: 10154.1005859375
Epoch 39200, Loss: 10154.1005859375
Epoch 39300, Loss: 10154.1005859375
Epoch 39400, Loss: 10154.099609375
Epoch 39500, Loss: 10154.1005859375
Epoch 39600, Loss: 10154.099609375
Epoch 39700, Loss: 10154.099609375
Epoch 39800, Loss: 10154.099609375
Epoch 39900, Loss: 10154.1005859375
Epoch 40000, Loss: 10154.099609375
Epoch 40100, Loss: 10154.099609375
Epoch 40200, Loss: 10154.1005859375
Epoch 40300, Loss: 10154.099609375
Epoch 40400, Loss: 10154.099609375
Epoch 40500, Loss: 10154.1005859375
Epoch 40600, Loss: 10154.0986328125
Epoch 40700, Loss: 10154.1005859375
Epoch 40800, Loss: 10154.1005859375
Epoch 40900, Loss: 10154.099609375
Epoch 41000, Loss: 10154.099609375
Epoch 41100, Loss: 10154.099609375
Epoch 41200, Loss: 10154.1005859375
Epoch 41300, Loss: 10154.1005859375
Epoch 41400, Loss: 10154.1015625
Epoch 41500, Loss: 10154.1005859375
Epoch 41600, Loss: 10154.0986328125
Epoch 41700, Loss: 10154.099609375
Epoch 41800, Loss: 10154.1005859375
Epoch 41900, Loss: 10154.099609375
Epoch 42000, Loss: 10154.0986328125
Epoch 42100, Loss: 10154.099609375
Epoch 42200, Loss: 10154.099609375
Epoch 42300, Loss: 10154.1005859375
Epoch 42400, Loss: 10154.1005859375
Epoch 42500, Loss: 10154.099609375
Epoch 42600, Loss: 10154.1005859375
Epoch 42700, Loss: 10154.099609375
Epoch 42800, Loss: 10154.099609375
Epoch 42900, Loss: 10154.1005859375
Epoch 43000, Loss: 10154.1005859375
Epoch 43100, Loss: 10154.099609375
Epoch 43200, Loss: 10154.099609375
Epoch 43300, Loss: 10154.099609375
Epoch 43400, Loss: 10154.1005859375
Epoch 43500, Loss: 10154.1005859375
Epoch 43600, Loss: 10154.099609375
Epoch 43700, Loss: 10154.099609375
Epoch 43800, Loss: 10154.1005859375
Epoch 43900, Loss: 10154.1005859375
Epoch 44000, Loss: 10154.099609375
Epoch 44100, Loss: 10154.1005859375
Epoch 44200, Loss: 10154.099609375
Epoch 44300, Loss: 10154.1005859375
Epoch 44400, Loss: 10154.1005859375
Epoch 44500, Loss: 10154.099609375
Epoch 44600, Loss: 10154.1005859375
Epoch 44700, Loss: 10154.099609375
Epoch 44800, Loss: 10154.1005859375
Epoch 44900, Loss: 10154.1005859375
Epoch 45000, Loss: 10154.099609375
Epoch 45100, Loss: 10154.1005859375
Epoch 45200, Loss: 10154.1005859375
Epoch 45300, Loss: 10154.1005859375
Epoch 45400, Loss: 10154.1005859375
Epoch 45500, Loss: 10154.0986328125
Epoch 45600, Loss: 10154.099609375
Epoch 45700, Loss: 10154.1005859375
Epoch 45800, Loss: 10154.1005859375
Epoch 45900, Loss: 10154.099609375
Epoch 46000, Loss: 10154.099609375
Epoch 46100, Loss: 10154.1005859375
Epoch 46200, Loss: 10154.1005859375
Epoch 46300, Loss: 10154.099609375
Epoch 46400, Loss: 10154.099609375
Epoch 46500, Loss: 10154.099609375
Epoch 46600, Loss: 10154.099609375
Epoch 46700, Loss: 10154.1005859375
Epoch 46800, Loss: 10154.1015625
Epoch 46900, Loss: 10154.1005859375
Epoch 47000, Loss: 10154.1005859375
Epoch 47100, Loss: 10154.1005859375
Epoch 47200, Loss: 10154.1005859375
Epoch 47300, Loss: 10154.099609375
Epoch 47400, Loss: 10154.1005859375
Epoch 47500, Loss: 10154.099609375
Epoch 47600, Loss: 10154.099609375
Epoch 47700, Loss: 10154.099609375
Epoch 47800, Loss: 10154.099609375
Epoch 47900, Loss: 10154.099609375
Epoch 48000, Loss: 10154.1005859375
Epoch 48100, Loss: 10154.099609375
Epoch 48200, Loss: 10154.1005859375
Epoch 48300, Loss: 10154.099609375
Epoch 48400, Loss: 10154.099609375
Epoch 48500, Loss: 10154.099609375
Epoch 48600, Loss: 10154.099609375
Epoch 48700, Loss: 10154.1005859375
Epoch 48800, Loss: 10154.1005859375
Epoch 48900, Loss: 10154.099609375
Epoch 49000, Loss: 10154.1005859375
Epoch 49100, Loss: 10154.0986328125
Epoch 49200, Loss: 10154.1005859375
Epoch 49300, Loss: 10154.099609375
Epoch 49400, Loss: 10154.099609375
Epoch 49500, Loss: 10154.099609375
Epoch 49600, Loss: 10154.099609375
Epoch 49700, Loss: 10154.1005859375
Epoch 49800, Loss: 10154.0986328125
Epoch 49900, Loss: 10154.1005859375
Epoch 0, Loss: 63828.96875
Epoch 100, Loss: 60927.3984375
Epoch 200, Loss: 58214.3125
Epoch 300, Loss: 55675.796875
Epoch 400, Loss: 53299.29296875
Epoch 500, Loss: 51073.7109375
Epoch 600, Loss: 48990.171875
Epoch 700, Loss: 47039.97265625
Epoch 800, Loss: 45215.41015625
Epoch 900, Loss: 43509.640625
Epoch 1000, Loss: 41915.6953125
Epoch 1100, Loss: 40426.69921875
Epoch 1200, Loss: 39036.26953125
Epoch 1300, Loss: 37738.25
Epoch 1400, Loss: 36526.8828125
Epoch 1500, Loss: 35396.7421875
Epoch 1600, Loss: 34341.9375
Epoch 1700, Loss: 33357.19140625
Epoch 1800, Loss: 32437.53125
Epoch 1900, Loss: 31578.298828125
Epoch 2000, Loss: 30775.13671875
Epoch 2100, Loss: 30024.052734375
Epoch 2200, Loss: 29321.71484375
Epoch 2300, Loss: 28664.1328125
Epoch 2400, Loss: 28048.8828125
Epoch 2500, Loss: 27472.16796875
Epoch 2600, Loss: 26931.01171875
Epoch 2700, Loss: 26422.76953125
Epoch 2800, Loss: 25945.00390625
Epoch 2900, Loss: 25495.44140625
Epoch 3000, Loss: 25071.953125
Epoch 3100, Loss: 24672.564453125
Epoch 3200, Loss: 24295.404296875
Epoch 3300, Loss: 23938.88671875
Epoch 3400, Loss: 23601.447265625
Epoch 3500, Loss: 23281.728515625
Epoch 3600, Loss: 22978.103515625
Epoch 3700, Loss: 22688.861328125
Epoch 3800, Loss: 22412.7265625
Epoch 3900, Loss: 22148.5078125
Epoch 4000, Loss: 21895.115234375
Epoch 4100, Loss: 21651.95703125
Epoch 4200, Loss: 21417.748046875
Epoch 4300, Loss: 21191.619140625
Epoch 4400, Loss: 20972.900390625
Epoch 4500, Loss: 20761.197265625
Epoch 4600, Loss: 20555.611328125
Epoch 4700, Loss: 20355.640625
Epoch 4800, Loss: 20160.8515625
Epoch 4900, Loss: 19970.875
Epoch 5000, Loss: 19785.376953125
Epoch 5100, Loss: 19604.08203125
Epoch 5200, Loss: 19426.73828125
Epoch 5300, Loss: 19253.1328125
Epoch 5400, Loss: 19083.078125
Epoch 5500, Loss: 18916.400390625
Epoch 5600, Loss: 18752.951171875
Epoch 5700, Loss: 18592.595703125
Epoch 5800, Loss: 18435.21484375
Epoch 5900, Loss: 18280.703125
Epoch 6000, Loss: 18129.2421875
Epoch 6100, Loss: 17980.447265625
Epoch 6200, Loss: 17834.224609375
Epoch 6300, Loss: 17690.494140625
Epoch 6400, Loss: 17549.1953125
Epoch 6500, Loss: 17410.25390625
Epoch 6600, Loss: 17273.615234375
Epoch 6700, Loss: 17139.22265625
Epoch 6800, Loss: 17007.021484375
Epoch 6900, Loss: 16876.966796875
Epoch 7000, Loss: 16749.00390625
Epoch 7100, Loss: 16623.142578125
Epoch 7200, Loss: 16499.55078125
Epoch 7300, Loss: 16378.0791015625
Epoch 7400, Loss: 16258.65234375
Epoch 7500, Loss: 16141.244140625
Epoch 7600, Loss: 16025.62890625
Epoch 7700, Loss: 15911.7734375
Epoch 7800, Loss: 15799.6416015625
Epoch 7900, Loss: 15689.197265625
Epoch 8000, Loss: 15580.400390625
Epoch 8100, Loss: 15473.234375
Epoch 8200, Loss: 15367.6572265625
Epoch 8300, Loss: 15263.6328125
Epoch 8400, Loss: 15161.1337890625
Epoch 8500, Loss: 15060.1416015625
Epoch 8600, Loss: 14960.6220703125
Epoch 8700, Loss: 14862.5556640625
Epoch 8800, Loss: 14765.9189453125
Epoch 8900, Loss: 14670.693359375
Epoch 9000, Loss: 14576.85546875
Epoch 9100, Loss: 14484.53515625
Epoch 9200, Loss: 14393.7451171875
Epoch 9300, Loss: 14304.3046875
Epoch 9400, Loss: 14216.2080078125
Epoch 9500, Loss: 14129.4306640625
Epoch 9600, Loss: 14043.96484375
Epoch 9700, Loss: 13959.796875
Epoch 9800, Loss: 13876.91796875
Epoch 9900, Loss: 13795.3232421875
Epoch 10000, Loss: 13715.001953125
Epoch 10100, Loss: 13635.9482421875
Epoch 10200, Loss: 13558.158203125
Epoch 10300, Loss: 13481.6279296875
Epoch 10400, Loss: 13406.349609375
Epoch 10500, Loss: 13332.318359375
Epoch 10600, Loss: 13259.53125
Epoch 10700, Loss: 13187.9833984375
Epoch 10800, Loss: 13117.6650390625
Epoch 10900, Loss: 13048.5693359375
Epoch 11000, Loss: 12980.6806640625
Epoch 11100, Loss: 12913.9990234375
Epoch 11200, Loss: 12848.5048828125
Epoch 11300, Loss: 12784.1904296875
Epoch 11400, Loss: 12721.0791015625
Epoch 11500, Loss: 12659.138671875
Epoch 11600, Loss: 12598.3388671875
Epoch 11700, Loss: 12538.66015625
Epoch 11800, Loss: 12480.0810546875
Epoch 11900, Loss: 12422.580078125
Epoch 12000, Loss: 12366.134765625
Epoch 12100, Loss: 12310.7294921875
Epoch 12200, Loss: 12256.62109375
Epoch 12300, Loss: 12203.5712890625
Epoch 12400, Loss: 12151.5126953125
Epoch 12500, Loss: 12100.41015625
Epoch 12600, Loss: 12050.255859375
Epoch 12700, Loss: 12001.0185546875
Epoch 12800, Loss: 11952.689453125
Epoch 12900, Loss: 11905.25390625
Epoch 13000, Loss: 11858.69140625
Epoch 13100, Loss: 11813.0263671875
Epoch 13200, Loss: 11768.35546875
Epoch 13300, Loss: 11724.56640625
Epoch 13400, Loss: 11681.609375
Epoch 13500, Loss: 11639.4736328125
Epoch 13600, Loss: 11598.1533203125
Epoch 13700, Loss: 11557.6279296875
Epoch 13800, Loss: 11517.8876953125
Epoch 13900, Loss: 11478.9287109375
Epoch 14000, Loss: 11440.736328125
Epoch 14100, Loss: 11403.5126953125
Epoch 14200, Loss: 11367.1533203125
Epoch 14300, Loss: 11331.6240234375
Epoch 14400, Loss: 11296.8798828125
Epoch 14500, Loss: 11262.8740234375
Epoch 14600, Loss: 11229.6328125
Epoch 14700, Loss: 11197.1806640625
Epoch 14800, Loss: 11165.4765625
Epoch 14900, Loss: 11134.49609375
Epoch 15000, Loss: 11104.3017578125
Epoch 15100, Loss: 11074.8740234375
Epoch 15200, Loss: 11046.1376953125
Epoch 15300, Loss: 11018.0791015625
Epoch 15400, Loss: 10990.6865234375
Epoch 15500, Loss: 10963.9599609375
Epoch 15600, Loss: 10937.8876953125
Epoch 15700, Loss: 10912.4638671875
Epoch 15800, Loss: 10887.681640625
Epoch 15900, Loss: 10863.5537109375
Epoch 16000, Loss: 10840.0908203125
Epoch 16100, Loss: 10817.26953125
Epoch 16200, Loss: 10795.0751953125
Epoch 16300, Loss: 10773.4892578125
Epoch 16400, Loss: 10752.4970703125
Epoch 16500, Loss: 10732.064453125
Epoch 16600, Loss: 10712.18359375
Epoch 16700, Loss: 10692.908203125
Epoch 16800, Loss: 10674.1865234375
Epoch 16900, Loss: 10655.9912109375
Epoch 17000, Loss: 10638.310546875
Epoch 17100, Loss: 10621.1328125
Epoch 17200, Loss: 10604.4541015625
Epoch 17300, Loss: 10588.259765625
Epoch 17400, Loss: 10572.5498046875
Epoch 17500, Loss: 10557.3232421875
Epoch 17600, Loss: 10542.595703125
Epoch 17700, Loss: 10528.3388671875
Epoch 17800, Loss: 10514.544921875
Epoch 17900, Loss: 10501.205078125
Epoch 18000, Loss: 10488.314453125
Epoch 18100, Loss: 10475.86328125
Epoch 18200, Loss: 10463.8525390625
Epoch 18300, Loss: 10452.2763671875
Epoch 18400, Loss: 10441.125
Epoch 18500, Loss: 10430.4140625
Epoch 18600, Loss: 10420.138671875
Epoch 18700, Loss: 10410.2900390625
Epoch 18800, Loss: 10400.8623046875
Epoch 18900, Loss: 10391.84765625
Epoch 19000, Loss: 10383.234375
Epoch 19100, Loss: 10375.01953125
Epoch 19200, Loss: 10367.2802734375
Epoch 19300, Loss: 10359.98046875
Epoch 19400, Loss: 10353.08203125
Epoch 19500, Loss: 10346.5556640625
Epoch 19600, Loss: 10340.3837890625
Epoch 19700, Loss: 10334.5498046875
Epoch 19800, Loss: 10329.0419921875
Epoch 19900, Loss: 10323.8544921875
Epoch 20000, Loss: 10318.9736328125
Epoch 20100, Loss: 10314.3916015625
Epoch 20200, Loss: 10310.0966796875
Epoch 20300, Loss: 10306.0791015625
Epoch 20400, Loss: 10302.328125
Epoch 20500, Loss: 10298.83203125
Epoch 20600, Loss: 10295.5791015625
Epoch 20700, Loss: 10292.5654296875
Epoch 20800, Loss: 10289.775390625
Epoch 20900, Loss: 10287.1923828125
Epoch 21000, Loss: 10284.8017578125
Epoch 21100, Loss: 10282.5908203125
Epoch 21200, Loss: 10280.5439453125
Epoch 21300, Loss: 10278.650390625
Epoch 21400, Loss: 10276.8974609375
Epoch 21500, Loss: 10275.2734375
Epoch 21600, Loss: 10273.7666015625
Epoch 21700, Loss: 10272.3671875
Epoch 21800, Loss: 10271.0634765625
Epoch 21900, Loss: 10269.845703125
Epoch 22000, Loss: 10268.703125
Epoch 22100, Loss: 10267.6259765625
Epoch 22200, Loss: 10266.6064453125
Epoch 22300, Loss: 10265.6376953125
Epoch 22400, Loss: 10264.70703125
Epoch 22500, Loss: 10263.8095703125
Epoch 22600, Loss: 10262.94140625
Epoch 22700, Loss: 10262.0908203125
Epoch 22800, Loss: 10261.2548828125
Epoch 22900, Loss: 10260.4287109375
Epoch 23000, Loss: 10259.6044921875
Epoch 23100, Loss: 10258.783203125
Epoch 23200, Loss: 10257.9560546875
Epoch 23300, Loss: 10257.1240234375
Epoch 23400, Loss: 10256.279296875
Epoch 23500, Loss: 10255.4365234375
Epoch 23600, Loss: 10254.6494140625
Epoch 23700, Loss: 10253.8642578125
Epoch 23800, Loss: 10253.072265625
Epoch 23900, Loss: 10252.2744140625
Epoch 24000, Loss: 10251.478515625
Epoch 24100, Loss: 10250.6962890625
Epoch 24200, Loss: 10249.9267578125
Epoch 24300, Loss: 10249.171875
Epoch 24400, Loss: 10248.427734375
Epoch 24500, Loss: 10247.697265625
Epoch 24600, Loss: 10246.9794921875
Epoch 24700, Loss: 10246.2734375
Epoch 24800, Loss: 10245.5791015625
Epoch 24900, Loss: 10244.892578125
Epoch 25000, Loss: 10244.203125
Epoch 25100, Loss: 10243.494140625
Epoch 25200, Loss: 10242.7705078125
Epoch 25300, Loss: 10242.0263671875
Epoch 25400, Loss: 10241.265625
Epoch 25500, Loss: 10240.484375
Epoch 25600, Loss: 10239.685546875
Epoch 25700, Loss: 10238.8671875
Epoch 25800, Loss: 10238.0322265625
Epoch 25900, Loss: 10237.201171875
Epoch 26000, Loss: 10236.404296875
Epoch 26100, Loss: 10235.6376953125
Epoch 26200, Loss: 10234.90234375
Epoch 26300, Loss: 10234.220703125
Epoch 26400, Loss: 10233.6171875
Epoch 26500, Loss: 10233.0810546875
Epoch 26600, Loss: 10232.6005859375
Epoch 26700, Loss: 10232.1640625
Epoch 26800, Loss: 10231.7607421875
Epoch 26900, Loss: 10231.37109375
Epoch 27000, Loss: 10230.9765625
Epoch 27100, Loss: 10230.5751953125
Epoch 27200, Loss: 10230.1650390625
Epoch 27300, Loss: 10229.7412109375
Epoch 27400, Loss: 10229.306640625
Epoch 27500, Loss: 10228.8623046875
Epoch 27600, Loss: 10228.404296875
Epoch 27700, Loss: 10227.9345703125
Epoch 27800, Loss: 10227.4521484375
Epoch 27900, Loss: 10226.958984375
Epoch 28000, Loss: 10226.4560546875
Epoch 28100, Loss: 10225.9521484375
Epoch 28200, Loss: 10225.501953125
Epoch 28300, Loss: 10225.1044921875
Epoch 28400, Loss: 10224.755859375
Epoch 28500, Loss: 10224.4501953125
Epoch 28600, Loss: 10224.193359375
Epoch 28700, Loss: 10223.974609375
Epoch 28800, Loss: 10223.783203125
Epoch 28900, Loss: 10223.6123046875
Epoch 29000, Loss: 10223.4638671875
Epoch 29100, Loss: 10223.3466796875
Epoch 29200, Loss: 10223.25
Epoch 29300, Loss: 10223.17578125
Epoch 29400, Loss: 10223.1162109375
Epoch 29500, Loss: 10223.0703125
Epoch 29600, Loss: 10223.0322265625
Epoch 29700, Loss: 10223.0
Epoch 29800, Loss: 10222.97265625
Epoch 29900, Loss: 10222.9482421875
Epoch 30000, Loss: 10222.92578125
Epoch 30100, Loss: 10222.9091796875
Epoch 30200, Loss: 10222.8935546875
Epoch 30300, Loss: 10222.880859375
Epoch 30400, Loss: 10222.87109375
Epoch 30500, Loss: 10222.861328125
Epoch 30600, Loss: 10222.853515625
Epoch 30700, Loss: 10222.8486328125
Epoch 30800, Loss: 10222.8427734375
Epoch 30900, Loss: 10222.83984375
Epoch 31000, Loss: 10222.8359375
Epoch 31100, Loss: 10222.83203125
Epoch 31200, Loss: 10222.8291015625
Epoch 31300, Loss: 10222.8271484375
Epoch 31400, Loss: 10222.8251953125
Epoch 31500, Loss: 10222.8232421875
Epoch 31600, Loss: 10222.822265625
Epoch 31700, Loss: 10222.8212890625
Epoch 31800, Loss: 10222.8193359375
Epoch 31900, Loss: 10222.8193359375
Epoch 32000, Loss: 10222.8173828125
Epoch 32100, Loss: 10222.8173828125
Epoch 32200, Loss: 10222.81640625
Epoch 32300, Loss: 10222.81640625
Epoch 32400, Loss: 10222.81640625
Epoch 32500, Loss: 10222.81640625
Epoch 32600, Loss: 10222.81640625
Epoch 32700, Loss: 10222.8154296875
Epoch 32800, Loss: 10222.8154296875
Epoch 32900, Loss: 10222.8154296875
Epoch 33000, Loss: 10222.8154296875
Epoch 33100, Loss: 10222.8154296875
Epoch 33200, Loss: 10222.8154296875
Epoch 33300, Loss: 10222.8154296875
Epoch 33400, Loss: 10222.814453125
Epoch 33500, Loss: 10222.8154296875
Epoch 33600, Loss: 10222.814453125
Epoch 33700, Loss: 10222.814453125
Epoch 33800, Loss: 10222.814453125
Epoch 33900, Loss: 10222.8154296875
Epoch 34000, Loss: 10222.8154296875
Epoch 34100, Loss: 10222.814453125
Epoch 34200, Loss: 10222.8154296875
Epoch 34300, Loss: 10222.814453125
Epoch 34400, Loss: 10222.814453125
Epoch 34500, Loss: 10222.8154296875
Epoch 34600, Loss: 10222.814453125
Epoch 34700, Loss: 10222.81640625
Epoch 34800, Loss: 10222.8154296875
Epoch 34900, Loss: 10222.8154296875
Epoch 35000, Loss: 10222.8154296875
Epoch 35100, Loss: 10222.814453125
Epoch 35200, Loss: 10222.8154296875
Epoch 35300, Loss: 10222.8154296875
Epoch 35400, Loss: 10222.8154296875
Epoch 35500, Loss: 10222.8154296875
Epoch 35600, Loss: 10222.814453125
Epoch 35700, Loss: 10222.814453125
Epoch 35800, Loss: 10222.8154296875
Epoch 35900, Loss: 10222.8154296875
Epoch 36000, Loss: 10222.8154296875
Epoch 36100, Loss: 10222.8154296875
Epoch 36200, Loss: 10222.8154296875
Epoch 36300, Loss: 10222.814453125
Epoch 36400, Loss: 10222.8154296875
Epoch 36500, Loss: 10222.8154296875
Epoch 36600, Loss: 10222.814453125
Epoch 36700, Loss: 10222.8154296875
Epoch 36800, Loss: 10222.8154296875
Epoch 36900, Loss: 10222.8154296875
Epoch 37000, Loss: 10222.8154296875
Epoch 37100, Loss: 10222.8154296875
Epoch 37200, Loss: 10222.8154296875
Epoch 37300, Loss: 10222.8154296875
Epoch 37400, Loss: 10222.8154296875
Epoch 37500, Loss: 10222.814453125
Epoch 37600, Loss: 10222.8154296875
Epoch 37700, Loss: 10222.8154296875
Epoch 37800, Loss: 10222.8154296875
Epoch 37900, Loss: 10222.8154296875
Epoch 38000, Loss: 10222.8154296875
Epoch 38100, Loss: 10222.8154296875
Epoch 38200, Loss: 10222.814453125
Epoch 38300, Loss: 10222.814453125
Epoch 38400, Loss: 10222.8154296875
Epoch 38500, Loss: 10222.8154296875
Epoch 38600, Loss: 10222.8154296875
Epoch 38700, Loss: 10222.8154296875
Epoch 38800, Loss: 10222.8154296875
Epoch 38900, Loss: 10222.8154296875
Epoch 39000, Loss: 10222.814453125
Epoch 39100, Loss: 10222.8154296875
Epoch 39200, Loss: 10222.8154296875
Epoch 39300, Loss: 10222.8154296875
Epoch 39400, Loss: 10222.8154296875
Epoch 39500, Loss: 10222.8154296875
Epoch 39600, Loss: 10222.81640625
Epoch 39700, Loss: 10222.8154296875
Epoch 39800, Loss: 10222.8154296875
Epoch 39900, Loss: 10222.814453125
Epoch 40000, Loss: 10222.8154296875
Epoch 40100, Loss: 10222.8154296875
Epoch 40200, Loss: 10222.8154296875
Epoch 40300, Loss: 10222.8154296875
Epoch 40400, Loss: 10222.8154296875
Epoch 40500, Loss: 10222.8154296875
Epoch 40600, Loss: 10222.8154296875
Epoch 40700, Loss: 10222.8154296875
Epoch 40800, Loss: 10222.8154296875
Epoch 40900, Loss: 10222.814453125
Epoch 41000, Loss: 10222.814453125
Epoch 41100, Loss: 10222.8154296875
Epoch 41200, Loss: 10222.8154296875
Epoch 41300, Loss: 10222.8154296875
Epoch 41400, Loss: 10222.8154296875
Epoch 41500, Loss: 10222.8154296875
Epoch 41600, Loss: 10222.8154296875
Epoch 41700, Loss: 10222.8154296875
Epoch 41800, Loss: 10222.814453125
Epoch 41900, Loss: 10222.8154296875
Epoch 42000, Loss: 10222.8154296875
Epoch 42100, Loss: 10222.8154296875
Epoch 42200, Loss: 10222.8154296875
Epoch 42300, Loss: 10222.8154296875
Epoch 42400, Loss: 10222.8154296875
Epoch 42500, Loss: 10222.8154296875
Epoch 42600, Loss: 10222.8154296875
Epoch 42700, Loss: 10222.814453125
Epoch 42800, Loss: 10222.8154296875
Epoch 42900, Loss: 10222.8154296875
Epoch 43000, Loss: 10222.8154296875
Epoch 43100, Loss: 10222.8154296875
Epoch 43200, Loss: 10222.8154296875
Epoch 43300, Loss: 10222.8154296875
Epoch 43400, Loss: 10222.8154296875
Epoch 43500, Loss: 10222.8154296875
Epoch 43600, Loss: 10222.8154296875
Epoch 43700, Loss: 10222.814453125
Epoch 43800, Loss: 10222.8154296875
Epoch 43900, Loss: 10222.8154296875
Epoch 44000, Loss: 10222.8154296875
Epoch 44100, Loss: 10222.8154296875
Epoch 44200, Loss: 10222.8154296875
Epoch 44300, Loss: 10222.8154296875
Epoch 44400, Loss: 10222.8154296875
Epoch 44500, Loss: 10222.814453125
Epoch 44600, Loss: 10222.8154296875
Epoch 44700, Loss: 10222.8154296875
Epoch 44800, Loss: 10222.8154296875
Epoch 44900, Loss: 10222.8154296875
Epoch 45000, Loss: 10222.81640625
Epoch 45100, Loss: 10222.8154296875
Epoch 45200, Loss: 10222.814453125
Epoch 45300, Loss: 10222.8154296875
Epoch 45400, Loss: 10222.8154296875
Epoch 45500, Loss: 10222.8154296875
Epoch 45600, Loss: 10222.8154296875
Epoch 45700, Loss: 10222.8154296875
Epoch 45800, Loss: 10222.8154296875
Epoch 45900, Loss: 10222.8154296875
Epoch 46000, Loss: 10222.8154296875
Epoch 46100, Loss: 10222.814453125
Epoch 46200, Loss: 10222.8154296875
Epoch 46300, Loss: 10222.8154296875
Epoch 46400, Loss: 10222.8154296875
Epoch 46500, Loss: 10222.8154296875
Epoch 46600, Loss: 10222.814453125
Epoch 46700, Loss: 10222.8154296875
Epoch 46800, Loss: 10222.814453125
Epoch 46900, Loss: 10222.814453125
Epoch 47000, Loss: 10222.814453125
Epoch 47100, Loss: 10222.814453125
Epoch 47200, Loss: 10222.8154296875
Epoch 47300, Loss: 10222.814453125
Epoch 47400, Loss: 10222.814453125
Epoch 47500, Loss: 10222.814453125
Epoch 47600, Loss: 10222.8154296875
Epoch 47700, Loss: 10222.814453125
Epoch 47800, Loss: 10222.814453125
Epoch 47900, Loss: 10222.814453125
Epoch 48000, Loss: 10222.8154296875
Epoch 48100, Loss: 10222.814453125
Epoch 48200, Loss: 10222.814453125
Epoch 48300, Loss: 10222.814453125
Epoch 48400, Loss: 10222.814453125
Epoch 48500, Loss: 10222.814453125
Epoch 48600, Loss: 10222.814453125
Epoch 48700, Loss: 10222.814453125
Epoch 48800, Loss: 10222.8154296875
Epoch 48900, Loss: 10222.8154296875
Epoch 49000, Loss: 10222.814453125
Epoch 49100, Loss: 10222.814453125
Epoch 49200, Loss: 10222.8154296875
Epoch 49300, Loss: 10222.8154296875
Epoch 49400, Loss: 10222.814453125
Epoch 49500, Loss: 10222.814453125
Epoch 49600, Loss: 10222.814453125
Epoch 49700, Loss: 10222.814453125
Epoch 49800, Loss: 10222.814453125
Epoch 49900, Loss: 10222.8154296875
Epoch 0, Loss: 63828.96875
Epoch 100, Loss: 60928.9765625
Epoch 200, Loss: 58217.36328125
Epoch 300, Loss: 55680.1171875
Epoch 400, Loss: 53304.94921875
Epoch 500, Loss: 51080.6484375
Epoch 600, Loss: 48998.37890625
Epoch 700, Loss: 47049.35546875
Epoch 800, Loss: 45225.8671875
Epoch 900, Loss: 43521.2421875
Epoch 1000, Loss: 41928.359375
Epoch 1100, Loss: 40440.36328125
Epoch 1200, Loss: 39050.8515625
Epoch 1300, Loss: 37753.66796875
Epoch 1400, Loss: 36543.1640625
Epoch 1500, Loss: 35413.87890625
Epoch 1600, Loss: 34359.8515625
Epoch 1700, Loss: 33375.8125
Epoch 1800, Loss: 32456.810546875
Epoch 1900, Loss: 31598.1640625
Epoch 2000, Loss: 30795.546875
Epoch 2100, Loss: 30045.0625
Epoch 2200, Loss: 29343.34375
Epoch 2300, Loss: 28686.416015625
Epoch 2400, Loss: 28071.998046875
Epoch 2500, Loss: 27496.07421875
Epoch 2600, Loss: 26955.6640625
Epoch 2700, Loss: 26448.142578125
Epoch 2800, Loss: 25971.056640625
Epoch 2900, Loss: 25522.140625
Epoch 3000, Loss: 25099.27734375
Epoch 3100, Loss: 24700.482421875
Epoch 3200, Loss: 24323.896484375
Epoch 3300, Loss: 23967.974609375
Epoch 3400, Loss: 23631.208984375
Epoch 3500, Loss: 23312.28515625
Epoch 3600, Loss: 23009.4609375
Epoch 3700, Loss: 22721.0
Epoch 3800, Loss: 22445.611328125
Epoch 3900, Loss: 22182.1171875
Epoch 4000, Loss: 21929.45703125
Epoch 4100, Loss: 21687.126953125
Epoch 4200, Loss: 21453.68359375
Epoch 4300, Loss: 21228.302734375
Epoch 4400, Loss: 21010.3671875
Epoch 4500, Loss: 20799.458984375
Epoch 4600, Loss: 20594.6484375
Epoch 4700, Loss: 20395.431640625
Epoch 4800, Loss: 20201.37890625
Epoch 4900, Loss: 20012.11328125
Epoch 5000, Loss: 19827.314453125
Epoch 5100, Loss: 19646.69921875
Epoch 5200, Loss: 19470.017578125
Epoch 5300, Loss: 19297.060546875
Epoch 5400, Loss: 19127.634765625
Epoch 5500, Loss: 18961.57421875
Epoch 5600, Loss: 18798.732421875
Epoch 5700, Loss: 18638.966796875
Epoch 5800, Loss: 18482.15625
Epoch 5900, Loss: 18328.32421875
Epoch 6000, Loss: 18177.490234375
Epoch 6100, Loss: 18029.296875
Epoch 6200, Loss: 17883.662109375
Epoch 6300, Loss: 17740.5078125
Epoch 6400, Loss: 17599.763671875
Epoch 6500, Loss: 17461.3671875
Epoch 6600, Loss: 17325.259765625
Epoch 6700, Loss: 17191.38671875
Epoch 6800, Loss: 17059.6875
Epoch 6900, Loss: 16930.119140625
Epoch 7000, Loss: 16802.630859375
Epoch 7100, Loss: 16677.296875
Epoch 7200, Loss: 16554.333984375
Epoch 7300, Loss: 16433.39453125
Epoch 7400, Loss: 16314.49609375
Epoch 7500, Loss: 16197.6845703125
Epoch 7600, Loss: 16082.642578125
Epoch 7700, Loss: 15969.3525390625
Epoch 7800, Loss: 15857.755859375
Epoch 7900, Loss: 15747.8271484375
Epoch 8000, Loss: 15639.533203125
Epoch 8100, Loss: 15532.833984375
Epoch 8200, Loss: 15427.7060546875
Epoch 8300, Loss: 15324.115234375
Epoch 8400, Loss: 15222.0517578125
Epoch 8500, Loss: 15121.4638671875
Epoch 8600, Loss: 15022.33984375
Epoch 8700, Loss: 14924.6572265625
Epoch 8800, Loss: 14828.38671875
Epoch 8900, Loss: 14733.521484375
Epoch 9000, Loss: 14640.0791015625
Epoch 9100, Loss: 14548.3056640625
Epoch 9200, Loss: 14457.9111328125
Epoch 9300, Loss: 14368.859375
Epoch 9400, Loss: 14281.138671875
Epoch 9500, Loss: 14194.7333984375
Epoch 9600, Loss: 14109.6328125
Epoch 9700, Loss: 14025.8203125
Epoch 9800, Loss: 13943.2890625
Epoch 9900, Loss: 13862.0283203125
Epoch 10000, Loss: 13782.0400390625
Epoch 10100, Loss: 13703.3095703125
Epoch 10200, Loss: 13625.8388671875
Epoch 10300, Loss: 13549.6123046875
Epoch 10400, Loss: 13474.6357421875
Epoch 10500, Loss: 13400.8974609375
Epoch 10600, Loss: 13328.39453125
Epoch 10700, Loss: 13257.1181640625
Epoch 10800, Loss: 13187.0625
Epoch 10900, Loss: 13118.2177734375
Epoch 11000, Loss: 13050.5791015625
Epoch 11100, Loss: 12984.130859375
Epoch 11200, Loss: 12918.8671875
Epoch 11300, Loss: 12854.7958984375
Epoch 11400, Loss: 12791.9140625
Epoch 11500, Loss: 12730.18359375
Epoch 11600, Loss: 12669.58203125
Epoch 11700, Loss: 12610.0927734375
Epoch 11800, Loss: 12551.6943359375
Epoch 11900, Loss: 12494.3623046875
Epoch 12000, Loss: 12438.080078125
Epoch 12100, Loss: 12382.927734375
Epoch 12200, Loss: 12329.076171875
Epoch 12300, Loss: 12276.23046875
Epoch 12400, Loss: 12224.357421875
Epoch 12500, Loss: 12173.4423828125
Epoch 12600, Loss: 12123.44921875
Epoch 12700, Loss: 12074.373046875
Epoch 12800, Loss: 12026.3271484375
Epoch 12900, Loss: 11979.287109375
Epoch 13000, Loss: 11933.13671875
Epoch 13100, Loss: 11887.84765625
Epoch 13200, Loss: 11843.4033203125
Epoch 13300, Loss: 11799.796875
Epoch 13400, Loss: 11757.013671875
Epoch 13500, Loss: 11715.041015625
Epoch 13600, Loss: 11673.8681640625
Epoch 13700, Loss: 11633.4873046875
Epoch 13800, Loss: 11593.890625
Epoch 13900, Loss: 11555.0625
Epoch 14000, Loss: 11517.1484375
Epoch 14100, Loss: 11480.142578125
Epoch 14200, Loss: 11443.9775390625
Epoch 14300, Loss: 11408.6376953125
Epoch 14400, Loss: 11374.0888671875
Epoch 14500, Loss: 11340.3935546875
Epoch 14600, Loss: 11307.623046875
Epoch 14700, Loss: 11275.6494140625
Epoch 14800, Loss: 11244.4208984375
Epoch 14900, Loss: 11213.9267578125
Epoch 15000, Loss: 11184.1474609375
Epoch 15100, Loss: 11155.048828125
Epoch 15200, Loss: 11126.62109375
Epoch 15300, Loss: 11098.8583984375
Epoch 15400, Loss: 11071.751953125
Epoch 15500, Loss: 11045.2958984375
Epoch 15600, Loss: 11019.484375
Epoch 15700, Loss: 10994.3076171875
Epoch 15800, Loss: 10969.767578125
Epoch 15900, Loss: 10945.888671875
Epoch 16000, Loss: 10922.654296875
Epoch 16100, Loss: 10900.05859375
Epoch 16200, Loss: 10878.1416015625
Epoch 16300, Loss: 10856.90234375
Epoch 16400, Loss: 10836.275390625
Epoch 16500, Loss: 10816.2294921875
Epoch 16600, Loss: 10796.728515625
Epoch 16700, Loss: 10777.75390625
Epoch 16800, Loss: 10759.296875
Epoch 16900, Loss: 10741.3515625
Epoch 17000, Loss: 10723.908203125
Epoch 17100, Loss: 10706.9560546875
Epoch 17200, Loss: 10690.5107421875
Epoch 17300, Loss: 10674.5810546875
Epoch 17400, Loss: 10659.134765625
Epoch 17500, Loss: 10644.1650390625
Epoch 17600, Loss: 10629.65625
Epoch 17700, Loss: 10615.609375
Epoch 17800, Loss: 10602.01171875
Epoch 17900, Loss: 10588.86328125
Epoch 18000, Loss: 10576.154296875
Epoch 18100, Loss: 10563.8837890625
Epoch 18200, Loss: 10552.044921875
Epoch 18300, Loss: 10540.6328125
Epoch 18400, Loss: 10529.6416015625
Epoch 18500, Loss: 10519.078125
Epoch 18600, Loss: 10508.94921875
Epoch 18700, Loss: 10499.357421875
Epoch 18800, Loss: 10490.2275390625
Epoch 18900, Loss: 10481.5205078125
Epoch 19000, Loss: 10473.2177734375
Epoch 19100, Loss: 10465.32421875
Epoch 19200, Loss: 10457.8330078125
Epoch 19300, Loss: 10450.71875
Epoch 19400, Loss: 10443.978515625
Epoch 19500, Loss: 10437.5966796875
Epoch 19600, Loss: 10431.5634765625
Epoch 19700, Loss: 10425.8701171875
Epoch 19800, Loss: 10420.5029296875
Epoch 19900, Loss: 10415.451171875
Epoch 20000, Loss: 10410.70703125
Epoch 20100, Loss: 10406.26171875
Epoch 20200, Loss: 10402.0986328125
Epoch 20300, Loss: 10398.197265625
Epoch 20400, Loss: 10394.552734375
Epoch 20500, Loss: 10391.1484375
Epoch 20600, Loss: 10387.9736328125
Epoch 20700, Loss: 10385.017578125
Epoch 20800, Loss: 10382.271484375
Epoch 20900, Loss: 10379.7158203125
Epoch 21000, Loss: 10377.3447265625
Epoch 21100, Loss: 10375.138671875
Epoch 21200, Loss: 10373.0908203125
Epoch 21300, Loss: 10371.1845703125
Epoch 21400, Loss: 10369.404296875
Epoch 21500, Loss: 10367.73828125
Epoch 21600, Loss: 10366.1767578125
Epoch 21700, Loss: 10364.7041015625
Epoch 21800, Loss: 10363.30859375
Epoch 21900, Loss: 10361.982421875
Epoch 22000, Loss: 10360.7138671875
Epoch 22100, Loss: 10359.494140625
Epoch 22200, Loss: 10358.3134765625
Epoch 22300, Loss: 10357.1630859375
Epoch 22400, Loss: 10356.0361328125
Epoch 22500, Loss: 10354.9267578125
Epoch 22600, Loss: 10353.8251953125
Epoch 22700, Loss: 10352.7275390625
Epoch 22800, Loss: 10351.7001953125
Epoch 22900, Loss: 10350.705078125
Epoch 23000, Loss: 10349.7109375
Epoch 23100, Loss: 10348.72265625
Epoch 23200, Loss: 10347.7421875
Epoch 23300, Loss: 10346.7705078125
Epoch 23400, Loss: 10345.80859375
Epoch 23500, Loss: 10344.8515625
Epoch 23600, Loss: 10343.9072265625
Epoch 23700, Loss: 10342.9677734375
Epoch 23800, Loss: 10342.0390625
Epoch 23900, Loss: 10341.119140625
Epoch 24000, Loss: 10340.2080078125
Epoch 24100, Loss: 10339.306640625
Epoch 24200, Loss: 10338.416015625
Epoch 24300, Loss: 10337.53125
Epoch 24400, Loss: 10336.6494140625
Epoch 24500, Loss: 10335.7548828125
Epoch 24600, Loss: 10334.841796875
Epoch 24700, Loss: 10333.9072265625
Epoch 24800, Loss: 10332.9521484375
Epoch 24900, Loss: 10331.974609375
Epoch 25000, Loss: 10330.9736328125
Epoch 25100, Loss: 10329.9501953125
Epoch 25200, Loss: 10328.9140625
Epoch 25300, Loss: 10327.8955078125
Epoch 25400, Loss: 10326.9052734375
Epoch 25500, Loss: 10325.9423828125
Epoch 25600, Loss: 10325.009765625
Epoch 25700, Loss: 10324.142578125
Epoch 25800, Loss: 10323.349609375
Epoch 25900, Loss: 10322.6240234375
Epoch 26000, Loss: 10321.9599609375
Epoch 26100, Loss: 10321.3505859375
Epoch 26200, Loss: 10320.7919921875
Epoch 26300, Loss: 10320.26953125
Epoch 26400, Loss: 10319.765625
Epoch 26500, Loss: 10319.2646484375
Epoch 26600, Loss: 10318.7529296875
Epoch 26700, Loss: 10318.2294921875
Epoch 26800, Loss: 10317.693359375
Epoch 26900, Loss: 10317.1435546875
Epoch 27000, Loss: 10316.5810546875
Epoch 27100, Loss: 10316.001953125
Epoch 27200, Loss: 10315.412109375
Epoch 27300, Loss: 10314.8056640625
Epoch 27400, Loss: 10314.1845703125
Epoch 27500, Loss: 10313.5546875
Epoch 27600, Loss: 10312.958984375
Epoch 27700, Loss: 10312.41796875
Epoch 27800, Loss: 10311.9287109375
Epoch 27900, Loss: 10311.482421875
Epoch 28000, Loss: 10311.091796875
Epoch 28100, Loss: 10310.76171875
Epoch 28200, Loss: 10310.4833984375
Epoch 28300, Loss: 10310.2412109375
Epoch 28400, Loss: 10310.0205078125
Epoch 28500, Loss: 10309.826171875
Epoch 28600, Loss: 10309.662109375
Epoch 28700, Loss: 10309.5224609375
Epoch 28800, Loss: 10309.40625
Epoch 28900, Loss: 10309.310546875
Epoch 29000, Loss: 10309.23046875
Epoch 29100, Loss: 10309.166015625
Epoch 29200, Loss: 10309.1123046875
Epoch 29300, Loss: 10309.0693359375
Epoch 29400, Loss: 10309.03125
Epoch 29500, Loss: 10308.9970703125
Epoch 29600, Loss: 10308.9677734375
Epoch 29700, Loss: 10308.9423828125
Epoch 29800, Loss: 10308.9189453125
Epoch 29900, Loss: 10308.900390625
Epoch 30000, Loss: 10308.8818359375
Epoch 30100, Loss: 10308.869140625
Epoch 30200, Loss: 10308.85546875
Epoch 30300, Loss: 10308.845703125
Epoch 30400, Loss: 10308.8369140625
Epoch 30500, Loss: 10308.828125
Epoch 30600, Loss: 10308.822265625
Epoch 30700, Loss: 10308.81640625
Epoch 30800, Loss: 10308.8125
Epoch 30900, Loss: 10308.8076171875
Epoch 31000, Loss: 10308.8056640625
Epoch 31100, Loss: 10308.8046875
Epoch 31200, Loss: 10308.7998046875
Epoch 31300, Loss: 10308.798828125
Epoch 31400, Loss: 10308.796875
Epoch 31500, Loss: 10308.7958984375
Epoch 31600, Loss: 10308.7958984375
Epoch 31700, Loss: 10308.7939453125
Epoch 31800, Loss: 10308.7939453125
Epoch 31900, Loss: 10308.7919921875
Epoch 32000, Loss: 10308.7919921875
Epoch 32100, Loss: 10308.7919921875
Epoch 32200, Loss: 10308.791015625
Epoch 32300, Loss: 10308.7919921875
Epoch 32400, Loss: 10308.791015625
Epoch 32500, Loss: 10308.791015625
Epoch 32600, Loss: 10308.791015625
Epoch 32700, Loss: 10308.7900390625
Epoch 32800, Loss: 10308.7890625
Epoch 32900, Loss: 10308.7900390625
Epoch 33000, Loss: 10308.7900390625
Epoch 33100, Loss: 10308.7890625
Epoch 33200, Loss: 10308.7890625
Epoch 33300, Loss: 10308.7890625
Epoch 33400, Loss: 10308.7900390625
Epoch 33500, Loss: 10308.7900390625
Epoch 33600, Loss: 10308.7900390625
Epoch 33700, Loss: 10308.7900390625
Epoch 33800, Loss: 10308.7900390625
Epoch 33900, Loss: 10308.7890625
Epoch 34000, Loss: 10308.7890625
Epoch 34100, Loss: 10308.7900390625
Epoch 34200, Loss: 10308.7900390625
Epoch 34300, Loss: 10308.7900390625
Epoch 34400, Loss: 10308.7900390625
Epoch 34500, Loss: 10308.7890625
Epoch 34600, Loss: 10308.7900390625
Epoch 34700, Loss: 10308.7900390625
Epoch 34800, Loss: 10308.7890625
Epoch 34900, Loss: 10308.7900390625
Epoch 35000, Loss: 10308.7900390625
Epoch 35100, Loss: 10308.7900390625
Epoch 35200, Loss: 10308.7900390625
Epoch 35300, Loss: 10308.7900390625
Epoch 35400, Loss: 10308.7900390625
Epoch 35500, Loss: 10308.7900390625
Epoch 35600, Loss: 10308.7900390625
Epoch 35700, Loss: 10308.7890625
Epoch 35800, Loss: 10308.7900390625
Epoch 35900, Loss: 10308.7890625
Epoch 36000, Loss: 10308.7890625
Epoch 36100, Loss: 10308.7900390625
Epoch 36200, Loss: 10308.7900390625
Epoch 36300, Loss: 10308.7890625
Epoch 36400, Loss: 10308.7900390625
Epoch 36500, Loss: 10308.7890625
Epoch 36600, Loss: 10308.7900390625
Epoch 36700, Loss: 10308.7890625
Epoch 36800, Loss: 10308.791015625
Epoch 36900, Loss: 10308.7900390625
Epoch 37000, Loss: 10308.7890625
Epoch 37100, Loss: 10308.7900390625
Epoch 37200, Loss: 10308.7900390625
Epoch 37300, Loss: 10308.7900390625
Epoch 37400, Loss: 10308.7890625
Epoch 37500, Loss: 10308.7890625
Epoch 37600, Loss: 10308.7900390625
Epoch 37700, Loss: 10308.7900390625
Epoch 37800, Loss: 10308.7900390625
Epoch 37900, Loss: 10308.7900390625
Epoch 38000, Loss: 10308.7900390625
Epoch 38100, Loss: 10308.7900390625
Epoch 38200, Loss: 10308.7890625
Epoch 38300, Loss: 10308.791015625
Epoch 38400, Loss: 10308.7900390625
Epoch 38500, Loss: 10308.7900390625
Epoch 38600, Loss: 10308.7890625
Epoch 38700, Loss: 10308.7900390625
Epoch 38800, Loss: 10308.7890625
Epoch 38900, Loss: 10308.7890625
Epoch 39000, Loss: 10308.7900390625
Epoch 39100, Loss: 10308.7890625
Epoch 39200, Loss: 10308.7900390625
Epoch 39300, Loss: 10308.7890625
Epoch 39400, Loss: 10308.7900390625
Epoch 39500, Loss: 10308.7890625
Epoch 39600, Loss: 10308.7900390625
Epoch 39700, Loss: 10308.7890625
Epoch 39800, Loss: 10308.791015625
Epoch 39900, Loss: 10308.7890625
Epoch 40000, Loss: 10308.7890625
Epoch 40100, Loss: 10308.7890625
Epoch 40200, Loss: 10308.7890625
Epoch 40300, Loss: 10308.7890625
Epoch 40400, Loss: 10308.7890625
Epoch 40500, Loss: 10308.7890625
Epoch 40600, Loss: 10308.7900390625
Epoch 40700, Loss: 10308.7890625
Epoch 40800, Loss: 10308.7890625
Epoch 40900, Loss: 10308.7890625
Epoch 41000, Loss: 10308.7900390625
Epoch 41100, Loss: 10308.7890625
Epoch 41200, Loss: 10308.7890625
Epoch 41300, Loss: 10308.7890625
Epoch 41400, Loss: 10308.7890625
Epoch 41500, Loss: 10308.7900390625
Epoch 41600, Loss: 10308.7900390625
Epoch 41700, Loss: 10308.791015625
Epoch 41800, Loss: 10308.7900390625
Epoch 41900, Loss: 10308.7900390625
Epoch 42000, Loss: 10308.7900390625
Epoch 42100, Loss: 10308.7890625
Epoch 42200, Loss: 10308.7900390625
Epoch 42300, Loss: 10308.7890625
Epoch 42400, Loss: 10308.7900390625
Epoch 42500, Loss: 10308.791015625
Epoch 42600, Loss: 10308.7890625
Epoch 42700, Loss: 10308.7890625
Epoch 42800, Loss: 10308.7900390625
Epoch 42900, Loss: 10308.7900390625
Epoch 43000, Loss: 10308.7880859375
Epoch 43100, Loss: 10308.7900390625
Epoch 43200, Loss: 10308.7890625
Epoch 43300, Loss: 10308.7890625
Epoch 43400, Loss: 10308.791015625
Epoch 43500, Loss: 10308.7900390625
Epoch 43600, Loss: 10308.7900390625
Epoch 43700, Loss: 10308.7880859375
Epoch 43800, Loss: 10308.7900390625
Epoch 43900, Loss: 10308.7900390625
Epoch 44000, Loss: 10308.791015625
Epoch 44100, Loss: 10308.7900390625
Epoch 44200, Loss: 10308.7900390625
Epoch 44300, Loss: 10308.7900390625
Epoch 44400, Loss: 10308.7890625
Epoch 44500, Loss: 10308.7900390625
Epoch 44600, Loss: 10308.7890625
Epoch 44700, Loss: 10308.7900390625
Epoch 44800, Loss: 10308.7900390625
Epoch 44900, Loss: 10308.7900390625
Epoch 45000, Loss: 10308.7900390625
Epoch 45100, Loss: 10308.7890625
Epoch 45200, Loss: 10308.7890625
Epoch 45300, Loss: 10308.7890625
Epoch 45400, Loss: 10308.7900390625
Epoch 45500, Loss: 10308.7900390625
Epoch 45600, Loss: 10308.7900390625
Epoch 45700, Loss: 10308.7890625
Epoch 45800, Loss: 10308.7890625
Epoch 45900, Loss: 10308.7900390625
Epoch 46000, Loss: 10308.7890625
Epoch 46100, Loss: 10308.7900390625
Epoch 46200, Loss: 10308.7900390625
Epoch 46300, Loss: 10308.7900390625
Epoch 46400, Loss: 10308.7890625
Epoch 46500, Loss: 10308.7890625
Epoch 46600, Loss: 10308.7890625
Epoch 46700, Loss: 10308.7900390625
Epoch 46800, Loss: 10308.7900390625
Epoch 46900, Loss: 10308.7890625
Epoch 47000, Loss: 10308.7900390625
Epoch 47100, Loss: 10308.7890625
Epoch 47200, Loss: 10308.7890625
Epoch 47300, Loss: 10308.7900390625
Epoch 47400, Loss: 10308.7890625
Epoch 47500, Loss: 10308.7900390625
Epoch 47600, Loss: 10308.7890625
Epoch 47700, Loss: 10308.7900390625
Epoch 47800, Loss: 10308.7890625
Epoch 47900, Loss: 10308.7890625
Epoch 48000, Loss: 10308.7900390625
Epoch 48100, Loss: 10308.7890625
Epoch 48200, Loss: 10308.7900390625
Epoch 48300, Loss: 10308.7900390625
Epoch 48400, Loss: 10308.7900390625
Epoch 48500, Loss: 10308.7890625
Epoch 48600, Loss: 10308.7890625
Epoch 48700, Loss: 10308.7900390625
Epoch 48800, Loss: 10308.7890625
Epoch 48900, Loss: 10308.791015625
Epoch 49000, Loss: 10308.7900390625
Epoch 49100, Loss: 10308.7900390625
Epoch 49200, Loss: 10308.7890625
Epoch 49300, Loss: 10308.7890625
Epoch 49400, Loss: 10308.7900390625
Epoch 49500, Loss: 10308.7890625
Epoch 49600, Loss: 10308.7900390625
Epoch 49700, Loss: 10308.7890625
Epoch 49800, Loss: 10308.7900390625
Epoch 49900, Loss: 10308.7890625
Epoch 0, Loss: 63828.96875
Epoch 100, Loss: 60930.94140625
Epoch 200, Loss: 58221.23828125
Epoch 300, Loss: 55685.62109375
Epoch 400, Loss: 53312.18359375
Epoch 500, Loss: 51089.53515625
Epoch 600, Loss: 49008.88671875
Epoch 700, Loss: 47061.34765625
Epoch 800, Loss: 45239.21875
Epoch 900, Loss: 43536.0625
Epoch 1000, Loss: 41944.55078125
Epoch 1100, Loss: 40457.8203125
Epoch 1200, Loss: 39069.47265625
Epoch 1300, Loss: 37773.35546875
Epoch 1400, Loss: 36563.984375
Epoch 1500, Loss: 35435.76953125
Epoch 1600, Loss: 34382.7265625
Epoch 1700, Loss: 33399.59375
Epoch 1800, Loss: 32481.404296875
Epoch 1900, Loss: 31623.513671875
Epoch 2000, Loss: 30821.595703125
Epoch 2100, Loss: 30071.91015625
Epoch 2200, Loss: 29370.978515625
Epoch 2300, Loss: 28714.9140625
Epoch 2400, Loss: 28101.564453125
Epoch 2500, Loss: 27526.625
Epoch 2600, Loss: 26987.16015625
Epoch 2700, Loss: 26480.53515625
Epoch 2800, Loss: 26004.3046875
Epoch 2900, Loss: 25556.205078125
Epoch 3000, Loss: 25134.1171875
Epoch 3100, Loss: 24736.06640625
Epoch 3200, Loss: 24360.20703125
Epoch 3300, Loss: 24005.02734375
Epoch 3400, Loss: 23669.123046875
Epoch 3500, Loss: 23351.21875
Epoch 3600, Loss: 23049.39453125
Epoch 3700, Loss: 22761.89453125
Epoch 3800, Loss: 22487.443359375
Epoch 3900, Loss: 22224.8515625
Epoch 4000, Loss: 21973.181640625
Epoch 4100, Loss: 21731.83984375
Epoch 4200, Loss: 21499.353515625
Epoch 4300, Loss: 21274.904296875
Epoch 4400, Loss: 21057.955078125
Epoch 4500, Loss: 20848.041015625
Epoch 4600, Loss: 20644.193359375
Epoch 4700, Loss: 20445.91015625
Epoch 4800, Loss: 20252.763671875
Epoch 4900, Loss: 20064.390625
Epoch 5000, Loss: 19880.453125
Epoch 5100, Loss: 19700.68359375
Epoch 5200, Loss: 19524.822265625
Epoch 5300, Loss: 19352.66796875
Epoch 5400, Loss: 19184.03125
Epoch 5500, Loss: 19018.734375
Epoch 5600, Loss: 18856.63671875
Epoch 5700, Loss: 18697.599609375
Epoch 5800, Loss: 18541.505859375
Epoch 5900, Loss: 18388.578125
Epoch 6000, Loss: 18238.505859375
Epoch 6100, Loss: 18091.0546875
Epoch 6200, Loss: 17946.140625
Epoch 6300, Loss: 17803.689453125
Epoch 6400, Loss: 17663.6328125
Epoch 6500, Loss: 17525.91015625
Epoch 6600, Loss: 17390.451171875
Epoch 6700, Loss: 17257.2109375
Epoch 6800, Loss: 17126.12890625
Epoch 6900, Loss: 16997.162109375
Epoch 7000, Loss: 16870.259765625
Epoch 7100, Loss: 16745.6484375
Epoch 7200, Loss: 16623.486328125
Epoch 7300, Loss: 16503.20703125
Epoch 7400, Loss: 16384.931640625
Epoch 7500, Loss: 16268.8466796875
Epoch 7600, Loss: 16154.50390625
Epoch 7700, Loss: 16041.869140625
Epoch 7800, Loss: 15930.9140625
Epoch 7900, Loss: 15821.603515625
Epoch 8000, Loss: 15713.8994140625
Epoch 8100, Loss: 15607.775390625
Epoch 8200, Loss: 15503.2099609375
Epoch 8300, Loss: 15400.1630859375
Epoch 8400, Loss: 15298.6064453125
Epoch 8500, Loss: 15198.517578125
Epoch 8600, Loss: 15099.876953125
Epoch 8700, Loss: 15002.6552734375
Epoch 8800, Loss: 14906.83984375
Epoch 8900, Loss: 14812.4033203125
Epoch 9000, Loss: 14719.72265625
Epoch 9100, Loss: 14628.4384765625
Epoch 9200, Loss: 14538.51171875
Epoch 9300, Loss: 14449.9287109375
Epoch 9400, Loss: 14362.6630859375
Epoch 9500, Loss: 14276.70703125
Epoch 9600, Loss: 14192.0390625
Epoch 9700, Loss: 14108.6484375
Epoch 9800, Loss: 14026.5341796875
Epoch 9900, Loss: 13945.681640625
Epoch 10000, Loss: 13866.08203125
Epoch 10100, Loss: 13787.734375
Epoch 10200, Loss: 13710.6298828125
Epoch 10300, Loss: 13634.763671875
Epoch 10400, Loss: 13560.1318359375
Epoch 10500, Loss: 13486.7294921875
Epoch 10600, Loss: 13414.5478515625
Epoch 10700, Loss: 13343.5869140625
Epoch 10800, Loss: 13273.8330078125
Epoch 10900, Loss: 13205.2822265625
Epoch 11000, Loss: 13137.921875
Epoch 11100, Loss: 13071.7431640625
Epoch 11200, Loss: 13006.75
Epoch 11300, Loss: 12942.9619140625
Epoch 11400, Loss: 12880.3271484375
Epoch 11500, Loss: 12818.8291015625
Epoch 11600, Loss: 12758.447265625
Epoch 11700, Loss: 12699.158203125
Epoch 11800, Loss: 12640.9453125
Epoch 11900, Loss: 12583.783203125
Epoch 12000, Loss: 12527.6552734375
Epoch 12100, Loss: 12472.9189453125
Epoch 12200, Loss: 12419.28515625
Epoch 12300, Loss: 12366.6474609375
Epoch 12400, Loss: 12315.017578125
Epoch 12500, Loss: 12264.5390625
Epoch 12600, Loss: 12215.083984375
Epoch 12700, Loss: 12166.5556640625
Epoch 12800, Loss: 12118.9091796875
Epoch 12900, Loss: 12072.1318359375
Epoch 13000, Loss: 12026.21875
Epoch 13100, Loss: 11981.1484375
Epoch 13200, Loss: 11936.9091796875
Epoch 13300, Loss: 11893.4931640625
Epoch 13400, Loss: 11850.8955078125
Epoch 13500, Loss: 11809.0966796875
Epoch 13600, Loss: 11768.0947265625
Epoch 13700, Loss: 11727.873046875
Epoch 13800, Loss: 11688.4208984375
Epoch 13900, Loss: 11649.884765625
Epoch 14000, Loss: 11612.3505859375
Epoch 14100, Loss: 11575.7958984375
Epoch 14200, Loss: 11540.142578125
Epoch 14300, Loss: 11505.32421875
Epoch 14400, Loss: 11471.3095703125
Epoch 14500, Loss: 11438.1572265625
Epoch 14600, Loss: 11405.7919921875
Epoch 14700, Loss: 11374.1826171875
Epoch 14800, Loss: 11343.318359375
Epoch 14900, Loss: 11313.1865234375
Epoch 15000, Loss: 11283.775390625
Epoch 15100, Loss: 11255.046875
Epoch 15200, Loss: 11226.9853515625
Epoch 15300, Loss: 11199.5732421875
Epoch 15400, Loss: 11172.798828125
Epoch 15500, Loss: 11146.6630859375
Epoch 15600, Loss: 11121.154296875
Epoch 15700, Loss: 11096.2900390625
Epoch 15800, Loss: 11072.1611328125
Epoch 15900, Loss: 11048.7275390625
Epoch 16000, Loss: 11025.9462890625
Epoch 16100, Loss: 11003.796875
Epoch 16200, Loss: 10982.26171875
Epoch 16300, Loss: 10961.33203125
Epoch 16400, Loss: 10940.994140625
Epoch 16500, Loss: 10921.232421875
Epoch 16600, Loss: 10902.03125
Epoch 16700, Loss: 10883.37109375
Epoch 16800, Loss: 10865.2177734375
Epoch 16900, Loss: 10847.5791015625
Epoch 17000, Loss: 10830.482421875
Epoch 17100, Loss: 10813.884765625
Epoch 17200, Loss: 10797.7734375
Epoch 17300, Loss: 10782.1357421875
Epoch 17400, Loss: 10766.9638671875
Epoch 17500, Loss: 10752.2548828125
Epoch 17600, Loss: 10737.9990234375
Epoch 17700, Loss: 10724.1923828125
Epoch 17800, Loss: 10710.828125
Epoch 17900, Loss: 10697.90234375
Epoch 18000, Loss: 10685.4091796875
Epoch 18100, Loss: 10673.3837890625
Epoch 18200, Loss: 10661.9169921875
Epoch 18300, Loss: 10650.9130859375
Epoch 18400, Loss: 10640.3427734375
Epoch 18500, Loss: 10630.19140625
Epoch 18600, Loss: 10620.46484375
Epoch 18700, Loss: 10611.15234375
Epoch 18800, Loss: 10602.25
Epoch 18900, Loss: 10593.7744140625
Epoch 19000, Loss: 10585.6982421875
Epoch 19100, Loss: 10578.013671875
Epoch 19200, Loss: 10570.7080078125
Epoch 19300, Loss: 10563.7685546875
Epoch 19400, Loss: 10557.197265625
Epoch 19500, Loss: 10550.9873046875
Epoch 19600, Loss: 10545.115234375
Epoch 19700, Loss: 10539.5751953125
Epoch 19800, Loss: 10534.349609375
Epoch 19900, Loss: 10529.427734375
Epoch 20000, Loss: 10524.802734375
Epoch 20100, Loss: 10520.4580078125
Epoch 20200, Loss: 10516.3828125
Epoch 20300, Loss: 10512.564453125
Epoch 20400, Loss: 10508.9873046875
Epoch 20500, Loss: 10505.642578125
Epoch 20600, Loss: 10502.5146484375
Epoch 20700, Loss: 10499.5908203125
Epoch 20800, Loss: 10496.857421875
Epoch 20900, Loss: 10494.30078125
Epoch 21000, Loss: 10491.904296875
Epoch 21100, Loss: 10489.6591796875
Epoch 21200, Loss: 10487.55078125
Epoch 21300, Loss: 10485.5654296875
Epoch 21400, Loss: 10483.689453125
Epoch 21500, Loss: 10481.9072265625
Epoch 21600, Loss: 10480.20703125
Epoch 21700, Loss: 10478.5771484375
Epoch 21800, Loss: 10477.005859375
Epoch 21900, Loss: 10475.48046875
Epoch 22000, Loss: 10473.994140625
Epoch 22100, Loss: 10472.5888671875
Epoch 22200, Loss: 10471.29296875
Epoch 22300, Loss: 10470.029296875
Epoch 22400, Loss: 10468.7900390625
Epoch 22500, Loss: 10467.5654296875
Epoch 22600, Loss: 10466.357421875
Epoch 22700, Loss: 10465.1591796875
Epoch 22800, Loss: 10463.970703125
Epoch 22900, Loss: 10462.7939453125
Epoch 23000, Loss: 10461.6220703125
Epoch 23100, Loss: 10460.4560546875
Epoch 23200, Loss: 10459.294921875
Epoch 23300, Loss: 10458.138671875
Epoch 23400, Loss: 10456.986328125
Epoch 23500, Loss: 10455.83984375
Epoch 23600, Loss: 10454.6962890625
Epoch 23700, Loss: 10453.55859375
Epoch 23800, Loss: 10452.4228515625
Epoch 23900, Loss: 10451.27734375
Epoch 24000, Loss: 10450.11328125
Epoch 24100, Loss: 10448.9306640625
Epoch 24200, Loss: 10447.72265625
Epoch 24300, Loss: 10446.486328125
Epoch 24400, Loss: 10445.2255859375
Epoch 24500, Loss: 10443.947265625
Epoch 24600, Loss: 10442.6796875
Epoch 24700, Loss: 10441.435546875
Epoch 24800, Loss: 10440.208984375
Epoch 24900, Loss: 10439.005859375
Epoch 25000, Loss: 10437.853515625
Epoch 25100, Loss: 10436.7734375
Epoch 25200, Loss: 10435.763671875
Epoch 25300, Loss: 10434.8173828125
Epoch 25400, Loss: 10433.931640625
Epoch 25500, Loss: 10433.1025390625
Epoch 25600, Loss: 10432.326171875
Epoch 25700, Loss: 10431.599609375
Epoch 25800, Loss: 10430.9169921875
Epoch 25900, Loss: 10430.2646484375
Epoch 26000, Loss: 10429.61328125
Epoch 26100, Loss: 10428.9560546875
Epoch 26200, Loss: 10428.2861328125
Epoch 26300, Loss: 10427.6025390625
Epoch 26400, Loss: 10426.9052734375
Epoch 26500, Loss: 10426.189453125
Epoch 26600, Loss: 10425.45703125
Epoch 26700, Loss: 10424.7060546875
Epoch 26800, Loss: 10423.939453125
Epoch 26900, Loss: 10423.1572265625
Epoch 27000, Loss: 10422.41015625
Epoch 27100, Loss: 10421.712890625
Epoch 27200, Loss: 10421.0615234375
Epoch 27300, Loss: 10420.45703125
Epoch 27400, Loss: 10419.9130859375
Epoch 27500, Loss: 10419.4365234375
Epoch 27600, Loss: 10419.01953125
Epoch 27700, Loss: 10418.65234375
Epoch 27800, Loss: 10418.330078125
Epoch 27900, Loss: 10418.048828125
Epoch 28000, Loss: 10417.80078125
Epoch 28100, Loss: 10417.5830078125
Epoch 28200, Loss: 10417.390625
Epoch 28300, Loss: 10417.2236328125
Epoch 28400, Loss: 10417.078125
Epoch 28500, Loss: 10416.951171875
Epoch 28600, Loss: 10416.8447265625
Epoch 28700, Loss: 10416.7529296875
Epoch 28800, Loss: 10416.6748046875
Epoch 28900, Loss: 10416.6083984375
Epoch 29000, Loss: 10416.5537109375
Epoch 29100, Loss: 10416.5068359375
Epoch 29200, Loss: 10416.4658203125
Epoch 29300, Loss: 10416.4287109375
Epoch 29400, Loss: 10416.396484375
Epoch 29500, Loss: 10416.3662109375
Epoch 29600, Loss: 10416.3408203125
Epoch 29700, Loss: 10416.3173828125
Epoch 29800, Loss: 10416.296875
Epoch 29900, Loss: 10416.279296875
Epoch 30000, Loss: 10416.2626953125
Epoch 30100, Loss: 10416.2509765625
Epoch 30200, Loss: 10416.23828125
Epoch 30300, Loss: 10416.2294921875
Epoch 30400, Loss: 10416.220703125
Epoch 30500, Loss: 10416.2138671875
Epoch 30600, Loss: 10416.208984375
Epoch 30700, Loss: 10416.2041015625
Epoch 30800, Loss: 10416.19921875
Epoch 30900, Loss: 10416.1962890625
Epoch 31000, Loss: 10416.1943359375
Epoch 31100, Loss: 10416.193359375
Epoch 31200, Loss: 10416.19140625
Epoch 31300, Loss: 10416.189453125
Epoch 31400, Loss: 10416.189453125
Epoch 31500, Loss: 10416.1865234375
Epoch 31600, Loss: 10416.1865234375
Epoch 31700, Loss: 10416.185546875
Epoch 31800, Loss: 10416.1865234375
Epoch 31900, Loss: 10416.18359375
Epoch 32000, Loss: 10416.1845703125
Epoch 32100, Loss: 10416.1845703125
Epoch 32200, Loss: 10416.18359375
Epoch 32300, Loss: 10416.18359375
Epoch 32400, Loss: 10416.1826171875
Epoch 32500, Loss: 10416.1826171875
Epoch 32600, Loss: 10416.1845703125
Epoch 32700, Loss: 10416.18359375
Epoch 32800, Loss: 10416.18359375
Epoch 32900, Loss: 10416.1826171875
Epoch 33000, Loss: 10416.181640625
Epoch 33100, Loss: 10416.1826171875
Epoch 33200, Loss: 10416.1826171875
Epoch 33300, Loss: 10416.18359375
Epoch 33400, Loss: 10416.181640625
Epoch 33500, Loss: 10416.181640625
Epoch 33600, Loss: 10416.181640625
Epoch 33700, Loss: 10416.181640625
Epoch 33800, Loss: 10416.1826171875
Epoch 33900, Loss: 10416.181640625
Epoch 34000, Loss: 10416.1826171875
Epoch 34100, Loss: 10416.18359375
Epoch 34200, Loss: 10416.1826171875
Epoch 34300, Loss: 10416.1826171875
Epoch 34400, Loss: 10416.1826171875
Epoch 34500, Loss: 10416.1826171875
Epoch 34600, Loss: 10416.181640625
Epoch 34700, Loss: 10416.1826171875
Epoch 34800, Loss: 10416.1826171875
Epoch 34900, Loss: 10416.181640625
Epoch 35000, Loss: 10416.18359375
Epoch 35100, Loss: 10416.18359375
Epoch 35200, Loss: 10416.181640625
Epoch 35300, Loss: 10416.1826171875
Epoch 35400, Loss: 10416.1826171875
Epoch 35500, Loss: 10416.181640625
Epoch 35600, Loss: 10416.1826171875
Epoch 35700, Loss: 10416.18359375
Epoch 35800, Loss: 10416.1826171875
Epoch 35900, Loss: 10416.1826171875
Epoch 36000, Loss: 10416.1826171875
Epoch 36100, Loss: 10416.1826171875
Epoch 36200, Loss: 10416.1826171875
Epoch 36300, Loss: 10416.1826171875
Epoch 36400, Loss: 10416.1826171875
Epoch 36500, Loss: 10416.181640625
Epoch 36600, Loss: 10416.1826171875
Epoch 36700, Loss: 10416.181640625
Epoch 36800, Loss: 10416.1826171875
Epoch 36900, Loss: 10416.1826171875
Epoch 37000, Loss: 10416.181640625
Epoch 37100, Loss: 10416.181640625
Epoch 37200, Loss: 10416.181640625
Epoch 37300, Loss: 10416.1826171875
Epoch 37400, Loss: 10416.1826171875
Epoch 37500, Loss: 10416.1826171875
Epoch 37600, Loss: 10416.1826171875
Epoch 37700, Loss: 10416.1826171875
Epoch 37800, Loss: 10416.181640625
Epoch 37900, Loss: 10416.181640625
Epoch 38000, Loss: 10416.1826171875
Epoch 38100, Loss: 10416.1826171875
Epoch 38200, Loss: 10416.181640625
Epoch 38300, Loss: 10416.181640625
Epoch 38400, Loss: 10416.1826171875
Epoch 38500, Loss: 10416.1826171875
Epoch 38600, Loss: 10416.1826171875
Epoch 38700, Loss: 10416.181640625
Epoch 38800, Loss: 10416.181640625
Epoch 38900, Loss: 10416.181640625
Epoch 39000, Loss: 10416.181640625
Epoch 39100, Loss: 10416.1826171875
Epoch 39200, Loss: 10416.18359375
Epoch 39300, Loss: 10416.1826171875
Epoch 39400, Loss: 10416.1826171875
Epoch 39500, Loss: 10416.1826171875
Epoch 39600, Loss: 10416.1826171875
Epoch 39700, Loss: 10416.1826171875
Epoch 39800, Loss: 10416.181640625
Epoch 39900, Loss: 10416.1826171875
Epoch 40000, Loss: 10416.1826171875
Epoch 40100, Loss: 10416.181640625
Epoch 40200, Loss: 10416.1826171875
Epoch 40300, Loss: 10416.181640625
Epoch 40400, Loss: 10416.1826171875
Epoch 40500, Loss: 10416.181640625
Epoch 40600, Loss: 10416.181640625
Epoch 40700, Loss: 10416.1826171875
Epoch 40800, Loss: 10416.181640625
Epoch 40900, Loss: 10416.181640625
Epoch 41000, Loss: 10416.1826171875
Epoch 41100, Loss: 10416.1826171875
Epoch 41200, Loss: 10416.181640625
Epoch 41300, Loss: 10416.181640625
Epoch 41400, Loss: 10416.181640625
Epoch 41500, Loss: 10416.181640625
Epoch 41600, Loss: 10416.18359375
Epoch 41700, Loss: 10416.18359375
Epoch 41800, Loss: 10416.181640625
Epoch 41900, Loss: 10416.181640625
Epoch 42000, Loss: 10416.181640625
Epoch 42100, Loss: 10416.1826171875
Epoch 42200, Loss: 10416.1826171875
Epoch 42300, Loss: 10416.1826171875
Epoch 42400, Loss: 10416.1826171875
Epoch 42500, Loss: 10416.1826171875
Epoch 42600, Loss: 10416.1826171875
Epoch 42700, Loss: 10416.181640625
Epoch 42800, Loss: 10416.1826171875
Epoch 42900, Loss: 10416.181640625
Epoch 43000, Loss: 10416.181640625
Epoch 43100, Loss: 10416.181640625
Epoch 43200, Loss: 10416.1826171875
Epoch 43300, Loss: 10416.181640625
Epoch 43400, Loss: 10416.1826171875
Epoch 43500, Loss: 10416.1826171875
Epoch 43600, Loss: 10416.181640625
Epoch 43700, Loss: 10416.181640625
Epoch 43800, Loss: 10416.181640625
Epoch 43900, Loss: 10416.1826171875
Epoch 44000, Loss: 10416.1826171875
Epoch 44100, Loss: 10416.1826171875
Epoch 44200, Loss: 10416.1826171875
Epoch 44300, Loss: 10416.1826171875
Epoch 44400, Loss: 10416.181640625
Epoch 44500, Loss: 10416.181640625
Epoch 44600, Loss: 10416.1826171875
Epoch 44700, Loss: 10416.1826171875
Epoch 44800, Loss: 10416.181640625
Epoch 44900, Loss: 10416.181640625
Epoch 45000, Loss: 10416.18359375
Epoch 45100, Loss: 10416.181640625
Epoch 45200, Loss: 10416.1826171875
Epoch 45300, Loss: 10416.181640625
Epoch 45400, Loss: 10416.181640625
Epoch 45500, Loss: 10416.1826171875
Epoch 45600, Loss: 10416.1826171875
Epoch 45700, Loss: 10416.1826171875
Epoch 45800, Loss: 10416.1826171875
Epoch 45900, Loss: 10416.1826171875
Epoch 46000, Loss: 10416.1826171875
Epoch 46100, Loss: 10416.1826171875
Epoch 46200, Loss: 10416.181640625
Epoch 46300, Loss: 10416.1826171875
Epoch 46400, Loss: 10416.1826171875
Epoch 46500, Loss: 10416.1826171875
Epoch 46600, Loss: 10416.1826171875
Epoch 46700, Loss: 10416.1826171875
Epoch 46800, Loss: 10416.1826171875
Epoch 46900, Loss: 10416.181640625
Epoch 47000, Loss: 10416.1826171875
Epoch 47100, Loss: 10416.1826171875
Epoch 47200, Loss: 10416.1826171875
Epoch 47300, Loss: 10416.1826171875
Epoch 47400, Loss: 10416.181640625
Epoch 47500, Loss: 10416.1826171875
Epoch 47600, Loss: 10416.1826171875
Epoch 47700, Loss: 10416.1826171875
Epoch 47800, Loss: 10416.1826171875
Epoch 47900, Loss: 10416.1826171875
Epoch 48000, Loss: 10416.181640625
Epoch 48100, Loss: 10416.181640625
Epoch 48200, Loss: 10416.1826171875
Epoch 48300, Loss: 10416.181640625
Epoch 48400, Loss: 10416.181640625
Epoch 48500, Loss: 10416.181640625
Epoch 48600, Loss: 10416.18359375
Epoch 48700, Loss: 10416.1826171875
Epoch 48800, Loss: 10416.181640625
Epoch 48900, Loss: 10416.1826171875
Epoch 49000, Loss: 10416.181640625
Epoch 49100, Loss: 10416.1826171875
Epoch 49200, Loss: 10416.1826171875
Epoch 49300, Loss: 10416.1826171875
Epoch 49400, Loss: 10416.1826171875
Epoch 49500, Loss: 10416.1826171875
Epoch 49600, Loss: 10416.1826171875
Epoch 49700, Loss: 10416.1826171875
Epoch 49800, Loss: 10416.181640625
Epoch 49900, Loss: 10416.181640625
Epoch 0, Loss: 63828.96875
Epoch 100, Loss: 60933.44921875
Epoch 200, Loss: 58226.15234375
Epoch 300, Loss: 55692.67578125
Epoch 400, Loss: 53321.5390625
Epoch 500, Loss: 51101.0859375
Epoch 600, Loss: 49022.53125
Epoch 700, Loss: 47076.921875
Epoch 800, Loss: 45256.5546875
Epoch 900, Loss: 43555.359375
Epoch 1000, Loss: 41965.6171875
Epoch 1100, Loss: 40480.5234375
Epoch 1200, Loss: 39093.671875
Epoch 1300, Loss: 37798.953125
Epoch 1400, Loss: 36591.09375
Epoch 1500, Loss: 35464.26171875
Epoch 1600, Loss: 34412.48828125
Epoch 1700, Loss: 33430.515625
Epoch 1800, Loss: 32513.38671875
Epoch 1900, Loss: 31656.47265625
Epoch 2000, Loss: 30855.447265625
Epoch 2100, Loss: 30106.85546875
Epoch 2200, Loss: 29406.92578125
Epoch 2300, Loss: 28752.064453125
Epoch 2400, Loss: 28140.06640625
Epoch 2500, Loss: 27566.37890625
Epoch 2600, Loss: 27028.109375
Epoch 2700, Loss: 26522.62109375
Epoch 2800, Loss: 26047.46484375
Epoch 2900, Loss: 25600.390625
Epoch 3000, Loss: 25179.283203125
Epoch 3100, Loss: 24782.177734375
Epoch 3200, Loss: 24407.21875
Epoch 3300, Loss: 24052.97265625
Epoch 3400, Loss: 23718.17578125
Epoch 3500, Loss: 23401.578125
Epoch 3600, Loss: 23101.0
Epoch 3700, Loss: 22814.703125
Epoch 3800, Loss: 22541.41015625
Epoch 3900, Loss: 22279.9375
Epoch 4000, Loss: 22029.568359375
Epoch 4100, Loss: 21789.44921875
Epoch 4200, Loss: 21558.142578125
Epoch 4300, Loss: 21334.837890625
Epoch 4400, Loss: 21119.15234375
Epoch 4500, Loss: 20910.458984375
Epoch 4600, Loss: 20707.791015625
Epoch 4700, Loss: 20510.6640625
Epoch 4800, Loss: 20318.640625
Epoch 4900, Loss: 20131.3515625
Epoch 5000, Loss: 19948.48046875
Epoch 5100, Loss: 19769.73828125
Epoch 5200, Loss: 19594.892578125
Epoch 5300, Loss: 19423.7265625
Epoch 5400, Loss: 19256.046875
Epoch 5500, Loss: 19091.69140625
Epoch 5600, Loss: 18930.5078125
Epoch 5700, Loss: 18772.357421875
Epoch 5800, Loss: 18617.2421875
Epoch 5900, Loss: 18465.40234375
Epoch 6000, Loss: 18316.25390625
Epoch 6100, Loss: 18169.701171875
Epoch 6200, Loss: 18025.671875
Epoch 6300, Loss: 17884.076171875
Epoch 6400, Loss: 17744.85546875
Epoch 6500, Loss: 17607.943359375
Epoch 6600, Loss: 17473.275390625
Epoch 6700, Loss: 17340.802734375
Epoch 6800, Loss: 17210.47265625
Epoch 6900, Loss: 17082.23046875
Epoch 7000, Loss: 16956.029296875
Epoch 7100, Loss: 16832.537109375
Epoch 7200, Loss: 16711.18359375
Epoch 7300, Loss: 16591.6875
Epoch 7400, Loss: 16474.142578125
Epoch 7500, Loss: 16358.927734375
Epoch 7600, Loss: 16245.4375
Epoch 7700, Loss: 16133.6240234375
Epoch 7800, Loss: 16023.451171875
Epoch 7900, Loss: 15914.8994140625
Epoch 8000, Loss: 15807.923828125
Epoch 8100, Loss: 15702.498046875
Epoch 8200, Loss: 15598.5947265625
Epoch 8300, Loss: 15496.185546875
Epoch 8400, Loss: 15395.251953125
Epoch 8500, Loss: 15295.76171875
Epoch 8600, Loss: 15197.693359375
Epoch 8700, Loss: 15101.029296875
Epoch 8800, Loss: 15005.7578125
Epoch 8900, Loss: 14912.265625
Epoch 9000, Loss: 14820.1826171875
Epoch 9100, Loss: 14729.4716796875
Epoch 9200, Loss: 14640.1123046875
Epoch 9300, Loss: 14552.080078125
Epoch 9400, Loss: 14465.353515625
Epoch 9500, Loss: 14379.919921875
Epoch 9600, Loss: 14295.7626953125
Epoch 9700, Loss: 14212.873046875
Epoch 9800, Loss: 14131.23828125
Epoch 9900, Loss: 14050.8515625
Epoch 10000, Loss: 13971.70703125
Epoch 10100, Loss: 13893.7958984375
Epoch 10200, Loss: 13817.1171875
Epoch 10300, Loss: 13741.6630859375
Epoch 10400, Loss: 13667.427734375
Epoch 10500, Loss: 13594.408203125
Epoch 10600, Loss: 13522.595703125
Epoch 10700, Loss: 13451.9853515625
Epoch 10800, Loss: 13382.568359375
Epoch 10900, Loss: 13314.3369140625
Epoch 11000, Loss: 13247.2822265625
Epoch 11100, Loss: 13181.416015625
Epoch 11200, Loss: 13116.751953125
Epoch 11300, Loss: 13053.2392578125
Epoch 11400, Loss: 12990.861328125
Epoch 11500, Loss: 12929.59765625
Epoch 11600, Loss: 12869.4296875
Epoch 11700, Loss: 12810.3310546875
Epoch 11800, Loss: 12752.2939453125
Epoch 11900, Loss: 12695.2861328125
Epoch 12000, Loss: 12639.552734375
Epoch 12100, Loss: 12585.2470703125
Epoch 12200, Loss: 12532.208984375
Epoch 12300, Loss: 12480.267578125
Epoch 12400, Loss: 12429.3017578125
Epoch 12500, Loss: 12379.267578125
Epoch 12600, Loss: 12330.1357421875
Epoch 12700, Loss: 12281.890625
Epoch 12800, Loss: 12234.509765625
Epoch 12900, Loss: 12187.98828125
Epoch 13000, Loss: 12142.3134765625
Epoch 13100, Loss: 12097.4716796875
Epoch 13200, Loss: 12053.455078125
Epoch 13300, Loss: 12010.2470703125
Epoch 13400, Loss: 11967.8408203125
Epoch 13500, Loss: 11926.2578125
Epoch 13600, Loss: 11885.642578125
Epoch 13700, Loss: 11845.9267578125
Epoch 13800, Loss: 11807.236328125
Epoch 13900, Loss: 11769.4462890625
Epoch 14000, Loss: 11732.509765625
Epoch 14100, Loss: 11696.4072265625
Epoch 14200, Loss: 11661.1240234375
Epoch 14300, Loss: 11626.642578125
Epoch 14400, Loss: 11593.044921875
Epoch 14500, Loss: 11560.287109375
Epoch 14600, Loss: 11528.34375
Epoch 14700, Loss: 11497.16796875
Epoch 14800, Loss: 11466.7265625
Epoch 14900, Loss: 11437.0029296875
Epoch 15000, Loss: 11407.9833984375
Epoch 15100, Loss: 11379.658203125
Epoch 15200, Loss: 11352.009765625
Epoch 15300, Loss: 11325.0986328125
Epoch 15400, Loss: 11298.884765625
Epoch 15500, Loss: 11273.3251953125
Epoch 15600, Loss: 11248.3974609375
Epoch 15700, Loss: 11224.109375
Epoch 15800, Loss: 11200.4609375
Epoch 15900, Loss: 11177.4443359375
Epoch 16000, Loss: 11155.044921875
Epoch 16100, Loss: 11133.2509765625
Epoch 16200, Loss: 11112.05078125
Epoch 16300, Loss: 11091.4375
Epoch 16400, Loss: 11071.396484375
Epoch 16500, Loss: 11051.9208984375
Epoch 16600, Loss: 11032.9951171875
Epoch 16700, Loss: 11014.6748046875
Epoch 16800, Loss: 10996.927734375
Epoch 16900, Loss: 10979.7119140625
Epoch 17000, Loss: 10963.001953125
Epoch 17100, Loss: 10946.76953125
Epoch 17200, Loss: 10931.0087890625
Epoch 17300, Loss: 10915.7080078125
Epoch 17400, Loss: 10900.857421875
Epoch 17500, Loss: 10886.4609375
Epoch 17600, Loss: 10872.6513671875
Epoch 17700, Loss: 10859.3759765625
Epoch 17800, Loss: 10846.564453125
Epoch 17900, Loss: 10834.1982421875
Epoch 18000, Loss: 10822.259765625
Epoch 18100, Loss: 10810.7451171875
Epoch 18200, Loss: 10799.64453125
Epoch 18300, Loss: 10788.9521484375
Epoch 18400, Loss: 10778.6630859375
Epoch 18500, Loss: 10768.779296875
Epoch 18600, Loss: 10759.3349609375
Epoch 18700, Loss: 10750.310546875
Epoch 18800, Loss: 10741.701171875
Epoch 18900, Loss: 10733.4892578125
Epoch 19000, Loss: 10725.662109375
Epoch 19100, Loss: 10718.208984375
Epoch 19200, Loss: 10711.125
Epoch 19300, Loss: 10704.3974609375
Epoch 19400, Loss: 10698.017578125
Epoch 19500, Loss: 10691.970703125
Epoch 19600, Loss: 10686.25
Epoch 19700, Loss: 10680.8427734375
Epoch 19800, Loss: 10675.7353515625
Epoch 19900, Loss: 10670.9228515625
Epoch 20000, Loss: 10666.3837890625
Epoch 20100, Loss: 10662.11328125
Epoch 20200, Loss: 10658.091796875
Epoch 20300, Loss: 10654.314453125
Epoch 20400, Loss: 10650.7734375
Epoch 20500, Loss: 10647.443359375
Epoch 20600, Loss: 10644.3173828125
Epoch 20700, Loss: 10641.3779296875
Epoch 20800, Loss: 10638.609375
Epoch 20900, Loss: 10635.99609375
Epoch 21000, Loss: 10633.5244140625
Epoch 21100, Loss: 10631.1787109375
Epoch 21200, Loss: 10628.943359375
Epoch 21300, Loss: 10626.8076171875
Epoch 21400, Loss: 10624.7529296875
Epoch 21500, Loss: 10622.8193359375
Epoch 21600, Loss: 10621.0771484375
Epoch 21700, Loss: 10619.404296875
Epoch 21800, Loss: 10617.78125
Epoch 21900, Loss: 10616.197265625
Epoch 22000, Loss: 10614.64453125
Epoch 22100, Loss: 10613.115234375
Epoch 22200, Loss: 10611.6083984375
Epoch 22300, Loss: 10610.115234375
Epoch 22400, Loss: 10608.630859375
Epoch 22500, Loss: 10607.15625
Epoch 22600, Loss: 10605.6845703125
Epoch 22700, Loss: 10604.216796875
Epoch 22800, Loss: 10602.7509765625
Epoch 22900, Loss: 10601.287109375
Epoch 23000, Loss: 10599.826171875
Epoch 23100, Loss: 10598.3662109375
Epoch 23200, Loss: 10596.9033203125
Epoch 23300, Loss: 10595.4306640625
Epoch 23400, Loss: 10593.9384765625
Epoch 23500, Loss: 10592.421875
Epoch 23600, Loss: 10590.8779296875
Epoch 23700, Loss: 10589.3095703125
Epoch 23800, Loss: 10587.7373046875
Epoch 23900, Loss: 10586.1796875
Epoch 24000, Loss: 10584.630859375
Epoch 24100, Loss: 10583.09765625
Epoch 24200, Loss: 10581.5810546875
Epoch 24300, Loss: 10580.1240234375
Epoch 24400, Loss: 10578.734375
Epoch 24500, Loss: 10577.4111328125
Epoch 24600, Loss: 10576.146484375
Epoch 24700, Loss: 10574.9423828125
Epoch 24800, Loss: 10573.7978515625
Epoch 24900, Loss: 10572.705078125
Epoch 25000, Loss: 10571.666015625
Epoch 25100, Loss: 10570.677734375
Epoch 25200, Loss: 10569.7392578125
Epoch 25300, Loss: 10568.8466796875
Epoch 25400, Loss: 10567.986328125
Epoch 25500, Loss: 10567.13671875
Epoch 25600, Loss: 10566.2841796875
Epoch 25700, Loss: 10565.4189453125
Epoch 25800, Loss: 10564.5419921875
Epoch 25900, Loss: 10563.64453125
Epoch 26000, Loss: 10562.728515625
Epoch 26100, Loss: 10561.7939453125
Epoch 26200, Loss: 10560.8369140625
Epoch 26300, Loss: 10559.8876953125
Epoch 26400, Loss: 10558.98046875
Epoch 26500, Loss: 10558.1171875
Epoch 26600, Loss: 10557.2958984375
Epoch 26700, Loss: 10556.525390625
Epoch 26800, Loss: 10555.8251953125
Epoch 26900, Loss: 10555.1904296875
Epoch 27000, Loss: 10554.615234375
Epoch 27100, Loss: 10554.0927734375
Epoch 27200, Loss: 10553.623046875
Epoch 27300, Loss: 10553.205078125
Epoch 27400, Loss: 10552.837890625
Epoch 27500, Loss: 10552.515625
Epoch 27600, Loss: 10552.2314453125
Epoch 27700, Loss: 10551.9765625
Epoch 27800, Loss: 10551.7490234375
Epoch 27900, Loss: 10551.5439453125
Epoch 28000, Loss: 10551.3603515625
Epoch 28100, Loss: 10551.1962890625
Epoch 28200, Loss: 10551.052734375
Epoch 28300, Loss: 10550.9248046875
Epoch 28400, Loss: 10550.8115234375
Epoch 28500, Loss: 10550.7138671875
Epoch 28600, Loss: 10550.62890625
Epoch 28700, Loss: 10550.55859375
Epoch 28800, Loss: 10550.4951171875
Epoch 28900, Loss: 10550.44140625
Epoch 29000, Loss: 10550.3935546875
Epoch 29100, Loss: 10550.3515625
Epoch 29200, Loss: 10550.3134765625
Epoch 29300, Loss: 10550.2783203125
Epoch 29400, Loss: 10550.2470703125
Epoch 29500, Loss: 10550.220703125
Epoch 29600, Loss: 10550.193359375
Epoch 29700, Loss: 10550.171875
Epoch 29800, Loss: 10550.15234375
Epoch 29900, Loss: 10550.134765625
Epoch 30000, Loss: 10550.119140625
Epoch 30100, Loss: 10550.109375
Epoch 30200, Loss: 10550.0966796875
Epoch 30300, Loss: 10550.0869140625
Epoch 30400, Loss: 10550.0810546875
Epoch 30500, Loss: 10550.07421875
Epoch 30600, Loss: 10550.068359375
Epoch 30700, Loss: 10550.064453125
Epoch 30800, Loss: 10550.0615234375
Epoch 30900, Loss: 10550.05859375
Epoch 31000, Loss: 10550.0546875
Epoch 31100, Loss: 10550.0546875
Epoch 31200, Loss: 10550.052734375
Epoch 31300, Loss: 10550.05078125
Epoch 31400, Loss: 10550.0498046875
Epoch 31500, Loss: 10550.0498046875
Epoch 31600, Loss: 10550.0498046875
Epoch 31700, Loss: 10550.0478515625
Epoch 31800, Loss: 10550.0478515625
Epoch 31900, Loss: 10550.0478515625
Epoch 32000, Loss: 10550.0478515625
Epoch 32100, Loss: 10550.0458984375
Epoch 32200, Loss: 10550.046875
Epoch 32300, Loss: 10550.0458984375
Epoch 32400, Loss: 10550.046875
Epoch 32500, Loss: 10550.046875
Epoch 32600, Loss: 10550.0458984375
Epoch 32700, Loss: 10550.046875
Epoch 32800, Loss: 10550.0458984375
Epoch 32900, Loss: 10550.0478515625
Epoch 33000, Loss: 10550.0458984375
Epoch 33100, Loss: 10550.046875
Epoch 33200, Loss: 10550.046875
Epoch 33300, Loss: 10550.0458984375
Epoch 33400, Loss: 10550.046875
Epoch 33500, Loss: 10550.0478515625
Epoch 33600, Loss: 10550.046875
Epoch 33700, Loss: 10550.046875
Epoch 33800, Loss: 10550.046875
Epoch 33900, Loss: 10550.0458984375
Epoch 34000, Loss: 10550.046875
Epoch 34100, Loss: 10550.046875
Epoch 34200, Loss: 10550.0478515625
Epoch 34300, Loss: 10550.0458984375
Epoch 34400, Loss: 10550.0478515625
Epoch 34500, Loss: 10550.046875
Epoch 34600, Loss: 10550.046875
Epoch 34700, Loss: 10550.046875
Epoch 34800, Loss: 10550.046875
Epoch 34900, Loss: 10550.046875
Epoch 35000, Loss: 10550.046875
Epoch 35100, Loss: 10550.046875
Epoch 35200, Loss: 10550.046875
Epoch 35300, Loss: 10550.0478515625
Epoch 35400, Loss: 10550.046875
Epoch 35500, Loss: 10550.0458984375
Epoch 35600, Loss: 10550.046875
Epoch 35700, Loss: 10550.0478515625
Epoch 35800, Loss: 10550.046875
Epoch 35900, Loss: 10550.0478515625
Epoch 36000, Loss: 10550.0458984375
Epoch 36100, Loss: 10550.0458984375
Epoch 36200, Loss: 10550.046875
Epoch 36300, Loss: 10550.046875
Epoch 36400, Loss: 10550.0478515625
Epoch 36500, Loss: 10550.046875
Epoch 36600, Loss: 10550.046875
Epoch 36700, Loss: 10550.046875
Epoch 36800, Loss: 10550.0478515625
Epoch 36900, Loss: 10550.0478515625
Epoch 37000, Loss: 10550.0478515625
Epoch 37100, Loss: 10550.046875
Epoch 37200, Loss: 10550.046875
Epoch 37300, Loss: 10550.0478515625
Epoch 37400, Loss: 10550.046875
Epoch 37500, Loss: 10550.046875
Epoch 37600, Loss: 10550.0478515625
Epoch 37700, Loss: 10550.0478515625
Epoch 37800, Loss: 10550.0478515625
Epoch 37900, Loss: 10550.046875
Epoch 38000, Loss: 10550.0478515625
Epoch 38100, Loss: 10550.046875
Epoch 38200, Loss: 10550.0478515625
Epoch 38300, Loss: 10550.0458984375
Epoch 38400, Loss: 10550.046875
Epoch 38500, Loss: 10550.0478515625
Epoch 38600, Loss: 10550.0478515625
Epoch 38700, Loss: 10550.046875
Epoch 38800, Loss: 10550.046875
Epoch 38900, Loss: 10550.046875
Epoch 39000, Loss: 10550.046875
Epoch 39100, Loss: 10550.046875
Epoch 39200, Loss: 10550.046875
Epoch 39300, Loss: 10550.046875
Epoch 39400, Loss: 10550.046875
Epoch 39500, Loss: 10550.0478515625
Epoch 39600, Loss: 10550.046875
Epoch 39700, Loss: 10550.046875
Epoch 39800, Loss: 10550.046875
Epoch 39900, Loss: 10550.0478515625
Epoch 40000, Loss: 10550.046875
Epoch 40100, Loss: 10550.0478515625
Epoch 40200, Loss: 10550.046875
Epoch 40300, Loss: 10550.046875
Epoch 40400, Loss: 10550.046875
Epoch 40500, Loss: 10550.046875
Epoch 40600, Loss: 10550.046875
Epoch 40700, Loss: 10550.0478515625
Epoch 40800, Loss: 10550.046875
Epoch 40900, Loss: 10550.046875
Epoch 41000, Loss: 10550.046875
Epoch 41100, Loss: 10550.0458984375
Epoch 41200, Loss: 10550.0478515625
Epoch 41300, Loss: 10550.046875
Epoch 41400, Loss: 10550.046875
Epoch 41500, Loss: 10550.0478515625
Epoch 41600, Loss: 10550.046875
Epoch 41700, Loss: 10550.046875
Epoch 41800, Loss: 10550.046875
Epoch 41900, Loss: 10550.0478515625
Epoch 42000, Loss: 10550.046875
Epoch 42100, Loss: 10550.0478515625
Epoch 42200, Loss: 10550.046875
Epoch 42300, Loss: 10550.046875
Epoch 42400, Loss: 10550.0478515625
Epoch 42500, Loss: 10550.046875
Epoch 42600, Loss: 10550.0478515625
Epoch 42700, Loss: 10550.046875
Epoch 42800, Loss: 10550.0478515625
Epoch 42900, Loss: 10550.0458984375
Epoch 43000, Loss: 10550.0478515625
Epoch 43100, Loss: 10550.0478515625
Epoch 43200, Loss: 10550.0458984375
Epoch 43300, Loss: 10550.046875
Epoch 43400, Loss: 10550.046875
Epoch 43500, Loss: 10550.0478515625
Epoch 43600, Loss: 10550.046875
Epoch 43700, Loss: 10550.0478515625
Epoch 43800, Loss: 10550.0458984375
Epoch 43900, Loss: 10550.046875
Epoch 44000, Loss: 10550.046875
Epoch 44100, Loss: 10550.046875
Epoch 44200, Loss: 10550.0458984375
Epoch 44300, Loss: 10550.046875
Epoch 44400, Loss: 10550.046875
Epoch 44500, Loss: 10550.046875
Epoch 44600, Loss: 10550.0478515625
Epoch 44700, Loss: 10550.046875
Epoch 44800, Loss: 10550.046875
Epoch 44900, Loss: 10550.046875
Epoch 45000, Loss: 10550.046875
Epoch 45100, Loss: 10550.0478515625
Epoch 45200, Loss: 10550.0478515625
Epoch 45300, Loss: 10550.046875
Epoch 45400, Loss: 10550.0478515625
Epoch 45500, Loss: 10550.0478515625
Epoch 45600, Loss: 10550.046875
Epoch 45700, Loss: 10550.0478515625
Epoch 45800, Loss: 10550.0478515625
Epoch 45900, Loss: 10550.046875
Epoch 46000, Loss: 10550.046875
Epoch 46100, Loss: 10550.046875
Epoch 46200, Loss: 10550.046875
Epoch 46300, Loss: 10550.046875
Epoch 46400, Loss: 10550.046875
Epoch 46500, Loss: 10550.046875
Epoch 46600, Loss: 10550.046875
Epoch 46700, Loss: 10550.0458984375
Epoch 46800, Loss: 10550.046875
Epoch 46900, Loss: 10550.0478515625
Epoch 47000, Loss: 10550.046875
Epoch 47100, Loss: 10550.046875
Epoch 47200, Loss: 10550.046875
Epoch 47300, Loss: 10550.046875
Epoch 47400, Loss: 10550.0478515625
Epoch 47500, Loss: 10550.0478515625
Epoch 47600, Loss: 10550.046875
Epoch 47700, Loss: 10550.046875
Epoch 47800, Loss: 10550.046875
Epoch 47900, Loss: 10550.0478515625
Epoch 48000, Loss: 10550.0478515625
Epoch 48100, Loss: 10550.046875
Epoch 48200, Loss: 10550.046875
Epoch 48300, Loss: 10550.0458984375
Epoch 48400, Loss: 10550.0458984375
Epoch 48500, Loss: 10550.046875
Epoch 48600, Loss: 10550.0478515625
Epoch 48700, Loss: 10550.046875
Epoch 48800, Loss: 10550.046875
Epoch 48900, Loss: 10550.048828125
Epoch 49000, Loss: 10550.046875
Epoch 49100, Loss: 10550.0478515625
Epoch 49200, Loss: 10550.046875
Epoch 49300, Loss: 10550.046875
Epoch 49400, Loss: 10550.046875
Epoch 49500, Loss: 10550.046875
Epoch 49600, Loss: 10550.046875
Epoch 49700, Loss: 10550.046875
Epoch 49800, Loss: 10550.046875
Epoch 49900, Loss: 10550.046875
Epoch 0, Loss: 63828.96875
Epoch 100, Loss: 60936.609375
Epoch 200, Loss: 58232.41015625
Epoch 300, Loss: 55702.01171875
Epoch 400, Loss: 53333.890625
Epoch 500, Loss: 51116.4453125
Epoch 600, Loss: 49040.71875
Epoch 700, Loss: 47097.6875
Epoch 800, Loss: 45279.8203125
Epoch 900, Loss: 43581.19921875
Epoch 1000, Loss: 41993.8203125
Epoch 1100, Loss: 40510.91015625
Epoch 1200, Loss: 39126.0859375
Epoch 1300, Loss: 37833.2109375
Epoch 1400, Loss: 36627.45703125
Epoch 1500, Loss: 35502.4609375
Epoch 1600, Loss: 34452.38671875
Epoch 1700, Loss: 33471.9609375
Epoch 1800, Loss: 32556.248046875
Epoch 1900, Loss: 31700.62890625
Epoch 2000, Loss: 30900.810546875
Epoch 2100, Loss: 30153.763671875
Epoch 2200, Loss: 29455.162109375
Epoch 2300, Loss: 28802.060546875
Epoch 2400, Loss: 28191.755859375
Epoch 2500, Loss: 27619.677734375
Epoch 2600, Loss: 27082.9375
Epoch 2700, Loss: 26578.8984375
Epoch 2800, Loss: 26105.123046875
Epoch 2900, Loss: 25659.35546875
Epoch 3000, Loss: 25239.49609375
Epoch 3100, Loss: 24843.578125
Epoch 3200, Loss: 24469.76171875
Epoch 3300, Loss: 24116.689453125
Epoch 3400, Loss: 23783.32421875
Epoch 3500, Loss: 23468.41796875
Epoch 3600, Loss: 23169.380859375
Epoch 3700, Loss: 22884.572265625
Epoch 3800, Loss: 22612.7109375
Epoch 3900, Loss: 22352.619140625
Epoch 4000, Loss: 22103.9453125
Epoch 4100, Loss: 21865.318359375
Epoch 4200, Loss: 21635.458984375
Epoch 4300, Loss: 21413.5546875
Epoch 4400, Loss: 21199.48046875
Epoch 4500, Loss: 20992.27734375
Epoch 4600, Loss: 20791.052734375
Epoch 4700, Loss: 20595.3203125
Epoch 4800, Loss: 20404.65625
Epoch 4900, Loss: 20218.689453125
Epoch 5000, Loss: 20037.103515625
Epoch 5100, Loss: 19859.61328125
Epoch 5200, Loss: 19685.982421875
Epoch 5300, Loss: 19516.0
Epoch 5400, Loss: 19349.4765625
Epoch 5500, Loss: 19186.244140625
Epoch 5600, Loss: 19026.15625
Epoch 5700, Loss: 18869.080078125
Epoch 5800, Loss: 18715.4296875
Epoch 5900, Loss: 18564.720703125
Epoch 6000, Loss: 18416.673828125
Epoch 6100, Loss: 18271.19921875
Epoch 6200, Loss: 18128.2109375
Epoch 6300, Loss: 17987.638671875
Epoch 6400, Loss: 17849.408203125
Epoch 6500, Loss: 17713.458984375
Epoch 6600, Loss: 17579.73046875
Epoch 6700, Loss: 17448.166015625
Epoch 6800, Loss: 17318.71484375
Epoch 6900, Loss: 17191.328125
Epoch 7000, Loss: 17066.248046875
Epoch 7100, Loss: 16943.986328125
Epoch 7200, Loss: 16823.6015625
Epoch 7300, Loss: 16705.041015625
Epoch 7400, Loss: 16588.3125
Epoch 7500, Loss: 16474.11328125
Epoch 7600, Loss: 16361.607421875
Epoch 7700, Loss: 16250.744140625
Epoch 7800, Loss: 16141.484375
Epoch 7900, Loss: 16033.8046875
Epoch 8000, Loss: 15927.669921875
Epoch 8100, Loss: 15823.06640625
Epoch 8200, Loss: 15719.953125
Epoch 8300, Loss: 15618.2978515625
Epoch 8400, Loss: 15518.08203125
Epoch 8500, Loss: 15419.287109375
Epoch 8600, Loss: 15321.8857421875
Epoch 8700, Loss: 15225.998046875
Epoch 8800, Loss: 15131.7919921875
Epoch 8900, Loss: 15039.0
Epoch 9000, Loss: 14947.59375
Epoch 9100, Loss: 14857.54296875
Epoch 9200, Loss: 14768.83203125
Epoch 9300, Loss: 14681.4296875
Epoch 9400, Loss: 14595.3154296875
Epoch 9500, Loss: 14510.4775390625
Epoch 9600, Loss: 14426.8994140625
Epoch 9700, Loss: 14344.568359375
Epoch 9800, Loss: 14263.47265625
Epoch 9900, Loss: 14183.6044921875
Epoch 10000, Loss: 14104.9609375
Epoch 10100, Loss: 14027.5341796875
Epoch 10200, Loss: 13951.3193359375
Epoch 10300, Loss: 13876.3076171875
Epoch 10400, Loss: 13802.4990234375
Epoch 10500, Loss: 13729.8837890625
Epoch 10600, Loss: 13658.458984375
Epoch 10700, Loss: 13588.2099609375
Epoch 10800, Loss: 13519.1328125
Epoch 10900, Loss: 13451.22265625
Epoch 11000, Loss: 13384.5087890625
Epoch 11100, Loss: 13318.984375
Epoch 11200, Loss: 13254.607421875
Epoch 11300, Loss: 13191.3603515625
Epoch 11400, Loss: 13129.2177734375
Epoch 11500, Loss: 13068.166015625
Epoch 11600, Loss: 13008.18359375
Epoch 11700, Loss: 12949.2646484375
Epoch 11800, Loss: 12891.6494140625
Epoch 11900, Loss: 12835.4521484375
Epoch 12000, Loss: 12780.7958984375
Epoch 12100, Loss: 12727.244140625
Epoch 12200, Loss: 12674.71875
Epoch 12300, Loss: 12623.166015625
Epoch 12400, Loss: 12572.548828125
Epoch 12500, Loss: 12522.8408203125
Epoch 12600, Loss: 12474.01953125
Epoch 12700, Loss: 12426.064453125
Epoch 12800, Loss: 12378.9619140625
Epoch 12900, Loss: 12332.69921875
Epoch 13000, Loss: 12287.2626953125
Epoch 13100, Loss: 12242.759765625
Epoch 13200, Loss: 12199.2685546875
Epoch 13300, Loss: 12156.7041015625
Epoch 13400, Loss: 12115.0185546875
Epoch 13500, Loss: 12074.1416015625
Epoch 13600, Loss: 12034.09375
Epoch 13700, Loss: 11995.0322265625
Epoch 13800, Loss: 11956.8349609375
Epoch 13900, Loss: 11919.486328125
Epoch 14000, Loss: 11882.970703125
Epoch 14100, Loss: 11847.2666015625
Epoch 14200, Loss: 11812.37890625
Epoch 14300, Loss: 11778.3935546875
Epoch 14400, Loss: 11745.244140625
Epoch 14500, Loss: 11712.9111328125
Epoch 14600, Loss: 11681.376953125
Epoch 14700, Loss: 11650.6220703125
Epoch 14800, Loss: 11620.6796875
Epoch 14900, Loss: 11591.5625
Epoch 15000, Loss: 11563.193359375
Epoch 15100, Loss: 11535.541015625
Epoch 15200, Loss: 11508.578125
Epoch 15300, Loss: 11482.2841796875
Epoch 15400, Loss: 11456.6357421875
Epoch 15500, Loss: 11431.6044921875
Epoch 15600, Loss: 11407.201171875
Epoch 15700, Loss: 11383.4169921875
Epoch 15800, Loss: 11360.24609375
Epoch 15900, Loss: 11337.673828125
Epoch 16000, Loss: 11315.6904296875
Epoch 16100, Loss: 11294.291015625
Epoch 16200, Loss: 11273.46484375
Epoch 16300, Loss: 11253.1982421875
Epoch 16400, Loss: 11233.490234375
Epoch 16500, Loss: 11214.4296875
Epoch 16600, Loss: 11195.9404296875
Epoch 16700, Loss: 11177.9951171875
Epoch 16800, Loss: 11160.5791015625
Epoch 16900, Loss: 11143.681640625
Epoch 17000, Loss: 11127.3837890625
Epoch 17100, Loss: 11111.7841796875
Epoch 17200, Loss: 11096.7373046875
Epoch 17300, Loss: 11082.1728515625
Epoch 17400, Loss: 11068.0712890625
Epoch 17500, Loss: 11054.4130859375
Epoch 17600, Loss: 11041.1845703125
Epoch 17700, Loss: 11028.3798828125
Epoch 17800, Loss: 11015.9921875
Epoch 17900, Loss: 11004.025390625
Epoch 18000, Loss: 10992.470703125
Epoch 18100, Loss: 10981.3203125
Epoch 18200, Loss: 10970.615234375
Epoch 18300, Loss: 10960.333984375
Epoch 18400, Loss: 10950.451171875
Epoch 18500, Loss: 10940.962890625
Epoch 18600, Loss: 10931.859375
Epoch 18700, Loss: 10923.138671875
Epoch 18800, Loss: 10914.8046875
Epoch 18900, Loss: 10906.845703125
Epoch 19000, Loss: 10899.255859375
Epoch 19100, Loss: 10892.0244140625
Epoch 19200, Loss: 10885.1376953125
Epoch 19300, Loss: 10878.5927734375
Epoch 19400, Loss: 10872.3740234375
Epoch 19500, Loss: 10866.478515625
Epoch 19600, Loss: 10860.8935546875
Epoch 19700, Loss: 10855.607421875
Epoch 19800, Loss: 10850.611328125
Epoch 19900, Loss: 10845.8896484375
Epoch 20000, Loss: 10841.431640625
Epoch 20100, Loss: 10837.21875
Epoch 20200, Loss: 10833.23828125
Epoch 20300, Loss: 10829.4736328125
Epoch 20400, Loss: 10825.9140625
Epoch 20500, Loss: 10822.537109375
Epoch 20600, Loss: 10819.33203125
Epoch 20700, Loss: 10816.283203125
Epoch 20800, Loss: 10813.3740234375
Epoch 20900, Loss: 10810.5869140625
Epoch 21000, Loss: 10808.0712890625
Epoch 21100, Loss: 10805.728515625
Epoch 21200, Loss: 10803.478515625
Epoch 21300, Loss: 10801.306640625
Epoch 21400, Loss: 10799.201171875
Epoch 21500, Loss: 10797.150390625
Epoch 21600, Loss: 10795.142578125
Epoch 21700, Loss: 10793.171875
Epoch 21800, Loss: 10791.2255859375
Epoch 21900, Loss: 10789.3046875
Epoch 22000, Loss: 10787.3984375
Epoch 22100, Loss: 10785.501953125
Epoch 22200, Loss: 10783.615234375
Epoch 22300, Loss: 10781.734375
Epoch 22400, Loss: 10779.8525390625
Epoch 22500, Loss: 10777.9755859375
Epoch 22600, Loss: 10776.08984375
Epoch 22700, Loss: 10774.185546875
Epoch 22800, Loss: 10772.26171875
Epoch 22900, Loss: 10770.3330078125
Epoch 23000, Loss: 10768.4111328125
Epoch 23100, Loss: 10766.48828125
Epoch 23200, Loss: 10764.5693359375
Epoch 23300, Loss: 10762.6572265625
Epoch 23400, Loss: 10760.7685546875
Epoch 23500, Loss: 10758.9345703125
Epoch 23600, Loss: 10757.1572265625
Epoch 23700, Loss: 10755.4375
Epoch 23800, Loss: 10753.7734375
Epoch 23900, Loss: 10752.1650390625
Epoch 24000, Loss: 10750.6083984375
Epoch 24100, Loss: 10749.107421875
Epoch 24200, Loss: 10747.66015625
Epoch 24300, Loss: 10746.26171875
Epoch 24400, Loss: 10744.91796875
Epoch 24500, Loss: 10743.623046875
Epoch 24600, Loss: 10742.37890625
Epoch 24700, Loss: 10741.1826171875
Epoch 24800, Loss: 10740.02734375
Epoch 24900, Loss: 10738.8974609375
Epoch 25000, Loss: 10737.7783203125
Epoch 25100, Loss: 10736.6591796875
Epoch 25200, Loss: 10735.53125
Epoch 25300, Loss: 10734.3876953125
Epoch 25400, Loss: 10733.2255859375
Epoch 25500, Loss: 10732.0537109375
Epoch 25600, Loss: 10730.9072265625
Epoch 25700, Loss: 10729.796875
Epoch 25800, Loss: 10728.7197265625
Epoch 25900, Loss: 10727.6806640625
Epoch 26000, Loss: 10726.703125
Epoch 26100, Loss: 10725.7919921875
Epoch 26200, Loss: 10724.9404296875
Epoch 26300, Loss: 10724.1494140625
Epoch 26400, Loss: 10723.4130859375
Epoch 26500, Loss: 10722.7314453125
Epoch 26600, Loss: 10722.1064453125
Epoch 26700, Loss: 10721.5439453125
Epoch 26800, Loss: 10721.029296875
Epoch 26900, Loss: 10720.5673828125
Epoch 27000, Loss: 10720.1494140625
Epoch 27100, Loss: 10719.7724609375
Epoch 27200, Loss: 10719.4326171875
Epoch 27300, Loss: 10719.1279296875
Epoch 27400, Loss: 10718.8525390625
Epoch 27500, Loss: 10718.6025390625
Epoch 27600, Loss: 10718.3740234375
Epoch 27700, Loss: 10718.1640625
Epoch 27800, Loss: 10717.9765625
Epoch 27900, Loss: 10717.8056640625
Epoch 28000, Loss: 10717.6533203125
Epoch 28100, Loss: 10717.517578125
Epoch 28200, Loss: 10717.3935546875
Epoch 28300, Loss: 10717.2890625
Epoch 28400, Loss: 10717.193359375
Epoch 28500, Loss: 10717.109375
Epoch 28600, Loss: 10717.0400390625
Epoch 28700, Loss: 10716.9775390625
Epoch 28800, Loss: 10716.9208984375
Epoch 28900, Loss: 10716.873046875
Epoch 29000, Loss: 10716.8271484375
Epoch 29100, Loss: 10716.78515625
Epoch 29200, Loss: 10716.75
Epoch 29300, Loss: 10716.71484375
Epoch 29400, Loss: 10716.6845703125
Epoch 29500, Loss: 10716.6572265625
Epoch 29600, Loss: 10716.6337890625
Epoch 29700, Loss: 10716.6123046875
Epoch 29800, Loss: 10716.5947265625
Epoch 29900, Loss: 10716.578125
Epoch 30000, Loss: 10716.5654296875
Epoch 30100, Loss: 10716.5537109375
Epoch 30200, Loss: 10716.5419921875
Epoch 30300, Loss: 10716.53515625
Epoch 30400, Loss: 10716.5283203125
Epoch 30500, Loss: 10716.5234375
Epoch 30600, Loss: 10716.5185546875
Epoch 30700, Loss: 10716.515625
Epoch 30800, Loss: 10716.5126953125
Epoch 30900, Loss: 10716.51171875
Epoch 31000, Loss: 10716.5078125
Epoch 31100, Loss: 10716.5078125
Epoch 31200, Loss: 10716.5068359375
Epoch 31300, Loss: 10716.5048828125
Epoch 31400, Loss: 10716.5048828125
Epoch 31500, Loss: 10716.5048828125
Epoch 31600, Loss: 10716.5048828125
Epoch 31700, Loss: 10716.505859375
Epoch 31800, Loss: 10716.5048828125
Epoch 31900, Loss: 10716.5068359375
Epoch 32000, Loss: 10716.505859375
Epoch 32100, Loss: 10716.505859375
Epoch 32200, Loss: 10716.5048828125
Epoch 32300, Loss: 10716.5048828125
Epoch 32400, Loss: 10716.5048828125
Epoch 32500, Loss: 10716.5048828125
Epoch 32600, Loss: 10716.50390625
Epoch 32700, Loss: 10716.505859375
Epoch 32800, Loss: 10716.5048828125
Epoch 32900, Loss: 10716.5048828125
Epoch 33000, Loss: 10716.50390625
Epoch 33100, Loss: 10716.5048828125
Epoch 33200, Loss: 10716.505859375
Epoch 33300, Loss: 10716.5048828125
Epoch 33400, Loss: 10716.5048828125
Epoch 33500, Loss: 10716.50390625
Epoch 33600, Loss: 10716.5048828125
Epoch 33700, Loss: 10716.5048828125
Epoch 33800, Loss: 10716.50390625
Epoch 33900, Loss: 10716.5048828125
Epoch 34000, Loss: 10716.5048828125
Epoch 34100, Loss: 10716.5048828125
Epoch 34200, Loss: 10716.5048828125
Epoch 34300, Loss: 10716.50390625
Epoch 34400, Loss: 10716.505859375
Epoch 34500, Loss: 10716.505859375
Epoch 34600, Loss: 10716.5048828125
Epoch 34700, Loss: 10716.5048828125
Epoch 34800, Loss: 10716.50390625
Epoch 34900, Loss: 10716.5048828125
Epoch 35000, Loss: 10716.50390625
Epoch 35100, Loss: 10716.505859375
Epoch 35200, Loss: 10716.50390625
Epoch 35300, Loss: 10716.5048828125
Epoch 35400, Loss: 10716.50390625
Epoch 35500, Loss: 10716.5048828125
Epoch 35600, Loss: 10716.5048828125
Epoch 35700, Loss: 10716.505859375
Epoch 35800, Loss: 10716.5048828125
Epoch 35900, Loss: 10716.5048828125
Epoch 36000, Loss: 10716.5048828125
Epoch 36100, Loss: 10716.50390625
Epoch 36200, Loss: 10716.5048828125
Epoch 36300, Loss: 10716.5048828125
Epoch 36400, Loss: 10716.5048828125
Epoch 36500, Loss: 10716.505859375
Epoch 36600, Loss: 10716.50390625
Epoch 36700, Loss: 10716.5048828125
Epoch 36800, Loss: 10716.5048828125
Epoch 36900, Loss: 10716.50390625
Epoch 37000, Loss: 10716.5048828125
Epoch 37100, Loss: 10716.5048828125
Epoch 37200, Loss: 10716.5048828125
Epoch 37300, Loss: 10716.5029296875
Epoch 37400, Loss: 10716.5048828125
Epoch 37500, Loss: 10716.5048828125
Epoch 37600, Loss: 10716.5048828125
Epoch 37700, Loss: 10716.5048828125
Epoch 37800, Loss: 10716.50390625
Epoch 37900, Loss: 10716.5048828125
Epoch 38000, Loss: 10716.5048828125
Epoch 38100, Loss: 10716.5048828125
Epoch 38200, Loss: 10716.5048828125
Epoch 38300, Loss: 10716.5048828125
Epoch 38400, Loss: 10716.505859375
Epoch 38500, Loss: 10716.5048828125
Epoch 38600, Loss: 10716.505859375
Epoch 38700, Loss: 10716.505859375
Epoch 38800, Loss: 10716.505859375
Epoch 38900, Loss: 10716.50390625
Epoch 39000, Loss: 10716.50390625
Epoch 39100, Loss: 10716.5048828125
Epoch 39200, Loss: 10716.50390625
Epoch 39300, Loss: 10716.5048828125
Epoch 39400, Loss: 10716.5048828125
Epoch 39500, Loss: 10716.5048828125
Epoch 39600, Loss: 10716.50390625
Epoch 39700, Loss: 10716.5048828125
Epoch 39800, Loss: 10716.5048828125
Epoch 39900, Loss: 10716.50390625
Epoch 40000, Loss: 10716.5048828125
Epoch 40100, Loss: 10716.5048828125
Epoch 40200, Loss: 10716.50390625
Epoch 40300, Loss: 10716.505859375
Epoch 40400, Loss: 10716.50390625
Epoch 40500, Loss: 10716.50390625
Epoch 40600, Loss: 10716.5048828125
Epoch 40700, Loss: 10716.5048828125
Epoch 40800, Loss: 10716.50390625
Epoch 40900, Loss: 10716.505859375
Epoch 41000, Loss: 10716.50390625
Epoch 41100, Loss: 10716.5048828125
Epoch 41200, Loss: 10716.505859375
Epoch 41300, Loss: 10716.505859375
Epoch 41400, Loss: 10716.5048828125
Epoch 41500, Loss: 10716.505859375
Epoch 41600, Loss: 10716.50390625
Epoch 41700, Loss: 10716.5048828125
Epoch 41800, Loss: 10716.50390625
Epoch 41900, Loss: 10716.505859375
Epoch 42000, Loss: 10716.5048828125
Epoch 42100, Loss: 10716.5048828125
Epoch 42200, Loss: 10716.505859375
Epoch 42300, Loss: 10716.5048828125
Epoch 42400, Loss: 10716.5048828125
Epoch 42500, Loss: 10716.5048828125
Epoch 42600, Loss: 10716.5048828125
Epoch 42700, Loss: 10716.5048828125
Epoch 42800, Loss: 10716.50390625
Epoch 42900, Loss: 10716.505859375
Epoch 43000, Loss: 10716.5048828125
Epoch 43100, Loss: 10716.5048828125
Epoch 43200, Loss: 10716.5048828125
Epoch 43300, Loss: 10716.50390625
Epoch 43400, Loss: 10716.505859375
Epoch 43500, Loss: 10716.50390625
Epoch 43600, Loss: 10716.50390625
Epoch 43700, Loss: 10716.5048828125
Epoch 43800, Loss: 10716.5048828125
Epoch 43900, Loss: 10716.50390625
Epoch 44000, Loss: 10716.5048828125
Epoch 44100, Loss: 10716.5048828125
Epoch 44200, Loss: 10716.5048828125
Epoch 44300, Loss: 10716.5048828125
Epoch 44400, Loss: 10716.5048828125
Epoch 44500, Loss: 10716.5048828125
Epoch 44600, Loss: 10716.5048828125
Epoch 44700, Loss: 10716.5048828125
Epoch 44800, Loss: 10716.5048828125
Epoch 44900, Loss: 10716.5048828125
Epoch 45000, Loss: 10716.50390625
Epoch 45100, Loss: 10716.505859375
Epoch 45200, Loss: 10716.50390625
Epoch 45300, Loss: 10716.50390625
Epoch 45400, Loss: 10716.50390625
Epoch 45500, Loss: 10716.5048828125
Epoch 45600, Loss: 10716.5048828125
Epoch 45700, Loss: 10716.5048828125
Epoch 45800, Loss: 10716.5048828125
Epoch 45900, Loss: 10716.50390625
Epoch 46000, Loss: 10716.505859375
Epoch 46100, Loss: 10716.505859375
Epoch 46200, Loss: 10716.50390625
Epoch 46300, Loss: 10716.5048828125
Epoch 46400, Loss: 10716.5048828125
Epoch 46500, Loss: 10716.505859375
Epoch 46600, Loss: 10716.5048828125
Epoch 46700, Loss: 10716.505859375
Epoch 46800, Loss: 10716.50390625
Epoch 46900, Loss: 10716.5048828125
Epoch 47000, Loss: 10716.5048828125
Epoch 47100, Loss: 10716.5048828125
Epoch 47200, Loss: 10716.50390625
Epoch 47300, Loss: 10716.5048828125
Epoch 47400, Loss: 10716.5048828125
Epoch 47500, Loss: 10716.5048828125
Epoch 47600, Loss: 10716.5048828125
Epoch 47700, Loss: 10716.50390625
Epoch 47800, Loss: 10716.5048828125
Epoch 47900, Loss: 10716.5048828125
Epoch 48000, Loss: 10716.50390625
Epoch 48100, Loss: 10716.5048828125
Epoch 48200, Loss: 10716.505859375
Epoch 48300, Loss: 10716.505859375
Epoch 48400, Loss: 10716.505859375
Epoch 48500, Loss: 10716.5048828125
Epoch 48600, Loss: 10716.5048828125
Epoch 48700, Loss: 10716.505859375
Epoch 48800, Loss: 10716.5048828125
Epoch 48900, Loss: 10716.50390625
Epoch 49000, Loss: 10716.50390625
Epoch 49100, Loss: 10716.5048828125
Epoch 49200, Loss: 10716.5048828125
Epoch 49300, Loss: 10716.5048828125
Epoch 49400, Loss: 10716.5048828125
Epoch 49500, Loss: 10716.5048828125
Epoch 49600, Loss: 10716.505859375
Epoch 49700, Loss: 10716.50390625
Epoch 49800, Loss: 10716.50390625
Epoch 49900, Loss: 10716.5048828125
Epoch 0, Loss: 63828.96875
Epoch 100, Loss: 60940.59765625
Epoch 200, Loss: 58240.16796875
Epoch 300, Loss: 55713.9921875
Epoch 400, Loss: 53349.96484375
Epoch 500, Loss: 51136.6640625
Epoch 600, Loss: 49064.69921875
Epoch 700, Loss: 47125.12109375
Epoch 800, Loss: 45310.76171875
Epoch 900, Loss: 43615.49609375
Epoch 1000, Loss: 42031.2578125
Epoch 1100, Loss: 40551.2734375
Epoch 1200, Loss: 39169.13671875
Epoch 1300, Loss: 37878.7734375
Epoch 1400, Loss: 36675.87109375
Epoch 1500, Loss: 35553.30859375
Epoch 1600, Loss: 34505.46875
Epoch 1700, Loss: 33527.08984375
Epoch 1800, Loss: 32613.2421875
Epoch 1900, Loss: 31759.341796875
Epoch 2000, Loss: 30961.166015625
Epoch 2100, Loss: 30216.2265625
Epoch 2200, Loss: 29519.34375
Epoch 2300, Loss: 28868.75
Epoch 2400, Loss: 28260.6015625
Epoch 2500, Loss: 27690.572265625
Epoch 2600, Loss: 27155.779296875
Epoch 2700, Loss: 26653.580078125
Epoch 2800, Loss: 26181.55078125
Epoch 2900, Loss: 25737.435546875
Epoch 3000, Loss: 25319.146484375
Epoch 3100, Loss: 24924.720703125
Epoch 3200, Loss: 24552.33984375
Epoch 3300, Loss: 24200.74609375
Epoch 3400, Loss: 23869.240234375
Epoch 3500, Loss: 23556.51953125
Epoch 3600, Loss: 23259.3828125
Epoch 3700, Loss: 22976.400390625
Epoch 3800, Loss: 22706.302734375
Epoch 3900, Loss: 22447.91015625
Epoch 4000, Loss: 22201.443359375
Epoch 4100, Loss: 21964.6328125
Epoch 4200, Loss: 21736.533203125
Epoch 4300, Loss: 21516.333984375
Epoch 4400, Loss: 21304.322265625
Epoch 4500, Loss: 21098.9140625
Epoch 4600, Loss: 20899.435546875
Epoch 4700, Loss: 20705.39453125
Epoch 4800, Loss: 20516.3671875
Epoch 4900, Loss: 20331.9921875
Epoch 5000, Loss: 20151.951171875
Epoch 5100, Loss: 19975.96484375
Epoch 5200, Loss: 19803.796875
Epoch 5300, Loss: 19635.236328125
Epoch 5400, Loss: 19470.095703125
Epoch 5500, Loss: 19308.20703125
Epoch 5600, Loss: 19149.427734375
Epoch 5700, Loss: 18993.998046875
Epoch 5800, Loss: 18841.90234375
Epoch 5900, Loss: 18692.53125
Epoch 6000, Loss: 18545.7890625
Epoch 6100, Loss: 18401.583984375
Epoch 6200, Loss: 18259.833984375
Epoch 6300, Loss: 18120.462890625
Epoch 6400, Loss: 17983.3984375
Epoch 6500, Loss: 17848.583984375
Epoch 6600, Loss: 17715.955078125
Epoch 6700, Loss: 17585.455078125
Epoch 6800, Loss: 17457.041015625
Epoch 6900, Loss: 17330.658203125
Epoch 7000, Loss: 17207.42578125
Epoch 7100, Loss: 17086.32421875
Epoch 7200, Loss: 16967.05078125
Epoch 7300, Loss: 16849.5546875
Epoch 7400, Loss: 16733.80078125
Epoch 7500, Loss: 16620.720703125
Epoch 7600, Loss: 16509.3828125
Epoch 7700, Loss: 16399.640625
Epoch 7800, Loss: 16291.4580078125
Epoch 7900, Loss: 16184.80859375
Epoch 8000, Loss: 16079.6611328125
Epoch 8100, Loss: 15975.9912109375
Epoch 8200, Loss: 15873.7802734375
Epoch 8300, Loss: 15772.994140625
Epoch 8400, Loss: 15673.60546875
Epoch 8500, Loss: 15575.60546875
Epoch 8600, Loss: 15479.3779296875
Epoch 8700, Loss: 15384.6005859375
Epoch 8800, Loss: 15291.2099609375
Epoch 8900, Loss: 15199.1904296875
Epoch 9000, Loss: 15108.5224609375
Epoch 9100, Loss: 15019.189453125
Epoch 9200, Loss: 14931.17578125
Epoch 9300, Loss: 14844.458984375
Epoch 9400, Loss: 14759.0166015625
Epoch 9500, Loss: 14674.830078125
Epoch 9600, Loss: 14591.8857421875
Epoch 9700, Loss: 14510.1669921875
Epoch 9800, Loss: 14429.6630859375
Epoch 9900, Loss: 14350.365234375
Epoch 10000, Loss: 14272.2666015625
Epoch 10100, Loss: 14195.3564453125
Epoch 10200, Loss: 14119.6318359375
Epoch 10300, Loss: 14045.0908203125
Epoch 10400, Loss: 13971.71875
Epoch 10500, Loss: 13899.5185546875
Epoch 10600, Loss: 13828.4765625
Epoch 10700, Loss: 13758.58984375
Epoch 10800, Loss: 13689.8515625
Epoch 10900, Loss: 13622.333984375
Epoch 11000, Loss: 13555.966796875
Epoch 11100, Loss: 13490.728515625
Epoch 11200, Loss: 13426.6064453125
Epoch 11300, Loss: 13363.578125
Epoch 11400, Loss: 13301.74609375
Epoch 11500, Loss: 13241.2998046875
Epoch 11600, Loss: 13182.09765625
Epoch 11700, Loss: 13124.0693359375
Epoch 11800, Loss: 13067.34765625
Epoch 11900, Loss: 13012.0380859375
Epoch 12000, Loss: 12957.83203125
Epoch 12100, Loss: 12904.6845703125
Epoch 12200, Loss: 12852.5390625
Epoch 12300, Loss: 12801.3525390625
Epoch 12400, Loss: 12751.0810546875
Epoch 12500, Loss: 12701.701171875
Epoch 12600, Loss: 12653.185546875
Epoch 12700, Loss: 12605.7138671875
Epoch 12800, Loss: 12559.2822265625
Epoch 12900, Loss: 12513.818359375
Epoch 13000, Loss: 12469.2802734375
Epoch 13100, Loss: 12425.603515625
Epoch 13200, Loss: 12382.736328125
Epoch 13300, Loss: 12340.6552734375
Epoch 13400, Loss: 12299.3779296875
Epoch 13500, Loss: 12259.0849609375
Epoch 13600, Loss: 12219.6865234375
Epoch 13700, Loss: 12181.1416015625
Epoch 13800, Loss: 12143.44921875
Epoch 13900, Loss: 12106.5927734375
Epoch 14000, Loss: 12070.560546875
Epoch 14100, Loss: 12035.359375
Epoch 14200, Loss: 12001.0380859375
Epoch 14300, Loss: 11967.5693359375
Epoch 14400, Loss: 11935.044921875
Epoch 14500, Loss: 11903.412109375
Epoch 14600, Loss: 11872.5908203125
Epoch 14700, Loss: 11842.5517578125
Epoch 14800, Loss: 11813.2744140625
Epoch 14900, Loss: 11784.740234375
Epoch 15000, Loss: 11756.92578125
Epoch 15100, Loss: 11729.8115234375
Epoch 15200, Loss: 11703.3671875
Epoch 15300, Loss: 11677.5732421875
Epoch 15400, Loss: 11652.419921875
Epoch 15500, Loss: 11627.91796875
Epoch 15600, Loss: 11604.052734375
Epoch 15700, Loss: 11580.7958984375
Epoch 15800, Loss: 11558.138671875
Epoch 15900, Loss: 11536.05859375
Epoch 16000, Loss: 11514.5458984375
Epoch 16100, Loss: 11493.5859375
Epoch 16200, Loss: 11473.1826171875
Epoch 16300, Loss: 11453.45703125
Epoch 16400, Loss: 11434.3134765625
Epoch 16500, Loss: 11416.00390625
Epoch 16600, Loss: 11398.390625
Epoch 16700, Loss: 11381.3583984375
Epoch 16800, Loss: 11364.859375
Epoch 16900, Loss: 11348.8564453125
Epoch 17000, Loss: 11333.3408203125
Epoch 17100, Loss: 11318.296875
Epoch 17200, Loss: 11303.708984375
Epoch 17300, Loss: 11289.5810546875
Epoch 17400, Loss: 11275.8974609375
Epoch 17500, Loss: 11262.650390625
Epoch 17600, Loss: 11249.8359375
Epoch 17700, Loss: 11237.443359375
Epoch 17800, Loss: 11225.5166015625
Epoch 17900, Loss: 11214.04296875
Epoch 18000, Loss: 11202.9775390625
Epoch 18100, Loss: 11192.31640625
Epoch 18200, Loss: 11182.0478515625
Epoch 18300, Loss: 11172.171875
Epoch 18400, Loss: 11162.6884765625
Epoch 18500, Loss: 11153.5869140625
Epoch 18600, Loss: 11144.857421875
Epoch 18700, Loss: 11136.4892578125
Epoch 18800, Loss: 11128.474609375
Epoch 18900, Loss: 11120.80859375
Epoch 19000, Loss: 11113.486328125
Epoch 19100, Loss: 11106.498046875
Epoch 19200, Loss: 11099.8349609375
Epoch 19300, Loss: 11093.4892578125
Epoch 19400, Loss: 11087.4501953125
Epoch 19500, Loss: 11081.7041015625
Epoch 19600, Loss: 11076.244140625
Epoch 19700, Loss: 11071.046875
Epoch 19800, Loss: 11066.107421875
Epoch 19900, Loss: 11061.404296875
Epoch 20000, Loss: 11056.927734375
Epoch 20100, Loss: 11052.6572265625
Epoch 20200, Loss: 11048.5791015625
Epoch 20300, Loss: 11044.67578125
Epoch 20400, Loss: 11040.9365234375
Epoch 20500, Loss: 11037.53515625
Epoch 20600, Loss: 11034.3330078125
Epoch 20700, Loss: 11031.2705078125
Epoch 20800, Loss: 11028.3203125
Epoch 20900, Loss: 11025.4765625
Epoch 21000, Loss: 11022.720703125
Epoch 21100, Loss: 11020.048828125
Epoch 21200, Loss: 11017.453125
Epoch 21300, Loss: 11014.9140625
Epoch 21400, Loss: 11012.4208984375
Epoch 21500, Loss: 11009.9560546875
Epoch 21600, Loss: 11007.5185546875
Epoch 21700, Loss: 11005.09765625
Epoch 21800, Loss: 11002.6953125
Epoch 21900, Loss: 11000.3017578125
Epoch 22000, Loss: 10997.916015625
Epoch 22100, Loss: 10995.53125
Epoch 22200, Loss: 10993.146484375
Epoch 22300, Loss: 10990.76171875
Epoch 22400, Loss: 10988.388671875
Epoch 22500, Loss: 10986.05078125
Epoch 22600, Loss: 10983.75390625
Epoch 22700, Loss: 10981.501953125
Epoch 22800, Loss: 10979.29296875
Epoch 22900, Loss: 10977.1279296875
Epoch 23000, Loss: 10975.0107421875
Epoch 23100, Loss: 10972.939453125
Epoch 23200, Loss: 10970.9189453125
Epoch 23300, Loss: 10968.94140625
Epoch 23400, Loss: 10967.021484375
Epoch 23500, Loss: 10965.14453125
Epoch 23600, Loss: 10963.318359375
Epoch 23700, Loss: 10961.5439453125
Epoch 23800, Loss: 10959.8193359375
Epoch 23900, Loss: 10958.1416015625
Epoch 24000, Loss: 10956.5166015625
Epoch 24100, Loss: 10954.939453125
Epoch 24200, Loss: 10953.404296875
Epoch 24300, Loss: 10951.896484375
Epoch 24400, Loss: 10950.408203125
Epoch 24500, Loss: 10948.9287109375
Epoch 24600, Loss: 10947.4765625
Epoch 24700, Loss: 10946.0556640625
Epoch 24800, Loss: 10944.6640625
Epoch 24900, Loss: 10943.30078125
Epoch 25000, Loss: 10941.96875
Epoch 25100, Loss: 10940.68359375
Epoch 25200, Loss: 10939.455078125
Epoch 25300, Loss: 10938.2861328125
Epoch 25400, Loss: 10937.1669921875
Epoch 25500, Loss: 10936.0986328125
Epoch 25600, Loss: 10935.0908203125
Epoch 25700, Loss: 10934.13671875
Epoch 25800, Loss: 10933.248046875
Epoch 25900, Loss: 10932.4169921875
Epoch 26000, Loss: 10931.6484375
Epoch 26100, Loss: 10930.9228515625
Epoch 26200, Loss: 10930.255859375
Epoch 26300, Loss: 10929.634765625
Epoch 26400, Loss: 10929.0634765625
Epoch 26500, Loss: 10928.53125
Epoch 26600, Loss: 10928.041015625
Epoch 26700, Loss: 10927.58984375
Epoch 26800, Loss: 10927.17578125
Epoch 26900, Loss: 10926.798828125
Epoch 27000, Loss: 10926.4521484375
Epoch 27100, Loss: 10926.13671875
Epoch 27200, Loss: 10925.849609375
Epoch 27300, Loss: 10925.5888671875
Epoch 27400, Loss: 10925.3447265625
Epoch 27500, Loss: 10925.125
Epoch 27600, Loss: 10924.919921875
Epoch 27700, Loss: 10924.7353515625
Epoch 27800, Loss: 10924.568359375
Epoch 27900, Loss: 10924.4169921875
Epoch 28000, Loss: 10924.2783203125
Epoch 28100, Loss: 10924.1552734375
Epoch 28200, Loss: 10924.046875
Epoch 28300, Loss: 10923.94921875
Epoch 28400, Loss: 10923.8662109375
Epoch 28500, Loss: 10923.791015625
Epoch 28600, Loss: 10923.724609375
Epoch 28700, Loss: 10923.6689453125
Epoch 28800, Loss: 10923.6171875
Epoch 28900, Loss: 10923.5693359375
Epoch 29000, Loss: 10923.525390625
Epoch 29100, Loss: 10923.486328125
Epoch 29200, Loss: 10923.4482421875
Epoch 29300, Loss: 10923.4169921875
Epoch 29400, Loss: 10923.38671875
Epoch 29500, Loss: 10923.3603515625
Epoch 29600, Loss: 10923.337890625
Epoch 29700, Loss: 10923.31640625
Epoch 29800, Loss: 10923.298828125
Epoch 29900, Loss: 10923.2841796875
Epoch 30000, Loss: 10923.271484375
Epoch 30100, Loss: 10923.2587890625
Epoch 30200, Loss: 10923.248046875
Epoch 30300, Loss: 10923.240234375
Epoch 30400, Loss: 10923.2333984375
Epoch 30500, Loss: 10923.228515625
Epoch 30600, Loss: 10923.2255859375
Epoch 30700, Loss: 10923.22265625
Epoch 30800, Loss: 10923.21875
Epoch 30900, Loss: 10923.216796875
Epoch 31000, Loss: 10923.2158203125
Epoch 31100, Loss: 10923.2138671875
Epoch 31200, Loss: 10923.2119140625
Epoch 31300, Loss: 10923.2109375
Epoch 31400, Loss: 10923.2109375
Epoch 31500, Loss: 10923.2109375
Epoch 31600, Loss: 10923.2109375
Epoch 31700, Loss: 10923.2119140625
Epoch 31800, Loss: 10923.2119140625
Epoch 31900, Loss: 10923.2099609375
Epoch 32000, Loss: 10923.2119140625
Epoch 32100, Loss: 10923.2109375
Epoch 32200, Loss: 10923.2099609375
Epoch 32300, Loss: 10923.2099609375
Epoch 32400, Loss: 10923.2099609375
Epoch 32500, Loss: 10923.208984375
Epoch 32600, Loss: 10923.208984375
Epoch 32700, Loss: 10923.2109375
Epoch 32800, Loss: 10923.208984375
Epoch 32900, Loss: 10923.2099609375
Epoch 33000, Loss: 10923.2099609375
Epoch 33100, Loss: 10923.2109375
Epoch 33200, Loss: 10923.2109375
Epoch 33300, Loss: 10923.2099609375
Epoch 33400, Loss: 10923.2109375
Epoch 33500, Loss: 10923.2109375
Epoch 33600, Loss: 10923.2099609375
Epoch 33700, Loss: 10923.2109375
Epoch 33800, Loss: 10923.2099609375
Epoch 33900, Loss: 10923.2109375
Epoch 34000, Loss: 10923.2109375
Epoch 34100, Loss: 10923.2099609375
Epoch 34200, Loss: 10923.2099609375
Epoch 34300, Loss: 10923.2099609375
Epoch 34400, Loss: 10923.2099609375
Epoch 34500, Loss: 10923.2119140625
Epoch 34600, Loss: 10923.2109375
Epoch 34700, Loss: 10923.2109375
Epoch 34800, Loss: 10923.2099609375
Epoch 34900, Loss: 10923.2099609375
Epoch 35000, Loss: 10923.2099609375
Epoch 35100, Loss: 10923.2109375
Epoch 35200, Loss: 10923.208984375
Epoch 35300, Loss: 10923.2099609375
Epoch 35400, Loss: 10923.2099609375
Epoch 35500, Loss: 10923.2099609375
Epoch 35600, Loss: 10923.2109375
Epoch 35700, Loss: 10923.2109375
Epoch 35800, Loss: 10923.2109375
Epoch 35900, Loss: 10923.2109375
Epoch 36000, Loss: 10923.2109375
Epoch 36100, Loss: 10923.2109375
Epoch 36200, Loss: 10923.208984375
Epoch 36300, Loss: 10923.2109375
Epoch 36400, Loss: 10923.2109375
Epoch 36500, Loss: 10923.2099609375
Epoch 36600, Loss: 10923.2099609375
Epoch 36700, Loss: 10923.2109375
Epoch 36800, Loss: 10923.2099609375
Epoch 36900, Loss: 10923.2109375
Epoch 37000, Loss: 10923.2109375
Epoch 37100, Loss: 10923.2109375
Epoch 37200, Loss: 10923.208984375
Epoch 37300, Loss: 10923.2109375
Epoch 37400, Loss: 10923.2109375
Epoch 37500, Loss: 10923.2109375
Epoch 37600, Loss: 10923.2109375
Epoch 37700, Loss: 10923.2109375
Epoch 37800, Loss: 10923.208984375
Epoch 37900, Loss: 10923.2119140625
Epoch 38000, Loss: 10923.2109375
Epoch 38100, Loss: 10923.2109375
Epoch 38200, Loss: 10923.2109375
Epoch 38300, Loss: 10923.208984375
Epoch 38400, Loss: 10923.2099609375
Epoch 38500, Loss: 10923.2109375
Epoch 38600, Loss: 10923.2080078125
Epoch 38700, Loss: 10923.2109375
Epoch 38800, Loss: 10923.2099609375
Epoch 38900, Loss: 10923.2109375
Epoch 39000, Loss: 10923.2109375
Epoch 39100, Loss: 10923.2109375
Epoch 39200, Loss: 10923.2109375
Epoch 39300, Loss: 10923.2119140625
Epoch 39400, Loss: 10923.2109375
Epoch 39500, Loss: 10923.2099609375
Epoch 39600, Loss: 10923.2099609375
Epoch 39700, Loss: 10923.2109375
Epoch 39800, Loss: 10923.2109375
Epoch 39900, Loss: 10923.2109375
Epoch 40000, Loss: 10923.2099609375
Epoch 40100, Loss: 10923.2109375
Epoch 40200, Loss: 10923.2109375
Epoch 40300, Loss: 10923.2109375
Epoch 40400, Loss: 10923.2109375
Epoch 40500, Loss: 10923.2109375
Epoch 40600, Loss: 10923.208984375
Epoch 40700, Loss: 10923.2109375
Epoch 40800, Loss: 10923.208984375
Epoch 40900, Loss: 10923.2119140625
Epoch 41000, Loss: 10923.2119140625
Epoch 41100, Loss: 10923.2119140625
Epoch 41200, Loss: 10923.2099609375
Epoch 41300, Loss: 10923.2099609375
Epoch 41400, Loss: 10923.2109375
Epoch 41500, Loss: 10923.2109375
Epoch 41600, Loss: 10923.2099609375
Epoch 41700, Loss: 10923.2099609375
Epoch 41800, Loss: 10923.208984375
Epoch 41900, Loss: 10923.2109375
Epoch 42000, Loss: 10923.2119140625
Epoch 42100, Loss: 10923.2099609375
Epoch 42200, Loss: 10923.208984375
Epoch 42300, Loss: 10923.2099609375
Epoch 42400, Loss: 10923.2109375
Epoch 42500, Loss: 10923.2099609375
Epoch 42600, Loss: 10923.2119140625
Epoch 42700, Loss: 10923.208984375
Epoch 42800, Loss: 10923.2099609375
Epoch 42900, Loss: 10923.2119140625
Epoch 43000, Loss: 10923.2109375
Epoch 43100, Loss: 10923.2099609375
Epoch 43200, Loss: 10923.2109375
Epoch 43300, Loss: 10923.2099609375
Epoch 43400, Loss: 10923.2099609375
Epoch 43500, Loss: 10923.2119140625
Epoch 43600, Loss: 10923.2109375
Epoch 43700, Loss: 10923.2119140625
Epoch 43800, Loss: 10923.2109375
Epoch 43900, Loss: 10923.2119140625
Epoch 44000, Loss: 10923.2099609375
Epoch 44100, Loss: 10923.2109375
Epoch 44200, Loss: 10923.2099609375
Epoch 44300, Loss: 10923.2099609375
Epoch 44400, Loss: 10923.2109375
Epoch 44500, Loss: 10923.2119140625
Epoch 44600, Loss: 10923.2109375
Epoch 44700, Loss: 10923.2099609375
Epoch 44800, Loss: 10923.208984375
Epoch 44900, Loss: 10923.2099609375
Epoch 45000, Loss: 10923.2099609375
Epoch 45100, Loss: 10923.2099609375
Epoch 45200, Loss: 10923.2109375
Epoch 45300, Loss: 10923.2109375
Epoch 45400, Loss: 10923.2109375
Epoch 45500, Loss: 10923.2109375
Epoch 45600, Loss: 10923.2109375
Epoch 45700, Loss: 10923.2080078125
Epoch 45800, Loss: 10923.2109375
Epoch 45900, Loss: 10923.2099609375
Epoch 46000, Loss: 10923.2099609375
Epoch 46100, Loss: 10923.2109375
Epoch 46200, Loss: 10923.2119140625
Epoch 46300, Loss: 10923.2109375
Epoch 46400, Loss: 10923.2109375
Epoch 46500, Loss: 10923.2109375
Epoch 46600, Loss: 10923.208984375
Epoch 46700, Loss: 10923.2099609375
Epoch 46800, Loss: 10923.2099609375
Epoch 46900, Loss: 10923.2099609375
Epoch 47000, Loss: 10923.2109375
Epoch 47100, Loss: 10923.2109375
Epoch 47200, Loss: 10923.208984375
Epoch 47300, Loss: 10923.2109375
Epoch 47400, Loss: 10923.2109375
Epoch 47500, Loss: 10923.2109375
Epoch 47600, Loss: 10923.208984375
Epoch 47700, Loss: 10923.2109375
Epoch 47800, Loss: 10923.2109375
Epoch 47900, Loss: 10923.2099609375
Epoch 48000, Loss: 10923.208984375
Epoch 48100, Loss: 10923.208984375
Epoch 48200, Loss: 10923.2109375
Epoch 48300, Loss: 10923.2109375
Epoch 48400, Loss: 10923.2099609375
Epoch 48500, Loss: 10923.2099609375
Epoch 48600, Loss: 10923.2109375
Epoch 48700, Loss: 10923.2109375
Epoch 48800, Loss: 10923.2109375
Epoch 48900, Loss: 10923.2119140625
Epoch 49000, Loss: 10923.2099609375
Epoch 49100, Loss: 10923.208984375
Epoch 49200, Loss: 10923.2109375
Epoch 49300, Loss: 10923.2109375
Epoch 49400, Loss: 10923.2099609375
Epoch 49500, Loss: 10923.2099609375
Epoch 49600, Loss: 10923.2099609375
Epoch 49700, Loss: 10923.2109375
Epoch 49800, Loss: 10923.2109375
Epoch 49900, Loss: 10923.2099609375
Epoch 0, Loss: 63828.96875
Epoch 100, Loss: 60945.62890625
Epoch 200, Loss: 58249.76953125
Epoch 300, Loss: 55729.1015625
Epoch 400, Loss: 53370.4375
Epoch 500, Loss: 51162.8984375
Epoch 600, Loss: 49096.1015625
Epoch 700, Loss: 47161.23046875
Epoch 800, Loss: 45351.796875
Epoch 900, Loss: 43661.06640625
Epoch 1000, Loss: 42081.0703125
Epoch 1100, Loss: 40605.01171875
Epoch 1200, Loss: 39226.4921875
Epoch 1300, Loss: 37939.9609375
Epoch 1400, Loss: 36740.59375
Epoch 1500, Loss: 35621.2734375
Epoch 1600, Loss: 34576.40234375
Epoch 1700, Loss: 33600.73828125
Epoch 1800, Loss: 32689.373046875
Epoch 1900, Loss: 31837.748046875
Epoch 2000, Loss: 31042.3046875
Epoch 2100, Loss: 30299.76953125
Epoch 2200, Loss: 29605.49609375
Epoch 2300, Loss: 28958.15234375
Epoch 2400, Loss: 28352.7421875
Epoch 2500, Loss: 27785.31640625
Epoch 2600, Loss: 27252.98828125
Epoch 2700, Loss: 26753.1171875
Epoch 2800, Loss: 26283.28125
Epoch 2900, Loss: 25841.236328125
Epoch 3000, Loss: 25424.904296875
Epoch 3100, Loss: 25032.34765625
Epoch 3200, Loss: 24661.74609375
Epoch 3300, Loss: 24312.0078125
Epoch 3400, Loss: 23983.310546875
Epoch 3500, Loss: 23673.048828125
Epoch 3600, Loss: 23378.23828125
Epoch 3700, Loss: 23097.48828125
Epoch 3800, Loss: 22829.53125
Epoch 3900, Loss: 22573.765625
Epoch 4000, Loss: 22329.62890625
Epoch 4100, Loss: 22095.021484375
Epoch 4200, Loss: 21869.04296875
Epoch 4300, Loss: 21651.19140625
Epoch 4400, Loss: 21441.513671875
Epoch 4500, Loss: 21238.259765625
Epoch 4600, Loss: 21040.861328125
Epoch 4700, Loss: 20848.83203125
Epoch 4800, Loss: 20661.7578125
Epoch 4900, Loss: 20479.275390625
Epoch 5000, Loss: 20301.064453125
Epoch 5100, Loss: 20126.857421875
Epoch 5200, Loss: 19956.41796875
Epoch 5300, Loss: 19789.533203125
Epoch 5400, Loss: 19626.017578125
Epoch 5500, Loss: 19465.708984375
Epoch 5600, Loss: 19308.724609375
Epoch 5700, Loss: 19155.435546875
Epoch 5800, Loss: 19004.94140625
Epoch 5900, Loss: 18857.1328125
Epoch 6000, Loss: 18711.908203125
Epoch 6100, Loss: 18569.177734375
Epoch 6200, Loss: 18428.861328125
Epoch 6300, Loss: 18290.87890625
Epoch 6400, Loss: 18155.162109375
Epoch 6500, Loss: 18021.654296875
Epoch 6600, Loss: 17890.291015625
Epoch 6700, Loss: 17761.013671875
Epoch 6800, Loss: 17633.7734375
Epoch 6900, Loss: 17509.455078125
Epoch 7000, Loss: 17387.828125
Epoch 7100, Loss: 17268.029296875
Epoch 7200, Loss: 17150.017578125
Epoch 7300, Loss: 17033.732421875
Epoch 7400, Loss: 16919.13671875
Epoch 7500, Loss: 16807.205078125
Epoch 7600, Loss: 16697.201171875
Epoch 7700, Loss: 16588.744140625
Epoch 7800, Loss: 16481.787109375
Epoch 7900, Loss: 16376.2919921875
Epoch 8000, Loss: 16272.2412109375
Epoch 8100, Loss: 16169.6171875
Epoch 8200, Loss: 16068.4052734375
Epoch 8300, Loss: 15968.5751953125
Epoch 8400, Loss: 15870.5107421875
Epoch 8500, Loss: 15773.9169921875
Epoch 8600, Loss: 15678.7080078125
Epoch 8700, Loss: 15584.861328125
Epoch 8800, Loss: 15492.3544921875
Epoch 8900, Loss: 15401.177734375
Epoch 9000, Loss: 15311.3076171875
Epoch 9100, Loss: 15222.73046875
Epoch 9200, Loss: 15135.4375
Epoch 9300, Loss: 15049.4111328125
Epoch 9400, Loss: 14964.6416015625
Epoch 9500, Loss: 14881.11328125
Epoch 9600, Loss: 14798.8134765625
Epoch 9700, Loss: 14717.720703125
Epoch 9800, Loss: 14637.8193359375
Epoch 9900, Loss: 14559.0966796875
Epoch 10000, Loss: 14481.55078125
Epoch 10100, Loss: 14405.1650390625
Epoch 10200, Loss: 14329.93359375
Epoch 10300, Loss: 14255.84765625
Epoch 10400, Loss: 14182.90234375
Epoch 10500, Loss: 14111.087890625
Epoch 10600, Loss: 14040.3974609375
Epoch 10700, Loss: 13970.888671875
Epoch 10800, Loss: 13902.55078125
Epoch 10900, Loss: 13835.3359375
Epoch 11000, Loss: 13769.2236328125
Epoch 11100, Loss: 13704.5439453125
Epoch 11200, Loss: 13641.224609375
Epoch 11300, Loss: 13579.1845703125
Epoch 11400, Loss: 13518.357421875
Epoch 11500, Loss: 13458.634765625
Epoch 11600, Loss: 13399.955078125
Epoch 11700, Loss: 13342.6943359375
Epoch 11800, Loss: 13286.7021484375
Epoch 11900, Loss: 13231.79296875
Epoch 12000, Loss: 13177.9423828125
Epoch 12100, Loss: 13125.123046875
Epoch 12200, Loss: 13073.3076171875
Epoch 12300, Loss: 13022.740234375
Epoch 12400, Loss: 12973.2802734375
Epoch 12500, Loss: 12924.865234375
Epoch 12600, Loss: 12877.4326171875
Epoch 12700, Loss: 12830.9375
Epoch 12800, Loss: 12785.296875
Epoch 12900, Loss: 12740.4775390625
Epoch 13000, Loss: 12696.462890625
Epoch 13100, Loss: 12653.2392578125
Epoch 13200, Loss: 12610.8251953125
Epoch 13300, Loss: 12569.4072265625
Epoch 13400, Loss: 12528.8994140625
Epoch 13500, Loss: 12489.2607421875
Epoch 13600, Loss: 12450.486328125
Epoch 13700, Loss: 12412.611328125
Epoch 13800, Loss: 12375.615234375
Epoch 13900, Loss: 12339.509765625
Epoch 14000, Loss: 12304.373046875
Epoch 14100, Loss: 12270.11328125
Epoch 14200, Loss: 12236.685546875
Epoch 14300, Loss: 12204.0830078125
Epoch 14400, Loss: 12172.26171875
Epoch 14500, Loss: 12141.2255859375
Epoch 14600, Loss: 12110.9404296875
Epoch 14700, Loss: 12081.4033203125
Epoch 14800, Loss: 12052.5947265625
Epoch 14900, Loss: 12024.5
Epoch 15000, Loss: 11997.1142578125
Epoch 15100, Loss: 11970.416015625
Epoch 15200, Loss: 11944.416015625
Epoch 15300, Loss: 11919.0947265625
Epoch 15400, Loss: 11894.4423828125
Epoch 15500, Loss: 11870.4326171875
Epoch 15600, Loss: 11847.04296875
Epoch 15700, Loss: 11824.2578125
Epoch 15800, Loss: 11802.115234375
Epoch 15900, Loss: 11780.697265625
Epoch 16000, Loss: 11760.16796875
Epoch 16100, Loss: 11740.537109375
Epoch 16200, Loss: 11721.5615234375
Epoch 16300, Loss: 11703.1611328125
Epoch 16400, Loss: 11685.283203125
Epoch 16500, Loss: 11667.9091796875
Epoch 16600, Loss: 11651.0166015625
Epoch 16700, Loss: 11634.599609375
Epoch 16800, Loss: 11618.6484375
Epoch 16900, Loss: 11603.1513671875
Epoch 17000, Loss: 11588.1044921875
Epoch 17100, Loss: 11573.4970703125
Epoch 17200, Loss: 11559.3349609375
Epoch 17300, Loss: 11545.6220703125
Epoch 17400, Loss: 11532.4287109375
Epoch 17500, Loss: 11519.7275390625
Epoch 17600, Loss: 11507.45703125
Epoch 17700, Loss: 11495.609375
Epoch 17800, Loss: 11484.177734375
Epoch 17900, Loss: 11473.15234375
Epoch 18000, Loss: 11462.52734375
Epoch 18100, Loss: 11452.294921875
Epoch 18200, Loss: 11442.4482421875
Epoch 18300, Loss: 11432.9765625
Epoch 18400, Loss: 11423.8759765625
Epoch 18500, Loss: 11415.1298828125
Epoch 18600, Loss: 11406.736328125
Epoch 18700, Loss: 11398.685546875
Epoch 18800, Loss: 11390.96875
Epoch 18900, Loss: 11383.5810546875
Epoch 19000, Loss: 11376.5009765625
Epoch 19100, Loss: 11369.7216796875
Epoch 19200, Loss: 11363.2275390625
Epoch 19300, Loss: 11357.0068359375
Epoch 19400, Loss: 11351.0439453125
Epoch 19500, Loss: 11345.3291015625
Epoch 19600, Loss: 11339.841796875
Epoch 19700, Loss: 11334.572265625
Epoch 19800, Loss: 11329.5048828125
Epoch 19900, Loss: 11324.625
Epoch 20000, Loss: 11320.1171875
Epoch 20100, Loss: 11315.8916015625
Epoch 20200, Loss: 11311.884765625
Epoch 20300, Loss: 11308.05859375
Epoch 20400, Loss: 11304.3828125
Epoch 20500, Loss: 11300.8466796875
Epoch 20600, Loss: 11297.421875
Epoch 20700, Loss: 11294.095703125
Epoch 20800, Loss: 11290.8447265625
Epoch 20900, Loss: 11287.66015625
Epoch 21000, Loss: 11284.5263671875
Epoch 21100, Loss: 11281.4453125
Epoch 21200, Loss: 11278.41015625
Epoch 21300, Loss: 11275.41796875
Epoch 21400, Loss: 11272.4599609375
Epoch 21500, Loss: 11269.537109375
Epoch 21600, Loss: 11266.6494140625
Epoch 21700, Loss: 11263.7939453125
Epoch 21800, Loss: 11260.9775390625
Epoch 21900, Loss: 11258.19140625
Epoch 22000, Loss: 11255.439453125
Epoch 22100, Loss: 11252.728515625
Epoch 22200, Loss: 11250.056640625
Epoch 22300, Loss: 11247.421875
Epoch 22400, Loss: 11244.828125
Epoch 22500, Loss: 11242.28125
Epoch 22600, Loss: 11239.7763671875
Epoch 22700, Loss: 11237.3203125
Epoch 22800, Loss: 11234.9091796875
Epoch 22900, Loss: 11232.5439453125
Epoch 23000, Loss: 11230.2275390625
Epoch 23100, Loss: 11227.96484375
Epoch 23200, Loss: 11225.7490234375
Epoch 23300, Loss: 11223.5849609375
Epoch 23400, Loss: 11221.4833984375
Epoch 23500, Loss: 11219.451171875
Epoch 23600, Loss: 11217.4912109375
Epoch 23700, Loss: 11215.5869140625
Epoch 23800, Loss: 11213.7392578125
Epoch 23900, Loss: 11211.943359375
Epoch 24000, Loss: 11210.201171875
Epoch 24100, Loss: 11208.5263671875
Epoch 24200, Loss: 11206.9140625
Epoch 24300, Loss: 11205.359375
Epoch 24400, Loss: 11203.8564453125
Epoch 24500, Loss: 11202.40234375
Epoch 24600, Loss: 11201.00390625
Epoch 24700, Loss: 11199.6611328125
Epoch 24800, Loss: 11198.376953125
Epoch 24900, Loss: 11197.1591796875
Epoch 25000, Loss: 11195.9921875
Epoch 25100, Loss: 11194.880859375
Epoch 25200, Loss: 11193.8212890625
Epoch 25300, Loss: 11192.8193359375
Epoch 25400, Loss: 11191.8642578125
Epoch 25500, Loss: 11190.962890625
Epoch 25600, Loss: 11190.10546875
Epoch 25700, Loss: 11189.2978515625
Epoch 25800, Loss: 11188.537109375
Epoch 25900, Loss: 11187.81640625
Epoch 26000, Loss: 11187.14453125
Epoch 26100, Loss: 11186.5126953125
Epoch 26200, Loss: 11185.919921875
Epoch 26300, Loss: 11185.3671875
Epoch 26400, Loss: 11184.8515625
Epoch 26500, Loss: 11184.3720703125
Epoch 26600, Loss: 11183.9287109375
Epoch 26700, Loss: 11183.51953125
Epoch 26800, Loss: 11183.140625
Epoch 26900, Loss: 11182.7939453125
Epoch 27000, Loss: 11182.4755859375
Epoch 27100, Loss: 11182.1806640625
Epoch 27200, Loss: 11181.9111328125
Epoch 27300, Loss: 11181.6640625
Epoch 27400, Loss: 11181.43359375
Epoch 27500, Loss: 11181.22265625
Epoch 27600, Loss: 11181.03125
Epoch 27700, Loss: 11180.849609375
Epoch 27800, Loss: 11180.6904296875
Epoch 27900, Loss: 11180.5419921875
Epoch 28000, Loss: 11180.41015625
Epoch 28100, Loss: 11180.29296875
Epoch 28200, Loss: 11180.1865234375
Epoch 28300, Loss: 11180.091796875
Epoch 28400, Loss: 11180.009765625
Epoch 28500, Loss: 11179.935546875
Epoch 28600, Loss: 11179.8701171875
Epoch 28700, Loss: 11179.810546875
Epoch 28800, Loss: 11179.7578125
Epoch 28900, Loss: 11179.7109375
Epoch 29000, Loss: 11179.6630859375
Epoch 29100, Loss: 11179.6220703125
Epoch 29200, Loss: 11179.583984375
Epoch 29300, Loss: 11179.548828125
Epoch 29400, Loss: 11179.5166015625
Epoch 29500, Loss: 11179.490234375
Epoch 29600, Loss: 11179.462890625
Epoch 29700, Loss: 11179.4423828125
Epoch 29800, Loss: 11179.419921875
Epoch 29900, Loss: 11179.4052734375
Epoch 30000, Loss: 11179.390625
Epoch 30100, Loss: 11179.376953125
Epoch 30200, Loss: 11179.3662109375
Epoch 30300, Loss: 11179.3583984375
Epoch 30400, Loss: 11179.34765625
Epoch 30500, Loss: 11179.3427734375
Epoch 30600, Loss: 11179.337890625
Epoch 30700, Loss: 11179.33203125
Epoch 30800, Loss: 11179.330078125
Epoch 30900, Loss: 11179.3271484375
Epoch 31000, Loss: 11179.32421875
Epoch 31100, Loss: 11179.32421875
Epoch 31200, Loss: 11179.32421875
Epoch 31300, Loss: 11179.322265625
Epoch 31400, Loss: 11179.3212890625
Epoch 31500, Loss: 11179.322265625
Epoch 31600, Loss: 11179.3193359375
Epoch 31700, Loss: 11179.3203125
Epoch 31800, Loss: 11179.3203125
Epoch 31900, Loss: 11179.3193359375
Epoch 32000, Loss: 11179.3203125
Epoch 32100, Loss: 11179.3203125
Epoch 32200, Loss: 11179.3212890625
Epoch 32300, Loss: 11179.3203125
Epoch 32400, Loss: 11179.3193359375
Epoch 32500, Loss: 11179.3212890625
Epoch 32600, Loss: 11179.3193359375
Epoch 32700, Loss: 11179.3203125
Epoch 32800, Loss: 11179.3203125
Epoch 32900, Loss: 11179.3212890625
Epoch 33000, Loss: 11179.3203125
Epoch 33100, Loss: 11179.3193359375
Epoch 33200, Loss: 11179.3203125
Epoch 33300, Loss: 11179.3212890625
Epoch 33400, Loss: 11179.3203125
Epoch 33500, Loss: 11179.3193359375
Epoch 33600, Loss: 11179.3203125
Epoch 33700, Loss: 11179.318359375
Epoch 33800, Loss: 11179.3203125
Epoch 33900, Loss: 11179.3203125
Epoch 34000, Loss: 11179.3193359375
Epoch 34100, Loss: 11179.3203125
Epoch 34200, Loss: 11179.3193359375
Epoch 34300, Loss: 11179.3212890625
Epoch 34400, Loss: 11179.322265625
Epoch 34500, Loss: 11179.3203125
Epoch 34600, Loss: 11179.318359375
Epoch 34700, Loss: 11179.3203125
Epoch 34800, Loss: 11179.3203125
Epoch 34900, Loss: 11179.3193359375
Epoch 35000, Loss: 11179.3203125
Epoch 35100, Loss: 11179.3193359375
Epoch 35200, Loss: 11179.3203125
Epoch 35300, Loss: 11179.3203125
Epoch 35400, Loss: 11179.3203125
Epoch 35500, Loss: 11179.3203125
Epoch 35600, Loss: 11179.3203125
Epoch 35700, Loss: 11179.3193359375
Epoch 35800, Loss: 11179.3203125
Epoch 35900, Loss: 11179.3203125
Epoch 36000, Loss: 11179.3203125
Epoch 36100, Loss: 11179.318359375
Epoch 36200, Loss: 11179.3193359375
Epoch 36300, Loss: 11179.3212890625
Epoch 36400, Loss: 11179.322265625
Epoch 36500, Loss: 11179.3203125
Epoch 36600, Loss: 11179.3212890625
Epoch 36700, Loss: 11179.3203125
Epoch 36800, Loss: 11179.3203125
Epoch 36900, Loss: 11179.3203125
Epoch 37000, Loss: 11179.318359375
Epoch 37100, Loss: 11179.3203125
Epoch 37200, Loss: 11179.3203125
Epoch 37300, Loss: 11179.318359375
Epoch 37400, Loss: 11179.322265625
Epoch 37500, Loss: 11179.3193359375
Epoch 37600, Loss: 11179.3212890625
Epoch 37700, Loss: 11179.3203125
Epoch 37800, Loss: 11179.318359375
Epoch 37900, Loss: 11179.3203125
Epoch 38000, Loss: 11179.322265625
Epoch 38100, Loss: 11179.3193359375
Epoch 38200, Loss: 11179.3212890625
Epoch 38300, Loss: 11179.3193359375
Epoch 38400, Loss: 11179.322265625
Epoch 38500, Loss: 11179.3193359375
Epoch 38600, Loss: 11179.3203125
Epoch 38700, Loss: 11179.3203125
Epoch 38800, Loss: 11179.3203125
Epoch 38900, Loss: 11179.318359375
Epoch 39000, Loss: 11179.3193359375
Epoch 39100, Loss: 11179.318359375
Epoch 39200, Loss: 11179.3173828125
Epoch 39300, Loss: 11179.318359375
Epoch 39400, Loss: 11179.318359375
Epoch 39500, Loss: 11179.3193359375
Epoch 39600, Loss: 11179.3212890625
Epoch 39700, Loss: 11179.3203125
Epoch 39800, Loss: 11179.3193359375
Epoch 39900, Loss: 11179.322265625
Epoch 40000, Loss: 11179.3212890625
Epoch 40100, Loss: 11179.318359375
Epoch 40200, Loss: 11179.3212890625
Epoch 40300, Loss: 11179.3193359375
Epoch 40400, Loss: 11179.3203125
Epoch 40500, Loss: 11179.3212890625
Epoch 40600, Loss: 11179.3212890625
Epoch 40700, Loss: 11179.3212890625
Epoch 40800, Loss: 11179.3212890625
Epoch 40900, Loss: 11179.3203125
Epoch 41000, Loss: 11179.3203125
Epoch 41100, Loss: 11179.322265625
Epoch 41200, Loss: 11179.3193359375
Epoch 41300, Loss: 11179.3203125
Epoch 41400, Loss: 11179.3203125
Epoch 41500, Loss: 11179.3203125
Epoch 41600, Loss: 11179.3193359375
Epoch 41700, Loss: 11179.318359375
Epoch 41800, Loss: 11179.3193359375
Epoch 41900, Loss: 11179.318359375
Epoch 42000, Loss: 11179.3203125
Epoch 42100, Loss: 11179.3203125
Epoch 42200, Loss: 11179.318359375
Epoch 42300, Loss: 11179.3212890625
Epoch 42400, Loss: 11179.3193359375
Epoch 42500, Loss: 11179.322265625
Epoch 42600, Loss: 11179.3203125
Epoch 42700, Loss: 11179.3203125
Epoch 42800, Loss: 11179.3193359375
Epoch 42900, Loss: 11179.3193359375
Epoch 43000, Loss: 11179.3203125
Epoch 43100, Loss: 11179.3193359375
Epoch 43200, Loss: 11179.3203125
Epoch 43300, Loss: 11179.3203125
Epoch 43400, Loss: 11179.3203125
Epoch 43500, Loss: 11179.3203125
Epoch 43600, Loss: 11179.3193359375
Epoch 43700, Loss: 11179.3193359375
Epoch 43800, Loss: 11179.3203125
Epoch 43900, Loss: 11179.3193359375
Epoch 44000, Loss: 11179.318359375
Epoch 44100, Loss: 11179.322265625
Epoch 44200, Loss: 11179.3212890625
Epoch 44300, Loss: 11179.3193359375
Epoch 44400, Loss: 11179.3212890625
Epoch 44500, Loss: 11179.318359375
Epoch 44600, Loss: 11179.3193359375
Epoch 44700, Loss: 11179.318359375
Epoch 44800, Loss: 11179.318359375
Epoch 44900, Loss: 11179.3203125
Epoch 45000, Loss: 11179.3193359375
Epoch 45100, Loss: 11179.3203125
Epoch 45200, Loss: 11179.3212890625
Epoch 45300, Loss: 11179.3203125
Epoch 45400, Loss: 11179.318359375
Epoch 45500, Loss: 11179.322265625
Epoch 45600, Loss: 11179.3193359375
Epoch 45700, Loss: 11179.3203125
Epoch 45800, Loss: 11179.3193359375
Epoch 45900, Loss: 11179.3193359375
Epoch 46000, Loss: 11179.3203125
Epoch 46100, Loss: 11179.3203125
Epoch 46200, Loss: 11179.322265625
Epoch 46300, Loss: 11179.318359375
Epoch 46400, Loss: 11179.318359375
Epoch 46500, Loss: 11179.3203125
Epoch 46600, Loss: 11179.3203125
Epoch 46700, Loss: 11179.318359375
Epoch 46800, Loss: 11179.3212890625
Epoch 46900, Loss: 11179.318359375
Epoch 47000, Loss: 11179.3203125
Epoch 47100, Loss: 11179.3212890625
Epoch 47200, Loss: 11179.3203125
Epoch 47300, Loss: 11179.3193359375
Epoch 47400, Loss: 11179.3203125
Epoch 47500, Loss: 11179.3203125
Epoch 47600, Loss: 11179.3203125
Epoch 47700, Loss: 11179.3193359375
Epoch 47800, Loss: 11179.3203125
Epoch 47900, Loss: 11179.3193359375
Epoch 48000, Loss: 11179.318359375
Epoch 48100, Loss: 11179.318359375
Epoch 48200, Loss: 11179.3193359375
Epoch 48300, Loss: 11179.3212890625
Epoch 48400, Loss: 11179.3203125
Epoch 48500, Loss: 11179.3203125
Epoch 48600, Loss: 11179.3212890625
Epoch 48700, Loss: 11179.3203125
Epoch 48800, Loss: 11179.3212890625
Epoch 48900, Loss: 11179.3203125
Epoch 49000, Loss: 11179.3193359375
Epoch 49100, Loss: 11179.3203125
Epoch 49200, Loss: 11179.318359375
Epoch 49300, Loss: 11179.3203125
Epoch 49400, Loss: 11179.3193359375
Epoch 49500, Loss: 11179.3193359375
Epoch 49600, Loss: 11179.3193359375
Epoch 49700, Loss: 11179.3193359375
Epoch 49800, Loss: 11179.3203125
Epoch 49900, Loss: 11179.3193359375
Epoch 0, Loss: 63828.96875
Epoch 100, Loss: 60951.91796875
Epoch 200, Loss: 58261.71875
Epoch 300, Loss: 55747.71484375
Epoch 400, Loss: 53396.109375
Epoch 500, Loss: 51196.2421875
Epoch 600, Loss: 49136.546875
Epoch 700, Loss: 47208.14453125
Epoch 800, Loss: 45405.6875
Epoch 900, Loss: 43721.14453125
Epoch 1000, Loss: 42146.89453125
Epoch 1100, Loss: 40676.140625
Epoch 1200, Loss: 39302.50390625
Epoch 1300, Loss: 38021.40625
Epoch 1400, Loss: 36826.72265625
Epoch 1500, Loss: 35711.71484375
Epoch 1600, Loss: 34670.78515625
Epoch 1700, Loss: 33698.70703125
Epoch 1800, Loss: 32790.625
Epoch 1900, Loss: 31942.009765625
Epoch 2000, Loss: 31150.509765625
Epoch 2100, Loss: 30411.07421875
Epoch 2200, Loss: 29720.75390625
Epoch 2300, Loss: 29077.54296875
Epoch 2400, Loss: 28475.6171875
Epoch 2500, Loss: 27911.49609375
Epoch 2600, Loss: 27382.27734375
Epoch 2700, Loss: 26885.330078125
Epoch 2800, Loss: 26418.240234375
Epoch 2900, Loss: 25978.783203125
Epoch 3000, Loss: 25564.8984375
Epoch 3100, Loss: 25174.65625
Epoch 3200, Loss: 24806.251953125
Epoch 3300, Loss: 24458.861328125
Epoch 3400, Loss: 24134.13671875
Epoch 3500, Loss: 23826.8125
Epoch 3600, Loss: 23534.8203125
Epoch 3700, Loss: 23256.767578125
Epoch 3800, Loss: 22991.38671875
Epoch 3900, Loss: 22739.16796875
Epoch 4000, Loss: 22497.76171875
Epoch 4100, Loss: 22265.779296875
Epoch 4200, Loss: 22042.32421875
Epoch 4300, Loss: 21827.630859375
Epoch 4400, Loss: 21620.599609375
Epoch 4500, Loss: 21419.888671875
Epoch 4600, Loss: 21224.951171875
Epoch 4700, Loss: 21035.294921875
Epoch 4800, Loss: 20850.515625
Epoch 4900, Loss: 20670.251953125
Epoch 5000, Loss: 20494.189453125
Epoch 5100, Loss: 20322.0625
Epoch 5200, Loss: 20153.6328125
Epoch 5300, Loss: 19988.69921875
Epoch 5400, Loss: 19827.07421875
Epoch 5500, Loss: 19668.86328125
Epoch 5600, Loss: 19514.6015625
Epoch 5700, Loss: 19363.208984375
Epoch 5800, Loss: 19214.5546875
Epoch 5900, Loss: 19068.53515625
Epoch 6000, Loss: 18925.044921875
Epoch 6100, Loss: 18783.994140625
Epoch 6200, Loss: 18645.30859375
Epoch 6300, Loss: 18508.90625
Epoch 6400, Loss: 18374.71875
Epoch 6500, Loss: 18242.681640625
Epoch 6600, Loss: 18112.73828125
Epoch 6700, Loss: 17984.830078125
Epoch 6800, Loss: 17859.80859375
Epoch 6900, Loss: 17737.71875
Epoch 7000, Loss: 17617.603515625
Epoch 7100, Loss: 17499.25390625
Epoch 7200, Loss: 17382.630859375
Epoch 7300, Loss: 17267.685546875
Epoch 7400, Loss: 17154.400390625
Epoch 7500, Loss: 17043.71484375
Epoch 7600, Loss: 16935.220703125
Epoch 7700, Loss: 16828.16015625
Epoch 7800, Loss: 16722.51171875
Epoch 7900, Loss: 16618.2578125
Epoch 8000, Loss: 16515.3828125
Epoch 8100, Loss: 16413.98828125
Epoch 8200, Loss: 16314.349609375
Epoch 8300, Loss: 16216.10546875
Epoch 8400, Loss: 16119.2470703125
Epoch 8500, Loss: 16023.7373046875
Epoch 8600, Loss: 15929.5615234375
Epoch 8700, Loss: 15836.6884765625
Epoch 8800, Loss: 15745.1103515625
Epoch 8900, Loss: 15654.7998046875
Epoch 9000, Loss: 15565.751953125
Epoch 9100, Loss: 15477.947265625
Epoch 9200, Loss: 15391.3720703125
Epoch 9300, Loss: 15306.017578125
Epoch 9400, Loss: 15221.8720703125
Epoch 9500, Loss: 15138.93359375
Epoch 9600, Loss: 15057.1845703125
Epoch 9700, Loss: 14976.619140625
Epoch 9800, Loss: 14897.234375
Epoch 9900, Loss: 14819.0126953125
Epoch 10000, Loss: 14741.939453125
Epoch 10100, Loss: 14666.005859375
Epoch 10200, Loss: 14591.193359375
Epoch 10300, Loss: 14517.4931640625
Epoch 10400, Loss: 14444.8935546875
Epoch 10500, Loss: 14373.4189453125
Epoch 10600, Loss: 14303.146484375
Epoch 10700, Loss: 14234.068359375
Epoch 10800, Loss: 14166.572265625
Epoch 10900, Loss: 14100.4033203125
Epoch 11000, Loss: 14035.53125
Epoch 11100, Loss: 13971.9111328125
Epoch 11200, Loss: 13909.4345703125
Epoch 11300, Loss: 13848.025390625
Epoch 11400, Loss: 13787.638671875
Epoch 11500, Loss: 13728.373046875
Epoch 11600, Loss: 13670.5986328125
Epoch 11700, Loss: 13613.9169921875
Epoch 11800, Loss: 13558.3330078125
Epoch 11900, Loss: 13504.130859375
Epoch 12000, Loss: 13451.158203125
Epoch 12100, Loss: 13399.3603515625
Epoch 12200, Loss: 13348.6923828125
Epoch 12300, Loss: 13299.1142578125
Epoch 12400, Loss: 13250.5634765625
Epoch 12500, Loss: 13202.94140625
Epoch 12600, Loss: 13156.1904296875
Epoch 12700, Loss: 13110.28125
Epoch 12800, Loss: 13065.2490234375
Epoch 12900, Loss: 13021.0712890625
Epoch 13000, Loss: 12977.873046875
Epoch 13100, Loss: 12935.673828125
Epoch 13200, Loss: 12894.373046875
Epoch 13300, Loss: 12853.96484375
Epoch 13400, Loss: 12814.43359375
Epoch 13500, Loss: 12775.759765625
Epoch 13600, Loss: 12737.95703125
Epoch 13700, Loss: 12701.005859375
Epoch 13800, Loss: 12664.9716796875
Epoch 13900, Loss: 12629.7802734375
Epoch 14000, Loss: 12595.421875
Epoch 14100, Loss: 12561.8671875
Epoch 14200, Loss: 12529.1083984375
Epoch 14300, Loss: 12497.111328125
Epoch 14400, Loss: 12465.8857421875
Epoch 14500, Loss: 12435.392578125
Epoch 14600, Loss: 12405.6259765625
Epoch 14700, Loss: 12376.578125
Epoch 14800, Loss: 12348.232421875
Epoch 14900, Loss: 12320.5869140625
Epoch 15000, Loss: 12293.63671875
Epoch 15100, Loss: 12267.515625
Epoch 15200, Loss: 12242.1953125
Epoch 15300, Loss: 12217.6123046875
Epoch 15400, Loss: 12193.7109375
Epoch 15500, Loss: 12170.4833984375
Epoch 15600, Loss: 12147.8984375
Epoch 15700, Loss: 12126.4892578125
Epoch 15800, Loss: 12105.87890625
Epoch 15900, Loss: 12085.9345703125
Epoch 16000, Loss: 12066.5869140625
Epoch 16100, Loss: 12047.7939453125
Epoch 16200, Loss: 12029.5556640625
Epoch 16300, Loss: 12011.8408203125
Epoch 16400, Loss: 11994.6328125
Epoch 16500, Loss: 11977.912109375
Epoch 16600, Loss: 11961.66796875
Epoch 16700, Loss: 11945.8837890625
Epoch 16800, Loss: 11930.5634765625
Epoch 16900, Loss: 11915.68359375
Epoch 17000, Loss: 11901.361328125
Epoch 17100, Loss: 11887.55078125
Epoch 17200, Loss: 11874.1728515625
Epoch 17300, Loss: 11861.205078125
Epoch 17400, Loss: 11848.650390625
Epoch 17500, Loss: 11836.501953125
Epoch 17600, Loss: 11824.7470703125
Epoch 17700, Loss: 11813.380859375
Epoch 17800, Loss: 11802.408203125
Epoch 17900, Loss: 11791.8076171875
Epoch 18000, Loss: 11781.5810546875
Epoch 18100, Loss: 11771.720703125
Epoch 18200, Loss: 11762.212890625
Epoch 18300, Loss: 11753.0498046875
Epoch 18400, Loss: 11744.216796875
Epoch 18500, Loss: 11735.71484375
Epoch 18600, Loss: 11727.521484375
Epoch 18700, Loss: 11719.63671875
Epoch 18800, Loss: 11712.0439453125
Epoch 18900, Loss: 11704.7412109375
Epoch 19000, Loss: 11697.71875
Epoch 19100, Loss: 11690.9638671875
Epoch 19200, Loss: 11684.4658203125
Epoch 19300, Loss: 11678.2109375
Epoch 19400, Loss: 11672.177734375
Epoch 19500, Loss: 11666.4541015625
Epoch 19600, Loss: 11661.2333984375
Epoch 19700, Loss: 11656.287109375
Epoch 19800, Loss: 11651.5517578125
Epoch 19900, Loss: 11647.005859375
Epoch 20000, Loss: 11642.6240234375
Epoch 20100, Loss: 11638.3837890625
Epoch 20200, Loss: 11634.271484375
Epoch 20300, Loss: 11630.271484375
Epoch 20400, Loss: 11626.3701171875
Epoch 20500, Loss: 11622.5546875
Epoch 20600, Loss: 11618.8095703125
Epoch 20700, Loss: 11615.134765625
Epoch 20800, Loss: 11611.51171875
Epoch 20900, Loss: 11607.9443359375
Epoch 21000, Loss: 11604.4130859375
Epoch 21100, Loss: 11600.927734375
Epoch 21200, Loss: 11597.4755859375
Epoch 21300, Loss: 11594.06640625
Epoch 21400, Loss: 11590.6884765625
Epoch 21500, Loss: 11587.3486328125
Epoch 21600, Loss: 11584.0458984375
Epoch 21700, Loss: 11580.7841796875
Epoch 21800, Loss: 11577.5556640625
Epoch 21900, Loss: 11574.375
Epoch 22000, Loss: 11571.232421875
Epoch 22100, Loss: 11568.1455078125
Epoch 22200, Loss: 11565.1337890625
Epoch 22300, Loss: 11562.197265625
Epoch 22400, Loss: 11559.3310546875
Epoch 22500, Loss: 11556.552734375
Epoch 22600, Loss: 11553.857421875
Epoch 22700, Loss: 11551.2578125
Epoch 22800, Loss: 11548.744140625
Epoch 22900, Loss: 11546.322265625
Epoch 23000, Loss: 11543.982421875
Epoch 23100, Loss: 11541.728515625
Epoch 23200, Loss: 11539.546875
Epoch 23300, Loss: 11537.439453125
Epoch 23400, Loss: 11535.400390625
Epoch 23500, Loss: 11533.44140625
Epoch 23600, Loss: 11531.55078125
Epoch 23700, Loss: 11529.7294921875
Epoch 23800, Loss: 11527.9794921875
Epoch 23900, Loss: 11526.294921875
Epoch 24000, Loss: 11524.671875
Epoch 24100, Loss: 11523.1162109375
Epoch 24200, Loss: 11521.625
Epoch 24300, Loss: 11520.1923828125
Epoch 24400, Loss: 11518.8193359375
Epoch 24500, Loss: 11517.4951171875
Epoch 24600, Loss: 11516.22265625
Epoch 24700, Loss: 11515.0009765625
Epoch 24800, Loss: 11513.8369140625
Epoch 24900, Loss: 11512.7080078125
Epoch 25000, Loss: 11511.6376953125
Epoch 25100, Loss: 11510.6044921875
Epoch 25200, Loss: 11509.6240234375
Epoch 25300, Loss: 11508.6845703125
Epoch 25400, Loss: 11507.79296875
Epoch 25500, Loss: 11506.94140625
Epoch 25600, Loss: 11506.134765625
Epoch 25700, Loss: 11505.365234375
Epoch 25800, Loss: 11504.6435546875
Epoch 25900, Loss: 11503.9599609375
Epoch 26000, Loss: 11503.3125
Epoch 26100, Loss: 11502.7080078125
Epoch 26200, Loss: 11502.134765625
Epoch 26300, Loss: 11501.603515625
Epoch 26400, Loss: 11501.1015625
Epoch 26500, Loss: 11500.63671875
Epoch 26600, Loss: 11500.208984375
Epoch 26700, Loss: 11499.8046875
Epoch 26800, Loss: 11499.4365234375
Epoch 26900, Loss: 11499.095703125
Epoch 27000, Loss: 11498.78125
Epoch 27100, Loss: 11498.4912109375
Epoch 27200, Loss: 11498.21875
Epoch 27300, Loss: 11497.9736328125
Epoch 27400, Loss: 11497.740234375
Epoch 27500, Loss: 11497.525390625
Epoch 27600, Loss: 11497.3330078125
Epoch 27700, Loss: 11497.154296875
Epoch 27800, Loss: 11496.990234375
Epoch 27900, Loss: 11496.837890625
Epoch 28000, Loss: 11496.7001953125
Epoch 28100, Loss: 11496.576171875
Epoch 28200, Loss: 11496.4638671875
Epoch 28300, Loss: 11496.3671875
Epoch 28400, Loss: 11496.28125
Epoch 28500, Loss: 11496.201171875
Epoch 28600, Loss: 11496.1328125
Epoch 28700, Loss: 11496.060546875
Epoch 28800, Loss: 11495.9990234375
Epoch 28900, Loss: 11495.9423828125
Epoch 29000, Loss: 11495.8896484375
Epoch 29100, Loss: 11495.841796875
Epoch 29200, Loss: 11495.7978515625
Epoch 29300, Loss: 11495.755859375
Epoch 29400, Loss: 11495.7197265625
Epoch 29500, Loss: 11495.68359375
Epoch 29600, Loss: 11495.6494140625
Epoch 29700, Loss: 11495.625
Epoch 29800, Loss: 11495.599609375
Epoch 29900, Loss: 11495.576171875
Epoch 30000, Loss: 11495.5556640625
Epoch 30100, Loss: 11495.5380859375
Epoch 30200, Loss: 11495.5234375
Epoch 30300, Loss: 11495.509765625
Epoch 30400, Loss: 11495.4990234375
Epoch 30500, Loss: 11495.48828125
Epoch 30600, Loss: 11495.4814453125
Epoch 30700, Loss: 11495.4765625
Epoch 30800, Loss: 11495.466796875
Epoch 30900, Loss: 11495.4638671875
Epoch 31000, Loss: 11495.462890625
Epoch 31100, Loss: 11495.45703125
Epoch 31200, Loss: 11495.4560546875
Epoch 31300, Loss: 11495.453125
Epoch 31400, Loss: 11495.4501953125
Epoch 31500, Loss: 11495.4482421875
Epoch 31600, Loss: 11495.44921875
Epoch 31700, Loss: 11495.4462890625
Epoch 31800, Loss: 11495.4482421875
Epoch 31900, Loss: 11495.44921875
Epoch 32000, Loss: 11495.4453125
Epoch 32100, Loss: 11495.4453125
Epoch 32200, Loss: 11495.4462890625
Epoch 32300, Loss: 11495.4462890625
Epoch 32400, Loss: 11495.447265625
Epoch 32500, Loss: 11495.4443359375
Epoch 32600, Loss: 11495.4443359375
Epoch 32700, Loss: 11495.4453125
Epoch 32800, Loss: 11495.4443359375
Epoch 32900, Loss: 11495.4423828125
Epoch 33000, Loss: 11495.4482421875
Epoch 33100, Loss: 11495.443359375
Epoch 33200, Loss: 11495.4453125
Epoch 33300, Loss: 11495.447265625
Epoch 33400, Loss: 11495.4443359375
Epoch 33500, Loss: 11495.447265625
Epoch 33600, Loss: 11495.4462890625
Epoch 33700, Loss: 11495.4462890625
Epoch 33800, Loss: 11495.4453125
Epoch 33900, Loss: 11495.4462890625
Epoch 34000, Loss: 11495.4482421875
Epoch 34100, Loss: 11495.447265625
Epoch 34200, Loss: 11495.4462890625
Epoch 34300, Loss: 11495.4462890625
Epoch 34400, Loss: 11495.4443359375
Epoch 34500, Loss: 11495.447265625
Epoch 34600, Loss: 11495.4453125
Epoch 34700, Loss: 11495.4453125
Epoch 34800, Loss: 11495.4482421875
Epoch 34900, Loss: 11495.4443359375
Epoch 35000, Loss: 11495.4443359375
Epoch 35100, Loss: 11495.4462890625
Epoch 35200, Loss: 11495.4443359375
Epoch 35300, Loss: 11495.4453125
Epoch 35400, Loss: 11495.4482421875
Epoch 35500, Loss: 11495.4443359375
Epoch 35600, Loss: 11495.4462890625
Epoch 35700, Loss: 11495.4453125
Epoch 35800, Loss: 11495.4443359375
Epoch 35900, Loss: 11495.4453125
Epoch 36000, Loss: 11495.4453125
Epoch 36100, Loss: 11495.4443359375
Epoch 36200, Loss: 11495.4453125
Epoch 36300, Loss: 11495.4453125
Epoch 36400, Loss: 11495.4462890625
Epoch 36500, Loss: 11495.4453125
Epoch 36600, Loss: 11495.44921875
Epoch 36700, Loss: 11495.447265625
Epoch 36800, Loss: 11495.4453125
Epoch 36900, Loss: 11495.443359375
Epoch 37000, Loss: 11495.4443359375
Epoch 37100, Loss: 11495.4462890625
Epoch 37200, Loss: 11495.443359375
Epoch 37300, Loss: 11495.447265625
Epoch 37400, Loss: 11495.4453125
Epoch 37500, Loss: 11495.4482421875
Epoch 37600, Loss: 11495.4462890625
Epoch 37700, Loss: 11495.4443359375
Epoch 37800, Loss: 11495.4453125
Epoch 37900, Loss: 11495.443359375
Epoch 38000, Loss: 11495.447265625
Epoch 38100, Loss: 11495.4462890625
Epoch 38200, Loss: 11495.4443359375
Epoch 38300, Loss: 11495.443359375
Epoch 38400, Loss: 11495.4443359375
Epoch 38500, Loss: 11495.4501953125
Epoch 38600, Loss: 11495.447265625
Epoch 38700, Loss: 11495.4453125
Epoch 38800, Loss: 11495.447265625
Epoch 38900, Loss: 11495.447265625
Epoch 39000, Loss: 11495.443359375
Epoch 39100, Loss: 11495.4443359375
Epoch 39200, Loss: 11495.447265625
Epoch 39300, Loss: 11495.4443359375
Epoch 39400, Loss: 11495.4462890625
Epoch 39500, Loss: 11495.447265625
Epoch 39600, Loss: 11495.4453125
Epoch 39700, Loss: 11495.4453125
Epoch 39800, Loss: 11495.4443359375
Epoch 39900, Loss: 11495.4453125
Epoch 40000, Loss: 11495.4462890625
Epoch 40100, Loss: 11495.4482421875
Epoch 40200, Loss: 11495.447265625
Epoch 40300, Loss: 11495.4453125
Epoch 40400, Loss: 11495.4453125
Epoch 40500, Loss: 11495.4453125
Epoch 40600, Loss: 11495.447265625
Epoch 40700, Loss: 11495.4443359375
Epoch 40800, Loss: 11495.4453125
Epoch 40900, Loss: 11495.4462890625
Epoch 41000, Loss: 11495.4443359375
Epoch 41100, Loss: 11495.4453125
Epoch 41200, Loss: 11495.4443359375
Epoch 41300, Loss: 11495.4453125
Epoch 41400, Loss: 11495.4453125
Epoch 41500, Loss: 11495.4443359375
Epoch 41600, Loss: 11495.4453125
Epoch 41700, Loss: 11495.4462890625
Epoch 41800, Loss: 11495.4453125
Epoch 41900, Loss: 11495.447265625
Epoch 42000, Loss: 11495.447265625
Epoch 42100, Loss: 11495.4443359375
Epoch 42200, Loss: 11495.4462890625
Epoch 42300, Loss: 11495.4443359375
Epoch 42400, Loss: 11495.447265625
Epoch 42500, Loss: 11495.4453125
Epoch 42600, Loss: 11495.4443359375
Epoch 42700, Loss: 11495.4453125
Epoch 42800, Loss: 11495.4443359375
Epoch 42900, Loss: 11495.4482421875
Epoch 43000, Loss: 11495.447265625
Epoch 43100, Loss: 11495.4453125
Epoch 43200, Loss: 11495.447265625
Epoch 43300, Loss: 11495.4453125
Epoch 43400, Loss: 11495.4462890625
Epoch 43500, Loss: 11495.4453125
Epoch 43600, Loss: 11495.4462890625
Epoch 43700, Loss: 11495.4443359375
Epoch 43800, Loss: 11495.4453125
Epoch 43900, Loss: 11495.4453125
Epoch 44000, Loss: 11495.4443359375
Epoch 44100, Loss: 11495.4443359375
Epoch 44200, Loss: 11495.4453125
Epoch 44300, Loss: 11495.4453125
Epoch 44400, Loss: 11495.4453125
Epoch 44500, Loss: 11495.4443359375
Epoch 44600, Loss: 11495.447265625
Epoch 44700, Loss: 11495.4443359375
Epoch 44800, Loss: 11495.4453125
Epoch 44900, Loss: 11495.4482421875
Epoch 45000, Loss: 11495.447265625
Epoch 45100, Loss: 11495.4453125
Epoch 45200, Loss: 11495.4453125
Epoch 45300, Loss: 11495.447265625
Epoch 45400, Loss: 11495.447265625
Epoch 45500, Loss: 11495.4453125
Epoch 45600, Loss: 11495.443359375
Epoch 45700, Loss: 11495.4453125
Epoch 45800, Loss: 11495.447265625
Epoch 45900, Loss: 11495.4462890625
Epoch 46000, Loss: 11495.4462890625
Epoch 46100, Loss: 11495.4453125
Epoch 46200, Loss: 11495.447265625
Epoch 46300, Loss: 11495.4462890625
Epoch 46400, Loss: 11495.4443359375
Epoch 46500, Loss: 11495.4453125
Epoch 46600, Loss: 11495.447265625
Epoch 46700, Loss: 11495.443359375
Epoch 46800, Loss: 11495.4453125
Epoch 46900, Loss: 11495.4443359375
Epoch 47000, Loss: 11495.4443359375
Epoch 47100, Loss: 11495.4453125
Epoch 47200, Loss: 11495.4443359375
Epoch 47300, Loss: 11495.447265625
Epoch 47400, Loss: 11495.443359375
Epoch 47500, Loss: 11495.4482421875
Epoch 47600, Loss: 11495.447265625
Epoch 47700, Loss: 11495.4423828125
Epoch 47800, Loss: 11495.4453125
Epoch 47900, Loss: 11495.443359375
Epoch 48000, Loss: 11495.4453125
Epoch 48100, Loss: 11495.4423828125
Epoch 48200, Loss: 11495.4443359375
Epoch 48300, Loss: 11495.4443359375
Epoch 48400, Loss: 11495.4462890625
Epoch 48500, Loss: 11495.4453125
Epoch 48600, Loss: 11495.4453125
Epoch 48700, Loss: 11495.4462890625
Epoch 48800, Loss: 11495.4453125
Epoch 48900, Loss: 11495.4462890625
Epoch 49000, Loss: 11495.4453125
Epoch 49100, Loss: 11495.443359375
Epoch 49200, Loss: 11495.4462890625
Epoch 49300, Loss: 11495.4453125
Epoch 49400, Loss: 11495.4443359375
Epoch 49500, Loss: 11495.4462890625
Epoch 49600, Loss: 11495.4453125
Epoch 49700, Loss: 11495.4462890625
Epoch 49800, Loss: 11495.4443359375
Epoch 49900, Loss: 11495.4453125
Epoch 0, Loss: 63828.96875
Epoch 100, Loss: 60959.69140625
Epoch 200, Loss: 58276.8359375
Epoch 300, Loss: 55770.62109375
Epoch 400, Loss: 53427.93359375
Epoch 500, Loss: 51237.875
Epoch 600, Loss: 49187.65234375
Epoch 700, Loss: 47268.4921875
Epoch 800, Loss: 45475.15234375
Epoch 900, Loss: 43798.89453125
Epoch 1000, Loss: 42232.31640625
Epoch 1100, Loss: 40768.59765625
Epoch 1200, Loss: 39401.58203125
Epoch 1300, Loss: 38127.8671875
Epoch 1400, Loss: 36939.33203125
Epoch 1500, Loss: 35829.96484375
Epoch 1600, Loss: 34794.1875
Epoch 1700, Loss: 33826.79296875
Epoch 1800, Loss: 32922.984375
Epoch 1900, Loss: 32079.12890625
Epoch 2000, Loss: 31292.41796875
Epoch 2100, Loss: 30557.271484375
Epoch 2200, Loss: 29872.8828125
Epoch 2300, Loss: 29234.62890625
Epoch 2400, Loss: 28637.119140625
Epoch 2500, Loss: 28077.15625
Epoch 2600, Loss: 27551.8359375
Epoch 2700, Loss: 27058.541015625
Epoch 2800, Loss: 26594.876953125
Epoch 2900, Loss: 26158.638671875
Epoch 3000, Loss: 25747.77734375
Epoch 3100, Loss: 25360.390625
Epoch 3200, Loss: 24994.689453125
Epoch 3300, Loss: 24651.552734375
Epoch 3400, Loss: 24331.083984375
Epoch 3500, Loss: 24027.318359375
Epoch 3600, Loss: 23738.724609375
Epoch 3700, Loss: 23463.91015625
Epoch 3800, Loss: 23202.5625
Epoch 3900, Loss: 22954.126953125
Epoch 4000, Loss: 22715.97265625
Epoch 4100, Loss: 22487.10546875
Epoch 4200, Loss: 22266.83203125
Epoch 4300, Loss: 22056.04296875
Epoch 4400, Loss: 21852.123046875
Epoch 4500, Loss: 21654.404296875
Epoch 4600, Loss: 21462.345703125
Epoch 4700, Loss: 21275.46484375
Epoch 4800, Loss: 21093.357421875
Epoch 4900, Loss: 20915.673828125
Epoch 5000, Loss: 20742.09765625
Epoch 5100, Loss: 20572.375
Epoch 5200, Loss: 20406.26953125
Epoch 5300, Loss: 20243.580078125
Epoch 5400, Loss: 20084.630859375
Epoch 5500, Loss: 19929.658203125
Epoch 5600, Loss: 19777.62109375
Epoch 5700, Loss: 19628.380859375
Epoch 5800, Loss: 19481.8125
Epoch 5900, Loss: 19337.814453125
Epoch 6000, Loss: 19196.279296875
Epoch 6100, Loss: 19057.12109375
Epoch 6200, Loss: 18920.2578125
Epoch 6300, Loss: 18785.6171875
Epoch 6400, Loss: 18653.125
Epoch 6500, Loss: 18522.71875
Epoch 6600, Loss: 18394.419921875
Epoch 6700, Loss: 18269.17578125
Epoch 6800, Loss: 18146.705078125
Epoch 6900, Loss: 18026.458984375
Epoch 7000, Loss: 17907.974609375
Epoch 7100, Loss: 17791.1875
Epoch 7200, Loss: 17676.169921875
Epoch 7300, Loss: 17562.771484375
Epoch 7400, Loss: 17450.880859375
Epoch 7500, Loss: 17341.337890625
Epoch 7600, Loss: 17234.4296875
Epoch 7700, Loss: 17128.87109375
Epoch 7800, Loss: 17024.693359375
Epoch 7900, Loss: 16922.310546875
Epoch 8000, Loss: 16821.3125
Epoch 8100, Loss: 16721.65234375
Epoch 8200, Loss: 16623.32421875
Epoch 8300, Loss: 16526.314453125
Epoch 8400, Loss: 16430.595703125
Epoch 8500, Loss: 16336.1650390625
Epoch 8600, Loss: 16242.9970703125
Epoch 8700, Loss: 16151.083984375
Epoch 8800, Loss: 16060.400390625
Epoch 8900, Loss: 15970.9345703125
Epoch 9000, Loss: 15882.6630859375
Epoch 9100, Loss: 15795.5751953125
Epoch 9200, Loss: 15709.6640625
Epoch 9300, Loss: 15624.9091796875
Epoch 9400, Loss: 15541.3095703125
Epoch 9500, Loss: 15458.8544921875
Epoch 9600, Loss: 15377.53515625
Epoch 9700, Loss: 15297.349609375
Epoch 9800, Loss: 15218.291015625
Epoch 9900, Loss: 15140.3505859375
Epoch 10000, Loss: 15063.5234375
Epoch 10100, Loss: 14987.7998046875
Epoch 10200, Loss: 14913.1767578125
Epoch 10300, Loss: 14839.650390625
Epoch 10400, Loss: 14767.71875
Epoch 10500, Loss: 14697.37890625
Epoch 10600, Loss: 14628.3896484375
Epoch 10700, Loss: 14560.720703125
Epoch 10800, Loss: 14494.337890625
Epoch 10900, Loss: 14429.1474609375
Epoch 11000, Loss: 14365.060546875
Epoch 11100, Loss: 14302.0146484375
Epoch 11200, Loss: 14239.970703125
Epoch 11300, Loss: 14178.9013671875
Epoch 11400, Loss: 14119.515625
Epoch 11500, Loss: 14061.595703125
Epoch 11600, Loss: 14004.947265625
Epoch 11700, Loss: 13949.517578125
Epoch 11800, Loss: 13895.3017578125
Epoch 11900, Loss: 13842.302734375
Epoch 12000, Loss: 13790.4287109375
Epoch 12100, Loss: 13739.658203125
Epoch 12200, Loss: 13689.9306640625
Epoch 12300, Loss: 13641.19921875
Epoch 12400, Loss: 13593.4296875
Epoch 12500, Loss: 13546.6123046875
Epoch 12600, Loss: 13500.7509765625
Epoch 12700, Loss: 13455.908203125
Epoch 12800, Loss: 13412.095703125
Epoch 12900, Loss: 13369.203125
Epoch 13000, Loss: 13327.2001953125
Epoch 13100, Loss: 13286.0703125
Epoch 13200, Loss: 13245.7919921875
Epoch 13300, Loss: 13206.357421875
Epoch 13400, Loss: 13167.7666015625
Epoch 13500, Loss: 13130.0439453125
Epoch 13600, Loss: 13093.201171875
Epoch 13700, Loss: 13057.1787109375
Epoch 13800, Loss: 13021.9677734375
Epoch 13900, Loss: 12987.5517578125
Epoch 14000, Loss: 12953.90625
Epoch 14100, Loss: 12921.03125
Epoch 14200, Loss: 12888.8955078125
Epoch 14300, Loss: 12857.5986328125
Epoch 14400, Loss: 12827.2353515625
Epoch 14500, Loss: 12797.6943359375
Epoch 14600, Loss: 12768.9453125
Epoch 14700, Loss: 12740.955078125
Epoch 14800, Loss: 12713.6943359375
Epoch 14900, Loss: 12687.14453125
Epoch 15000, Loss: 12661.2880859375
Epoch 15100, Loss: 12636.09765625
Epoch 15200, Loss: 12611.5615234375
Epoch 15300, Loss: 12587.76171875
Epoch 15400, Loss: 12565.2568359375
Epoch 15500, Loss: 12543.544921875
Epoch 15600, Loss: 12522.5263671875
Epoch 15700, Loss: 12502.1298828125
Epoch 15800, Loss: 12482.3271484375
Epoch 15900, Loss: 12463.0830078125
Epoch 16000, Loss: 12444.37890625
Epoch 16100, Loss: 12426.1953125
Epoch 16200, Loss: 12408.5185546875
Epoch 16300, Loss: 12391.3427734375
Epoch 16400, Loss: 12374.65234375
Epoch 16500, Loss: 12358.427734375
Epoch 16600, Loss: 12342.7578125
Epoch 16700, Loss: 12327.736328125
Epoch 16800, Loss: 12313.1796875
Epoch 16900, Loss: 12299.083984375
Epoch 17000, Loss: 12285.4375
Epoch 17100, Loss: 12272.21875
Epoch 17200, Loss: 12259.4150390625
Epoch 17300, Loss: 12247.013671875
Epoch 17400, Loss: 12235.0126953125
Epoch 17500, Loss: 12223.4013671875
Epoch 17600, Loss: 12212.16015625
Epoch 17700, Loss: 12201.2861328125
Epoch 17800, Loss: 12190.775390625
Epoch 17900, Loss: 12180.60546875
Epoch 18000, Loss: 12170.7783203125
Epoch 18100, Loss: 12161.2724609375
Epoch 18200, Loss: 12152.083984375
Epoch 18300, Loss: 12143.203125
Epoch 18400, Loss: 12134.62109375
Epoch 18500, Loss: 12126.31640625
Epoch 18600, Loss: 12118.2919921875
Epoch 18700, Loss: 12110.529296875
Epoch 18800, Loss: 12103.0205078125
Epoch 18900, Loss: 12095.7451171875
Epoch 19000, Loss: 12088.6953125
Epoch 19100, Loss: 12082.1044921875
Epoch 19200, Loss: 12076.0068359375
Epoch 19300, Loss: 12070.1962890625
Epoch 19400, Loss: 12064.6064453125
Epoch 19500, Loss: 12059.2177734375
Epoch 19600, Loss: 12054.0048828125
Epoch 19700, Loss: 12048.9560546875
Epoch 19800, Loss: 12044.044921875
Epoch 19900, Loss: 12039.2548828125
Epoch 20000, Loss: 12034.5859375
Epoch 20100, Loss: 12030.009765625
Epoch 20200, Loss: 12025.521484375
Epoch 20300, Loss: 12021.1083984375
Epoch 20400, Loss: 12016.78515625
Epoch 20500, Loss: 12012.5146484375
Epoch 20600, Loss: 12008.3232421875
Epoch 20700, Loss: 12004.18359375
Epoch 20800, Loss: 12000.1259765625
Epoch 20900, Loss: 11996.125
Epoch 21000, Loss: 11992.2080078125
Epoch 21100, Loss: 11988.361328125
Epoch 21200, Loss: 11984.5888671875
Epoch 21300, Loss: 11980.8916015625
Epoch 21400, Loss: 11977.2685546875
Epoch 21500, Loss: 11973.728515625
Epoch 21600, Loss: 11970.2734375
Epoch 21700, Loss: 11966.908203125
Epoch 21800, Loss: 11963.638671875
Epoch 21900, Loss: 11960.451171875
Epoch 22000, Loss: 11957.35546875
Epoch 22100, Loss: 11954.3603515625
Epoch 22200, Loss: 11951.443359375
Epoch 22300, Loss: 11948.6201171875
Epoch 22400, Loss: 11945.890625
Epoch 22500, Loss: 11943.2509765625
Epoch 22600, Loss: 11940.69140625
Epoch 22700, Loss: 11938.2177734375
Epoch 22800, Loss: 11935.8349609375
Epoch 22900, Loss: 11933.533203125
Epoch 23000, Loss: 11931.3076171875
Epoch 23100, Loss: 11929.1484375
Epoch 23200, Loss: 11927.0576171875
Epoch 23300, Loss: 11925.04296875
Epoch 23400, Loss: 11923.091796875
Epoch 23500, Loss: 11921.203125
Epoch 23600, Loss: 11919.3779296875
Epoch 23700, Loss: 11917.6171875
Epoch 23800, Loss: 11915.919921875
Epoch 23900, Loss: 11914.279296875
Epoch 24000, Loss: 11912.7041015625
Epoch 24100, Loss: 11911.1845703125
Epoch 24200, Loss: 11909.728515625
Epoch 24300, Loss: 11908.326171875
Epoch 24400, Loss: 11906.9833984375
Epoch 24500, Loss: 11905.6923828125
Epoch 24600, Loss: 11904.46484375
Epoch 24700, Loss: 11903.28125
Epoch 24800, Loss: 11902.1455078125
Epoch 24900, Loss: 11901.06640625
Epoch 25000, Loss: 11900.0224609375
Epoch 25100, Loss: 11899.025390625
Epoch 25200, Loss: 11898.076171875
Epoch 25300, Loss: 11897.1640625
Epoch 25400, Loss: 11896.296875
Epoch 25500, Loss: 11895.4755859375
Epoch 25600, Loss: 11894.6826171875
Epoch 25700, Loss: 11893.931640625
Epoch 25800, Loss: 11893.22265625
Epoch 25900, Loss: 11892.5517578125
Epoch 26000, Loss: 11891.908203125
Epoch 26100, Loss: 11891.30859375
Epoch 26200, Loss: 11890.7470703125
Epoch 26300, Loss: 11890.21484375
Epoch 26400, Loss: 11889.71875
Epoch 26500, Loss: 11889.2529296875
Epoch 26600, Loss: 11888.8203125
Epoch 26700, Loss: 11888.416015625
Epoch 26800, Loss: 11888.041015625
Epoch 26900, Loss: 11887.6953125
Epoch 27000, Loss: 11887.369140625
Epoch 27100, Loss: 11887.07421875
Epoch 27200, Loss: 11886.7880859375
Epoch 27300, Loss: 11886.5341796875
Epoch 27400, Loss: 11886.2890625
Epoch 27500, Loss: 11886.0673828125
Epoch 27600, Loss: 11885.86328125
Epoch 27700, Loss: 11885.6669921875
Epoch 27800, Loss: 11885.4921875
Epoch 27900, Loss: 11885.328125
Epoch 28000, Loss: 11885.1796875
Epoch 28100, Loss: 11885.0458984375
Epoch 28200, Loss: 11884.927734375
Epoch 28300, Loss: 11884.8154296875
Epoch 28400, Loss: 11884.7119140625
Epoch 28500, Loss: 11884.62109375
Epoch 28600, Loss: 11884.53515625
Epoch 28700, Loss: 11884.45703125
Epoch 28800, Loss: 11884.37890625
Epoch 28900, Loss: 11884.3115234375
Epoch 29000, Loss: 11884.240234375
Epoch 29100, Loss: 11884.1767578125
Epoch 29200, Loss: 11884.12109375
Epoch 29300, Loss: 11884.0673828125
Epoch 29400, Loss: 11884.0146484375
Epoch 29500, Loss: 11883.9697265625
Epoch 29600, Loss: 11883.9248046875
Epoch 29700, Loss: 11883.88671875
Epoch 29800, Loss: 11883.8525390625
Epoch 29900, Loss: 11883.8134765625
Epoch 30000, Loss: 11883.78515625
Epoch 30100, Loss: 11883.75390625
Epoch 30200, Loss: 11883.728515625
Epoch 30300, Loss: 11883.70703125
Epoch 30400, Loss: 11883.685546875
Epoch 30500, Loss: 11883.669921875
Epoch 30600, Loss: 11883.6533203125
Epoch 30700, Loss: 11883.6376953125
Epoch 30800, Loss: 11883.625
Epoch 30900, Loss: 11883.6162109375
Epoch 31000, Loss: 11883.6044921875
Epoch 31100, Loss: 11883.59375
Epoch 31200, Loss: 11883.5830078125
Epoch 31300, Loss: 11883.5791015625
Epoch 31400, Loss: 11883.5712890625
Epoch 31500, Loss: 11883.5654296875
Epoch 31600, Loss: 11883.5595703125
Epoch 31700, Loss: 11883.5537109375
Epoch 31800, Loss: 11883.55078125
Epoch 31900, Loss: 11883.544921875
Epoch 32000, Loss: 11883.5400390625
Epoch 32100, Loss: 11883.5361328125
Epoch 32200, Loss: 11883.529296875
Epoch 32300, Loss: 11883.5302734375
Epoch 32400, Loss: 11883.53125
Epoch 32500, Loss: 11883.5263671875
Epoch 32600, Loss: 11883.5244140625
Epoch 32700, Loss: 11883.5244140625
Epoch 32800, Loss: 11883.521484375
Epoch 32900, Loss: 11883.5166015625
Epoch 33000, Loss: 11883.521484375
Epoch 33100, Loss: 11883.51953125
Epoch 33200, Loss: 11883.513671875
Epoch 33300, Loss: 11883.5185546875
Epoch 33400, Loss: 11883.515625
Epoch 33500, Loss: 11883.513671875
Epoch 33600, Loss: 11883.5126953125
Epoch 33700, Loss: 11883.51171875
Epoch 33800, Loss: 11883.5107421875
Epoch 33900, Loss: 11883.515625
Epoch 34000, Loss: 11883.51171875
Epoch 34100, Loss: 11883.509765625
Epoch 34200, Loss: 11883.5126953125
Epoch 34300, Loss: 11883.5107421875
Epoch 34400, Loss: 11883.5087890625
Epoch 34500, Loss: 11883.51171875
Epoch 34600, Loss: 11883.51171875
Epoch 34700, Loss: 11883.51171875
Epoch 34800, Loss: 11883.5087890625
Epoch 34900, Loss: 11883.51171875
Epoch 35000, Loss: 11883.509765625
Epoch 35100, Loss: 11883.5146484375
Epoch 35200, Loss: 11883.5087890625
Epoch 35300, Loss: 11883.5087890625
Epoch 35400, Loss: 11883.509765625
Epoch 35500, Loss: 11883.51171875
Epoch 35600, Loss: 11883.509765625
Epoch 35700, Loss: 11883.5107421875
Epoch 35800, Loss: 11883.51171875
Epoch 35900, Loss: 11883.51171875
Epoch 36000, Loss: 11883.5126953125
Epoch 36100, Loss: 11883.5107421875
Epoch 36200, Loss: 11883.51171875
Epoch 36300, Loss: 11883.509765625
Epoch 36400, Loss: 11883.5126953125
Epoch 36500, Loss: 11883.5087890625
Epoch 36600, Loss: 11883.513671875
Epoch 36700, Loss: 11883.513671875
Epoch 36800, Loss: 11883.5087890625
Epoch 36900, Loss: 11883.515625
Epoch 37000, Loss: 11883.5087890625
Epoch 37100, Loss: 11883.5126953125
Epoch 37200, Loss: 11883.509765625
Epoch 37300, Loss: 11883.513671875
Epoch 37400, Loss: 11883.515625
Epoch 37500, Loss: 11883.509765625
Epoch 37600, Loss: 11883.51171875
Epoch 37700, Loss: 11883.513671875
Epoch 37800, Loss: 11883.513671875
Epoch 37900, Loss: 11883.51171875
Epoch 38000, Loss: 11883.5087890625
Epoch 38100, Loss: 11883.5087890625
Epoch 38200, Loss: 11883.509765625
Epoch 38300, Loss: 11883.5107421875
Epoch 38400, Loss: 11883.5107421875
Epoch 38500, Loss: 11883.509765625
Epoch 38600, Loss: 11883.5087890625
Epoch 38700, Loss: 11883.509765625
Epoch 38800, Loss: 11883.513671875
Epoch 38900, Loss: 11883.5107421875
Epoch 39000, Loss: 11883.51171875
Epoch 39100, Loss: 11883.5107421875
Epoch 39200, Loss: 11883.51171875
Epoch 39300, Loss: 11883.5107421875
Epoch 39400, Loss: 11883.5087890625
Epoch 39500, Loss: 11883.51171875
Epoch 39600, Loss: 11883.5126953125
Epoch 39700, Loss: 11883.513671875
Epoch 39800, Loss: 11883.5126953125
Epoch 39900, Loss: 11883.51171875
Epoch 40000, Loss: 11883.51171875
Epoch 40100, Loss: 11883.5126953125
Epoch 40200, Loss: 11883.5126953125
Epoch 40300, Loss: 11883.51171875
Epoch 40400, Loss: 11883.51171875
Epoch 40500, Loss: 11883.515625
Epoch 40600, Loss: 11883.51171875
Epoch 40700, Loss: 11883.509765625
Epoch 40800, Loss: 11883.51171875
Epoch 40900, Loss: 11883.51171875
Epoch 41000, Loss: 11883.509765625
Epoch 41100, Loss: 11883.513671875
Epoch 41200, Loss: 11883.5107421875
Epoch 41300, Loss: 11883.5087890625
Epoch 41400, Loss: 11883.515625
Epoch 41500, Loss: 11883.5126953125
Epoch 41600, Loss: 11883.509765625
Epoch 41700, Loss: 11883.509765625
Epoch 41800, Loss: 11883.5126953125
Epoch 41900, Loss: 11883.5126953125
Epoch 42000, Loss: 11883.513671875
Epoch 42100, Loss: 11883.513671875
Epoch 42200, Loss: 11883.5087890625
Epoch 42300, Loss: 11883.51171875
Epoch 42400, Loss: 11883.5107421875
Epoch 42500, Loss: 11883.513671875
Epoch 42600, Loss: 11883.5146484375
Epoch 42700, Loss: 11883.515625
Epoch 42800, Loss: 11883.513671875
Epoch 42900, Loss: 11883.5087890625
Epoch 43000, Loss: 11883.5087890625
Epoch 43100, Loss: 11883.5107421875
Epoch 43200, Loss: 11883.5126953125
Epoch 43300, Loss: 11883.51171875
Epoch 43400, Loss: 11883.5107421875
Epoch 43500, Loss: 11883.51171875
Epoch 43600, Loss: 11883.5146484375
Epoch 43700, Loss: 11883.5107421875
Epoch 43800, Loss: 11883.5126953125
Epoch 43900, Loss: 11883.5107421875
Epoch 44000, Loss: 11883.5126953125
Epoch 44100, Loss: 11883.51171875
Epoch 44200, Loss: 11883.5078125
Epoch 44300, Loss: 11883.509765625
Epoch 44400, Loss: 11883.5107421875
Epoch 44500, Loss: 11883.5087890625
Epoch 44600, Loss: 11883.5126953125
Epoch 44700, Loss: 11883.509765625
Epoch 44800, Loss: 11883.509765625
Epoch 44900, Loss: 11883.5107421875
Epoch 45000, Loss: 11883.515625
Epoch 45100, Loss: 11883.5107421875
Epoch 45200, Loss: 11883.5087890625
Epoch 45300, Loss: 11883.5107421875
Epoch 45400, Loss: 11883.5126953125
Epoch 45500, Loss: 11883.509765625
Epoch 45600, Loss: 11883.509765625
Epoch 45700, Loss: 11883.5107421875
Epoch 45800, Loss: 11883.5107421875
Epoch 45900, Loss: 11883.5126953125
Epoch 46000, Loss: 11883.51171875
Epoch 46100, Loss: 11883.513671875
Epoch 46200, Loss: 11883.513671875
Epoch 46300, Loss: 11883.5107421875
Epoch 46400, Loss: 11883.513671875
Epoch 46500, Loss: 11883.51171875
Epoch 46600, Loss: 11883.5126953125
Epoch 46700, Loss: 11883.509765625
Epoch 46800, Loss: 11883.515625
Epoch 46900, Loss: 11883.5126953125
Epoch 47000, Loss: 11883.5107421875
Epoch 47100, Loss: 11883.5107421875
Epoch 47200, Loss: 11883.5107421875
Epoch 47300, Loss: 11883.513671875
Epoch 47400, Loss: 11883.509765625
Epoch 47500, Loss: 11883.509765625
Epoch 47600, Loss: 11883.51171875
Epoch 47700, Loss: 11883.5126953125
Epoch 47800, Loss: 11883.51171875
Epoch 47900, Loss: 11883.5126953125
Epoch 48000, Loss: 11883.5107421875
Epoch 48100, Loss: 11883.51171875
Epoch 48200, Loss: 11883.51171875
Epoch 48300, Loss: 11883.51171875
Epoch 48400, Loss: 11883.51171875
Epoch 48500, Loss: 11883.5107421875
Epoch 48600, Loss: 11883.51171875
Epoch 48700, Loss: 11883.5107421875
Epoch 48800, Loss: 11883.51171875
Epoch 48900, Loss: 11883.513671875
Epoch 49000, Loss: 11883.5126953125
Epoch 49100, Loss: 11883.509765625
Epoch 49200, Loss: 11883.51171875
Epoch 49300, Loss: 11883.5107421875
Epoch 49400, Loss: 11883.513671875
Epoch 49500, Loss: 11883.513671875
Epoch 49600, Loss: 11883.5087890625
Epoch 49700, Loss: 11883.5107421875
Epoch 49800, Loss: 11883.51171875
Epoch 49900, Loss: 11883.513671875
Epoch 0, Loss: 63828.96875
Epoch 100, Loss: 60969.26171875
Epoch 200, Loss: 58295.921875
Epoch 300, Loss: 55798.828125
Epoch 400, Loss: 53466.9921875
Epoch 500, Loss: 51288.4296875
Epoch 600, Loss: 49250.1171875
Epoch 700, Loss: 47343.90234375
Epoch 800, Loss: 45562.37890625
Epoch 900, Loss: 43897.1171875
Epoch 1000, Loss: 42340.640625
Epoch 1100, Loss: 40886.15234375
Epoch 1200, Loss: 39529.34765625
Epoch 1300, Loss: 38264.31640625
Epoch 1400, Loss: 37083.765625
Epoch 1500, Loss: 35981.703125
Epoch 1600, Loss: 34952.55859375
Epoch 1700, Loss: 33991.19921875
Epoch 1800, Loss: 33092.890625
Epoch 1900, Loss: 32256.509765625
Epoch 2000, Loss: 31475.287109375
Epoch 2100, Loss: 30747.44921875
Epoch 2200, Loss: 30070.466796875
Epoch 2300, Loss: 29438.12109375
Epoch 2400, Loss: 28846.17578125
Epoch 2500, Loss: 28291.41796875
Epoch 2600, Loss: 27770.966796875
Epoch 2700, Loss: 27282.2109375
Epoch 2800, Loss: 26822.7890625
Epoch 2900, Loss: 26390.515625
Epoch 3000, Loss: 25983.3828125
Epoch 3100, Loss: 25599.5
Epoch 3200, Loss: 25237.255859375
Epoch 3300, Loss: 24901.013671875
Epoch 3400, Loss: 24585.015625
Epoch 3500, Loss: 24285.513671875
Epoch 3600, Loss: 24000.98046875
Epoch 3700, Loss: 23730.30078125
Epoch 3800, Loss: 23474.6484375
Epoch 3900, Loss: 23230.228515625
Epoch 4000, Loss: 22995.912109375
Epoch 4100, Loss: 22770.70703125
Epoch 4200, Loss: 22555.5078125
Epoch 4300, Loss: 22348.517578125
Epoch 4400, Loss: 22148.2109375
Epoch 4500, Loss: 21953.96484375
Epoch 4600, Loss: 21765.236328125
Epoch 4700, Loss: 21581.556640625
Epoch 4800, Loss: 21402.53125
Epoch 4900, Loss: 21227.8046875
Epoch 5000, Loss: 21057.080078125
Epoch 5100, Loss: 20890.1015625
Epoch 5200, Loss: 20726.634765625
Epoch 5300, Loss: 20567.63671875
Epoch 5400, Loss: 20412.236328125
Epoch 5500, Loss: 20259.837890625
Epoch 5600, Loss: 20110.28515625
Epoch 5700, Loss: 19963.443359375
Epoch 5800, Loss: 19819.197265625
Epoch 5900, Loss: 19677.4375
Epoch 6000, Loss: 19538.05859375
Epoch 6100, Loss: 19400.978515625
Epoch 6200, Loss: 19266.123046875
Epoch 6300, Loss: 19133.392578125
Epoch 6400, Loss: 19002.724609375
Epoch 6500, Loss: 18874.65625
Epoch 6600, Loss: 18749.236328125
Epoch 6700, Loss: 18626.732421875
Epoch 6800, Loss: 18506.529296875
Epoch 6900, Loss: 18388.07421875
Epoch 7000, Loss: 18271.560546875
Epoch 7100, Loss: 18156.626953125
Epoch 7200, Loss: 18043.181640625
Epoch 7300, Loss: 17931.1796875
Epoch 7400, Loss: 17820.595703125
Epoch 7500, Loss: 17712.333984375
Epoch 7600, Loss: 17607.701171875
Epoch 7700, Loss: 17504.40234375
Epoch 7800, Loss: 17402.388671875
Epoch 7900, Loss: 17301.6484375
Epoch 8000, Loss: 17202.166015625
Epoch 8100, Loss: 17103.91796875
Epoch 8200, Loss: 17006.89453125
Epoch 8300, Loss: 16911.09375
Epoch 8400, Loss: 16816.48828125
Epoch 8500, Loss: 16723.0703125
Epoch 8600, Loss: 16630.83203125
Epoch 8700, Loss: 16539.75390625
Epoch 8800, Loss: 16449.830078125
Epoch 8900, Loss: 16361.0517578125
Epoch 9000, Loss: 16273.4052734375
Epoch 9100, Loss: 16186.87890625
Epoch 9200, Loss: 16101.4658203125
Epoch 9300, Loss: 16017.150390625
Epoch 9400, Loss: 15933.92578125
Epoch 9500, Loss: 15851.779296875
Epoch 9600, Loss: 15770.7041015625
Epoch 9700, Loss: 15690.703125
Epoch 9800, Loss: 15611.7587890625
Epoch 9900, Loss: 15533.8720703125
Epoch 10000, Loss: 15457.0380859375
Epoch 10100, Loss: 15382.044921875
Epoch 10200, Loss: 15308.6083984375
Epoch 10300, Loss: 15236.580078125
Epoch 10400, Loss: 15165.904296875
Epoch 10500, Loss: 15096.5458984375
Epoch 10600, Loss: 15028.4609375
Epoch 10700, Loss: 14961.548828125
Epoch 10800, Loss: 14895.728515625
Epoch 10900, Loss: 14830.943359375
Epoch 11000, Loss: 14767.48828125
Epoch 11100, Loss: 14705.4580078125
Epoch 11200, Loss: 14645.3876953125
Epoch 11300, Loss: 14586.716796875
Epoch 11400, Loss: 14529.3125
Epoch 11500, Loss: 14473.1103515625
Epoch 11600, Loss: 14418.0546875
Epoch 11700, Loss: 14364.10546875
Epoch 11800, Loss: 14311.21875
Epoch 11900, Loss: 14259.3798828125
Epoch 12000, Loss: 14208.546875
Epoch 12100, Loss: 14158.7041015625
Epoch 12200, Loss: 14109.8623046875
Epoch 12300, Loss: 14062.0712890625
Epoch 12400, Loss: 14015.38671875
Epoch 12500, Loss: 13969.701171875
Epoch 12600, Loss: 13924.98828125
Epoch 12700, Loss: 13881.22265625
Epoch 12800, Loss: 13838.390625
Epoch 12900, Loss: 13796.4765625
Epoch 13000, Loss: 13755.45703125
Epoch 13100, Loss: 13715.306640625
Epoch 13200, Loss: 13676.08984375
Epoch 13300, Loss: 13637.732421875
Epoch 13400, Loss: 13600.2236328125
Epoch 13500, Loss: 13563.517578125
Epoch 13600, Loss: 13527.9140625
Epoch 13700, Loss: 13493.2568359375
Epoch 13800, Loss: 13459.46875
Epoch 13900, Loss: 13426.509765625
Epoch 14000, Loss: 13394.345703125
Epoch 14100, Loss: 13362.9755859375
Epoch 14200, Loss: 13332.3896484375
Epoch 14300, Loss: 13302.576171875
Epoch 14400, Loss: 13273.5009765625
Epoch 14500, Loss: 13245.138671875
Epoch 14600, Loss: 13217.44921875
Epoch 14700, Loss: 13190.4404296875
Epoch 14800, Loss: 13164.0703125
Epoch 14900, Loss: 13138.3466796875
Epoch 15000, Loss: 13113.4482421875
Epoch 15100, Loss: 13089.9267578125
Epoch 15200, Loss: 13067.205078125
Epoch 15300, Loss: 13045.1748046875
Epoch 15400, Loss: 13023.7822265625
Epoch 15500, Loss: 13003.013671875
Epoch 15600, Loss: 12982.826171875
Epoch 15700, Loss: 12963.193359375
Epoch 15800, Loss: 12944.1064453125
Epoch 15900, Loss: 12925.5263671875
Epoch 16000, Loss: 12907.44921875
Epoch 16100, Loss: 12889.85546875
Epoch 16200, Loss: 12872.93359375
Epoch 16300, Loss: 12856.6376953125
Epoch 16400, Loss: 12840.8369140625
Epoch 16500, Loss: 12825.494140625
Epoch 16600, Loss: 12810.6005859375
Epoch 16700, Loss: 12796.1572265625
Epoch 16800, Loss: 12782.13671875
Epoch 16900, Loss: 12768.5458984375
Epoch 17000, Loss: 12755.3583984375
Epoch 17100, Loss: 12742.576171875
Epoch 17200, Loss: 12730.1845703125
Epoch 17300, Loss: 12718.1796875
Epoch 17400, Loss: 12706.55078125
Epoch 17500, Loss: 12695.279296875
Epoch 17600, Loss: 12684.35546875
Epoch 17700, Loss: 12673.7783203125
Epoch 17800, Loss: 12663.541015625
Epoch 17900, Loss: 12653.6171875
Epoch 18000, Loss: 12644.0009765625
Epoch 18100, Loss: 12634.69140625
Epoch 18200, Loss: 12625.671875
Epoch 18300, Loss: 12616.923828125
Epoch 18400, Loss: 12608.4462890625
Epoch 18500, Loss: 12600.212890625
Epoch 18600, Loss: 12592.23046875
Epoch 18700, Loss: 12584.7255859375
Epoch 18800, Loss: 12577.7861328125
Epoch 18900, Loss: 12571.18359375
Epoch 19000, Loss: 12564.8388671875
Epoch 19100, Loss: 12558.724609375
Epoch 19200, Loss: 12552.8046875
Epoch 19300, Loss: 12547.0625
Epoch 19400, Loss: 12541.486328125
Epoch 19500, Loss: 12536.0556640625
Epoch 19600, Loss: 12530.7666015625
Epoch 19700, Loss: 12525.5927734375
Epoch 19800, Loss: 12520.5439453125
Epoch 19900, Loss: 12515.59375
Epoch 20000, Loss: 12510.7509765625
Epoch 20100, Loss: 12506.001953125
Epoch 20200, Loss: 12501.33984375
Epoch 20300, Loss: 12496.767578125
Epoch 20400, Loss: 12492.265625
Epoch 20500, Loss: 12487.8515625
Epoch 20600, Loss: 12483.51953125
Epoch 20700, Loss: 12479.2734375
Epoch 20800, Loss: 12475.1171875
Epoch 20900, Loss: 12471.037109375
Epoch 21000, Loss: 12467.041015625
Epoch 21100, Loss: 12463.1328125
Epoch 21200, Loss: 12459.32421875
Epoch 21300, Loss: 12455.58984375
Epoch 21400, Loss: 12451.9482421875
Epoch 21500, Loss: 12448.40625
Epoch 21600, Loss: 12444.9501953125
Epoch 21700, Loss: 12441.583984375
Epoch 21800, Loss: 12438.3125
Epoch 21900, Loss: 12435.1298828125
Epoch 22000, Loss: 12432.03515625
Epoch 22100, Loss: 12429.0390625
Epoch 22200, Loss: 12426.1318359375
Epoch 22300, Loss: 12423.30859375
Epoch 22400, Loss: 12420.583984375
Epoch 22500, Loss: 12417.9384765625
Epoch 22600, Loss: 12415.384765625
Epoch 22700, Loss: 12412.916015625
Epoch 22800, Loss: 12410.525390625
Epoch 22900, Loss: 12408.2099609375
Epoch 23000, Loss: 12405.974609375
Epoch 23100, Loss: 12403.806640625
Epoch 23200, Loss: 12401.7099609375
Epoch 23300, Loss: 12399.67578125
Epoch 23400, Loss: 12397.708984375
Epoch 23500, Loss: 12395.810546875
Epoch 23600, Loss: 12393.9716796875
Epoch 23700, Loss: 12392.193359375
Epoch 23800, Loss: 12390.482421875
Epoch 23900, Loss: 12388.822265625
Epoch 24000, Loss: 12387.232421875
Epoch 24100, Loss: 12385.6953125
Epoch 24200, Loss: 12384.2138671875
Epoch 24300, Loss: 12382.7939453125
Epoch 24400, Loss: 12381.435546875
Epoch 24500, Loss: 12380.1298828125
Epoch 24600, Loss: 12378.87109375
Epoch 24700, Loss: 12377.677734375
Epoch 24800, Loss: 12376.52734375
Epoch 24900, Loss: 12375.435546875
Epoch 25000, Loss: 12374.3896484375
Epoch 25100, Loss: 12373.390625
Epoch 25200, Loss: 12372.4453125
Epoch 25300, Loss: 12371.5380859375
Epoch 25400, Loss: 12370.6767578125
Epoch 25500, Loss: 12369.859375
Epoch 25600, Loss: 12369.0712890625
Epoch 25700, Loss: 12368.3330078125
Epoch 25800, Loss: 12367.6279296875
Epoch 25900, Loss: 12366.958984375
Epoch 26000, Loss: 12366.328125
Epoch 26100, Loss: 12365.7275390625
Epoch 26200, Loss: 12365.1630859375
Epoch 26300, Loss: 12364.6259765625
Epoch 26400, Loss: 12364.125
Epoch 26500, Loss: 12363.65234375
Epoch 26600, Loss: 12363.212890625
Epoch 26700, Loss: 12362.7998046875
Epoch 26800, Loss: 12362.412109375
Epoch 26900, Loss: 12362.05078125
Epoch 27000, Loss: 12361.7119140625
Epoch 27100, Loss: 12361.3994140625
Epoch 27200, Loss: 12361.099609375
Epoch 27300, Loss: 12360.8203125
Epoch 27400, Loss: 12360.55859375
Epoch 27500, Loss: 12360.3134765625
Epoch 27600, Loss: 12360.083984375
Epoch 27700, Loss: 12359.8740234375
Epoch 27800, Loss: 12359.673828125
Epoch 27900, Loss: 12359.498046875
Epoch 28000, Loss: 12359.330078125
Epoch 28100, Loss: 12359.169921875
Epoch 28200, Loss: 12359.0244140625
Epoch 28300, Loss: 12358.8955078125
Epoch 28400, Loss: 12358.775390625
Epoch 28500, Loss: 12358.6640625
Epoch 28600, Loss: 12358.5556640625
Epoch 28700, Loss: 12358.4541015625
Epoch 28800, Loss: 12358.3583984375
Epoch 28900, Loss: 12358.271484375
Epoch 29000, Loss: 12358.18359375
Epoch 29100, Loss: 12358.111328125
Epoch 29200, Loss: 12358.044921875
Epoch 29300, Loss: 12357.9775390625
Epoch 29400, Loss: 12357.9189453125
Epoch 29500, Loss: 12357.86328125
Epoch 29600, Loss: 12357.8173828125
Epoch 29700, Loss: 12357.7744140625
Epoch 29800, Loss: 12357.732421875
Epoch 29900, Loss: 12357.69140625
Epoch 30000, Loss: 12357.6591796875
Epoch 30100, Loss: 12357.6240234375
Epoch 30200, Loss: 12357.59375
Epoch 30300, Loss: 12357.5625
Epoch 30400, Loss: 12357.53515625
Epoch 30500, Loss: 12357.5078125
Epoch 30600, Loss: 12357.482421875
Epoch 30700, Loss: 12357.455078125
Epoch 30800, Loss: 12357.435546875
Epoch 30900, Loss: 12357.412109375
Epoch 31000, Loss: 12357.3916015625
Epoch 31100, Loss: 12357.3671875
Epoch 31200, Loss: 12357.349609375
Epoch 31300, Loss: 12357.330078125
Epoch 31400, Loss: 12357.3046875
Epoch 31500, Loss: 12357.29296875
Epoch 31600, Loss: 12357.271484375
Epoch 31700, Loss: 12357.2548828125
Epoch 31800, Loss: 12357.2421875
Epoch 31900, Loss: 12357.224609375
Epoch 32000, Loss: 12357.205078125
Epoch 32100, Loss: 12357.193359375
Epoch 32200, Loss: 12357.177734375
Epoch 32300, Loss: 12357.162109375
Epoch 32400, Loss: 12357.1484375
Epoch 32500, Loss: 12357.134765625
Epoch 32600, Loss: 12357.1318359375
Epoch 32700, Loss: 12357.111328125
Epoch 32800, Loss: 12357.1025390625
Epoch 32900, Loss: 12357.0888671875
Epoch 33000, Loss: 12357.0810546875
Epoch 33100, Loss: 12357.0732421875
Epoch 33200, Loss: 12357.060546875
Epoch 33300, Loss: 12357.052734375
Epoch 33400, Loss: 12357.048828125
Epoch 33500, Loss: 12357.044921875
Epoch 33600, Loss: 12357.03125
Epoch 33700, Loss: 12357.02734375
Epoch 33800, Loss: 12357.025390625
Epoch 33900, Loss: 12357.015625
Epoch 34000, Loss: 12357.0087890625
Epoch 34100, Loss: 12357.001953125
Epoch 34200, Loss: 12357.0009765625
Epoch 34300, Loss: 12356.99609375
Epoch 34400, Loss: 12356.9912109375
Epoch 34500, Loss: 12356.98828125
Epoch 34600, Loss: 12356.990234375
Epoch 34700, Loss: 12356.98046875
Epoch 34800, Loss: 12356.9775390625
Epoch 34900, Loss: 12356.978515625
Epoch 35000, Loss: 12356.9765625
Epoch 35100, Loss: 12356.9716796875
Epoch 35200, Loss: 12356.974609375
Epoch 35300, Loss: 12356.9697265625
Epoch 35400, Loss: 12356.9677734375
Epoch 35500, Loss: 12356.96875
Epoch 35600, Loss: 12356.96875
Epoch 35700, Loss: 12356.9599609375
Epoch 35800, Loss: 12356.96484375
Epoch 35900, Loss: 12356.9619140625
Epoch 36000, Loss: 12356.962890625
Epoch 36100, Loss: 12356.9609375
Epoch 36200, Loss: 12356.9619140625
Epoch 36300, Loss: 12356.9638671875
Epoch 36400, Loss: 12356.9619140625
Epoch 36500, Loss: 12356.96484375
Epoch 36600, Loss: 12356.96484375
Epoch 36700, Loss: 12356.9677734375
Epoch 36800, Loss: 12356.9619140625
Epoch 36900, Loss: 12356.962890625
Epoch 37000, Loss: 12356.9619140625
Epoch 37100, Loss: 12356.9599609375
Epoch 37200, Loss: 12356.958984375
Epoch 37300, Loss: 12356.962890625
Epoch 37400, Loss: 12356.9580078125
Epoch 37500, Loss: 12356.958984375
Epoch 37600, Loss: 12356.9599609375
Epoch 37700, Loss: 12356.962890625
Epoch 37800, Loss: 12356.9609375
Epoch 37900, Loss: 12356.9638671875
Epoch 38000, Loss: 12356.962890625
Epoch 38100, Loss: 12356.958984375
Epoch 38200, Loss: 12356.96484375
Epoch 38300, Loss: 12356.962890625
Epoch 38400, Loss: 12356.9638671875
Epoch 38500, Loss: 12356.966796875
Epoch 38600, Loss: 12356.9638671875
Epoch 38700, Loss: 12356.958984375
Epoch 38800, Loss: 12356.9609375
Epoch 38900, Loss: 12356.9609375
Epoch 39000, Loss: 12356.958984375
Epoch 39100, Loss: 12356.966796875
Epoch 39200, Loss: 12356.962890625
Epoch 39300, Loss: 12356.9638671875
Epoch 39400, Loss: 12356.9599609375
Epoch 39500, Loss: 12356.958984375
Epoch 39600, Loss: 12356.9638671875
Epoch 39700, Loss: 12356.9638671875
Epoch 39800, Loss: 12356.96484375
Epoch 39900, Loss: 12356.966796875
Epoch 40000, Loss: 12356.96484375
Epoch 40100, Loss: 12356.9609375
Epoch 40200, Loss: 12356.9638671875
Epoch 40300, Loss: 12356.962890625
Epoch 40400, Loss: 12356.962890625
Epoch 40500, Loss: 12356.9560546875
Epoch 40600, Loss: 12356.9609375
Epoch 40700, Loss: 12356.958984375
Epoch 40800, Loss: 12356.958984375
Epoch 40900, Loss: 12356.96484375
Epoch 41000, Loss: 12356.962890625
Epoch 41100, Loss: 12356.9560546875
Epoch 41200, Loss: 12356.96875
Epoch 41300, Loss: 12356.9619140625
Epoch 41400, Loss: 12356.9609375
Epoch 41500, Loss: 12356.9599609375
Epoch 41600, Loss: 12356.96484375
Epoch 41700, Loss: 12356.9609375
Epoch 41800, Loss: 12356.962890625
Epoch 41900, Loss: 12356.958984375
Epoch 42000, Loss: 12356.962890625
Epoch 42100, Loss: 12356.962890625
Epoch 42200, Loss: 12356.9609375
Epoch 42300, Loss: 12356.958984375
Epoch 42400, Loss: 12356.9609375
Epoch 42500, Loss: 12356.9609375
Epoch 42600, Loss: 12356.962890625
Epoch 42700, Loss: 12356.9619140625
Epoch 42800, Loss: 12356.96484375
Epoch 42900, Loss: 12356.96484375
Epoch 43000, Loss: 12356.9658203125
Epoch 43100, Loss: 12356.962890625
Epoch 43200, Loss: 12356.958984375
Epoch 43300, Loss: 12356.9609375
Epoch 43400, Loss: 12356.966796875
Epoch 43500, Loss: 12356.9609375
Epoch 43600, Loss: 12356.96484375
Epoch 43700, Loss: 12356.9609375
Epoch 43800, Loss: 12356.9619140625
Epoch 43900, Loss: 12356.96484375
Epoch 44000, Loss: 12356.962890625
Epoch 44100, Loss: 12356.9609375
Epoch 44200, Loss: 12356.962890625
Epoch 44300, Loss: 12356.9599609375
Epoch 44400, Loss: 12356.962890625
Epoch 44500, Loss: 12356.9619140625
Epoch 44600, Loss: 12356.9619140625
Epoch 44700, Loss: 12356.9609375
Epoch 44800, Loss: 12356.962890625
Epoch 44900, Loss: 12356.9580078125
Epoch 45000, Loss: 12356.9609375
Epoch 45100, Loss: 12356.9580078125
Epoch 45200, Loss: 12356.9609375
Epoch 45300, Loss: 12356.9599609375
Epoch 45400, Loss: 12356.96484375
Epoch 45500, Loss: 12356.95703125
Epoch 45600, Loss: 12356.9609375
Epoch 45700, Loss: 12356.9638671875
Epoch 45800, Loss: 12356.96484375
Epoch 45900, Loss: 12356.9609375
Epoch 46000, Loss: 12356.9638671875
Epoch 46100, Loss: 12356.9638671875
Epoch 46200, Loss: 12356.9609375
Epoch 46300, Loss: 12356.962890625
Epoch 46400, Loss: 12356.9580078125
Epoch 46500, Loss: 12356.9599609375
Epoch 46600, Loss: 12356.9658203125
Epoch 46700, Loss: 12356.962890625
Epoch 46800, Loss: 12356.9580078125
Epoch 46900, Loss: 12356.96484375
Epoch 47000, Loss: 12356.9609375
Epoch 47100, Loss: 12356.958984375
Epoch 47200, Loss: 12356.9599609375
Epoch 47300, Loss: 12356.9599609375
Epoch 47400, Loss: 12356.9599609375
Epoch 47500, Loss: 12356.9619140625
Epoch 47600, Loss: 12356.9609375
Epoch 47700, Loss: 12356.9609375
Epoch 47800, Loss: 12356.962890625
Epoch 47900, Loss: 12356.9619140625
Epoch 48000, Loss: 12356.958984375
Epoch 48100, Loss: 12356.962890625
Epoch 48200, Loss: 12356.95703125
Epoch 48300, Loss: 12356.958984375
Epoch 48400, Loss: 12356.9609375
Epoch 48500, Loss: 12356.9609375
Epoch 48600, Loss: 12356.9609375
Epoch 48700, Loss: 12356.9609375
Epoch 48800, Loss: 12356.9609375
Epoch 48900, Loss: 12356.958984375
Epoch 49000, Loss: 12356.96484375
Epoch 49100, Loss: 12356.958984375
Epoch 49200, Loss: 12356.9638671875
Epoch 49300, Loss: 12356.9609375
Epoch 49400, Loss: 12356.9619140625
Epoch 49500, Loss: 12356.962890625
Epoch 49600, Loss: 12356.962890625
Epoch 49700, Loss: 12356.958984375
Epoch 49800, Loss: 12356.9619140625
Epoch 49900, Loss: 12356.9609375
Epoch 0, Loss: 63828.96875
Epoch 100, Loss: 60981.52734375
Epoch 200, Loss: 58319.796875
Epoch 300, Loss: 55834.2265625
Epoch 400, Loss: 53514.84375
Epoch 500, Loss: 51349.33203125
Epoch 600, Loss: 49326.03125
Epoch 700, Loss: 47436.07421875
Epoch 800, Loss: 45669.8359375
Epoch 900, Loss: 44018.9765625
Epoch 1000, Loss: 42475.8046875
Epoch 1100, Loss: 41034.44921875
Epoch 1200, Loss: 39690.98828125
Epoch 1300, Loss: 38437.40625
Epoch 1400, Loss: 37267.33984375
Epoch 1500, Loss: 36174.80078125
Epoch 1600, Loss: 35154.30078125
Epoch 1700, Loss: 34200.77734375
Epoch 1800, Loss: 33312.5859375
Epoch 1900, Loss: 32483.96875
Epoch 2000, Loss: 31711.810546875
Epoch 2100, Loss: 30994.130859375
Epoch 2200, Loss: 30325.810546875
Epoch 2300, Loss: 29700.9921875
Epoch 2400, Loss: 29116.078125
Epoch 2500, Loss: 28567.869140625
Epoch 2600, Loss: 28053.501953125
Epoch 2700, Loss: 27570.404296875
Epoch 2800, Loss: 27116.23828125
Epoch 2900, Loss: 26688.86328125
Epoch 3000, Loss: 26286.296875
Epoch 3100, Loss: 25906.69140625
Epoch 3200, Loss: 25551.578125
Epoch 3300, Loss: 25222.48828125
Epoch 3400, Loss: 24911.8203125
Epoch 3500, Loss: 24617.384765625
Epoch 3600, Loss: 24337.65234375
Epoch 3700, Loss: 24074.560546875
Epoch 3800, Loss: 23823.861328125
Epoch 3900, Loss: 23584.126953125
Epoch 4000, Loss: 23354.2578125
Epoch 4100, Loss: 23134.6796875
Epoch 4200, Loss: 22924.80078125
Epoch 4300, Loss: 22722.123046875
Epoch 4400, Loss: 22525.94140625
Epoch 4500, Loss: 22335.62890625
Epoch 4600, Loss: 22150.66015625
Epoch 4700, Loss: 21970.578125
Epoch 4800, Loss: 21794.9921875
Epoch 4900, Loss: 21623.560546875
Epoch 5000, Loss: 21456.0
Epoch 5100, Loss: 21292.615234375
Epoch 5200, Loss: 21133.8046875
Epoch 5300, Loss: 20978.2421875
Epoch 5400, Loss: 20825.720703125
Epoch 5500, Loss: 20676.095703125
Epoch 5600, Loss: 20529.353515625
Epoch 5700, Loss: 20385.16796875
Epoch 5800, Loss: 20243.423828125
Epoch 5900, Loss: 20104.02734375
Epoch 6000, Loss: 19966.88671875
Epoch 6100, Loss: 19831.921875
Epoch 6200, Loss: 19699.052734375
Epoch 6300, Loss: 19568.318359375
Epoch 6400, Loss: 19440.84765625
Epoch 6500, Loss: 19315.3359375
Epoch 6600, Loss: 19193.125
Epoch 6700, Loss: 19073.052734375
Epoch 6800, Loss: 18955.03125
Epoch 6900, Loss: 18838.5625
Epoch 7000, Loss: 18723.58203125
Epoch 7100, Loss: 18610.28125
Epoch 7200, Loss: 18498.845703125
Epoch 7300, Loss: 18388.8515625
Epoch 7400, Loss: 18280.26171875
Epoch 7500, Loss: 18173.48828125
Epoch 7600, Loss: 18070.7578125
Epoch 7700, Loss: 17969.25390625
Epoch 7800, Loss: 17868.923828125
Epoch 7900, Loss: 17769.7421875
Epoch 8000, Loss: 17671.69921875
Epoch 8100, Loss: 17574.77734375
Epoch 8200, Loss: 17478.974609375
Epoch 8300, Loss: 17384.265625
Epoch 8400, Loss: 17290.640625
Epoch 8500, Loss: 17198.09765625
Epoch 8600, Loss: 17106.61328125
Epoch 8700, Loss: 17016.197265625
Epoch 8800, Loss: 16926.826171875
Epoch 8900, Loss: 16838.5
Epoch 9000, Loss: 16751.212890625
Epoch 9100, Loss: 16664.955078125
Epoch 9200, Loss: 16579.7265625
Epoch 9300, Loss: 16495.513671875
Epoch 9400, Loss: 16412.326171875
Epoch 9500, Loss: 16330.146484375
Epoch 9600, Loss: 16248.9755859375
Epoch 9700, Loss: 16169.216796875
Epoch 9800, Loss: 16091.2265625
Epoch 9900, Loss: 16014.55859375
Epoch 10000, Loss: 15939.3427734375
Epoch 10100, Loss: 15865.5625
Epoch 10200, Loss: 15793.1044921875
Epoch 10300, Loss: 15721.9375
Epoch 10400, Loss: 15652.0537109375
Epoch 10500, Loss: 15583.498046875
Epoch 10600, Loss: 15516.685546875
Epoch 10700, Loss: 15451.498046875
Epoch 10800, Loss: 15387.595703125
Epoch 10900, Loss: 15325.4111328125
Epoch 11000, Loss: 15264.640625
Epoch 11100, Loss: 15205.0966796875
Epoch 11200, Loss: 15146.720703125
Epoch 11300, Loss: 15089.494140625
Epoch 11400, Loss: 15033.3740234375
Epoch 11500, Loss: 14978.330078125
Epoch 11600, Loss: 14924.330078125
Epoch 11700, Loss: 14871.39453125
Epoch 11800, Loss: 14819.5830078125
Epoch 11900, Loss: 14768.92578125
Epoch 12000, Loss: 14719.3662109375
Epoch 12100, Loss: 14670.84375
Epoch 12200, Loss: 14623.314453125
Epoch 12300, Loss: 14576.7919921875
Epoch 12400, Loss: 14531.240234375
Epoch 12500, Loss: 14486.65234375
Epoch 12600, Loss: 14443.0
Epoch 12700, Loss: 14400.265625
Epoch 12800, Loss: 14358.525390625
Epoch 12900, Loss: 14317.7509765625
Epoch 13000, Loss: 14278.2099609375
Epoch 13100, Loss: 14239.6875
Epoch 13200, Loss: 14202.1220703125
Epoch 13300, Loss: 14165.4794921875
Epoch 13400, Loss: 14129.744140625
Epoch 13500, Loss: 14094.8984375
Epoch 13600, Loss: 14060.9482421875
Epoch 13700, Loss: 14027.8798828125
Epoch 13800, Loss: 13995.625
Epoch 13900, Loss: 13964.15234375
Epoch 14000, Loss: 13933.42578125
Epoch 14100, Loss: 13903.38671875
Epoch 14200, Loss: 13874.0498046875
Epoch 14300, Loss: 13845.3828125
Epoch 14400, Loss: 13817.37890625
Epoch 14500, Loss: 13790.025390625
Epoch 14600, Loss: 13763.2998046875
Epoch 14700, Loss: 13737.541015625
Epoch 14800, Loss: 13713.2529296875
Epoch 14900, Loss: 13689.7998046875
Epoch 15000, Loss: 13667.0517578125
Epoch 15100, Loss: 13644.962890625
Epoch 15200, Loss: 13623.462890625
Epoch 15300, Loss: 13602.537109375
Epoch 15400, Loss: 13582.154296875
Epoch 15500, Loss: 13562.302734375
Epoch 15600, Loss: 13542.951171875
Epoch 15700, Loss: 13524.2607421875
Epoch 15800, Loss: 13506.3330078125
Epoch 15900, Loss: 13488.9150390625
Epoch 16000, Loss: 13471.97265625
Epoch 16100, Loss: 13455.5
Epoch 16200, Loss: 13439.4853515625
Epoch 16300, Loss: 13423.9150390625
Epoch 16400, Loss: 13408.767578125
Epoch 16500, Loss: 13394.044921875
Epoch 16600, Loss: 13379.7314453125
Epoch 16700, Loss: 13365.818359375
Epoch 16800, Loss: 13352.310546875
Epoch 16900, Loss: 13339.16015625
Epoch 17000, Loss: 13326.40234375
Epoch 17100, Loss: 13314.0146484375
Epoch 17200, Loss: 13301.9892578125
Epoch 17300, Loss: 13290.30859375
Epoch 17400, Loss: 13278.9609375
Epoch 17500, Loss: 13267.9609375
Epoch 17600, Loss: 13257.267578125
Epoch 17700, Loss: 13246.9052734375
Epoch 17800, Loss: 13236.826171875
Epoch 17900, Loss: 13227.0546875
Epoch 18000, Loss: 13217.5546875
Epoch 18100, Loss: 13208.326171875
Epoch 18200, Loss: 13199.341796875
Epoch 18300, Loss: 13190.861328125
Epoch 18400, Loss: 13183.0341796875
Epoch 18500, Loss: 13175.6162109375
Epoch 18600, Loss: 13168.498046875
Epoch 18700, Loss: 13161.638671875
Epoch 18800, Loss: 13155.0107421875
Epoch 18900, Loss: 13148.5810546875
Epoch 19000, Loss: 13142.349609375
Epoch 19100, Loss: 13136.271484375
Epoch 19200, Loss: 13130.3671875
Epoch 19300, Loss: 13124.6103515625
Epoch 19400, Loss: 13118.9921875
Epoch 19500, Loss: 13113.48828125
Epoch 19600, Loss: 13108.1142578125
Epoch 19700, Loss: 13102.8515625
Epoch 19800, Loss: 13097.689453125
Epoch 19900, Loss: 13092.62109375
Epoch 20000, Loss: 13087.654296875
Epoch 20100, Loss: 13082.78125
Epoch 20200, Loss: 13077.9921875
Epoch 20300, Loss: 13073.3076171875
Epoch 20400, Loss: 13068.697265625
Epoch 20500, Loss: 13064.17578125
Epoch 20600, Loss: 13059.748046875
Epoch 20700, Loss: 13055.404296875
Epoch 20800, Loss: 13051.15234375
Epoch 20900, Loss: 13046.9970703125
Epoch 21000, Loss: 13042.935546875
Epoch 21100, Loss: 13038.955078125
Epoch 21200, Loss: 13035.076171875
Epoch 21300, Loss: 13031.3017578125
Epoch 21400, Loss: 13027.6064453125
Epoch 21500, Loss: 13024.01171875
Epoch 21600, Loss: 13020.5146484375
Epoch 21700, Loss: 13017.1103515625
Epoch 21800, Loss: 13013.7998046875
Epoch 21900, Loss: 13010.5888671875
Epoch 22000, Loss: 13007.4697265625
Epoch 22100, Loss: 13004.4365234375
Epoch 22200, Loss: 13001.4990234375
Epoch 22300, Loss: 12998.6513671875
Epoch 22400, Loss: 12995.896484375
Epoch 22500, Loss: 12993.2333984375
Epoch 22600, Loss: 12990.6484375
Epoch 22700, Loss: 12988.1552734375
Epoch 22800, Loss: 12985.73828125
Epoch 22900, Loss: 12983.3876953125
Epoch 23000, Loss: 12981.1240234375
Epoch 23100, Loss: 12978.919921875
Epoch 23200, Loss: 12976.79296875
Epoch 23300, Loss: 12974.7216796875
Epoch 23400, Loss: 12972.734375
Epoch 23500, Loss: 12970.7890625
Epoch 23600, Loss: 12968.92578125
Epoch 23700, Loss: 12967.1083984375
Epoch 23800, Loss: 12965.35546875
Epoch 23900, Loss: 12963.666015625
Epoch 24000, Loss: 12962.0390625
Epoch 24100, Loss: 12960.46484375
Epoch 24200, Loss: 12958.966796875
Epoch 24300, Loss: 12957.5048828125
Epoch 24400, Loss: 12956.1044921875
Epoch 24500, Loss: 12954.7724609375
Epoch 24600, Loss: 12953.4765625
Epoch 24700, Loss: 12952.240234375
Epoch 24800, Loss: 12951.0654296875
Epoch 24900, Loss: 12949.93359375
Epoch 25000, Loss: 12948.8525390625
Epoch 25100, Loss: 12947.830078125
Epoch 25200, Loss: 12946.837890625
Epoch 25300, Loss: 12945.904296875
Epoch 25400, Loss: 12945.0234375
Epoch 25500, Loss: 12944.1875
Epoch 25600, Loss: 12943.3955078125
Epoch 25700, Loss: 12942.650390625
Epoch 25800, Loss: 12941.951171875
Epoch 25900, Loss: 12941.2890625
Epoch 26000, Loss: 12940.6787109375
Epoch 26100, Loss: 12940.091796875
Epoch 26200, Loss: 12939.54296875
Epoch 26300, Loss: 12939.03515625
Epoch 26400, Loss: 12938.5625
Epoch 26500, Loss: 12938.1201171875
Epoch 26600, Loss: 12937.69921875
Epoch 26700, Loss: 12937.318359375
Epoch 26800, Loss: 12936.9619140625
Epoch 26900, Loss: 12936.6259765625
Epoch 27000, Loss: 12936.30859375
Epoch 27100, Loss: 12936.0126953125
Epoch 27200, Loss: 12935.7451171875
Epoch 27300, Loss: 12935.494140625
Epoch 27400, Loss: 12935.255859375
Epoch 27500, Loss: 12935.046875
Epoch 27600, Loss: 12934.84375
Epoch 27700, Loss: 12934.654296875
Epoch 27800, Loss: 12934.4873046875
Epoch 27900, Loss: 12934.3212890625
Epoch 28000, Loss: 12934.1728515625
Epoch 28100, Loss: 12934.03515625
Epoch 28200, Loss: 12933.9130859375
Epoch 28300, Loss: 12933.79296875
Epoch 28400, Loss: 12933.6787109375
Epoch 28500, Loss: 12933.58203125
Epoch 28600, Loss: 12933.48046875
Epoch 28700, Loss: 12933.3994140625
Epoch 28800, Loss: 12933.3173828125
Epoch 28900, Loss: 12933.240234375
Epoch 29000, Loss: 12933.1689453125
Epoch 29100, Loss: 12933.09765625
Epoch 29200, Loss: 12933.033203125
Epoch 29300, Loss: 12932.96875
Epoch 29400, Loss: 12932.904296875
Epoch 29500, Loss: 12932.8505859375
Epoch 29600, Loss: 12932.7890625
Epoch 29700, Loss: 12932.7392578125
Epoch 29800, Loss: 12932.6767578125
Epoch 29900, Loss: 12932.626953125
Epoch 30000, Loss: 12932.56640625
Epoch 30100, Loss: 12932.525390625
Epoch 30200, Loss: 12932.470703125
Epoch 30300, Loss: 12932.416015625
Epoch 30400, Loss: 12932.375
Epoch 30500, Loss: 12932.3212890625
Epoch 30600, Loss: 12932.2734375
Epoch 30700, Loss: 12932.23046875
Epoch 30800, Loss: 12932.177734375
Epoch 30900, Loss: 12932.13671875
Epoch 31000, Loss: 12932.09375
Epoch 31100, Loss: 12932.04296875
Epoch 31200, Loss: 12932.00390625
Epoch 31300, Loss: 12931.9541015625
Epoch 31400, Loss: 12931.916015625
Epoch 31500, Loss: 12931.8720703125
Epoch 31600, Loss: 12931.833984375
Epoch 31700, Loss: 12931.8017578125
Epoch 31800, Loss: 12931.755859375
Epoch 31900, Loss: 12931.716796875
Epoch 32000, Loss: 12931.6884765625
Epoch 32100, Loss: 12931.6484375
Epoch 32200, Loss: 12931.60546875
Epoch 32300, Loss: 12931.5830078125
Epoch 32400, Loss: 12931.5390625
Epoch 32500, Loss: 12931.5078125
Epoch 32600, Loss: 12931.470703125
Epoch 32700, Loss: 12931.4375
Epoch 32800, Loss: 12931.41015625
Epoch 32900, Loss: 12931.37890625
Epoch 33000, Loss: 12931.34765625
Epoch 33100, Loss: 12931.31640625
Epoch 33200, Loss: 12931.287109375
Epoch 33300, Loss: 12931.2568359375
Epoch 33400, Loss: 12931.234375
Epoch 33500, Loss: 12931.2119140625
Epoch 33600, Loss: 12931.177734375
Epoch 33700, Loss: 12931.1572265625
Epoch 33800, Loss: 12931.1259765625
Epoch 33900, Loss: 12931.10546875
Epoch 34000, Loss: 12931.08984375
Epoch 34100, Loss: 12931.056640625
Epoch 34200, Loss: 12931.033203125
Epoch 34300, Loss: 12931.01171875
Epoch 34400, Loss: 12930.998046875
Epoch 34500, Loss: 12930.9716796875
Epoch 34600, Loss: 12930.9580078125
Epoch 34700, Loss: 12930.9345703125
Epoch 34800, Loss: 12930.91796875
Epoch 34900, Loss: 12930.896484375
Epoch 35000, Loss: 12930.8857421875
Epoch 35100, Loss: 12930.865234375
Epoch 35200, Loss: 12930.84765625
Epoch 35300, Loss: 12930.833984375
Epoch 35400, Loss: 12930.826171875
Epoch 35500, Loss: 12930.80859375
Epoch 35600, Loss: 12930.7900390625
Epoch 35700, Loss: 12930.78125
Epoch 35800, Loss: 12930.76953125
Epoch 35900, Loss: 12930.75390625
Epoch 36000, Loss: 12930.7470703125
Epoch 36100, Loss: 12930.736328125
Epoch 36200, Loss: 12930.7275390625
Epoch 36300, Loss: 12930.716796875
Epoch 36400, Loss: 12930.70703125
Epoch 36500, Loss: 12930.6982421875
Epoch 36600, Loss: 12930.69140625
Epoch 36700, Loss: 12930.681640625
Epoch 36800, Loss: 12930.67578125
Epoch 36900, Loss: 12930.669921875
Epoch 37000, Loss: 12930.6630859375
Epoch 37100, Loss: 12930.6591796875
Epoch 37200, Loss: 12930.6533203125
Epoch 37300, Loss: 12930.64453125
Epoch 37400, Loss: 12930.640625
Epoch 37500, Loss: 12930.638671875
Epoch 37600, Loss: 12930.6328125
Epoch 37700, Loss: 12930.6298828125
Epoch 37800, Loss: 12930.62109375
Epoch 37900, Loss: 12930.6279296875
Epoch 38000, Loss: 12930.62890625
Epoch 38100, Loss: 12930.619140625
Epoch 38200, Loss: 12930.6181640625
Epoch 38300, Loss: 12930.615234375
Epoch 38400, Loss: 12930.6123046875
Epoch 38500, Loss: 12930.61328125
Epoch 38600, Loss: 12930.6123046875
Epoch 38700, Loss: 12930.611328125
Epoch 38800, Loss: 12930.6044921875
Epoch 38900, Loss: 12930.603515625
Epoch 39000, Loss: 12930.60546875
Epoch 39100, Loss: 12930.6025390625
Epoch 39200, Loss: 12930.60546875
Epoch 39300, Loss: 12930.609375
Epoch 39400, Loss: 12930.611328125
Epoch 39500, Loss: 12930.6025390625
Epoch 39600, Loss: 12930.603515625
Epoch 39700, Loss: 12930.6083984375
Epoch 39800, Loss: 12930.6044921875
Epoch 39900, Loss: 12930.60546875
Epoch 40000, Loss: 12930.6064453125
Epoch 40100, Loss: 12930.6015625
Epoch 40200, Loss: 12930.6064453125
Epoch 40300, Loss: 12930.60546875
Epoch 40400, Loss: 12930.6015625
Epoch 40500, Loss: 12930.6044921875
Epoch 40600, Loss: 12930.6025390625
Epoch 40700, Loss: 12930.603515625
Epoch 40800, Loss: 12930.6025390625
Epoch 40900, Loss: 12930.5986328125
Epoch 41000, Loss: 12930.603515625
Epoch 41100, Loss: 12930.6025390625
Epoch 41200, Loss: 12930.599609375
Epoch 41300, Loss: 12930.6025390625
Epoch 41400, Loss: 12930.609375
Epoch 41500, Loss: 12930.6015625
Epoch 41600, Loss: 12930.603515625
Epoch 41700, Loss: 12930.6044921875
Epoch 41800, Loss: 12930.599609375
Epoch 41900, Loss: 12930.599609375
Epoch 42000, Loss: 12930.6044921875
Epoch 42100, Loss: 12930.6064453125
Epoch 42200, Loss: 12930.5986328125
Epoch 42300, Loss: 12930.6015625
Epoch 42400, Loss: 12930.60546875
Epoch 42500, Loss: 12930.599609375
Epoch 42600, Loss: 12930.60546875
Epoch 42700, Loss: 12930.5966796875
Epoch 42800, Loss: 12930.59765625
Epoch 42900, Loss: 12930.599609375
Epoch 43000, Loss: 12930.6044921875
Epoch 43100, Loss: 12930.609375
Epoch 43200, Loss: 12930.59765625
Epoch 43300, Loss: 12930.603515625
Epoch 43400, Loss: 12930.59765625
Epoch 43500, Loss: 12930.6064453125
Epoch 43600, Loss: 12930.6044921875
Epoch 43700, Loss: 12930.6044921875
Epoch 43800, Loss: 12930.5986328125
Epoch 43900, Loss: 12930.6015625
Epoch 44000, Loss: 12930.6015625
Epoch 44100, Loss: 12930.6015625
Epoch 44200, Loss: 12930.6015625
Epoch 44300, Loss: 12930.60546875
Epoch 44400, Loss: 12930.6015625
Epoch 44500, Loss: 12930.6005859375
Epoch 44600, Loss: 12930.60546875
Epoch 44700, Loss: 12930.603515625
Epoch 44800, Loss: 12930.6025390625
Epoch 44900, Loss: 12930.6015625
Epoch 45000, Loss: 12930.6005859375
Epoch 45100, Loss: 12930.603515625
Epoch 45200, Loss: 12930.599609375
Epoch 45300, Loss: 12930.6064453125
Epoch 45400, Loss: 12930.6015625
Epoch 45500, Loss: 12930.60546875
Epoch 45600, Loss: 12930.6025390625
Epoch 45700, Loss: 12930.6015625
Epoch 45800, Loss: 12930.595703125
Epoch 45900, Loss: 12930.60546875
Epoch 46000, Loss: 12930.6015625
Epoch 46100, Loss: 12930.6025390625
Epoch 46200, Loss: 12930.603515625
Epoch 46300, Loss: 12930.6015625
Epoch 46400, Loss: 12930.59765625
Epoch 46500, Loss: 12930.60546875
Epoch 46600, Loss: 12930.6015625
Epoch 46700, Loss: 12930.6015625
Epoch 46800, Loss: 12930.599609375
Epoch 46900, Loss: 12930.6044921875
Epoch 47000, Loss: 12930.603515625
Epoch 47100, Loss: 12930.59765625
Epoch 47200, Loss: 12930.609375
Epoch 47300, Loss: 12930.599609375
Epoch 47400, Loss: 12930.5966796875
Epoch 47500, Loss: 12930.6025390625
Epoch 47600, Loss: 12930.603515625
Epoch 47700, Loss: 12930.6015625
Epoch 47800, Loss: 12930.599609375
Epoch 47900, Loss: 12930.60546875
Epoch 48000, Loss: 12930.5986328125
Epoch 48100, Loss: 12930.603515625
Epoch 48200, Loss: 12930.6025390625
Epoch 48300, Loss: 12930.6005859375
Epoch 48400, Loss: 12930.6064453125
Epoch 48500, Loss: 12930.6103515625
Epoch 48600, Loss: 12930.603515625
Epoch 48700, Loss: 12930.6015625
Epoch 48800, Loss: 12930.6005859375
Epoch 48900, Loss: 12930.6015625
Epoch 49000, Loss: 12930.6015625
Epoch 49100, Loss: 12930.60546875
Epoch 49200, Loss: 12930.60546875
Epoch 49300, Loss: 12930.60546875
Epoch 49400, Loss: 12930.603515625
Epoch 49500, Loss: 12930.5986328125
Epoch 49600, Loss: 12930.6015625
Epoch 49700, Loss: 12930.599609375
Epoch 49800, Loss: 12930.5986328125
Epoch 49900, Loss: 12930.6064453125
Epoch 0, Loss: 63828.96875
Epoch 100, Loss: 60996.9375
Epoch 200, Loss: 58349.37890625
Epoch 300, Loss: 55878.4921875
Epoch 400, Loss: 53573.59765625
Epoch 500, Loss: 51422.99609375
Epoch 600, Loss: 49418.08203125
Epoch 700, Loss: 47546.9453125
Epoch 800, Loss: 45799.41015625
Epoch 900, Loss: 44166.7109375
Epoch 1000, Loss: 42641.23046875
Epoch 1100, Loss: 41218.9296875
Epoch 1200, Loss: 39891.79296875
Epoch 1300, Loss: 38653.25390625
Epoch 1400, Loss: 37496.91796875
Epoch 1500, Loss: 36416.86328125
Epoch 1600, Loss: 35407.66015625
Epoch 1700, Loss: 34467.671875
Epoch 1800, Loss: 33590.98046875
Epoch 1900, Loss: 32774.44921875
Epoch 2000, Loss: 32015.833984375
Epoch 2100, Loss: 31311.072265625
Epoch 2200, Loss: 30653.103515625
Epoch 2300, Loss: 30037.916015625
Epoch 2400, Loss: 29461.9296875
Epoch 2500, Loss: 28921.97265625
Epoch 2600, Loss: 28415.224609375
Epoch 2700, Loss: 27939.15234375
Epoch 2800, Loss: 27491.482421875
Epoch 2900, Loss: 27070.12109375
Epoch 3000, Loss: 26673.384765625
Epoch 3100, Loss: 26301.947265625
Epoch 3200, Loss: 25956.23828125
Epoch 3300, Loss: 25634.263671875
Epoch 3400, Loss: 25329.873046875
Epoch 3500, Loss: 25041.3984375
Epoch 3600, Loss: 24771.2265625
Epoch 3700, Loss: 24514.32421875
Epoch 3800, Loss: 24269.275390625
Epoch 3900, Loss: 24034.865234375
Epoch 4000, Loss: 23811.521484375
Epoch 4100, Loss: 23598.94921875
Epoch 4200, Loss: 23394.12109375
Epoch 4300, Loss: 23196.232421875
Epoch 4400, Loss: 23004.58203125
Epoch 4500, Loss: 22818.5703125
Epoch 4600, Loss: 22637.6875
Epoch 4700, Loss: 22461.486328125
Epoch 4800, Loss: 22289.5859375
Epoch 4900, Loss: 22122.046875
Epoch 5000, Loss: 21959.755859375
Epoch 5100, Loss: 21800.990234375
Epoch 5200, Loss: 21645.87109375
Epoch 5300, Loss: 21493.779296875
Epoch 5400, Loss: 21344.4765625
Epoch 5500, Loss: 21197.82421875
Epoch 5600, Loss: 21053.677734375
Epoch 5700, Loss: 20911.931640625
Epoch 5800, Loss: 20772.4765625
Epoch 5900, Loss: 20635.224609375
Epoch 6000, Loss: 20500.0859375
Epoch 6100, Loss: 20366.984375
Epoch 6200, Loss: 20237.326171875
Epoch 6300, Loss: 20109.7265625
Epoch 6400, Loss: 19984.490234375
Epoch 6500, Loss: 19862.9375
Epoch 6600, Loss: 19743.271484375
Epoch 6700, Loss: 19625.90234375
Epoch 6800, Loss: 19510.0703125
Epoch 6900, Loss: 19395.705078125
Epoch 7000, Loss: 19282.7734375
Epoch 7100, Loss: 19171.21875
Epoch 7200, Loss: 19061.017578125
Epoch 7300, Loss: 18952.109375
Epoch 7400, Loss: 18844.4921875
Epoch 7500, Loss: 18738.40234375
Epoch 7600, Loss: 18637.13671875
Epoch 7700, Loss: 18537.041015625
Epoch 7800, Loss: 18438.015625
Epoch 7900, Loss: 18340.0390625
Epoch 8000, Loss: 18243.09765625
Epoch 8100, Loss: 18147.1796875
Epoch 8200, Loss: 18052.265625
Epoch 8300, Loss: 17958.330078125
Epoch 8400, Loss: 17865.369140625
Epoch 8500, Loss: 17773.375
Epoch 8600, Loss: 17682.3359375
Epoch 8700, Loss: 17592.240234375
Epoch 8800, Loss: 17503.08984375
Epoch 8900, Loss: 17414.875
Epoch 9000, Loss: 17327.59375
Epoch 9100, Loss: 17241.240234375
Epoch 9200, Loss: 17155.859375
Epoch 9300, Loss: 17071.43359375
Epoch 9400, Loss: 16989.2265625
Epoch 9500, Loss: 16908.373046875
Epoch 9600, Loss: 16828.81640625
Epoch 9700, Loss: 16750.5546875
Epoch 9800, Loss: 16673.68359375
Epoch 9900, Loss: 16598.2734375
Epoch 10000, Loss: 16524.201171875
Epoch 10100, Loss: 16452.529296875
Epoch 10200, Loss: 16382.771484375
Epoch 10300, Loss: 16314.39453125
Epoch 10400, Loss: 16247.3046875
Epoch 10500, Loss: 16181.4560546875
Epoch 10600, Loss: 16117.42578125
Epoch 10700, Loss: 16054.6640625
Epoch 10800, Loss: 15993.1513671875
Epoch 10900, Loss: 15932.798828125
Epoch 11000, Loss: 15873.55859375
Epoch 11100, Loss: 15815.41015625
Epoch 11200, Loss: 15758.4521484375
Epoch 11300, Loss: 15702.6748046875
Epoch 11400, Loss: 15648.10546875
Epoch 11500, Loss: 15594.685546875
Epoch 11600, Loss: 15542.2958984375
Epoch 11700, Loss: 15490.98046875
Epoch 11800, Loss: 15440.6640625
Epoch 11900, Loss: 15391.35546875
Epoch 12000, Loss: 15343.04296875
Epoch 12100, Loss: 15295.6923828125
Epoch 12200, Loss: 15249.326171875
Epoch 12300, Loss: 15203.9921875
Epoch 12400, Loss: 15159.66796875
Epoch 12500, Loss: 15116.7294921875
Epoch 12600, Loss: 15074.884765625
Epoch 12700, Loss: 15034.0654296875
Epoch 12800, Loss: 14994.2724609375
Epoch 12900, Loss: 14955.482421875
Epoch 13000, Loss: 14917.7412109375
Epoch 13100, Loss: 14880.9853515625
Epoch 13200, Loss: 14845.1474609375
Epoch 13300, Loss: 14810.16796875
Epoch 13400, Loss: 14776.029296875
Epoch 13500, Loss: 14742.69140625
Epoch 13600, Loss: 14710.12109375
Epoch 13700, Loss: 14678.2890625
Epoch 13800, Loss: 14647.177734375
Epoch 13900, Loss: 14616.767578125
Epoch 14000, Loss: 14587.05078125
Epoch 14100, Loss: 14557.9833984375
Epoch 14200, Loss: 14529.568359375
Epoch 14300, Loss: 14501.7783203125
Epoch 14400, Loss: 14475.8837890625
Epoch 14500, Loss: 14450.96484375
Epoch 14600, Loss: 14426.822265625
Epoch 14700, Loss: 14403.3525390625
Epoch 14800, Loss: 14380.521484375
Epoch 14900, Loss: 14358.30078125
Epoch 15000, Loss: 14336.611328125
Epoch 15100, Loss: 14315.4736328125
Epoch 15200, Loss: 14295.265625
Epoch 15300, Loss: 14275.705078125
Epoch 15400, Loss: 14256.6591796875
Epoch 15500, Loss: 14238.11328125
Epoch 15600, Loss: 14220.064453125
Epoch 15700, Loss: 14202.458984375
Epoch 15800, Loss: 14185.318359375
Epoch 15900, Loss: 14168.626953125
Epoch 16000, Loss: 14152.384765625
Epoch 16100, Loss: 14136.564453125
Epoch 16200, Loss: 14121.1728515625
Epoch 16300, Loss: 14106.1806640625
Epoch 16400, Loss: 14091.6015625
Epoch 16500, Loss: 14077.4140625
Epoch 16600, Loss: 14063.611328125
Epoch 16700, Loss: 14050.1845703125
Epoch 16800, Loss: 14037.14453125
Epoch 16900, Loss: 14024.453125
Epoch 17000, Loss: 14012.1103515625
Epoch 17100, Loss: 14000.087890625
Epoch 17200, Loss: 13988.3828125
Epoch 17300, Loss: 13977.0078125
Epoch 17400, Loss: 13965.9345703125
Epoch 17500, Loss: 13955.150390625
Epoch 17600, Loss: 13944.642578125
Epoch 17700, Loss: 13934.404296875
Epoch 17800, Loss: 13924.4482421875
Epoch 17900, Loss: 13915.234375
Epoch 18000, Loss: 13906.609375
Epoch 18100, Loss: 13898.4033203125
Epoch 18200, Loss: 13890.4892578125
Epoch 18300, Loss: 13882.8662109375
Epoch 18400, Loss: 13875.48828125
Epoch 18500, Loss: 13868.33203125
Epoch 18600, Loss: 13861.3642578125
Epoch 18700, Loss: 13854.583984375
Epoch 18800, Loss: 13847.9755859375
Epoch 18900, Loss: 13841.53125
Epoch 19000, Loss: 13835.2275390625
Epoch 19100, Loss: 13829.05859375
Epoch 19200, Loss: 13823.017578125
Epoch 19300, Loss: 13817.125
Epoch 19400, Loss: 13811.345703125
Epoch 19500, Loss: 13805.671875
Epoch 19600, Loss: 13800.1162109375
Epoch 19700, Loss: 13794.66015625
Epoch 19800, Loss: 13789.3203125
Epoch 19900, Loss: 13784.0712890625
Epoch 20000, Loss: 13778.916015625
Epoch 20100, Loss: 13773.876953125
Epoch 20200, Loss: 13768.9140625
Epoch 20300, Loss: 13764.052734375
Epoch 20400, Loss: 13759.302734375
Epoch 20500, Loss: 13754.6376953125
Epoch 20600, Loss: 13750.0791015625
Epoch 20700, Loss: 13745.623046875
Epoch 20800, Loss: 13741.251953125
Epoch 20900, Loss: 13737.0107421875
Epoch 21000, Loss: 13732.853515625
Epoch 21100, Loss: 13728.7978515625
Epoch 21200, Loss: 13724.8515625
Epoch 21300, Loss: 13721.0126953125
Epoch 21400, Loss: 13717.265625
Epoch 21500, Loss: 13713.625
Epoch 21600, Loss: 13710.0849609375
Epoch 21700, Loss: 13706.6640625
Epoch 21800, Loss: 13703.3291015625
Epoch 21900, Loss: 13700.09375
Epoch 22000, Loss: 13696.970703125
Epoch 22100, Loss: 13693.94140625
Epoch 22200, Loss: 13691.0087890625
Epoch 22300, Loss: 13688.16796875
Epoch 22400, Loss: 13685.4150390625
Epoch 22500, Loss: 13682.7548828125
Epoch 22600, Loss: 13680.1787109375
Epoch 22700, Loss: 13677.693359375
Epoch 22800, Loss: 13675.2705078125
Epoch 22900, Loss: 13672.9375
Epoch 23000, Loss: 13670.67578125
Epoch 23100, Loss: 13668.4892578125
Epoch 23200, Loss: 13666.373046875
Epoch 23300, Loss: 13664.32421875
Epoch 23400, Loss: 13662.361328125
Epoch 23500, Loss: 13660.4599609375
Epoch 23600, Loss: 13658.6279296875
Epoch 23700, Loss: 13656.84375
Epoch 23800, Loss: 13655.146484375
Epoch 23900, Loss: 13653.5
Epoch 24000, Loss: 13651.9248046875
Epoch 24100, Loss: 13650.404296875
Epoch 24200, Loss: 13648.953125
Epoch 24300, Loss: 13647.552734375
Epoch 24400, Loss: 13646.2099609375
Epoch 24500, Loss: 13644.9404296875
Epoch 24600, Loss: 13643.71484375
Epoch 24700, Loss: 13642.5390625
Epoch 24800, Loss: 13641.4296875
Epoch 24900, Loss: 13640.3544921875
Epoch 25000, Loss: 13639.3427734375
Epoch 25100, Loss: 13638.37109375
Epoch 25200, Loss: 13637.44921875
Epoch 25300, Loss: 13636.583984375
Epoch 25400, Loss: 13635.759765625
Epoch 25500, Loss: 13634.974609375
Epoch 25600, Loss: 13634.2353515625
Epoch 25700, Loss: 13633.525390625
Epoch 25800, Loss: 13632.876953125
Epoch 25900, Loss: 13632.244140625
Epoch 26000, Loss: 13631.654296875
Epoch 26100, Loss: 13631.107421875
Epoch 26200, Loss: 13630.58984375
Epoch 26300, Loss: 13630.0986328125
Epoch 26400, Loss: 13629.638671875
Epoch 26500, Loss: 13629.224609375
Epoch 26600, Loss: 13628.8330078125
Epoch 26700, Loss: 13628.4521484375
Epoch 26800, Loss: 13628.111328125
Epoch 26900, Loss: 13627.7939453125
Epoch 27000, Loss: 13627.501953125
Epoch 27100, Loss: 13627.2275390625
Epoch 27200, Loss: 13626.9794921875
Epoch 27300, Loss: 13626.736328125
Epoch 27400, Loss: 13626.52734375
Epoch 27500, Loss: 13626.32421875
Epoch 27600, Loss: 13626.1376953125
Epoch 27700, Loss: 13625.95703125
Epoch 27800, Loss: 13625.7939453125
Epoch 27900, Loss: 13625.640625
Epoch 28000, Loss: 13625.4892578125
Epoch 28100, Loss: 13625.3564453125
Epoch 28200, Loss: 13625.23828125
Epoch 28300, Loss: 13625.1171875
Epoch 28400, Loss: 13625.005859375
Epoch 28500, Loss: 13624.884765625
Epoch 28600, Loss: 13624.771484375
Epoch 28700, Loss: 13624.67578125
Epoch 28800, Loss: 13624.564453125
Epoch 28900, Loss: 13624.46875
Epoch 29000, Loss: 13624.365234375
Epoch 29100, Loss: 13624.2783203125
Epoch 29200, Loss: 13624.1796875
Epoch 29300, Loss: 13624.087890625
Epoch 29400, Loss: 13624.005859375
Epoch 29500, Loss: 13623.9072265625
Epoch 29600, Loss: 13623.8271484375
Epoch 29700, Loss: 13623.73046875
Epoch 29800, Loss: 13623.65234375
Epoch 29900, Loss: 13623.56640625
Epoch 30000, Loss: 13623.484375
Epoch 30100, Loss: 13623.3955078125
Epoch 30200, Loss: 13623.330078125
Epoch 30300, Loss: 13623.2451171875
Epoch 30400, Loss: 13623.1533203125
Epoch 30500, Loss: 13623.072265625
Epoch 30600, Loss: 13622.990234375
Epoch 30700, Loss: 13622.9150390625
Epoch 30800, Loss: 13622.8359375
Epoch 30900, Loss: 13622.76953125
Epoch 31000, Loss: 13622.6806640625
Epoch 31100, Loss: 13622.60546875
Epoch 31200, Loss: 13622.537109375
Epoch 31300, Loss: 13622.45703125
Epoch 31400, Loss: 13622.390625
Epoch 31500, Loss: 13622.3125
Epoch 31600, Loss: 13622.2421875
Epoch 31700, Loss: 13622.1748046875
Epoch 31800, Loss: 13622.1015625
Epoch 31900, Loss: 13622.0380859375
Epoch 32000, Loss: 13621.95703125
Epoch 32100, Loss: 13621.9013671875
Epoch 32200, Loss: 13621.830078125
Epoch 32300, Loss: 13621.7607421875
Epoch 32400, Loss: 13621.6943359375
Epoch 32500, Loss: 13621.630859375
Epoch 32600, Loss: 13621.5703125
Epoch 32700, Loss: 13621.509765625
Epoch 32800, Loss: 13621.453125
Epoch 32900, Loss: 13621.3876953125
Epoch 33000, Loss: 13621.3232421875
Epoch 33100, Loss: 13621.2685546875
Epoch 33200, Loss: 13621.2041015625
Epoch 33300, Loss: 13621.1552734375
Epoch 33400, Loss: 13621.0986328125
Epoch 33500, Loss: 13621.037109375
Epoch 33600, Loss: 13620.98046875
Epoch 33700, Loss: 13620.921875
Epoch 33800, Loss: 13620.8720703125
Epoch 33900, Loss: 13620.818359375
Epoch 34000, Loss: 13620.77734375
Epoch 34100, Loss: 13620.724609375
Epoch 34200, Loss: 13620.6630859375
Epoch 34300, Loss: 13620.61328125
Epoch 34400, Loss: 13620.564453125
Epoch 34500, Loss: 13620.51953125
Epoch 34600, Loss: 13620.4677734375
Epoch 34700, Loss: 13620.4228515625
Epoch 34800, Loss: 13620.3740234375
Epoch 34900, Loss: 13620.330078125
Epoch 35000, Loss: 13620.29296875
Epoch 35100, Loss: 13620.240234375
Epoch 35200, Loss: 13620.20703125
Epoch 35300, Loss: 13620.1611328125
Epoch 35400, Loss: 13620.119140625
Epoch 35500, Loss: 13620.0810546875
Epoch 35600, Loss: 13620.041015625
Epoch 35700, Loss: 13620.0009765625
Epoch 35800, Loss: 13619.9658203125
Epoch 35900, Loss: 13619.923828125
Epoch 36000, Loss: 13619.884765625
Epoch 36100, Loss: 13619.85546875
Epoch 36200, Loss: 13619.8173828125
Epoch 36300, Loss: 13619.77734375
Epoch 36400, Loss: 13619.751953125
Epoch 36500, Loss: 13619.7119140625
Epoch 36600, Loss: 13619.68359375
Epoch 36700, Loss: 13619.6396484375
Epoch 36800, Loss: 13619.6171875
Epoch 36900, Loss: 13619.59375
Epoch 37000, Loss: 13619.5595703125
Epoch 37100, Loss: 13619.537109375
Epoch 37200, Loss: 13619.498046875
Epoch 37300, Loss: 13619.466796875
Epoch 37400, Loss: 13619.4501953125
Epoch 37500, Loss: 13619.416015625
Epoch 37600, Loss: 13619.38671875
Epoch 37700, Loss: 13619.3701171875
Epoch 37800, Loss: 13619.3515625
Epoch 37900, Loss: 13619.337890625
Epoch 38000, Loss: 13619.349609375
Epoch 38100, Loss: 13619.34375
Epoch 38200, Loss: 13619.341796875
Epoch 38300, Loss: 13619.337890625
Epoch 38400, Loss: 13619.341796875
Epoch 38500, Loss: 13619.341796875
Epoch 38600, Loss: 13619.3349609375
Epoch 38700, Loss: 13619.3466796875
Epoch 38800, Loss: 13619.33984375
Epoch 38900, Loss: 13619.341796875
Epoch 39000, Loss: 13619.337890625
Epoch 39100, Loss: 13619.3388671875
Epoch 39200, Loss: 13619.337890625
Epoch 39300, Loss: 13619.345703125
Epoch 39400, Loss: 13619.33984375
Epoch 39500, Loss: 13619.337890625
Epoch 39600, Loss: 13619.345703125
Epoch 39700, Loss: 13619.337890625
Epoch 39800, Loss: 13619.3388671875
Epoch 39900, Loss: 13619.3408203125
Epoch 40000, Loss: 13619.333984375
Epoch 40100, Loss: 13619.3359375
Epoch 40200, Loss: 13619.341796875
Epoch 40300, Loss: 13619.3447265625
Epoch 40400, Loss: 13619.341796875
Epoch 40500, Loss: 13619.333984375
Epoch 40600, Loss: 13619.337890625
Epoch 40700, Loss: 13619.341796875
Epoch 40800, Loss: 13619.33984375
Epoch 40900, Loss: 13619.345703125
Epoch 41000, Loss: 13619.337890625
Epoch 41100, Loss: 13619.341796875
Epoch 41200, Loss: 13619.345703125
Epoch 41300, Loss: 13619.341796875
Epoch 41400, Loss: 13619.34375
Epoch 41500, Loss: 13619.3388671875
Epoch 41600, Loss: 13619.3359375
Epoch 41700, Loss: 13619.34765625
Epoch 41800, Loss: 13619.333984375
Epoch 41900, Loss: 13619.3408203125
Epoch 42000, Loss: 13619.3388671875
Epoch 42100, Loss: 13619.3408203125
Epoch 42200, Loss: 13619.3388671875
Epoch 42300, Loss: 13619.3330078125
Epoch 42400, Loss: 13619.345703125
Epoch 42500, Loss: 13619.337890625
Epoch 42600, Loss: 13619.33203125
Epoch 42700, Loss: 13619.3310546875
Epoch 42800, Loss: 13619.34375
Epoch 42900, Loss: 13619.3359375
Epoch 43000, Loss: 13619.3427734375
Epoch 43100, Loss: 13619.3388671875
Epoch 43200, Loss: 13619.3349609375
Epoch 43300, Loss: 13619.3388671875
Epoch 43400, Loss: 13619.3408203125
Epoch 43500, Loss: 13619.34375
Epoch 43600, Loss: 13619.3369140625
Epoch 43700, Loss: 13619.3349609375
Epoch 43800, Loss: 13619.337890625
Epoch 43900, Loss: 13619.3427734375
Epoch 44000, Loss: 13619.34375
Epoch 44100, Loss: 13619.33203125
Epoch 44200, Loss: 13619.33984375
Epoch 44300, Loss: 13619.3427734375
Epoch 44400, Loss: 13619.3359375
Epoch 44500, Loss: 13619.341796875
Epoch 44600, Loss: 13619.3388671875
Epoch 44700, Loss: 13619.34375
Epoch 44800, Loss: 13619.337890625
Epoch 44900, Loss: 13619.33984375
Epoch 45000, Loss: 13619.3359375
Epoch 45100, Loss: 13619.345703125
Epoch 45200, Loss: 13619.345703125
Epoch 45300, Loss: 13619.341796875
Epoch 45400, Loss: 13619.34375
Epoch 45500, Loss: 13619.3408203125
Epoch 45600, Loss: 13619.3388671875
Epoch 45700, Loss: 13619.333984375
Epoch 45800, Loss: 13619.33984375
Epoch 45900, Loss: 13619.33984375
Epoch 46000, Loss: 13619.341796875
Epoch 46100, Loss: 13619.3447265625
Epoch 46200, Loss: 13619.33203125
Epoch 46300, Loss: 13619.3330078125
Epoch 46400, Loss: 13619.3408203125
Epoch 46500, Loss: 13619.3505859375
Epoch 46600, Loss: 13619.3427734375
Epoch 46700, Loss: 13619.337890625
Epoch 46800, Loss: 13619.345703125
Epoch 46900, Loss: 13619.3427734375
Epoch 47000, Loss: 13619.3408203125
Epoch 47100, Loss: 13619.3369140625
Epoch 47200, Loss: 13619.337890625
Epoch 47300, Loss: 13619.345703125
Epoch 47400, Loss: 13619.345703125
Epoch 47500, Loss: 13619.341796875
Epoch 47600, Loss: 13619.3408203125
Epoch 47700, Loss: 13619.341796875
Epoch 47800, Loss: 13619.33984375
Epoch 47900, Loss: 13619.3447265625
Epoch 48000, Loss: 13619.3505859375
Epoch 48100, Loss: 13619.3408203125
Epoch 48200, Loss: 13619.341796875
Epoch 48300, Loss: 13619.337890625
Epoch 48400, Loss: 13619.3369140625
Epoch 48500, Loss: 13619.3310546875
Epoch 48600, Loss: 13619.3427734375
Epoch 48700, Loss: 13619.3369140625
Epoch 48800, Loss: 13619.3349609375
Epoch 48900, Loss: 13619.33984375
Epoch 49000, Loss: 13619.3447265625
Epoch 49100, Loss: 13619.3349609375
Epoch 49200, Loss: 13619.337890625
Epoch 49300, Loss: 13619.3369140625
Epoch 49400, Loss: 13619.33984375
Epoch 49500, Loss: 13619.34375
Epoch 49600, Loss: 13619.3359375
Epoch 49700, Loss: 13619.330078125
Epoch 49800, Loss: 13619.33984375
Epoch 49900, Loss: 13619.341796875
Epoch 0, Loss: 63828.96875
Epoch 100, Loss: 61016.08203125
Epoch 200, Loss: 58386.9296875
Epoch 300, Loss: 55933.87109375
Epoch 400, Loss: 53645.8828125
Epoch 500, Loss: 51514.65234375
Epoch 600, Loss: 49529.55078125
Epoch 700, Loss: 47679.4921875
Epoch 800, Loss: 45953.59375
Epoch 900, Loss: 44343.03515625
Epoch 1000, Loss: 42842.19140625
Epoch 1100, Loss: 41441.9609375
Epoch 1200, Loss: 40135.21875
Epoch 1300, Loss: 38915.5078125
Epoch 1400, Loss: 37776.58203125
Epoch 1500, Loss: 36712.94921875
Epoch 1600, Loss: 35722.59765625
Epoch 1700, Loss: 34798.4453125
Epoch 1800, Loss: 33938.85546875
Epoch 1900, Loss: 33140.40625
Epoch 2000, Loss: 32399.69921875
Epoch 2100, Loss: 31709.287109375
Epoch 2200, Loss: 31064.7109375
Epoch 2300, Loss: 30461.958984375
Epoch 2400, Loss: 29897.462890625
Epoch 2500, Loss: 29368.080078125
Epoch 2600, Loss: 28871.044921875
Epoch 2700, Loss: 28403.884765625
Epoch 2800, Loss: 27964.388671875
Epoch 2900, Loss: 27552.05078125
Epoch 3000, Loss: 27165.337890625
Epoch 3100, Loss: 26804.6015625
Epoch 3200, Loss: 26469.478515625
Epoch 3300, Loss: 26155.396484375
Epoch 3400, Loss: 25859.599609375
Epoch 3500, Loss: 25582.515625
Epoch 3600, Loss: 25319.658203125
Epoch 3700, Loss: 25069.52734375
Epoch 3800, Loss: 24830.794921875
Epoch 3900, Loss: 24604.845703125
Epoch 4000, Loss: 24389.802734375
Epoch 4100, Loss: 24183.046875
Epoch 4200, Loss: 23983.666015625
Epoch 4300, Loss: 23790.8828125
Epoch 4400, Loss: 23604.015625
Epoch 4500, Loss: 23422.501953125
Epoch 4600, Loss: 23245.837890625
Epoch 4700, Loss: 23074.392578125
Epoch 4800, Loss: 22908.6953125
Epoch 4900, Loss: 22747.318359375
Epoch 5000, Loss: 22589.349609375
Epoch 5100, Loss: 22434.53125
Epoch 5200, Loss: 22282.65234375
Epoch 5300, Loss: 22133.50390625
Epoch 5400, Loss: 21986.93359375
Epoch 5500, Loss: 21842.798828125
Epoch 5600, Loss: 21700.974609375
Epoch 5700, Loss: 21561.3359375
Epoch 5800, Loss: 21423.806640625
Epoch 5900, Loss: 21288.4296875
Epoch 6000, Loss: 21156.572265625
Epoch 6100, Loss: 21026.99609375
Epoch 6200, Loss: 20900.203125
Epoch 6300, Loss: 20777.70703125
Epoch 6400, Loss: 20657.0390625
Epoch 6500, Loss: 20537.9140625
Epoch 6600, Loss: 20420.220703125
Epoch 6700, Loss: 20303.93359375
Epoch 6800, Loss: 20188.984375
Epoch 6900, Loss: 20075.33203125
Epoch 7000, Loss: 19962.947265625
Epoch 7100, Loss: 19851.79296875
Epoch 7200, Loss: 19741.84765625
Epoch 7300, Loss: 19633.078125
Epoch 7400, Loss: 19525.47265625
Epoch 7500, Loss: 19419.279296875
Epoch 7600, Loss: 19318.642578125
Epoch 7700, Loss: 19219.12890625
Epoch 7800, Loss: 19120.609375
Epoch 7900, Loss: 19023.0546875
Epoch 8000, Loss: 18926.5546875
Epoch 8100, Loss: 18831.076171875
Epoch 8200, Loss: 18736.509765625
Epoch 8300, Loss: 18642.837890625
Epoch 8400, Loss: 18550.072265625
Epoch 8500, Loss: 18458.203125
Epoch 8600, Loss: 18367.22265625
Epoch 8700, Loss: 18277.123046875
Epoch 8800, Loss: 18187.890625
Epoch 8900, Loss: 18099.52734375
Epoch 9000, Loss: 18013.0703125
Epoch 9100, Loss: 17928.38671875
Epoch 9200, Loss: 17844.939453125
Epoch 9300, Loss: 17762.728515625
Epoch 9400, Loss: 17681.728515625
Epoch 9500, Loss: 17601.97265625
Epoch 9600, Loss: 17524.89453125
Epoch 9700, Loss: 17450.4140625
Epoch 9800, Loss: 17377.4609375
Epoch 9900, Loss: 17305.95703125
Epoch 10000, Loss: 17235.890625
Epoch 10100, Loss: 17167.15625
Epoch 10200, Loss: 17100.2421875
Epoch 10300, Loss: 17034.67578125
Epoch 10400, Loss: 16970.28125
Epoch 10500, Loss: 16907.033203125
Epoch 10600, Loss: 16845.091796875
Epoch 10700, Loss: 16784.462890625
Epoch 10800, Loss: 16725.123046875
Epoch 10900, Loss: 16666.91796875
Epoch 11000, Loss: 16609.755859375
Epoch 11100, Loss: 16553.662109375
Epoch 11200, Loss: 16498.572265625
Epoch 11300, Loss: 16444.515625
Epoch 11400, Loss: 16391.453125
Epoch 11500, Loss: 16339.396484375
Epoch 11600, Loss: 16288.3369140625
Epoch 11700, Loss: 16238.380859375
Epoch 11800, Loss: 16189.4140625
Epoch 11900, Loss: 16141.72265625
Epoch 12000, Loss: 16095.49609375
Epoch 12100, Loss: 16050.4052734375
Epoch 12200, Loss: 16006.4619140625
Epoch 12300, Loss: 15963.8046875
Epoch 12400, Loss: 15922.2099609375
Epoch 12500, Loss: 15881.671875
Epoch 12600, Loss: 15842.10546875
Epoch 12700, Loss: 15803.5
Epoch 12800, Loss: 15765.7958984375
Epoch 12900, Loss: 15728.96484375
Epoch 13000, Loss: 15692.9765625
Epoch 13100, Loss: 15657.7978515625
Epoch 13200, Loss: 15623.4140625
Epoch 13300, Loss: 15589.78125
Epoch 13400, Loss: 15556.8798828125
Epoch 13500, Loss: 15524.6923828125
Epoch 13600, Loss: 15493.2041015625
Epoch 13700, Loss: 15462.37109375
Epoch 13800, Loss: 15432.1796875
Epoch 13900, Loss: 15402.640625
Epoch 14000, Loss: 15374.35546875
Epoch 14100, Loss: 15347.84765625
Epoch 14200, Loss: 15322.205078125
Epoch 14300, Loss: 15297.298828125
Epoch 14400, Loss: 15273.060546875
Epoch 14500, Loss: 15249.4521484375
Epoch 14600, Loss: 15226.9541015625
Epoch 14700, Loss: 15205.173828125
Epoch 14800, Loss: 15183.953125
Epoch 14900, Loss: 15163.265625
Epoch 15000, Loss: 15143.078125
Epoch 15100, Loss: 15123.3916015625
Epoch 15200, Loss: 15104.1787109375
Epoch 15300, Loss: 15085.431640625
Epoch 15400, Loss: 15067.1474609375
Epoch 15500, Loss: 15049.296875
Epoch 15600, Loss: 15031.892578125
Epoch 15700, Loss: 15014.9140625
Epoch 15800, Loss: 14998.3564453125
Epoch 15900, Loss: 14982.2314453125
Epoch 16000, Loss: 14966.486328125
Epoch 16100, Loss: 14951.162109375
Epoch 16200, Loss: 14936.2197265625
Epoch 16300, Loss: 14921.67578125
Epoch 16400, Loss: 14907.498046875
Epoch 16500, Loss: 14893.7099609375
Epoch 16600, Loss: 14880.255859375
Epoch 16700, Loss: 14867.1572265625
Epoch 16800, Loss: 14854.40625
Epoch 16900, Loss: 14841.986328125
Epoch 17000, Loss: 14829.87890625
Epoch 17100, Loss: 14818.0703125
Epoch 17200, Loss: 14806.5478515625
Epoch 17300, Loss: 14795.291015625
Epoch 17400, Loss: 14784.759765625
Epoch 17500, Loss: 14775.0078125
Epoch 17600, Loss: 14765.708984375
Epoch 17700, Loss: 14756.83984375
Epoch 17800, Loss: 14748.2666015625
Epoch 17900, Loss: 14739.9931640625
Epoch 18000, Loss: 14731.953125
Epoch 18100, Loss: 14724.1396484375
Epoch 18200, Loss: 14716.5595703125
Epoch 18300, Loss: 14709.123046875
Epoch 18400, Loss: 14701.890625
Epoch 18500, Loss: 14694.841796875
Epoch 18600, Loss: 14687.94140625
Epoch 18700, Loss: 14681.193359375
Epoch 18800, Loss: 14674.580078125
Epoch 18900, Loss: 14668.1171875
Epoch 19000, Loss: 14661.75390625
Epoch 19100, Loss: 14655.5068359375
Epoch 19200, Loss: 14649.3818359375
Epoch 19300, Loss: 14643.3583984375
Epoch 19400, Loss: 14637.44921875
Epoch 19500, Loss: 14631.65625
Epoch 19600, Loss: 14625.9501953125
Epoch 19700, Loss: 14620.3466796875
Epoch 19800, Loss: 14614.859375
Epoch 19900, Loss: 14609.46875
Epoch 20000, Loss: 14604.189453125
Epoch 20100, Loss: 14599.025390625
Epoch 20200, Loss: 14593.9638671875
Epoch 20300, Loss: 14589.017578125
Epoch 20400, Loss: 14584.1845703125
Epoch 20500, Loss: 14579.4501953125
Epoch 20600, Loss: 14574.8310546875
Epoch 20700, Loss: 14570.3359375
Epoch 20800, Loss: 14565.9365234375
Epoch 20900, Loss: 14561.65625
Epoch 21000, Loss: 14557.4931640625
Epoch 21100, Loss: 14553.447265625
Epoch 21200, Loss: 14549.48046875
Epoch 21300, Loss: 14545.65234375
Epoch 21400, Loss: 14541.9306640625
Epoch 21500, Loss: 14538.314453125
Epoch 21600, Loss: 14534.8037109375
Epoch 21700, Loss: 14531.4013671875
Epoch 21800, Loss: 14528.103515625
Epoch 21900, Loss: 14524.900390625
Epoch 22000, Loss: 14521.8173828125
Epoch 22100, Loss: 14518.8212890625
Epoch 22200, Loss: 14515.9248046875
Epoch 22300, Loss: 14513.14453125
Epoch 22400, Loss: 14510.4267578125
Epoch 22500, Loss: 14507.8037109375
Epoch 22600, Loss: 14505.29296875
Epoch 22700, Loss: 14502.859375
Epoch 22800, Loss: 14500.5205078125
Epoch 22900, Loss: 14498.248046875
Epoch 23000, Loss: 14496.0615234375
Epoch 23100, Loss: 14493.9228515625
Epoch 23200, Loss: 14491.88671875
Epoch 23300, Loss: 14489.91796875
Epoch 23400, Loss: 14488.0107421875
Epoch 23500, Loss: 14486.1689453125
Epoch 23600, Loss: 14484.4013671875
Epoch 23700, Loss: 14482.6904296875
Epoch 23800, Loss: 14481.052734375
Epoch 23900, Loss: 14479.45703125
Epoch 24000, Loss: 14477.9462890625
Epoch 24100, Loss: 14476.47265625
Epoch 24200, Loss: 14475.0634765625
Epoch 24300, Loss: 14473.7060546875
Epoch 24400, Loss: 14472.4287109375
Epoch 24500, Loss: 14471.1748046875
Epoch 24600, Loss: 14469.99609375
Epoch 24700, Loss: 14468.8623046875
Epoch 24800, Loss: 14467.779296875
Epoch 24900, Loss: 14466.7412109375
Epoch 25000, Loss: 14465.75390625
Epoch 25100, Loss: 14464.8193359375
Epoch 25200, Loss: 14463.935546875
Epoch 25300, Loss: 14463.083984375
Epoch 25400, Loss: 14462.26171875
Epoch 25500, Loss: 14461.5087890625
Epoch 25600, Loss: 14460.77734375
Epoch 25700, Loss: 14460.10546875
Epoch 25800, Loss: 14459.4599609375
Epoch 25900, Loss: 14458.84765625
Epoch 26000, Loss: 14458.2607421875
Epoch 26100, Loss: 14457.7177734375
Epoch 26200, Loss: 14457.2041015625
Epoch 26300, Loss: 14456.7236328125
Epoch 26400, Loss: 14456.2763671875
Epoch 26500, Loss: 14455.833984375
Epoch 26600, Loss: 14455.4384765625
Epoch 26700, Loss: 14455.0625
Epoch 26800, Loss: 14454.724609375
Epoch 26900, Loss: 14454.3818359375
Epoch 27000, Loss: 14454.06640625
Epoch 27100, Loss: 14453.78515625
Epoch 27200, Loss: 14453.521484375
Epoch 27300, Loss: 14453.2587890625
Epoch 27400, Loss: 14453.025390625
Epoch 27500, Loss: 14452.8037109375
Epoch 27600, Loss: 14452.5810546875
Epoch 27700, Loss: 14452.3896484375
Epoch 27800, Loss: 14452.201171875
Epoch 27900, Loss: 14452.0166015625
Epoch 28000, Loss: 14451.845703125
Epoch 28100, Loss: 14451.681640625
Epoch 28200, Loss: 14451.529296875
Epoch 28300, Loss: 14451.3720703125
Epoch 28400, Loss: 14451.2119140625
Epoch 28500, Loss: 14451.0791015625
Epoch 28600, Loss: 14450.91796875
Epoch 28700, Loss: 14450.794921875
Epoch 28800, Loss: 14450.62890625
Epoch 28900, Loss: 14450.4921875
Epoch 29000, Loss: 14450.3720703125
Epoch 29100, Loss: 14450.234375
Epoch 29200, Loss: 14450.1005859375
Epoch 29300, Loss: 14449.966796875
Epoch 29400, Loss: 14449.83203125
Epoch 29500, Loss: 14449.6962890625
Epoch 29600, Loss: 14449.568359375
Epoch 29700, Loss: 14449.4521484375
Epoch 29800, Loss: 14449.3203125
Epoch 29900, Loss: 14449.193359375
Epoch 30000, Loss: 14449.05859375
Epoch 30100, Loss: 14448.9365234375
Epoch 30200, Loss: 14448.81640625
Epoch 30300, Loss: 14448.681640625
Epoch 30400, Loss: 14448.55859375
Epoch 30500, Loss: 14448.44140625
Epoch 30600, Loss: 14448.322265625
Epoch 30700, Loss: 14448.2041015625
Epoch 30800, Loss: 14448.0703125
Epoch 30900, Loss: 14447.95703125
Epoch 31000, Loss: 14447.8408203125
Epoch 31100, Loss: 14447.7265625
Epoch 31200, Loss: 14447.6181640625
Epoch 31300, Loss: 14447.490234375
Epoch 31400, Loss: 14447.376953125
Epoch 31500, Loss: 14447.2685546875
Epoch 31600, Loss: 14447.1484375
Epoch 31700, Loss: 14447.0498046875
Epoch 31800, Loss: 14446.9267578125
Epoch 31900, Loss: 14446.8173828125
Epoch 32000, Loss: 14446.7177734375
Epoch 32100, Loss: 14446.59375
Epoch 32200, Loss: 14446.4931640625
Epoch 32300, Loss: 14446.376953125
Epoch 32400, Loss: 14446.27734375
Epoch 32500, Loss: 14446.177734375
Epoch 32600, Loss: 14446.0625
Epoch 32700, Loss: 14445.9736328125
Epoch 32800, Loss: 14445.8583984375
Epoch 32900, Loss: 14445.75
Epoch 33000, Loss: 14445.64453125
Epoch 33100, Loss: 14445.5546875
Epoch 33200, Loss: 14445.4736328125
Epoch 33300, Loss: 14445.3623046875
Epoch 33400, Loss: 14445.267578125
Epoch 33500, Loss: 14445.1728515625
Epoch 33600, Loss: 14445.072265625
Epoch 33700, Loss: 14444.96484375
Epoch 33800, Loss: 14444.888671875
Epoch 33900, Loss: 14444.78125
Epoch 34000, Loss: 14444.7060546875
Epoch 34100, Loss: 14444.6025390625
Epoch 34200, Loss: 14444.50390625
Epoch 34300, Loss: 14444.423828125
Epoch 34400, Loss: 14444.33984375
Epoch 34500, Loss: 14444.2451171875
Epoch 34600, Loss: 14444.16015625
Epoch 34700, Loss: 14444.0654296875
Epoch 34800, Loss: 14443.9912109375
Epoch 34900, Loss: 14443.900390625
Epoch 35000, Loss: 14443.8212890625
Epoch 35100, Loss: 14443.7197265625
Epoch 35200, Loss: 14443.642578125
Epoch 35300, Loss: 14443.5712890625
Epoch 35400, Loss: 14443.494140625
Epoch 35500, Loss: 14443.404296875
Epoch 35600, Loss: 14443.3427734375
Epoch 35700, Loss: 14443.2431640625
Epoch 35800, Loss: 14443.17578125
Epoch 35900, Loss: 14443.0927734375
Epoch 36000, Loss: 14443.033203125
Epoch 36100, Loss: 14442.94140625
Epoch 36200, Loss: 14442.8671875
Epoch 36300, Loss: 14442.7998046875
Epoch 36400, Loss: 14442.7275390625
Epoch 36500, Loss: 14442.671875
Epoch 36600, Loss: 14442.6845703125
Epoch 36700, Loss: 14442.6640625
Epoch 36800, Loss: 14442.673828125
Epoch 36900, Loss: 14442.6572265625
Epoch 37000, Loss: 14442.6748046875
Epoch 37100, Loss: 14442.6650390625
Epoch 37200, Loss: 14442.6748046875
Epoch 37300, Loss: 14442.6640625
Epoch 37400, Loss: 14442.669921875
Epoch 37500, Loss: 14442.6640625
Epoch 37600, Loss: 14442.6591796875
Epoch 37700, Loss: 14442.6708984375
Epoch 37800, Loss: 14442.6669921875
Epoch 37900, Loss: 14442.666015625
Epoch 38000, Loss: 14442.66015625
Epoch 38100, Loss: 14442.6640625
Epoch 38200, Loss: 14442.6640625
Epoch 38300, Loss: 14442.65234375
Epoch 38400, Loss: 14442.6640625
Epoch 38500, Loss: 14442.6630859375
Epoch 38600, Loss: 14442.6708984375
Epoch 38700, Loss: 14442.6650390625
Epoch 38800, Loss: 14442.6669921875
Epoch 38900, Loss: 14442.669921875
Epoch 39000, Loss: 14442.6572265625
Epoch 39100, Loss: 14442.6708984375
Epoch 39200, Loss: 14442.662109375
Epoch 39300, Loss: 14442.6572265625
Epoch 39400, Loss: 14442.6650390625
Epoch 39500, Loss: 14442.6591796875
Epoch 39600, Loss: 14442.658203125
Epoch 39700, Loss: 14442.6591796875
Epoch 39800, Loss: 14442.673828125
Epoch 39900, Loss: 14442.66015625
Epoch 40000, Loss: 14442.6630859375
Epoch 40100, Loss: 14442.6640625
Epoch 40200, Loss: 14442.658203125
Epoch 40300, Loss: 14442.6708984375
Epoch 40400, Loss: 14442.669921875
Epoch 40500, Loss: 14442.6591796875
Epoch 40600, Loss: 14442.6611328125
Epoch 40700, Loss: 14442.6650390625
Epoch 40800, Loss: 14442.66015625
Epoch 40900, Loss: 14442.6748046875
Epoch 41000, Loss: 14442.6708984375
Epoch 41100, Loss: 14442.662109375
Epoch 41200, Loss: 14442.66015625
Epoch 41300, Loss: 14442.6630859375
Epoch 41400, Loss: 14442.6669921875
Epoch 41500, Loss: 14442.6669921875
Epoch 41600, Loss: 14442.6669921875
Epoch 41700, Loss: 14442.658203125
Epoch 41800, Loss: 14442.662109375
Epoch 41900, Loss: 14442.6591796875
Epoch 42000, Loss: 14442.669921875
Epoch 42100, Loss: 14442.6640625
Epoch 42200, Loss: 14442.6728515625
Epoch 42300, Loss: 14442.65625
Epoch 42400, Loss: 14442.662109375
Epoch 42500, Loss: 14442.6708984375
Epoch 42600, Loss: 14442.6669921875
Epoch 42700, Loss: 14442.6640625
Epoch 42800, Loss: 14442.671875
Epoch 42900, Loss: 14442.658203125
Epoch 43000, Loss: 14442.666015625
Epoch 43100, Loss: 14442.6669921875
Epoch 43200, Loss: 14442.6826171875
Epoch 43300, Loss: 14442.6650390625
Epoch 43400, Loss: 14442.6708984375
Epoch 43500, Loss: 14442.671875
Epoch 43600, Loss: 14442.662109375
Epoch 43700, Loss: 14442.662109375
Epoch 43800, Loss: 14442.654296875
Epoch 43900, Loss: 14442.6728515625
Epoch 44000, Loss: 14442.6650390625
Epoch 44100, Loss: 14442.6826171875
Epoch 44200, Loss: 14442.669921875
Epoch 44300, Loss: 14442.673828125
Epoch 44400, Loss: 14442.6669921875
Epoch 44500, Loss: 14442.67578125
Epoch 44600, Loss: 14442.6630859375
Epoch 44700, Loss: 14442.666015625
Epoch 44800, Loss: 14442.66015625
Epoch 44900, Loss: 14442.6669921875
Epoch 45000, Loss: 14442.6640625
Epoch 45100, Loss: 14442.669921875
Epoch 45200, Loss: 14442.669921875
Epoch 45300, Loss: 14442.6708984375
Epoch 45400, Loss: 14442.666015625
Epoch 45500, Loss: 14442.6640625
Epoch 45600, Loss: 14442.6650390625
Epoch 45700, Loss: 14442.6640625
Epoch 45800, Loss: 14442.654296875
Epoch 45900, Loss: 14442.666015625
Epoch 46000, Loss: 14442.6669921875
Epoch 46100, Loss: 14442.66015625
Epoch 46200, Loss: 14442.658203125
Epoch 46300, Loss: 14442.66015625
Epoch 46400, Loss: 14442.666015625
Epoch 46500, Loss: 14442.6669921875
Epoch 46600, Loss: 14442.6669921875
Epoch 46700, Loss: 14442.6669921875
Epoch 46800, Loss: 14442.6640625
Epoch 46900, Loss: 14442.6689453125
Epoch 47000, Loss: 14442.6689453125
Epoch 47100, Loss: 14442.6640625
Epoch 47200, Loss: 14442.6591796875
Epoch 47300, Loss: 14442.669921875
Epoch 47400, Loss: 14442.65625
Epoch 47500, Loss: 14442.6591796875
Epoch 47600, Loss: 14442.654296875
Epoch 47700, Loss: 14442.66796875
Epoch 47800, Loss: 14442.662109375
Epoch 47900, Loss: 14442.6630859375
Epoch 48000, Loss: 14442.6630859375
Epoch 48100, Loss: 14442.666015625
Epoch 48200, Loss: 14442.67578125
Epoch 48300, Loss: 14442.66015625
Epoch 48400, Loss: 14442.6669921875
Epoch 48500, Loss: 14442.6611328125
Epoch 48600, Loss: 14442.662109375
Epoch 48700, Loss: 14442.6669921875
Epoch 48800, Loss: 14442.6640625
Epoch 48900, Loss: 14442.6669921875
Epoch 49000, Loss: 14442.6630859375
Epoch 49100, Loss: 14442.669921875
Epoch 49200, Loss: 14442.673828125
Epoch 49300, Loss: 14442.6640625
Epoch 49400, Loss: 14442.6640625
Epoch 49500, Loss: 14442.6552734375
Epoch 49600, Loss: 14442.669921875
Epoch 49700, Loss: 14442.666015625
Epoch 49800, Loss: 14442.669921875
Epoch 49900, Loss: 14442.6572265625</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="06-regularization_files/figure-html/cell-32-output-2.png" class="figure-img" width="804" height="275"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-lasso-regression-model-selection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.12: Model selection in LASSO regression on the bikeshare data set. (Left): training and validation MSE as a function of regularization strength. (Right): learned weights as a function of regularization strength.
</figcaption>
</figure>
</div>
<p>We find that LASSO regression achieves a slightly lower MSE on validation data than ridge regression.</p>
<p>Let‚Äôs train the LASSO model one last time with the best regularization strength and take a look at the learned coefficients:</p>
<div id="d619185b" class="cell" data-execution_count="32">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>reg_strength <span class="op">=</span> reg_strengths[val_mses.index(<span class="bu">min</span>(val_mses))]</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>LR <span class="op">=</span> LinearRegression(n_params<span class="op">=</span>X_train.shape[<span class="dv">1</span>])</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>train_model(LR, X_train, y_train, lr<span class="op">=</span><span class="fl">1e-1</span>, n_epochs<span class="op">=</span><span class="dv">20000</span>, tol <span class="op">=</span> <span class="fl">1e-4</span>, regularization <span class="op">=</span> <span class="kw">lambda</span> w: reg_strength<span class="op">*</span>ell_1_regularization(w))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We can inspect the coefficients to see which features the model found most important:</p>
<div id="92534442" class="cell" data-execution_count="33">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>coefs <span class="op">=</span> pd.DataFrame({</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Feature'</span>: X.columns,</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Coefficient'</span>: LR.w.detach().numpy().flatten()</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>}).sort_values(by<span class="op">=</span><span class="st">'Coefficient'</span>, key<span class="op">=</span><span class="bu">abs</span>, ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>coefs.head()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Feature</th>
<th data-quarto-table-cell-role="th">Coefficient</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">43</th>
<td>hr_17</td>
<td>244.998688</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">34</th>
<td>hr_8</td>
<td>219.405945</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">2</th>
<td>temp</td>
<td>174.332703</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">44</th>
<td>hr_18</td>
<td>144.762268</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">30</th>
<td>hr_4</td>
<td>-144.750198</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>We see that rush hours and high temperatures are identified by the model as highly predictive of bike rental demand.</p>
<p>Finally, let‚Äôs evaluate our chosen model on the test set to get a final estimate of performance:</p>
<div id="3564267f" class="cell" data-execution_count="34">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>y_pred_test <span class="op">=</span> LR.forward(X_test)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>test_mse <span class="op">=</span> mse(y_pred_test, y_test)</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test MSE: </span><span class="sc">{</span>test_mse<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Fraction of base rate: </span><span class="sc">{</span>test_mse<span class="sc">.</span>item()<span class="op">/</span>base_rate_mse<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Test MSE: 11488.4326
Fraction of base rate: 0.3417</code></pre>
</div>
</div>
<p>Our final model achieves a test MSE considerably lower than the base rate, suggesting that it has successfully learned to predict bike rental demand using the available features.</p>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-fanaee-tEventLabelingCombining2013" class="csl-entry" role="listitem">
Fanaee-T, Hadi, and Joao Gama. 2013. <span>‚ÄúEvent Labeling Combining Ensemble Detectors and Background Knowledge.‚Äù</span> <em>Progress in Artificial Intelligence</em>, 1‚Äì15. <a href="https://doi.org/10.1007/s13748-013-0040-3">https://doi.org/10.1007/s13748-013-0040-3</a>.
</div>
</div>
</section>

<p><br> <br> <span style="color:grey;">¬© Phil Chodrow, 2025</span></p></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/04-more-gradients.html" class="pagination-link" aria-label="Higher Dimensions">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Higher Dimensions</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->




</body></html>