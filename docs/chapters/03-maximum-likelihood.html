<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; Maximum Likelihood Estimation and Gradients ‚Äì Machine Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/04-more-gradients.html" rel="next">
<link href="../chapters/02-signal-noise.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-a14e345711173c227b21482e2eb4cc70.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-d8b0bb70be28f5ebcc1c8c98afdc075d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>
div.callout-question.callout {
  border-left-color: lightblue;
}
div.callout-question.callout-style-default > .callout-header {
  background-color: rgb(from lightblue r g b / 13%);
}
div.callout-question .callout-toggle::before {  background-image: url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="rgb(33, 37, 41)" class="bi bi-chevron-down" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M1.646 4.646a.5.5 0 0 1 .708 0L8 10.293l5.646-5.647a.5.5 0 0 1 .708.708l-6 6a.5.5 0 0 1-.708 0l-6-6a.5.5 0 0 1 0-.708z"/></svg>');}
div.callout-question.callout-style-default .callout-icon::before, div.callout-question.callout-titled .callout-icon::before {
  content: '‚ùì';
  background-image: none;
}
div.callout-sol.callout {
  border-left-color: pink;
}
div.callout-sol.callout-style-default > .callout-header {
  background-color: rgb(from pink r g b / 13%);
}
div.callout-sol .callout-toggle::before {  background-image: url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="rgb(33, 37, 41)" class="bi bi-chevron-down" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M1.646 4.646a.5.5 0 0 1 .708 0L8 10.293l5.646-5.647a.5.5 0 0 1 .708.708l-6 6a.5.5 0 0 1-.708 0l-6-6a.5.5 0 0 1 0-.708z"/></svg>');}
div.callout-sol.callout-style-default .callout-icon::before, div.callout-sol.callout-titled .callout-icon::before {
  content: 'üìù';
  background-image: none;
}
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../assets/styles.css">
</head>

<body class="nav-sidebar floating slimcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/02-signal-noise.html">Machine Learning Fundamentals</a></li><li class="breadcrumb-item"><a href="../chapters/03-maximum-likelihood.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Maximum Likelihood Estimation and Gradients</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Machine Learning</a> 
    </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Machine Learning Fundamentals</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/02-signal-noise.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Data = Signal + Noise</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/03-maximum-likelihood.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Maximum Likelihood Estimation and Gradients</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/04-more-gradients.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Higher Dimensions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/06-regularization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Features and Regularization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/07-bias-variance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">More on Overfitting</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Classification</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/10-intro-classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Introduction: Binary Labels</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/11-assessment-of-classifiers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Assessment of Classifiers</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/12-decision-theory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Decision Theory in Classification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/15-multinomial-classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Multinomial Classification</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#recap-log-likelihood-of-the-linear-gaussian-model" id="toc-recap-log-likelihood-of-the-linear-gaussian-model" class="nav-link active" data-scroll-target="#recap-log-likelihood-of-the-linear-gaussian-model">Recap: Log-Likelihood of the Linear-Gaussian Model</a></li>
  <li><a href="#the-gradient-of-a-multivariate-function" id="toc-the-gradient-of-a-multivariate-function" class="nav-link" data-scroll-target="#the-gradient-of-a-multivariate-function">The Gradient of a Multivariate Function</a>
  <ul class="collapse">
  <li><a href="#checking-gradients-with-torch" id="toc-checking-gradients-with-torch" class="nav-link" data-scroll-target="#checking-gradients-with-torch">Checking Gradients with <code>torch</code></a></li>
  <li><a href="#the-gradient-points-in-the-direction-of-greatest-increase" id="toc-the-gradient-points-in-the-direction-of-greatest-increase" class="nav-link" data-scroll-target="#the-gradient-points-in-the-direction-of-greatest-increase">The Gradient Points In the Direction of Greatest Increase</a></li>
  </ul></li>
  <li><a href="#critical-points-and-local-extrema" id="toc-critical-points-and-local-extrema" class="nav-link" data-scroll-target="#critical-points-and-local-extrema">Critical Points and Local Extrema</a></li>
  <li><a href="#revisiting-the-linear-gaussian-log-likelihood" id="toc-revisiting-the-linear-gaussian-log-likelihood" class="nav-link" data-scroll-target="#revisiting-the-linear-gaussian-log-likelihood">Revisiting the Linear-Gaussian Log-Likelihood</a></li>
  <li><a href="#a-first-look-gradient-descent-for-maximum-likelihood-estimation" id="toc-a-first-look-gradient-descent-for-maximum-likelihood-estimation" class="nav-link" data-scroll-target="#a-first-look-gradient-descent-for-maximum-likelihood-estimation">A First Look: Gradient Descent for Maximum Likelihood Estimation</a>
  <ul class="collapse">
  <li><a href="#object-oriented-api-for-ml-models" id="toc-object-oriented-api-for-ml-models" class="nav-link" data-scroll-target="#object-oriented-api-for-ml-models">Object-Oriented API for ML Models</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/02-signal-noise.html">Machine Learning Fundamentals</a></li><li class="breadcrumb-item"><a href="../chapters/03-maximum-likelihood.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Maximum Likelihood Estimation and Gradients</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Maximum Likelihood Estimation and Gradients</span></h1>
<p class="subtitle lead">Estimating parameters of data-generating distributions using calculus.</p>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><em><a href="https://colab.research.google.com/github/philchodrow/ml-notes-update/blob/main/docs/live-notebooks/03-maximum-likelihood.ipynb">Open the live notebook</a> in Google Colab.</em></p>
<section id="recap-log-likelihood-of-the-linear-gaussian-model" class="level2">
<h2 class="anchored" data-anchor-id="recap-log-likelihood-of-the-linear-gaussian-model">Recap: Log-Likelihood of the Linear-Gaussian Model</h2>
<p><a href="../chapters/02-signal-noise.html">Last time</a>, we introduced the idea of modeling data as signal + noise, studied the Gaussian distribution as a model of noise, and introduced the linear-Gaussian model for prediction in the context of linear trends. We also derived the log-likelihood function for the linear-Gaussian model and introduced the idea that we could learn the signal of the data by maximizing the log-likelihood with respect to the model parameters. In this chapter, we‚Äôll begin our study of how to maximize the likelihood systematically using tools from calculus.</p>
</section>
<section id="the-gradient-of-a-multivariate-function" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-gradient-of-a-multivariate-function">The Gradient of a Multivariate Function</h2>

<div class="no-row-height column-margin column-container"><div class="margin-aside">
<p>As you can study in courses dedicated to multivariable calculus, the existence of all of a function‚Äôs partial derivatives does not necessarily imply that the function is <em>multivariate differentiable</em>. In this course, we‚Äôll exclusively treat functions which are indeed multivariate differentiable unless otherwise noted, and so this distinction will not be an issue for us.</p>
</div></div><div id="def-partial-derivative" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.1</strong></span> Let <span class="math inline">\(f:\mathbb{R}^p\rightarrow \mathbb{R}\)</span> be a function which accepts a vector input <span class="math inline">\(\mathbf{w}=(w_1,\ldots,w_p)^T\in \mathbb{R}^p\)</span> and returns a scalar output <span class="math inline">\(f(\mathbf{w})\in \mathbb{R}\)</span>. The <strong>partial derivative</strong> of <span class="math inline">\(f\)</span> with respect to the <span class="math inline">\(j\)</span>-th coordinate <span class="math inline">\(w_j\)</span> is defined as the limit</p>
<p><span class="math display">\[
\begin{aligned}
    \frac{\partial f}{\partial w_i} &amp;= \lim_{h \rightarrow 0} \frac{f(w_1,\ldots,w_i + h, \ldots w_p) - f(w_1,\ldots,w_i, \ldots w_p)}{h} \\
    &amp;= \lim_{h \rightarrow 0} \frac{f(\mathbf{w}+ h\mathbf{e}_i) - f(\mathbf{w})}{h}\;,
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{e}_i = (0,0,\ldots,1,\ldots,0,0)^T\)</span> is the <span class="math inline">\(i\)</span>-th standard basis vector in <span class="math inline">\(\mathbb{R}^p\)</span>, i.e., the vector with a 1 in the <span class="math inline">\(i\)</span>-th position and 0‚Äôs elsewhere. If this limit does not exist, then the partial derivative is said to be undefined.</p>
</div>
<p>Just like in single-variable calculus, it‚Äôs not usually convenient to work directly with the limit definition of the partial derivative. Instead we use the following heuristic:</p>
<div id="prp-partial-derivative-rules" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 2.1</strong></span> To compute <span class="math inline">\(\frac{\partial f}{\partial w_i}\)</span>, treat all other variables <span class="math inline">\(w_j\)</span> for <span class="math inline">\(j\neq i\)</span> as constants, and differentiate <span class="math inline">\(f\)</span> with respect to <span class="math inline">\(w_i\)</span> using the usual rules of single-variable calculus (power rule, product rule, chain rule, etc.).</p>
</div>
<div id="exr-partial-derivative-example-1" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.1 (Practice with Partial Derivatives)</strong></span> Let <span class="math inline">\(f:\mathbb{R}^3\rightarrow \mathbb{R}\)</span> be defined by <span class="math inline">\(f(x,y,z) = x^2\sin y + yz + z^3x\)</span>. Compute <span class="math inline">\(\frac{\partial f}{\partial x}\)</span>, <span class="math inline">\(\frac{\partial f}{\partial y}\)</span>, and <span class="math inline">\(\frac{\partial f}{\partial z}\)</span>.</p>
</div>
<div class="callout callout-style-simple callout-sol no-icon callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Sol</span>Solution for <a href="#exr-partial-derivative-example-1" class="quarto-xref">Exercise&nbsp;<span>2.1</span></a>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>To compute <span class="math inline">\(\frac{\partial f}{\partial x}\)</span>, we treat <span class="math inline">\(y\)</span> and <span class="math inline">\(z\)</span> as constants, which yields</p>
<p><span class="math display">\[
\frac{\partial f}{\partial x} = 2x \sin y + z^3\;.
\]</span></p>
<p>Similarly, we can compute <span class="math inline">\(\frac{\partial f}{\partial y}\)</span> and <span class="math inline">\(\frac{\partial f}{\partial z}\)</span>:</p>
<p><span class="math display">\[
\begin{align}
    \frac{\partial f}{\partial y} &amp;= x^2 \cos y + z \\
    \frac{\partial f}{\partial z} &amp;= y + 3z^2 x\;.    
\end{align}
\]</span></p>
</div>
</div>
</div>
<div id="def-gradient" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.2</strong></span> Let <span class="math inline">\(f:\mathbb{R}^p\rightarrow \mathbb{R}\)</span> be a differentiable function which accepts a vector input <span class="math inline">\(\mathbf{w}=(w_1,\ldots,w_p)^T\in \mathbb{R}^p\)</span> and returns a scalar output <span class="math inline">\(f(\mathbf{w})\in \mathbb{R}\)</span>. The <strong>gradient</strong> of <span class="math inline">\(f\)</span> at <span class="math inline">\(\mathbf{w}\)</span>, written <span class="math inline">\(\nabla f(\mathbf{w}) \in \mathbb{R}^p\)</span>, is the vector of partial derivatives</p>
<p><span class="math display">\[
\begin{align}
    \nabla f(\mathbf{w}) &amp;= \begin{pmatrix}
    \frac{\partial f}{\partial w_1} \\
    \frac{\partial f}{\partial w_2} \\
    \vdots \\
    \frac{\partial f}{\partial w_p}
    \end{pmatrix}\;.
\end{align}
\]</span></p>
</div>
<div id="exr-gradient-example-1" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.2 (Writing Gradients)</strong></span> Write the gradient of the function in <a href="#exr-partial-derivative-example-1" class="quarto-xref">Exercise&nbsp;<span>2.1</span></a>.</p>
</div>
<div class="callout callout-style-simple callout-sol no-icon callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Sol</span>Solution for <a href="#exr-gradient-example-1" class="quarto-xref">Exercise&nbsp;<span>2.2</span></a>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In the function from <a href="#exr-partial-derivative-example-1" class="quarto-xref">Exercise&nbsp;<span>2.1</span></a>, the gradient is given by stacking the partial derivatives we computed into a single vector:</p>
<p><span class="math display">\[
\begin{aligned}
    \nabla f(x, y, z) =
     \begin{pmatrix}
    \frac{\partial f}{\partial x} \\
    \frac{\partial f}{\partial y} \\
    \frac{\partial f}{\partial z}
    \end{pmatrix} &amp;=
    \begin{pmatrix}
    2x \sin y + z^3 \\
    x^2 \cos y + z \\
    y + 3z^2 x  
    \end{pmatrix}
    \in \mathbb{R}^3\;.
\end{aligned}
\]</span></p>
</div>
</div>
</div>
<div id="exr-linear-gaussian-gradient" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.3</strong></span> Consider the <em>mean-squared error</em> function for a simple linear model with parameters <span class="math inline">\(w_0\)</span> and <span class="math inline">\(w_1\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
    R(\mathbf{x}, \mathbf{y}; w_0, w_1) = \frac{1}{n} \sum_{i = 1}^n (y_i - w_1x_i - w_0)^2\;.
\end{aligned}
\]</span></p>
<p>Compute the gradient <span class="math inline">\(\nabla R(\mathbf{x}, \mathbf{y}; w_0, w_1)\)</span> with respect to the parameters <span class="math inline">\(w_0\)</span> and <span class="math inline">\(w_1\)</span>.</p>
<p><em>Note: we‚Äôll soon see that this function is closely related to the log-likelihood of the linear-Gaussian model</em>.</p>
</div>
<div class="callout callout-style-simple callout-sol no-icon callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Sol</span>Solution for <a href="#exr-linear-gaussian-gradient" class="quarto-xref">Exercise&nbsp;<span>2.3</span></a>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We can compute the gradient by computing each partial derivative in turn:</p>
<p><span class="math display">\[
\begin{aligned}
    \frac{\partial R}{\partial w_0} &amp;= \frac{-2}{n} (y_i - w_1x_i - w_0) \\
    \frac{\partial R}{\partial w_1} &amp;= \frac{-2}{n} x_i(y_i - w_1x_i - w_0) \;,
\end{aligned}
\]</span> where we‚Äôve used the rules for derivatives. Stacking these into a vector gives</p>
<p><span class="math display">\[
\begin{aligned}
    \nabla R(\mathbf{x}, \mathbf{y}; w_0, w_1) = \frac{2}{n} \begin{pmatrix}
    \sum_{i=1}^n (y_i - w_1x_i - w_0) \\
    \sum_{i=1}^n x_i (y_i - w_1x_i - w_0)
    \end{pmatrix}
\end{aligned}
\]</span></p>
</div>
</div>
</div>
<section id="checking-gradients-with-torch" class="level3">
<h3 class="anchored" data-anchor-id="checking-gradients-with-torch">Checking Gradients with <code>torch</code></h3>
<p>The <code>pytorch</code> package, which we‚Äôll use throughout this course, implements <em>automatic differentiation</em>. Automatic differentiation is an extraordinarily powerful tool which we‚Äôll study later in the course. For now, we‚Äôll just note that it provides a handy way to check calculations of derivatives and gradients. For example, we can use <code>torch</code> to check the gradient we computed in <a href="#exr-gradient-example-1" class="quarto-xref">Exercise&nbsp;<span>2.2</span></a> as follows:</p>
<div id="adc4adce" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="annotated-cell-1"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-1-1"><a href="#annotated-cell-1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="annotated-cell-1-2"><a href="#annotated-cell-1-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor([<span class="fl">1.0</span>, <span class="fl">2.0</span>, <span class="fl">3.0</span>])</span>
<span id="annotated-cell-1-3"><a href="#annotated-cell-1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-1-4"><a href="#annotated-cell-1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># function to differentiate</span></span>
<span id="annotated-cell-1-5"><a href="#annotated-cell-1-5" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="kw">lambda</span> x: x[<span class="dv">0</span>]<span class="op">**</span><span class="dv">2</span> <span class="op">*</span> torch.sin(x[<span class="dv">1</span>]) <span class="op">+</span> x[<span class="dv">1</span>]<span class="op">*</span>x[<span class="dv">2</span>] <span class="op">+</span> x[<span class="dv">2</span>]<span class="op">**</span><span class="dv">3</span> <span class="op">*</span> x[<span class="dv">0</span>]</span>
<span id="annotated-cell-1-6"><a href="#annotated-cell-1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-1-7"><a href="#annotated-cell-1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># compute the gradient by hand using the formula we derived</span></span>
<span id="annotated-cell-1-8"><a href="#annotated-cell-1-8" aria-hidden="true" tabindex="-1"></a>our_grad <span class="op">=</span> torch.tensor([</span>
<span id="annotated-cell-1-9"><a href="#annotated-cell-1-9" aria-hidden="true" tabindex="-1"></a>    <span class="dv">2</span> <span class="op">*</span> x[<span class="dv">0</span>] <span class="op">*</span> torch.sin(x[<span class="dv">1</span>]) <span class="op">+</span> x[<span class="dv">2</span>]<span class="op">**</span><span class="dv">3</span>,</span>
<span id="annotated-cell-1-10"><a href="#annotated-cell-1-10" aria-hidden="true" tabindex="-1"></a>    x[<span class="dv">0</span>]<span class="op">**</span><span class="dv">2</span> <span class="op">*</span> torch.cos(x[<span class="dv">1</span>]) <span class="op">+</span> x[<span class="dv">2</span>],</span>
<span id="annotated-cell-1-11"><a href="#annotated-cell-1-11" aria-hidden="true" tabindex="-1"></a>    x[<span class="dv">1</span>] <span class="op">+</span> <span class="dv">3</span> <span class="op">*</span> x[<span class="dv">2</span>]<span class="op">**</span><span class="dv">2</span> <span class="op">*</span> x[<span class="dv">0</span>]</span>
<span id="annotated-cell-1-12"><a href="#annotated-cell-1-12" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="annotated-cell-1-13"><a href="#annotated-cell-1-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(our_grad)</span>
<span id="annotated-cell-1-14"><a href="#annotated-cell-1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-1-15"><a href="#annotated-cell-1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># compute the gradient using automatic differentiation</span></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-1-16" class="code-annotation-target"><a href="#annotated-cell-1-16" aria-hidden="true" tabindex="-1"></a>x.requires_grad_()</span>
<span id="annotated-cell-1-17"><a href="#annotated-cell-1-17" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> f(x)</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-1-18" class="code-annotation-target"><a href="#annotated-cell-1-18" aria-hidden="true" tabindex="-1"></a>y.backward()</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="3" onclick="event.preventDefault();">3</a><span id="annotated-cell-1-19" class="code-annotation-target"><a href="#annotated-cell-1-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x.grad)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-annotation">
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-1" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-1" data-code-lines="16,17" data-code-annotation="1">First, we compute the value of the function we want to differentiate and store the result to a variable (in this case called <code>y</code>).</span>
</dd>
<dt data-target-cell="annotated-cell-1" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-1" data-code-lines="18" data-code-annotation="2">Next, we call the <code>backward()</code> method on <code>y</code>, which computes the gradient of <code>y</code> with respect to its inputs (in this case, the vector <code>x</code>) using automatic differentiation.</span>
</dd>
<dt data-target-cell="annotated-cell-1" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-1" data-code-lines="19" data-code-annotation="3">Finally, we can access the computed gradient via the <code>grad</code> attribute of the input tensor <code>x</code>.</span>
</dd>
</dl>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([28.8186,  2.5839, 29.0000])
tensor([28.8186,  2.5839, 29.0000])</code></pre>
</div>
</div>
<p>The two approaches agree! As we grow comfortable with the calculus, we‚Äôll begin to rely more on torch‚Äôs automatic differentiation capabilities to compute gradients for us.</p>
</section>
<section id="the-gradient-points-in-the-direction-of-greatest-increase" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-gradient-points-in-the-direction-of-greatest-increase">The Gradient Points In the Direction of Greatest Increase</h3>
<p>An important feature of the gradient is that it tells us the direction in which a small change in the function inputs <span class="math inline">\(\mathbf{w}\)</span> could produce the greatest increase in the function output <span class="math inline">\(f(\mathbf{w})\)</span>. Equivalently, the gradient points directly <em>away</em> from the direction of greatest decrease in the function output. Here‚Äôs an example using the function from <a href="#exr-linear-gaussian-gradient" class="quarto-xref">Exercise&nbsp;<span>2.3</span></a>. <code>torch</code> makes it very easy to implement this function.</p>
<div id="39bfc02f" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> MSE(x, y, w0, w1):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ((y <span class="op">-</span> (w1 <span class="op">*</span> x <span class="op">+</span> w0))<span class="op">**</span><span class="dv">2</span>).mean()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We first plot the function as a function of the parameters <span class="math inline">\(w_0\)</span> and <span class="math inline">\(w_1\)</span> and then we overlay arrows representing the gradients at various points in the <span class="math inline">\((w_0, w_1)\)</span> space, with the gradients calculated via automatic differentiation in <code>torch</code>.</p>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># create the grid of (mu, sigma^2) values and the data</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>w0_grid <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>w1_grid <span class="op">=</span> torch.linspace(<span class="fl">0.1</span>, <span class="dv">2</span>, <span class="dv">100</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>W0, W1 <span class="op">=</span> torch.meshgrid(w0_grid, w1_grid, indexing<span class="op">=</span><span class="st">'ij'</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor([<span class="fl">0.5</span>, <span class="op">-</span><span class="fl">1.0</span>, <span class="fl">1.0</span>, <span class="fl">0.7</span>, <span class="fl">0.3</span>])  <span class="co"># example data points</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.tensor([<span class="fl">1.0</span>, <span class="fl">0.0</span>, <span class="fl">2.0</span>, <span class="fl">1.5</span>, <span class="fl">0.5</span>])</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>LL <span class="op">=</span> torch.zeros(W0.shape)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(W0.shape[<span class="dv">0</span>]):</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(W0.shape[<span class="dv">1</span>]):</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        LL[i, j] <span class="op">=</span> MSE(x, y, W0[i, j], W1[i, j])</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize the figure </span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="co"># show the log-likelihood as a contour plot</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>im <span class="op">=</span> ax.contourf(LL.numpy(), levels<span class="op">=</span><span class="dv">100</span>, cmap<span class="op">=</span><span class="st">'inferno_r'</span>, extent <span class="op">=</span> [w1_grid[<span class="dv">0</span>], w1_grid[<span class="op">-</span><span class="dv">1</span>], w0_grid[<span class="dv">0</span>], w0_grid[<span class="op">-</span><span class="dv">1</span>]])</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>ax.contour(LL.numpy(), levels<span class="op">=</span><span class="dv">100</span>, colors<span class="op">=</span><span class="st">'black'</span>, linewidths<span class="op">=</span><span class="fl">0.5</span>, extent <span class="op">=</span> [w1_grid[<span class="dv">0</span>], w1_grid[<span class="op">-</span><span class="dv">1</span>], w0_grid[<span class="dv">0</span>], w0_grid[<span class="op">-</span><span class="dv">1</span>]])</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r'</span><span class="dv">$</span><span class="vs">w_1</span><span class="dv">$</span><span class="vs">'</span>)</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="vs">r'</span><span class="dv">$</span><span class="vs">w_0</span><span class="dv">$</span><span class="vs">'</span>)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a><span class="co"># compute and plot the gradients at various points</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> w0, w1 <span class="kw">in</span> [( <span class="op">-</span><span class="fl">0.5</span>, <span class="fl">1.5</span>), (<span class="fl">0.0</span>, <span class="fl">1.0</span>), (<span class="fl">0.5</span>, <span class="fl">1.5</span>), (<span class="fl">.25</span>, <span class="fl">0.5</span>), (<span class="fl">0.75</span>, <span class="fl">1.0</span>)]:</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>    w0_tensor <span class="op">=</span> torch.tensor(w0, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>    w1_tensor <span class="op">=</span> torch.tensor(w1, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>    ll <span class="op">=</span> MSE(x, y, w0_tensor, w1_tensor)</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>    ll.backward()</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>    grad_w0 <span class="op">=</span> w0_tensor.grad.item()</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>    grad_w1 <span class="op">=</span> w1_tensor.grad.item()</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>    ax.quiver(w1, w0, <span class="op">-</span>grad_w1, <span class="op">-</span>grad_w0, color<span class="op">=</span><span class="st">'black'</span>, scale<span class="op">=</span><span class="dv">20</span>, width<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>    ax.scatter(w1, w0, color<span class="op">=</span><span class="st">'black'</span>, s<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>plt.colorbar(im)</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Gradients of the mean-squared error'</span>)</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div id="fig-mse" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-mse-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="4f6746b5" class="cell" data-execution_count="6">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03-maximum-likelihood_files/figure-html/cell-6-output-1.png" class="figure-img" width="567" height="442"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-mse-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.1: Visualization of the gradients of the mean-squared error function with respect to the parameters <span class="math inline">\(w_0\)</span> and <span class="math inline">\(w_1\)</span>. The background color indicates the value of the mean-squared error, with lighter colors representing higher values. Dotted curves give contours along which the function is constant. The black arrows represent the gradients at various points in the <span class="math inline">\((w_0, w_1)\)</span> space, pointing in the direction of greatest increase of the mean-squared error function.
</figcaption>
</figure>
</div>
<p>Two observations about <a href="#fig-mse" class="quarto-xref">Figure&nbsp;<span>2.1</span></a> are worth noting:</p>
<ol type="1">
<li>The gradient arrows always point uphill and are orthogonal (at right angles with) to the contour lines of the function.</li>
<li>The gradient arrows get smaller as we approach the maximum of the log-likelihood function, eventually becoming zero at the maximum itself.</li>
</ol>
<p>Both of these features are possible to prove mathematically, although we won‚Äôt do so here.</p>
</section>
</section>
<section id="critical-points-and-local-extrema" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="critical-points-and-local-extrema">Critical Points and Local Extrema</h2>
<p>One way we can use gradients is by analytically computing the local extrema of a function: solve the equation <span class="math inline">\(\nabla \mathcal{L}(\mathbf{w}) = 0\)</span> for <span class="math inline">\(\mathbf{w}\)</span> to find critical points of the log-likelihood, and then check which of these points are local maxima.</p>
<p>A critical point of a multivariate function is a point at which all of its partial derivatives are equal to zero. Critical points are candidates for local maxima or minima of the function, and so they are of interest when performing maximum-likelihood estimation by solving <span class="math inline">\(\nabla \mathcal{L}(\mathbf{w}) = 0\)</span>.</p>
<div id="def-critical-point" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.3 (Critical Points of Multivariate Functions)</strong></span> A <em>critical point</em> of a differentiable function <span class="math inline">\(f:\mathbb{R}^p\rightarrow \mathbb{R}\)</span> is a point <span class="math inline">\(\mathbf{w}^* \in \mathbb{R}^p\)</span> such that <span class="math inline">\(\nabla f(\mathbf{w}^*) = \mathbf{0}\)</span> (the zero vector in <span class="math inline">\(\mathbb{R}^p\)</span>).</p>
</div>
<p>All critical points of a function can be identified by solving the system of equations <span class="math inline">\(\nabla f(\mathbf{w}) = \mathbf{0}\)</span>. In a few rare cases, it‚Äôs possible to solve this system analytically to find all critical points.</p>

<div class="no-row-height column-margin column-container"><div class="margin-aside">
<p>The notation <span class="math inline">\(\lVert \mathbf{v} \rVert_2\)</span> refers to the Euclidean norm of <span class="math inline">\(\mathbf{v}\)</span>, with formula</p>
<p><span class="math display">\[
\begin{aligned}
    \lVert \mathbf{v} \rVert_2 = \sqrt{\sum_{i = 1}^p v_i^2}\;.
\end{aligned}
\]</span></p>
</div></div><div id="def-local-extrema" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.4 (Local Minima and Maxima)</strong></span> A <em>local minimum</em> of a differentiable function <span class="math inline">\(f:\mathbb{R}^p\rightarrow \mathbb{R}\)</span> is a point <span class="math inline">\(\mathbf{w}^* \in \mathbb{R}^p\)</span> such that there exists some radius <span class="math inline">\(r&gt;0\)</span> such that for all <span class="math inline">\(\mathbf{w}\)</span> with <span class="math inline">\(\|\mathbf{w}- \mathbf{w}^*\|_2 &lt; r\)</span>, we have <span class="math inline">\(f(\mathbf{w}) \geq f(\mathbf{w}^*)\)</span>. A <em>local maximum</em> is defined similarly, with the inequality reversed: for all <span class="math inline">\(\mathbf{w}\)</span> with <span class="math inline">\(\|\mathbf{w}- \mathbf{w}^*\|_2 &lt; r\)</span>, we have <span class="math inline">\(f(\mathbf{w}) \leq f(\mathbf{w}^*)\)</span>.</p>
</div>

<div class="no-row-height column-margin column-container"><div class="margin-aside">
<p>The Mild Conditions of <a href="#thm-extrema" class="quarto-xref">Theorem&nbsp;<span>2.1</span></a> are that <span class="math inline">\(f\)</span> is continuously differentiable in an open neighborhood around <span class="math inline">\(\mathbf{w}^*\)</span>.</p>
</div></div><div id="thm-extrema" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.1 (Local Extrema are Critical Points)</strong></span> Under Mild Conditions*, if <span class="math inline">\(\mathbf{w}^*\)</span> is a local extremum (minimum or maximum) of a differentiable function <span class="math inline">\(f:\mathbb{R}^p\rightarrow \mathbb{R}\)</span>, then <span class="math inline">\(\mathbf{w}^*\)</span> is a critical point of <span class="math inline">\(f\)</span>.</p>
</div>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>We Always Minimize
</div>
</div>
<div class="callout-body-container callout-body">
<p>Although our motivating problem is still <em>maximum</em> likelihood estimation, it is conventional in the literature on statistics, machine learning, and optimization to always seek <em>minima</em> of a given function. This works because maximizing <span class="math inline">\(\mathcal{L}(\mathbf{w})\)</span> is equivalent to minimizing <span class="math inline">\(-\mathcal{L}(\mathbf{w})\)</span>. Therefore, in the remainder of this chapter and in subsequent chapters, we will often refer to ‚Äúminimizing the negative log-likelihood‚Äù rather than ‚Äúmaximizing the log-likelihood.‚Äù Perhaps confusingly, we‚Äôll still refer to the result as the ‚Äúmaximum likelihood estimate‚Äù (MLE).</p>
</div>
</div>
<div class="page-columns page-full"><p><a href="#thm-extrema" class="quarto-xref">Theorem&nbsp;<span>2.1</span></a> tells us that we can try to find the maximum likelihood estimate of a parameter vector <span class="math inline">\(\mathbf{w}\)</span> by solving the equation <span class="math inline">\(\nabla \mathcal{L}(\mathbf{w}) = \mathbf{0}\)</span>. In principle, we should then check that the critical points we find are indeed minima of <span class="math inline">\(-\mathcal{L}(\mathbf{w})\)</span> rather than maxima or saddle points, which can sometimes be done using the multivariate second-derivative test. In practice, however, this second step is often skipped. </p><div class="no-row-height column-margin column-container"><span class="margin-aside">Skipping the second-derivative test can be justified if it is known that <span class="math inline">\(-\mathcal{L}\)</span> is a <em>convex</em> function.</span></div></div>
<p>Equipped with the concept of critical points, we are ready to find maximum likelihood estimates by solving the equation <span class="math inline">\(\nabla \mathcal{L}(\mathbf{w}) = \mathbf{0}\)</span>.</p>
<p>By convention, the maximum-likelihood estimate of a parameter is given a ‚Äúhat‚Äù symbol, so we would write the MLE estimators we found above as <span class="math inline">\(\hat{\mu}\)</span> and <span class="math inline">\(\hat{\sigma}^2\)</span>.</p>
</section>
<section id="revisiting-the-linear-gaussian-log-likelihood" class="level2">
<h2 class="anchored" data-anchor-id="revisiting-the-linear-gaussian-log-likelihood">Revisiting the Linear-Gaussian Log-Likelihood</h2>
<p>Let‚Äôs now consider the linear-Gaussian model from <a href="../chapters/02-signal-noise.html">last chapter</a>. In this model, we assume that each observed target variable <span class="math inline">\(y_i\)</span> is sampled from a Gaussian distribution with mean equal to a linear function of the corresponding feature vector <span class="math inline">\(x_i\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
    y_i &amp;\sim \mathcal{N}(w_1 x_i + w_0, \sigma^2)\;,
\end{aligned}
\]</span></p>
<p>To find the maximum-likelihood estimates given a data set of pairs <span class="math inline">\((x_i,y_i)\)</span> for <span class="math inline">\(i=1,\ldots,n\)</span>, we need to compute the log-likelihood function for this model, which as per last chapter is</p>
<p><span id="eq-linear-gaussian-to-mse"><span class="math display">\[
\begin{aligned}
    \mathcal{L}(\mathbf{x}, \mathbf{y}; w_1, w_0) &amp;= \sum_{i = 1}^n \log p_y(y_i;w_1x_i + w_0; \sigma^2) \\
    &amp;= \sum_{i = 1}^n \log \left( \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left( -\frac{(y_i - (w_1 x_i + w_0))^2}{2\sigma^2} \right) \right) \\
    &amp;= \sum_{i = 1}^n \left( -\frac{1}{2} \log(2\pi \sigma^2) - \frac{(y_i - (w_1 x_i + w_0))^2}{2\sigma^2} \right) \\
    &amp;= \underbrace{-\frac{n}{2} \log(2\pi \sigma^2)}_{C} - \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - (w_1 x_i + w_0))^2 \\
    &amp;= C - \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - w_1 x_i - w_0)^2 \\
    &amp;= C - \frac{n}{2\sigma^2} \cdot \frac{1}{n} \sum_{i=1}^n (y_i - w_1 x_i - w_0)^2 \\
    &amp;= C - \frac{n}{2\sigma^2} R(\mathbf{x}, \mathbf{y}; w_0, w_1)\;.
\end{aligned}
\tag{2.1}\]</span></span></p>
<p>We‚Äôve collected terms that do not depend on <span class="math inline">\(w_0\)</span> or <span class="math inline">\(w_1\)</span> into a constant term <span class="math inline">\(C\)</span>, and noticed that there‚Äôs a copy of the mean-squared error function <span class="math inline">\(R(\mathbf{x}, \mathbf{y}; w_0, w_1) = \frac{1}{n}\sum_{i=1}^n (y_i - w_1 x_i - w_0)^2\)</span> from <a href="#exr-linear-gaussian-gradient" class="quarto-xref">Exercise&nbsp;<span>2.3</span></a> appearing in the likelihood expression. Indeed, this term is the only that involves the parameters <span class="math inline">\(w_0\)</span> and <span class="math inline">\(w_1\)</span>. This means:</p>
<blockquote class="blockquote">
<p>To <strong>maximize</strong> the likelihood <span class="math inline">\(\mathcal{L}\)</span> with respect to <span class="math inline">\(w_0\)</span> and <span class="math inline">\(w_1\)</span>, we can equivalently <strong>minimize</strong> the mean-squared error <span class="math inline">\(R(\mathbf{x}, \mathbf{y}; w_0, w_1)\)</span>.</p>
</blockquote>
</section>
<section id="a-first-look-gradient-descent-for-maximum-likelihood-estimation" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="a-first-look-gradient-descent-for-maximum-likelihood-estimation">A First Look: Gradient Descent for Maximum Likelihood Estimation</h2>
<p>Now that we have tools to compute gradients, we can use these gradients to find maximum-likelihood estimates numerically using a <em>gradient method</em>. There are many kinds of gradient methods, and they all have in common a simple idea:</p>
<div id="def-gradient-method" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.5 (Gradient Methods)</strong></span> A <em>gradient method</em> for optimizing a multivariate function <span class="math inline">\(f:\mathbb{R}^p\rightarrow \mathbb{R}\)</span> is an iterative algorithm which starts from an initial guess <span class="math inline">\(\mathbf{w}^{(0)} \in \mathbb{R}^p\)</span> and produces a sequence of estimates <span class="math inline">\(\mathbf{w}^{(1)}, \mathbf{w}^{(2)}, \ldots\)</span> by repeatedly updating the current estimate <span class="math inline">\(\mathbf{w}^{(t)}\)</span> in the direction of the negative gradient <span class="math inline">\(-\nabla f(\mathbf{w}^{(t)})\)</span>, or some approximation thereof.</p>
</div>
<p>The simplest gradient method is <em>gradient descent</em> with <em>fixed learning rate</em>:</p>
<div id="def-gradient-descent" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.6 (Gradient Descent)</strong></span> <em>Gradient descent</em> is an algorithm that iterates the update</p>
<p><span class="math display">\[
\begin{aligned}
    \mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} - \alpha\nabla f(\mathbf{w}^{(t)})\;,
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\alpha \in \mathbb{R}_{&gt;0}\)</span> is a fixed hyperparameter called the <em>learning rate</em>.</p>
</div>
<p>Let‚Äôs use gradient descent to find maximum-likelihood estimates for the parameters of the linear-Gaussian model in a simple example. To visualize gradient descent, we‚Äôll start by implementing a function for the linear-Gaussian log-likelihood in terms of the parameters <span class="math inline">\(w_0\)</span> and <span class="math inline">\(w_1\)</span>:</p>
<p>Then we‚Äôll generate some synthetic data from a linear-Gaussian model with known parameters:</p>
<div id="fa015371" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># true parameters</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>w0 <span class="op">=</span> torch.tensor(<span class="fl">0.0</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>w1 <span class="op">=</span> torch.tensor(<span class="fl">2.0</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>sigma2 <span class="op">=</span> torch.tensor(<span class="fl">1.0</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># observed data</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">101</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>signal <span class="op">=</span> w1 <span class="op">*</span> x <span class="op">+</span> w0</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>noise <span class="op">=</span> torch.sqrt(sigma2) <span class="op">*</span> torch.randn_like(x)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> signal <span class="op">+</span> noise</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize <span class="op">=</span> (<span class="dv">6</span>, <span class="dv">4</span>))</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>ax.scatter(x, y, color<span class="op">=</span><span class="st">'steelblue'</span>, s<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Feature (x)'</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Target (y)'</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> ax.set_title(<span class="st">'Observed Data from Linear-Gaussian Model'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div id="fig-linear-gaussian-data" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-linear-gaussian-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="dd69710a" class="cell" data-execution_count="8">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03-maximum-likelihood_files/figure-html/cell-8-output-1.png" class="figure-img" width="503" height="368"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-linear-gaussian-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.2: Sample data from a linear-Gaussian model with true parameters <span class="math inline">\(w_0 = 0\)</span>, <span class="math inline">\(w_1 = 2\)</span>, and <span class="math inline">\(\sigma^2 = 1\)</span>.
</figcaption>
</figure>
</div>
<p>Our aim in maximum likelihood estimation is to find estimators <span class="math inline">\(\hat{w}_0\)</span> and <span class="math inline">\(\hat{w}_1\)</span> which maximize the log-likelihood of the observed data. As we saw in <a href="#eq-linear-gaussian-to-mse" class="quarto-xref">Equation&nbsp;<span>2.1</span></a>, maximizing the log-likelihood is equivalent to minimizing the mean-squared error between the observed targets <span class="math inline">\(y_i\)</span> and the linear predictions <span class="math inline">\(w_1 x_i + w_0\)</span>. Let‚Äôs go ahead and plot the mean-squared error:</p>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>w0_grid <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>w1_grid <span class="op">=</span> torch.linspace(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">100</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>W0, W1 <span class="op">=</span> torch.meshgrid(w0_grid, w1_grid, indexing<span class="op">=</span><span class="st">'ij'</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>LL <span class="op">=</span> torch.zeros(W0.shape)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(W0.shape[<span class="dv">0</span>]):</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(W0.shape[<span class="dv">1</span>]):</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        LL[i, j] <span class="op">=</span> MSE(x, y, W0[i, j], W1[i, j])</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co"># visualize the log-likelihood surface</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">5</span>))</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>im <span class="op">=</span> ax.contourf(LL.numpy(), levels<span class="op">=</span><span class="dv">20</span>, cmap<span class="op">=</span><span class="st">'inferno_r'</span>, extent <span class="op">=</span> [w1_grid[<span class="dv">0</span>], w1_grid[<span class="op">-</span><span class="dv">1</span>], w0_grid[<span class="dv">0</span>], w0_grid[<span class="op">-</span><span class="dv">1</span>]])</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>ax.contour(LL.numpy(), levels<span class="op">=</span><span class="dv">20</span>, colors<span class="op">=</span><span class="st">'black'</span>, linewidths<span class="op">=</span><span class="fl">0.5</span>, extent <span class="op">=</span> [w1_grid[<span class="dv">0</span>], w1_grid[<span class="op">-</span><span class="dv">1</span>], w0_grid[<span class="dv">0</span>], w0_grid[<span class="op">-</span><span class="dv">1</span>]])</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r'Intercept </span><span class="kw">(</span><span class="dv">$</span><span class="vs">w_0</span><span class="dv">$</span><span class="kw">)</span><span class="vs">'</span>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="vs">r'Slope </span><span class="kw">(</span><span class="dv">$</span><span class="vs">w_1</span><span class="dv">$</span><span class="kw">)</span><span class="vs">'</span>)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>plt.colorbar(im, ax<span class="op">=</span>ax)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Mean-squared error for linear-Gaussian model'</span>)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>ax.scatter([w1], [w0], color<span class="op">=</span><span class="st">'black'</span>, s<span class="op">=</span><span class="dv">50</span>, label<span class="op">=</span><span class="st">'True Parameters'</span>, facecolors<span class="op">=</span><span class="st">'white'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div id="fig-linear-gaussian-log-likelihood" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-linear-gaussian-log-likelihood-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="a164ad7e" class="cell" data-execution_count="9">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03-maximum-likelihood_files/figure-html/cell-9-output-1.png" class="figure-img" width="505" height="444"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-linear-gaussian-log-likelihood-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.3: Mean-squared error <span class="math inline">\(R(\mathbf{x}, \mathbf{y}; w_0, w_1)\)</span> for the linear-Gaussian model as a function of the intercept <span class="math inline">\(w_0\)</span> and slope <span class="math inline">\(w_1\)</span> with the data shown in <a href="#fig-linear-gaussian-data" class="quarto-xref">Figure&nbsp;<span>2.2</span></a>. The dot indicates the location of the true parameters used to generate the data.
</figcaption>
</figure>
</div>
<div class="callout callout-style-simple callout-question no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Question</span>Question
</div>
</div>
<div class="callout-body-container callout-body">
<blockquote class="blockquote">
<p>Why does this surface have only one minimum, and why is that minimum so close to the true parameters used to generate the data? Seems pretty convenient!</p>
</blockquote>
<p>It is convenient! The uniqueness of the local minimum is due to a property called <em>convexity</em> that we‚Äôll study later in the course. The closeness of the minimum to the true parameters is a consequence of the <em>consistency</em> property of maximum-likelihood estimators, which you can study in courses on statistical inference.</p>
</div>
</div>
<p>The gradient descent algorithm will start from an initial guess for the parameters <span class="math inline">\((w_0, w_1)\)</span> and iteratively update this guess in the direction of the negative gradient of the negative log-likelihood.</p>
<section id="object-oriented-api-for-ml-models" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="object-oriented-api-for-ml-models">Object-Oriented API for ML Models</h3>
<p>Here we‚Äôll introduce our the <em>object-oriented API</em> for machine learning models which we‚Äôll use throughout this course. For this we need a <em>model class</em> which will represent the linear-Gaussian model and an <em>optimizer class</em> which will implement the gradient descent algorithm.</p>
<section id="model-class" class="level4">
<h4 class="anchored" data-anchor-id="model-class">Model Class</h4>
<p>The primary responsibility of the model class is to store the weight parameters and to implement a method called <code>forward</code> which computes the model‚Äôs predictions (estimate of the signal) given an input <span class="math inline">\(x\)</span>. Here‚Äôs a simple implementation of a model class for 1d linear regression:</p>
<div id="784dc905" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="annotated-cell-4"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-4-1"><a href="#annotated-cell-4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LinearRegression1D:</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-4" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-4-2" class="code-annotation-target"><a href="#annotated-cell-4-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="annotated-cell-4-3"><a href="#annotated-cell-4-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w0 <span class="op">=</span> torch.tensor(<span class="fl">1.0</span>)</span>
<span id="annotated-cell-4-4"><a href="#annotated-cell-4-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w1 <span class="op">=</span> torch.tensor(<span class="fl">1.0</span>)</span>
<span id="annotated-cell-4-5"><a href="#annotated-cell-4-5" aria-hidden="true" tabindex="-1"></a>    </span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-4" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-4-6" class="code-annotation-target"><a href="#annotated-cell-4-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="annotated-cell-4-7"><a href="#annotated-cell-4-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.w1 <span class="op">*</span> x <span class="op">+</span> <span class="va">self</span>.w0</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-annotation">
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-4" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-4" data-code-lines="2" data-code-annotation="1">Initialize the model with a guess for the parameters <span class="math inline">\(w_0\)</span> and <span class="math inline">\(w_1\)</span>. Later, we‚Äôll use just a single instance variable which holds a vector of weights.</span>
</dd>
<dt data-target-cell="annotated-cell-4" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-4" data-code-lines="6" data-code-annotation="2">Method for computing the model‚Äôs predictions (estimate of the signal) given an input <span class="math inline">\(x\)</span>.</span>
</dd>
</dl>
</div>
</div>
</section>
<section id="optimizer-class" class="level4">
<h4 class="anchored" data-anchor-id="optimizer-class">Optimizer Class</h4>
<p>The primary responsibility of the optimizer class is to implement the optimization algorithm of choice in the <code>step</code> method. If we aren‚Äôt using automatic differentiation, then the optimizer is also a good place to compute the gradients of the loss function with respect to the model parameters. Here‚Äôs a simple implementation of a gradient descent optimizer for 1d linear regression:</p>
<div id="41fcf83a" class="cell" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="annotated-cell-5"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-5-1"><a href="#annotated-cell-5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GradientDescentOptimizer1D: </span>
<span id="annotated-cell-5-2"><a href="#annotated-cell-5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-5-3"><a href="#annotated-cell-5-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model, lr<span class="op">=</span><span class="fl">0.01</span>):</span>
<span id="annotated-cell-5-4"><a href="#annotated-cell-5-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="annotated-cell-5-5"><a href="#annotated-cell-5-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lr <span class="op">=</span> lr</span>
<span id="annotated-cell-5-6"><a href="#annotated-cell-5-6" aria-hidden="true" tabindex="-1"></a></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-5" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-5-7" class="code-annotation-target"><a href="#annotated-cell-5-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> grad_func(<span class="va">self</span>, x, y):</span>
<span id="annotated-cell-5-8"><a href="#annotated-cell-5-8" aria-hidden="true" tabindex="-1"></a>        n <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="annotated-cell-5-9"><a href="#annotated-cell-5-9" aria-hidden="true" tabindex="-1"></a>        w0_grad <span class="op">=</span> <span class="op">-</span><span class="dv">2</span><span class="op">/</span>n <span class="op">*</span> torch.<span class="bu">sum</span>(y <span class="op">-</span> (<span class="va">self</span>.model.w1 <span class="op">*</span> x <span class="op">+</span> <span class="va">self</span>.model.w0))  </span>
<span id="annotated-cell-5-10"><a href="#annotated-cell-5-10" aria-hidden="true" tabindex="-1"></a>        w1_grad <span class="op">=</span> <span class="op">-</span><span class="dv">2</span><span class="op">/</span>n <span class="op">*</span> torch.<span class="bu">sum</span>(x <span class="op">*</span> (y <span class="op">-</span> (<span class="va">self</span>.model.w1 <span class="op">*</span> x <span class="op">+</span> <span class="va">self</span>.model.w0)))  </span>
<span id="annotated-cell-5-11"><a href="#annotated-cell-5-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> w0_grad, w1_grad</span>
<span id="annotated-cell-5-12"><a href="#annotated-cell-5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-5-13"><a href="#annotated-cell-5-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>, x, y): </span>
<span id="annotated-cell-5-14"><a href="#annotated-cell-5-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># compute the gradients</span></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-5" data-target-annotation="3" onclick="event.preventDefault();">3</a><span id="annotated-cell-5-15" class="code-annotation-target"><a href="#annotated-cell-5-15" aria-hidden="true" tabindex="-1"></a>        w0_grad, w1_grad <span class="op">=</span> <span class="va">self</span>.grad_func(x, y)</span>
<span id="annotated-cell-5-16"><a href="#annotated-cell-5-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># update the parameters</span></span>
<span id="annotated-cell-5-17"><a href="#annotated-cell-5-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model.w0 <span class="op">=</span> <span class="va">self</span>.model.w0 <span class="op">-</span> <span class="va">self</span>.lr <span class="op">*</span> w0_grad</span>
<span id="annotated-cell-5-18"><a href="#annotated-cell-5-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model.w1 <span class="op">=</span> <span class="va">self</span>.model.w1 <span class="op">-</span> <span class="va">self</span>.lr <span class="op">*</span> w1_grad</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-annotation">
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-5" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-5" data-code-lines="7" data-code-annotation="1">Method for computing the gradients of the negative log-likelihood with respect to the parameters <span class="math inline">\(w_0\)</span> and <span class="math inline">\(w_1\)</span>, which we did in <a href="#exr-linear-gaussian-gradient" class="quarto-xref">Exercise&nbsp;<span>2.3</span></a>.</span>
</dd>
<dt data-target-cell="annotated-cell-5" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-5" data-code-lines="15" data-code-annotation="3">Update the parameters by taking a step in the direction of the negative gradient, scaled by the learning rate.</span>
</dd>
</dl>
</div>
</div>
</section>
<section id="training-loop" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="training-loop">Training Loop</h4>
<p>Once we‚Äôve implemented our two classes, the main ‚Äútraining loop‚Äù just requires us to repeatedly call the <code>step</code> method of the optimizer, which updates <code>model.w0</code> and <code>model.w1</code> at each iteration. We can also keep track of the history of parameter values across iterations to visualize the trajectory of the algorithm on the surface of the mean-squared error function.</p>
<div id="899168f1" class="cell" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LinearRegression1D()</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> GradientDescentOptimizer1D(model, lr<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>w0_history <span class="op">=</span> [model.w0.item()]</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>w1_history <span class="op">=</span> [model.w1.item()]</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(epochs): </span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    opt.step(x, y)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    w0_history.append(model.w0.item())</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    w1_history.append(model.w1.item())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p><a href="#fig-linear-gaussian-gradient-descent" class="quarto-xref">Figure&nbsp;<span>2.4</span></a> shows the trajectory of the gradient descent algorithm on the negative log-likelihood surface, ultimately landing near the minimal value of the MSE and close to the true parameters used to generate the data. We also show the fitted linear model corresponding to the final estimates of <span class="math inline">\(w_0\)</span> and <span class="math inline">\(w_1\)</span>, which visually agrees well with the data.</p>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">3</span>))</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>im <span class="op">=</span> ax[<span class="dv">0</span>].contourf(LL.numpy(), levels<span class="op">=</span><span class="dv">20</span>, cmap<span class="op">=</span><span class="st">'inferno_r'</span>, extent <span class="op">=</span> [w1_grid[<span class="dv">0</span>], w1_grid[<span class="op">-</span><span class="dv">1</span>], w0_grid[<span class="dv">0</span>], w0_grid[<span class="op">-</span><span class="dv">1</span>]])</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].contour(LL.numpy(), levels<span class="op">=</span><span class="dv">20</span>, colors<span class="op">=</span><span class="st">'black'</span>, linewidths<span class="op">=</span><span class="fl">0.5</span>, extent <span class="op">=</span> [w1_grid[<span class="dv">0</span>], w1_grid[<span class="op">-</span><span class="dv">1</span>], w0_grid[<span class="dv">0</span>], w0_grid[<span class="op">-</span><span class="dv">1</span>]])</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="vs">r'Intercept </span><span class="kw">(</span><span class="dv">$</span><span class="vs">w_0</span><span class="dv">$</span><span class="kw">)</span><span class="vs">'</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="vs">r'Slope </span><span class="kw">(</span><span class="dv">$</span><span class="vs">w_1</span><span class="dv">$</span><span class="kw">)</span><span class="vs">'</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>plt.colorbar(im, ax<span class="op">=</span>ax[<span class="dv">0</span>])</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">'Mean-squared error for</span><span class="ch">\n</span><span class="st">linear-Gaussian model'</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(w1_history, w0_history, marker<span class="op">=</span><span class="st">'o'</span>, color<span class="op">=</span><span class="st">'black'</span>, label<span class="op">=</span><span class="st">'Gradient Descent Path'</span>, markersize<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].scatter([w1], [w0], color<span class="op">=</span><span class="st">'black'</span>, s<span class="op">=</span><span class="dv">50</span>, label<span class="op">=</span><span class="st">'True Parameters'</span>, facecolors<span class="op">=</span><span class="st">'white'</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].scatter(x, y, color<span class="op">=</span><span class="st">'steelblue'</span>, s<span class="op">=</span><span class="dv">10</span>, label<span class="op">=</span><span class="st">'Observed Data'</span>)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(x, model.w1.detach().item() <span class="op">*</span> x <span class="op">+</span> model.w0.detach().item(), color<span class="op">=</span><span class="st">'black'</span>, label<span class="op">=</span><span class="st">'Fitted Line'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].legend()</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">'Feature (x)'</span>)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="st">'Target (y)'</span>)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> ax[<span class="dv">1</span>].set_title(<span class="st">'Fitted model'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div id="fig-linear-gaussian-gradient-descent" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-linear-gaussian-gradient-descent-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="3fbd7c5d" class="cell" data-execution_count="13">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03-maximum-likelihood_files/figure-html/cell-13-output-1.png" class="figure-img" width="672" height="313"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-linear-gaussian-gradient-descent-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.4: Trajectory of the gradient descent algorithm (black line with dots) on the negative log-likelihood surface for the linear-Gaussian model from <a href="#fig-linear-gaussian-log-likelihood" class="quarto-xref">Figure&nbsp;<span>2.3</span></a>. The starting point of the algorithm is at the beginning of the line, and each dot represents an iteration of the algorithm. The dot indicates the location of the true parameters used to generate the data.
</figcaption>
</figure>
</div>
<p>We have just completed a simple implementation of our first machine learning algorithm: gradient descent for 1d linear-Gaussian regression via maximum likelihood estimation. This algorithm appears to successfully learn the linear trend present in the data, as shown in the right panel of <a href="#fig-linear-gaussian-gradient-descent" class="quarto-xref">Figure&nbsp;<span>2.4</span></a>.</p>


</section>
</section>
</section>

<p><br> <br> <span style="color:grey;">¬© Phil Chodrow, 2025</span></p></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/02-signal-noise.html" class="pagination-link" aria-label="Data = Signal + Noise">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Data = Signal + Noise</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/04-more-gradients.html" class="pagination-link" aria-label="Higher Dimensions">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Higher Dimensions</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>