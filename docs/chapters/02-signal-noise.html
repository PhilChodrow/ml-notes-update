<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>1&nbsp; Data = Signal + Noise ‚Äì Machine Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/03-maximum-likelihood.html" rel="next">
<link href="../index.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-a14e345711173c227b21482e2eb4cc70.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-d8b0bb70be28f5ebcc1c8c98afdc075d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>
div.callout-sol.callout {
  border-left-color: pink;
}
div.callout-sol.callout-style-default > .callout-header {
  background-color: rgb(from pink r g b / 13%);
}
div.callout-sol .callout-toggle::before {  background-image: url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="rgb(33, 37, 41)" class="bi bi-chevron-down" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M1.646 4.646a.5.5 0 0 1 .708 0L8 10.293l5.646-5.647a.5.5 0 0 1 .708.708l-6 6a.5.5 0 0 1-.708 0l-6-6a.5.5 0 0 1 0-.708z"/></svg>');}
div.callout-sol.callout-style-default .callout-icon::before, div.callout-sol.callout-titled .callout-icon::before {
  content: 'üìù';
  background-image: none;
}
div.callout-question.callout {
  border-left-color: lightblue;
}
div.callout-question.callout-style-default > .callout-header {
  background-color: rgb(from lightblue r g b / 13%);
}
div.callout-question .callout-toggle::before {  background-image: url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="rgb(33, 37, 41)" class="bi bi-chevron-down" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M1.646 4.646a.5.5 0 0 1 .708 0L8 10.293l5.646-5.647a.5.5 0 0 1 .708.708l-6 6a.5.5 0 0 1-.708 0l-6-6a.5.5 0 0 1 0-.708z"/></svg>');}
div.callout-question.callout-style-default .callout-icon::before, div.callout-question.callout-titled .callout-icon::before {
  content: '‚ùì';
  background-image: none;
}
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../assets/styles.css">
</head>

<body class="nav-sidebar floating slimcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/02-signal-noise.html">Machine Learning Fundamentals</a></li><li class="breadcrumb-item"><a href="../chapters/02-signal-noise.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Data = Signal + Noise</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Machine Learning</a> 
    </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Machine Learning Fundamentals</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/02-signal-noise.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Data = Signal + Noise</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/03-maximum-likelihood.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Maximum Likelihood Estimation and Gradients</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/04-more-gradients.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Higher Dimensions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/06-regularization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Features and Regularization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/07-bias-variance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">More on Overfitting</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Classification</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/10-intro-classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Introduction: Binary Labels</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/15-multinomial-classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Multinomial Classification</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction-data-signal-and-noise" id="toc-introduction-data-signal-and-noise" class="nav-link active" data-scroll-target="#introduction-data-signal-and-noise">Introduction: Data, Signal, and Noise</a></li>
  <li><a href="#overfitting" id="toc-overfitting" class="nav-link" data-scroll-target="#overfitting">Overfitting</a></li>
  <li><a href="#modeling-the-noise" id="toc-modeling-the-noise" class="nav-link" data-scroll-target="#modeling-the-noise">Modeling the Noise</a></li>
  <li><a href="#properties-of-the-gaussian-distribution" id="toc-properties-of-the-gaussian-distribution" class="nav-link" data-scroll-target="#properties-of-the-gaussian-distribution">Properties of the Gaussian Distribution</a></li>
  <li><a href="#the-standard-gaussian" id="toc-the-standard-gaussian" class="nav-link" data-scroll-target="#the-standard-gaussian">The Standard Gaussian</a></li>
  <li><a href="#gaussian-noise" id="toc-gaussian-noise" class="nav-link" data-scroll-target="#gaussian-noise">Gaussian Noise</a>
  <ul class="collapse">
  <li><a href="#data-generating-distributions" id="toc-data-generating-distributions" class="nav-link" data-scroll-target="#data-generating-distributions">Data Generating Distributions</a></li>
  </ul></li>
  <li><a href="#model-likelihood" id="toc-model-likelihood" class="nav-link" data-scroll-target="#model-likelihood">Model Likelihood</a>
  <ul class="collapse">
  <li><a href="#a-first-look-likelihood-maximization" id="toc-a-first-look-likelihood-maximization" class="nav-link" data-scroll-target="#a-first-look-likelihood-maximization">A First Look: Likelihood Maximization</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/02-signal-noise.html">Machine Learning Fundamentals</a></li><li class="breadcrumb-item"><a href="../chapters/02-signal-noise.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Data = Signal + Noise</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Data = Signal + Noise</span></h1>
<p class="subtitle lead">Introducing data-generating distributions for machine learning</p>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><em><a href="https://colab.research.google.com/github/philchodrow/ml-notes-update/blob/main/docs/live-notebooks/02-signal-noise.ipynb">Open the live notebook</a> in Google Colab.</em></p>
<section id="introduction-data-signal-and-noise" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="introduction-data-signal-and-noise">Introduction: Data, Signal, and Noise</h2>
<p>In these notes, we‚Äôll expand on the following idea:</p>
<blockquote class="blockquote">
<p>Machine learning is the science and practice of building algorithms that distinguish between signal and noise in real-world data.</p>
</blockquote>
<p>Consider the following simple data set:</p>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>scatterplot_kwargs <span class="op">=</span> <span class="bu">dict</span>(color <span class="op">=</span> <span class="st">"black"</span>, label <span class="op">=</span> <span class="st">"data"</span>, facecolors <span class="op">=</span> <span class="st">"none"</span>, s <span class="op">=</span> <span class="dv">40</span>, alpha <span class="op">=</span> <span class="fl">0.6</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>n_points <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.linspace(<span class="dv">0</span>, <span class="dv">10</span>, n_points)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>signal <span class="op">=</span> <span class="fl">2.0</span> <span class="op">*</span> x <span class="op">+</span> <span class="fl">1.0</span>  <span class="co"># underlying pattern (signal)</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>noise <span class="op">=</span> torch.randn(n_points) <span class="op">*</span> <span class="fl">3.0</span>  <span class="co"># random noise</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> signal <span class="op">+</span> noise</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot residual segments</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize <span class="op">=</span> (<span class="dv">6</span>, <span class="dv">4</span>))</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>ax.scatter(x.numpy(), y.numpy(), <span class="op">**</span>scatterplot_kwargs)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>ax.plot(x.numpy(), signal.numpy(), color <span class="op">=</span> <span class="st">"black"</span>, linestyle <span class="op">=</span> <span class="st">"--"</span>, label <span class="op">=</span> <span class="vs">r"Signal: </span><span class="dv">$</span><span class="vs">f</span><span class="kw">(</span><span class="vs">x_i</span><span class="kw">)</span><span class="vs"> = 2x_i </span><span class="op">+</span><span class="vs"> 1</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_points):</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>: </span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        ax.plot([x[i].item(), x[i].item()], [signal[i].item(), y[i].item()], color <span class="op">=</span> <span class="st">"red"</span>, alpha <span class="op">=</span> <span class="fl">0.3</span>, linewidth <span class="op">=</span> <span class="fl">0.8</span>, label <span class="op">=</span> <span class="vs">r"Noise: </span><span class="dv">$</span><span class="er">\</span><span class="vs">epsilon_i</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>: </span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        ax.plot([x[i].item(), x[i].item()], [signal[i].item(), y[i].item()], color <span class="op">=</span> <span class="st">"red"</span>, alpha <span class="op">=</span> <span class="fl">0.3</span>, linewidth <span class="op">=</span> <span class="fl">0.8</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="vs">r"</span><span class="dv">$</span><span class="vs">x</span><span class="dv">$</span><span class="vs">"</span>, ylabel <span class="op">=</span> <span class="vs">r"</span><span class="dv">$</span><span class="vs">y</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div id="fig-signal-noise" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-signal-noise-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="238ca096" class="cell" data-execution_count="4">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02-signal-noise_files/figure-html/cell-4-output-1.png" class="figure-img" width="503" height="349"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-signal-noise-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.1: An illustrative decomposition of a data set (points) into a hypothesized underlying signal (dashed line) and noise (red segments).
</figcaption>
</figure>
</div>
<p>We can think of this data as consisting of two components: a <strong>signal</strong> expressed by a relationship <span class="math inline">\(y \approx f(x)\)</span> (in this case <span class="math inline">\(f(x) = 2x + 1\)</span>), and some <strong>noise</strong> that partially obscures this relationship. Schematically, we can write this relationship as:</p>
<p><span id="eq-signal-plus-noise"><span class="math display">\[
\begin{aligned}
    y_i = f(x_i) + \epsilon_i\;,
\end{aligned}
\tag{1.1}\]</span></span></p>
<p>which says that the <span class="math inline">\(i\)</span>th value of the target is equal to some function <span class="math inline">\(f(x_i)\)</span> of the input variable <span class="math inline">\(x_i\)</span>, plus some random noise term <span class="math inline">\(\epsilon_i\)</span>.</p>
</section>
<section id="overfitting" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="overfitting">Overfitting</h2>
<p>It‚Äôs important to emphasize here that the thing we want to learn is <em>not</em> the individual targets <span class="math inline">\(y_i\)</span>, but rather the underlying function <span class="math inline">\(f(x)\)</span>. To see why, let‚Äôs consider an example of what goes wrong if we try to learn the targets <span class="math inline">\(y_i\)</span> exactly. This is called <em>interpolation</em>, and is illustrated by <a href="#fig-interpolation" class="quarto-xref">Figure&nbsp;<span>1.2</span></a>:</p>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> interpolate</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an interpolating function</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>f_interp <span class="op">=</span> interpolate.interp1d(x.numpy(), y.numpy(), kind<span class="op">=</span><span class="st">'cubic'</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>x_dense <span class="op">=</span> torch.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>y_interp <span class="op">=</span> f_interp(x_dense.numpy())</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize <span class="op">=</span> (<span class="dv">6</span>, <span class="dv">4</span>))</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>ax.scatter(x.numpy(), y.numpy(), <span class="op">**</span>scatterplot_kwargs)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>ax.plot(x_dense.numpy(), y_interp, color <span class="op">=</span> <span class="st">"red"</span>, linestyle <span class="op">=</span> <span class="st">"--"</span>, label <span class="op">=</span> <span class="st">"interpolating fit"</span>, zorder <span class="op">=</span> <span class="op">-</span><span class="dv">10</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="vs">r"</span><span class="dv">$</span><span class="vs">x</span><span class="dv">$</span><span class="vs">"</span>, ylabel <span class="op">=</span> <span class="vs">r"</span><span class="dv">$</span><span class="vs">y</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div id="fig-interpolation" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-interpolation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="08e9040c" class="cell" data-execution_count="5">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02-signal-noise_files/figure-html/cell-5-output-1.png" class="figure-img" width="503" height="349"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-interpolation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.2: A function which exactly interpolates the data, perfectly fitting both signal and noise.
</figcaption>
</figure>
</div>
<p>The problem with interpolation is that while we have perfectly fit our training data, we have not learned the underlying <em>signal</em> <span class="math inline">\(f(x)\)</span>. If we were to generate new data with the same signal but with different noise, we would likely find that our interpolating function doesn‚Äôt actually make very good predictions about that data at all: many of its bends and wiggles don‚Äôt have any correspondence to features in the new data.</p>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate new data</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>y_new <span class="op">=</span> <span class="fl">2.0</span> <span class="op">*</span> x_dense <span class="op">+</span> <span class="fl">1.0</span> <span class="op">+</span> torch.randn(<span class="dv">100</span>) <span class="op">*</span> <span class="fl">3.0</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize <span class="op">=</span> (<span class="dv">6</span>, <span class="dv">4</span>))</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>ax.scatter(x_dense.numpy(), y_new.numpy(), <span class="op">**</span>scatterplot_kwargs)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>ax.plot(x_dense.numpy(), y_interp, color <span class="op">=</span> <span class="st">"red"</span>, linestyle <span class="op">=</span> <span class="st">"--"</span>, label <span class="op">=</span> <span class="st">"interpolating fit"</span>, zorder <span class="op">=</span> <span class="op">-</span><span class="dv">10</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="vs">r"</span><span class="dv">$</span><span class="vs">x</span><span class="dv">$</span><span class="vs">"</span>, ylabel <span class="op">=</span> <span class="vs">r"</span><span class="dv">$</span><span class="vs">y</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div id="fig-interpolation-2" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-interpolation-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="45db6794" class="cell" data-execution_count="6">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02-signal-noise_files/figure-html/cell-6-output-1.png" class="figure-img" width="503" height="349"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-interpolation-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.3: New data plotted alongside the interpolating fit from the previous figure.
</figcaption>
</figure>
</div>
<p>We observe a number of irrelevant fluctuations in the interpolating fit that do not correspond to the underlying pattern. This phenomenon is called <strong>overfitting</strong>: by trying to fit the noise in our training data, we have failed to learn the true signal. We‚Äôll learn how to quantify overfitting once we begin to study measures of model quality.</p>
</section>
<section id="modeling-the-noise" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="modeling-the-noise">Modeling the Noise</h2>
<p>If we want to learn the signal <span class="math inline">\(f\)</span> in <a href="#eq-signal-plus-noise" class="quarto-xref">Equation&nbsp;<span>1.1</span></a>, it‚Äôs helpful to learn how to talk mathematically about the noise term <span class="math inline">\(\epsilon_i\)</span>. To do this, we need to step into the language of probability theory. Our standing assumption will be that the noise terms <span class="math inline">\(\epsilon_i\)</span> are <em>random variables</em> drawn independently and identically-distributed from some probability distribution. This means that each time we observe a new data point, we get a different value of <span class="math inline">\(\epsilon_i\)</span> drawn from the same distribution, without any influence from other values of <span class="math inline">\(\epsilon_j\)</span> for <span class="math inline">\(j \neq i\)</span>.</p>
<p>The noise distribution we will usually consider is the Gaussian distribution, also called the Gaussian distribution.</p>
<div id="cell-fig-gaussian-area" class="cell page-columns page-full" data-execution_count="7">

<div class="no-row-height column-margin column-container"><div class="cell-output cell-output-display">
<div id="fig-gaussian-area" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gaussian-area-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="02-signal-noise_files/figure-html/fig-gaussian-area-output-1.png" width="286" height="239" class="figure-img">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig" id="fig-gaussian-area-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.4: The probability that a Gaussian random variable lies between two values <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> is given by the area under its PDF between those values.
</figcaption>
</figure>
</div>
</div></div></div>
<div id="def-gaussian" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.1 (Gaussian (Normal) Distribution)</strong></span> A random variable <span class="math inline">\(\epsilon\)</span> has a Gaussian distribution with <em>parameters</em> mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span> if the probability that <span class="math inline">\(\epsilon\)</span> has a value between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> is given by:</p>
<p><span class="math display">\[
\begin{aligned}
    \mathbb{P}(a \leq \epsilon \leq b) = \int_a^b p_\epsilon(x;\mu, \sigma) \, dx\;,
\end{aligned}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\begin{aligned}
p_\epsilon(x;\mu, \sigma) = \frac{1}{\sigma \sqrt{2 \pi}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)\;.
\end{aligned}
\]</span></p>
<p>The function <span class="math inline">\(p_\epsilon(x;\mu, \sigma)\)</span> is called the <em>probability density function</em> (PDF) of the Gaussian distribution. We use the shorthand notation <span class="math inline">\(\epsilon \sim \mathcal{N}(\mu, \sigma^2)\)</span> to indicate that the random variable <span class="math inline">\(\epsilon\)</span> has a Gaussian distribution with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>.</p>
<p>Gaussian distributions are often called <em>normal</em> distributions in the statistics literature.</p>
</div>
<p>Here‚Äôs a simple vectorized implementation of the Gaussian PDF, which can be evaluated on a PyTorch tensor of inputs:</p>
<div id="7ee11c77" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>pi <span class="op">=</span> <span class="fl">3.141592653589793</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> normal_pdf(x, mu, sigma):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (sigma <span class="op">*</span> (<span class="dv">2</span> <span class="op">*</span> pi)<span class="op">**</span><span class="fl">0.5</span>) <span class="op">*</span> torch.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> ((x <span class="op">-</span> mu) <span class="op">/</span> sigma) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>normal_pdf(torch.tensor([<span class="fl">0.0</span>, <span class="fl">1.0</span>, <span class="fl">2.0</span>]), mu<span class="op">=</span><span class="fl">0.0</span>, sigma<span class="op">=</span><span class="fl">1.0</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>tensor([0.3989, 0.2420, 0.0540])</code></pre>
</div>
</div>
<div id="cell-fig-normal-pdfs" class="cell page-columns page-full" data-execution_count="9">

<div class="no-row-height column-margin column-container"><div class="cell-output cell-output-display">
<div id="fig-normal-pdfs" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-normal-pdfs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="02-signal-noise_files/figure-html/fig-normal-pdfs-output-1.png" width="283" height="221" class="figure-img">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig" id="fig-normal-pdfs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.5: Normal distribution PDFs for different values.
</figcaption>
</figure>
</div>
</div></div></div>
<p>The two parameters of the Gaussian distribution describe its ‚Äúshape:‚Äù The mean <span class="math inline">\(\mu\)</span> indicates the ‚Äúcenter‚Äù of the distribution, while the standard deviation <span class="math inline">\(\sigma\)</span> indicates how ‚Äúspread out‚Äù the distribution is.</p>
<p>‚ÄúGaussian noise‚Äù refers to random variables drawn from a Gaussian distribution. We can generate Gaussian noise from a given Gaussian distribution using many functions. We‚Äôll focus on PyTorch‚Äôs implementation, which allows us to generate tensors of Gaussian noise easily:</p>
<div id="8cc03c81" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> torch.normal(mu, sigma, size<span class="op">=</span>(<span class="dv">10</span>,))</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>epsilon</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>tensor([ 0.1864,  1.0350,  0.8385,  0.1005, -0.8591,  0.1861, -1.9333, -1.0409,
        -0.1336,  1.1946])</code></pre>
</div>
</div>
<p>If we take many samples of Gaussian noise and plot a histogram of their values, then we‚Äôll find (via the <em>law of large numbers</em>) that the histogram approximates the PDF of the Gaussian distribution we sampled from, as illustrated in <a href="#fig-normal-histogram-comparison" class="quarto-xref">Figure&nbsp;<span>1.6</span></a>:</p>
<div id="cell-fig-normal-histogram-comparison" class="cell page-columns page-full" data-execution_count="11">

<div class="no-row-height column-margin column-container"><div class="cell-output cell-output-display">
<div id="fig-normal-histogram-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-normal-histogram-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="02-signal-noise_files/figure-html/fig-normal-histogram-comparison-output-1.png" width="286" height="201" class="figure-img">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig" id="fig-normal-histogram-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.6: Histogram of samples from a Gaussian distribution compared to its PDF.
</figcaption>
</figure>
</div>
</div></div></div>
</section>
<section id="properties-of-the-gaussian-distribution" class="level2">
<h2 class="anchored" data-anchor-id="properties-of-the-gaussian-distribution">Properties of the Gaussian Distribution</h2>
<p>The Gaussian distribution has a number of useful properties for modeling noise in data. Here are a few:</p>
<div id="thm-translation" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1.1 (Translation)</strong></span> If <span class="math inline">\(\epsilon\)</span> is normally distributed with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>, then the random variable <span class="math inline">\(\epsilon + c\)</span> (where <span class="math inline">\(c\)</span> is a constant) is normally distributed with mean <span class="math inline">\(\mu + c\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>. In other words, if <span class="math inline">\(\epsilon \sim \mathcal{N}(\mu, \sigma^2)\)</span>, then <span class="math inline">\(\epsilon + c \sim \mathcal{N}(\mu + c, \sigma^2)\)</span>.</p>
</div>
<div id="thm-scaling" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1.2 (Scaling)</strong></span> If <span class="math inline">\(\epsilon\)</span> is normally distributed with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>, then the random variable <span class="math inline">\(a \epsilon\)</span> (where <span class="math inline">\(a\)</span> is a constant) is normally distributed with mean <span class="math inline">\(a \mu\)</span> and standard deviation <span class="math inline">\(|a| \sigma\)</span>. In other words, if <span class="math inline">\(\epsilon \sim \mathcal{N}(\mu, \sigma^2)\)</span>, then <span class="math inline">\(a \epsilon \sim \mathcal{N}(a \mu, (a \sigma)^2)\)</span>.</p>
</div>
<p>The <em>expectation</em> or <em>mean</em> of a continuous-valued random variable <span class="math inline">\(\epsilon\)</span> with probability density function <span class="math inline">\(p_\epsilon(x)\)</span> is defined as:</p>
<p><span class="math display">\[
\begin{aligned}
    \mathbb{E}[\epsilon] = \int_{-\infty}^{\infty} x \, p_\epsilon(x) \, dx\;.
\end{aligned}
\]</span></p>
<p>The <em>variance</em> of a continuous-valued random variable <span class="math inline">\(\epsilon\)</span> with probability density function <span class="math inline">\(p_\epsilon(x)\)</span> is defined as:</p>
<p><span class="math display">\[
\begin{aligned}
    \text{Var}(\epsilon) = \mathbb{E}[(\epsilon - \mathbb{E}[\epsilon])^2] = \int_{-\infty}^{\infty} (x - \mathbb{E}[\epsilon])^2 \, p_\epsilon(x) \, dx\;.
\end{aligned}
\]</span></p>
<p>We can interpret the variance as a measure of how far away <span class="math inline">\(\epsilon\)</span> ‚Äúusually, on average‚Äù lies from its mean value.</p>
<div id="thm-moments" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1.3 (Mean and Variance of the Gaussian)</strong></span> For a normally distributed random variable with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
    \mathbb{E}[\epsilon] &amp;= \mu \\
    \mathrm{Var}[\epsilon] &amp;= \sigma^2.
\end{aligned}
\]</span></p>
</div>
<p>These two properties can be proven using some integration tricks which are beyond our scope here.</p>
</section>
<section id="the-standard-gaussian" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-standard-gaussian">The Standard Gaussian</h2>
<p>The standard Gaussian distribution is the Gaussian with mean zero and standard deviation one: <span class="math inline">\(\mathcal{N}(0, 1)\)</span>. We often use the symbol <span class="math inline">\(Z\)</span> to Due to the properties above, we can make any Gaussian random variable from a standard Gaussian: if <span class="math inline">\(X \sim \mathcal{N}(\mu, \sigma^2)\)</span>, then, using <a href="#thm-scaling" class="quarto-xref">Theorem&nbsp;<span>1.2</span></a> and <a href="#thm-translation" class="quarto-xref">Theorem&nbsp;<span>1.1</span></a>, we can write <span class="math inline">\(X\)</span> as</p>
<p><span class="math display">\[
\begin{aligned}
    X \sim \sigma Z + \mu\;.
\end{aligned}
\]</span></p>
<p>We can check that this random variable has the correct mean and variance:</p>
<p><span class="math display">\[
\begin{aligned}
    \mathbb{E}[X] = \mathbb{E}[\sigma Z + \mu] = \sigma \mathbb{E}[Z] + \mu = \sigma \cdot 0 + \mu = \mu\;,
\end{aligned}
\]</span></p>
<p>where we‚Äôve used linearity of expectation and the fact that <span class="math inline">\(\mathbb{E}[Z] = 0\)</span>. To calculate the variance, we use the fact that <span class="math inline">\(\mathbb{E}[Z]= 0\)</span>, again, so that</p>
<p><span class="math display">\[
\begin{aligned}
    \mathrm{Var}[X] = \mathrm{Var}[\sigma Z + \mu] = \mathrm{Var}[\sigma Z] = \sigma^2 \mathrm{Var}[Z] = \sigma^2 \cdot 1 = \sigma^2\;.
\end{aligned}
\]</span></p>

<div class="no-row-height column-margin column-container"><div class="margin-aside">
<p>We‚Äôve used the variance properties</p>
<p><span class="math display">\[
\begin{aligned}
    \mathrm{Var}[aX] = a^2\mathrm{Var}[X]&amp; \\
    \mathrm{Var}[X + b] = \mathrm{Var}[X]&amp;\;
\end{aligned}
\]</span></p>
<p>for constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.</p>
</div></div></section>
<section id="gaussian-noise" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="gaussian-noise">Gaussian Noise</h2>
<p>With the Gaussian distribution in mind, let‚Äôs now return to our signal-plus-noise paradigm:</p>
<p><span class="math display">\[
\begin{aligned}
    y_i = f(x_i) + \epsilon_i\;,
\end{aligned}
\]</span></p>
<p>If we assume that <span class="math inline">\(\epsilon_i \sim \mathcal{N}(0, \sigma^2)\)</span> (i.e., that the noise terms are independent and identically distributed Gaussian random variables with mean zero and standard deviation <span class="math inline">\(\sigma\)</span>), then we can use the translation property of the Gaussian distribution (<a href="#thm-translation" class="quarto-xref">Theorem&nbsp;<span>1.1</span></a>) to deduce that</p>
<p><span id="eq-normal-noise"><span class="math display">\[
\begin{aligned}
    y_i &amp;\sim \mathcal{N}(f(x_i), \sigma^2)\;.
\end{aligned}
\tag{1.2}\]</span></span></p>
<div class="page-columns page-full"><p>So, we are modeling each data point <span class="math inline">\(y_i\)</span> as a Gaussian random variable whose mean is given by the underlying signal <span class="math inline">\(f(x_i)\)</span>, and whose standard deviation is given by the noise level <span class="math inline">\(\sigma\)</span>. This modeling approach is important enough to merit a name: </p><div class="no-row-height column-margin column-container"><span class="margin-aside">Technically, the model described below is an <em>additive</em> Gaussian model, but we won‚Äôt worry about non-additive models here and therefore won‚Äôt bother repeating ‚Äúadditive.‚Äù</span></div></div>
<div id="def-gaussian-model" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.2 (Gaussian Model)</strong></span> The model</p>
<p><span id="eq-gaussian-model"><span class="math display">\[
\begin{aligned}
    y &amp;= f(x_i) + \epsilon_i \\    
    \epsilon_i &amp;\sim \mathcal{N}(0, \sigma^2)
\end{aligned}
\tag{1.3}\]</span></span></p>
<p>is called a <em>Gaussian data generating model</em>.</p>
</div>
<p>Often, as illustrated so far in these notes, we choose <span class="math inline">\(f\)</span> to be a linear function of <span class="math inline">\(x_i\)</span>:</p>
<div id="def-linear-gaussian-model" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.3 (Linear-Gaussian Model)</strong></span> A Gaussian model in which</p>
<p><span id="eq-linear-signal-1d"><span class="math display">\[
\begin{aligned}
    f(x_i) = w_0 + w_1 x_i   
\end{aligned}
\tag{1.4}\]</span></span></p>
<p>for parameters <span class="math inline">\(w_0, w_1 \in \mathbb{R}\)</span> is called a (1-dimensional) <em>linear-Gaussian model</em>.</p>
</div>
<p>Here‚Äôs a schematic picture of the linear-Gaussian model.</p>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># from https://stackoverflow.com/questions/47597119/plot-a-vertical-normal-distribution-in-python</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> draw_gaussian_at(support, sd<span class="op">=</span><span class="fl">1.0</span>, height<span class="op">=</span><span class="fl">1.0</span>, </span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>        xpos<span class="op">=</span><span class="fl">0.0</span>, ypos<span class="op">=</span><span class="fl">0.0</span>, ax<span class="op">=</span><span class="va">None</span>, <span class="op">**</span>kwargs):</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> ax <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>        ax <span class="op">=</span> plt.gca()</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    gaussian <span class="op">=</span> torch.exp((<span class="op">-</span>support <span class="op">**</span> <span class="fl">2.0</span>) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> sd <span class="op">**</span> <span class="fl">2.0</span>))</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    gaussian <span class="op">/=</span> gaussian.<span class="bu">max</span>()</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    gaussian <span class="op">*=</span> height</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ax.plot(gaussian <span class="op">+</span> xpos, support <span class="op">+</span> ypos, <span class="op">**</span>kwargs)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>support <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">1000</span>)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>ax.plot(x, signal, color <span class="op">=</span> <span class="st">"black"</span>, linestyle <span class="op">=</span> <span class="st">"--"</span>, label <span class="op">=</span> <span class="vs">r"Signal: </span><span class="dv">$</span><span class="vs">f</span><span class="kw">(</span><span class="vs">x_i</span><span class="kw">)</span><span class="vs"> = 2x_i </span><span class="op">+</span><span class="vs"> 1</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> each <span class="kw">in</span> x:</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    draw_gaussian_at(support, sd<span class="op">=</span><span class="dv">3</span>, height<span class="op">=</span><span class="fl">0.4</span>, xpos<span class="op">=</span>each, ypos<span class="op">=</span><span class="fl">2.0</span> <span class="op">*</span> each <span class="op">+</span> <span class="fl">1.0</span>, ax<span class="op">=</span>ax, color<span class="op">=</span><span class="st">'C0'</span>, alpha<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>ax.scatter(x.numpy(), y.numpy(), <span class="op">**</span>scatterplot_kwargs)</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"</span><span class="dv">$</span><span class="vs">x</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"</span><span class="dv">$</span><span class="vs">y</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Data points modeled as Gaussians around the signal"</span>)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div id="fig-gaussian-trend" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-gaussian-trend-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="9a15d051" class="cell" data-execution_count="12">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02-signal-noise_files/figure-html/cell-12-output-1.png" class="figure-img" width="585" height="442"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-gaussian-trend-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.7: Illustration of the Gaussian noise model <a href="#eq-normal-noise" class="quarto-xref">Equation&nbsp;<span>1.2</span></a>, showing data points (black circles) as samples from Gaussian distributions (blue curves) centered on the signal (dashed line).
</figcaption>
</figure>
</div>
<p>We are modeling <span class="math inline">\(y_i\)</span> as a noisy sample from a Gaussian distribution centered at the signal value <span class="math inline">\(f(x_i)\)</span>, with noise level <span class="math inline">\(\sigma\)</span> controlling how spread out the distribution is. A larger value of <span class="math inline">\(\sigma\)</span> means that the observed data points <span class="math inline">\(y_i\)</span> will tend to be further away from the signal <span class="math inline">\(f(x_i)\)</span>, while a smaller value of <span class="math inline">\(\sigma\)</span> means that the data points will tend to be closer to the signal. Larger values of the noise level <span class="math inline">\(\sigma\)</span> create noisier data sets where it is more difficult to discern the underlying signal.</p>
<section id="data-generating-distributions" class="level3">
<h3 class="anchored" data-anchor-id="data-generating-distributions">Data Generating Distributions</h3>
<p>One of the primary reasons to define models like <a href="#eq-normal-noise" class="quarto-xref">Equation&nbsp;<span>1.2</span></a> is that they give us principled ways for thinking about what our models should learn ‚Äì the signal ‚Äì and what they should ignore ‚Äì the noise. These models are also very helpful as models of <em>where the data comes from</em> ‚Äì that is, as <em>data generating distributions</em>.</p>
<div id="def-data-generating-distribution" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.4 (Data Generating Distribution)</strong></span> A <em>data generating distribution</em> (also called a <em>data generating model</em>) is a probabilistic model that describes how data points (especially targets <span class="math inline">\(y\)</span>) are generated in terms of random variables and their distributions.</p>
</div>
<p>The linear-Gaussian model is a simple example of a data generating model: it describes a recipe to simulate each target <span class="math inline">\(y_i\)</span> by first computing the signal value <span class="math inline">\(f(x_i)\)</span>, then sampling a noise term <span class="math inline">\(\epsilon_i\)</span> from a Gaussian distribution, and finally adding the two together to get <span class="math inline">\(y_i = f(x_i) + \epsilon_i\)</span>.</p>
<p>A very useful feature of data generating models is that they also give us tools to measure how well our learned signal fits observed data, via the concept of a <em>likelihood</em>.</p>
</section>
</section>
<section id="model-likelihood" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="model-likelihood">Model Likelihood</h2>
<p>Given a data-generating distribution, we can compute the <em>likelihood</em> of the observed data under that model. Recall that the PDF of a single data point <span class="math inline">\(y_i\)</span> under the Gaussian noise model <a href="#eq-normal-noise" class="quarto-xref">Equation&nbsp;<span>1.2</span></a> with predictor value <span class="math inline">\(x_i\)</span> is given by the formula</p>
<p><span class="math display">\[
\begin{aligned}
    p_{y}(y_i; f(x_i), \sigma^2) = \frac{1}{\sigma \sqrt{2 \pi}} \exp\left(-\frac{(y_i - f(x_i))^2}{2\sigma^2}\right)\;.
\end{aligned}
\]</span></p>
<p>The likelihood of the complete data set is simply the product of the individual data point PDFs, evaluated at their corresponding observed values. The likelihood is a function of the predictors which we‚Äôll collect into a vector <span class="math inline">\(\mathbf{x}= (x_1,\ldots,x_n)^T\)</span>, the targets which we‚Äôll collect into a vector <span class="math inline">\(\mathbf{y}= (y_1,\ldots,y_n)^T\)</span>, and the parameters of the model (in this case, the function <span class="math inline">\(f\)</span> and the noise level <span class="math inline">\(\sigma\)</span>):</p>
<div id="def-gaussian-likelihood" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.5 (Gaussian Likelihood, Log-Likelihood)</strong></span> The <em>likelihood</em> of the observed data <span class="math inline">\((\mathbf{x}, \mathbf{y})\)</span> under a 1d Gaussian model with function <span class="math inline">\(f\)</span> and noise level <span class="math inline">\(\sigma\)</span> is given by:</p>
<p><span class="math display">\[
\begin{aligned}
    L(\mathbf{x}, \mathbf{y}; f, \sigma) = \prod_{i = 1}^n p_{y}(y_i; f(x_i), \sigma^2)
\end{aligned}
\]</span></p>
<p>The <em>log-likelihood</em> is the logarithm of the likelihood:</p>
<p><span id="eq-log-likelihood"><span class="math display">\[
\begin{aligned}
    \mathcal{L}(\mathbf{x}, \mathbf{y}; f, \sigma) = \log L(\mathbf{x}, \mathbf{y}; f, \sigma) = \sum_{i = 1}^n \log p_{y}(y_i; f(x_i), \sigma^2)\;.
\end{aligned}
\tag{1.5}\]</span></span></p>
</div>
<p>The log-likelihood <span class="math inline">\(\mathcal{L}\)</span> in <a href="#eq-log-likelihood" class="quarto-xref">Equation&nbsp;<span>1.5</span></a> is almost always the tool we work with in applied contexts ‚Äì it turns products into sums, which is very useful in computational practice.</p>
<section id="a-first-look-likelihood-maximization" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="a-first-look-likelihood-maximization">A First Look: Likelihood Maximization</h3>
<p>Let‚Äôs now finally fit a machine learning model! We‚Äôll assume that the data is sampled from the linear-Gaussian model specified by <a href="#eq-gaussian-model" class="quarto-xref">Equation&nbsp;<span>1.3</span></a> and <a href="#eq-linear-signal-1d" class="quarto-xref">Equation&nbsp;<span>1.4</span></a>, and try to fit the parameters <span class="math inline">\(w_0, w_1\)</span> of the function <span class="math inline">\(f\)</span> to maximize the likelihood of the observed data. For now, we‚Äôll do this simply by choosing the combination of parameters that achieves the best likelihood from among many candidates:</p>
<p>To see how the likelihood can give us a tool to assess model fit, we can compute the likelihood of our observed data for different choices of the function <span class="math inline">\(f\)</span>. This can be done in a simple grid search over possible values of the parameters <span class="math inline">\(w_0, w_1\)</span> in the linear function <span class="math inline">\(f(x) = w_0 + w_1 x\)</span>.</p>
<div id="54581c6a" class="cell" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="annotated-cell-3"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-3-1"><a href="#annotated-cell-3-1" aria-hidden="true" tabindex="-1"></a>sig <span class="op">=</span> <span class="fl">3.0</span>  <span class="co"># assumed noise level</span></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-3-2" class="code-annotation-target"><a href="#annotated-cell-3-2" aria-hidden="true" tabindex="-1"></a>best_ll <span class="op">=</span> <span class="op">-</span><span class="bu">float</span>(<span class="st">'inf'</span>)</span>
<span id="annotated-cell-3-3"><a href="#annotated-cell-3-3" aria-hidden="true" tabindex="-1"></a>best_w <span class="op">=</span> <span class="va">None</span></span>
<span id="annotated-cell-3-4"><a href="#annotated-cell-3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-3-5"><a href="#annotated-cell-3-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> w0 <span class="kw">in</span> torch.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">20</span>):</span>
<span id="annotated-cell-3-6"><a href="#annotated-cell-3-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> w1 <span class="kw">in</span> torch.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">20</span>):</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-3-7" class="code-annotation-target"><a href="#annotated-cell-3-7" aria-hidden="true" tabindex="-1"></a>        f <span class="op">=</span> <span class="kw">lambda</span> x: w0 <span class="op">+</span> w1 <span class="op">*</span> x</span>
<span id="annotated-cell-3-8"><a href="#annotated-cell-3-8" aria-hidden="true" tabindex="-1"></a>        ll <span class="op">=</span> <span class="fl">0.0</span></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="3" onclick="event.preventDefault();">3</a><span id="annotated-cell-3-9" class="code-annotation-target"><a href="#annotated-cell-3-9" aria-hidden="true" tabindex="-1"></a>        ll <span class="op">+=</span> normal_pdf(y, f(x), sig).log().<span class="bu">sum</span>().item()</span>
<span id="annotated-cell-3-10"><a href="#annotated-cell-3-10" aria-hidden="true" tabindex="-1"></a></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="4" onclick="event.preventDefault();">4</a><span id="annotated-cell-3-11" class="code-annotation-target"><a href="#annotated-cell-3-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> ll <span class="op">&gt;</span> best_ll:</span>
<span id="annotated-cell-3-12"><a href="#annotated-cell-3-12" aria-hidden="true" tabindex="-1"></a>            best_ll <span class="op">=</span> ll</span>
<span id="annotated-cell-3-13"><a href="#annotated-cell-3-13" aria-hidden="true" tabindex="-1"></a>            best_w <span class="op">=</span> (w0.item(), w1.item())</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-annotation">
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-3" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="2" data-code-annotation="1">Initialize the best log-likelihood and best parameters.</span>
</dd>
<dt data-target-cell="annotated-cell-3" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="7" data-code-annotation="2">Define the predictor function <span class="math inline">\(f\)</span> with current parameters.</span>
</dd>
<dt data-target-cell="annotated-cell-3" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="9" data-code-annotation="3">Compute the data log-likelihood. The <code>normal_pdf</code> function computes the PDF values for all data points at once, which we then log and sum to get the log-likelihood.</span>
</dd>
<dt data-target-cell="annotated-cell-3" data-target-annotation="4">4</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="11" data-code-annotation="4">Update the best log-likelihood and parameters if the current log-likelihood is better.</span>
</dd>
</dl>
</div>
</div>
<p>Let‚Äôs check the predictor function <span class="math inline">\(f\)</span> we learned by heuristically maximizing the log-likelihood against the observed data:</p>
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize <span class="op">=</span> (<span class="dv">6</span>, <span class="dv">4</span>))</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>ax.scatter(x.numpy(), y.numpy(), <span class="op">**</span>scatterplot_kwargs)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>ax.plot(x.numpy(), (best_w[<span class="dv">0</span>] <span class="op">+</span> best_w[<span class="dv">1</span>] <span class="op">*</span> x).numpy(), color <span class="op">=</span> <span class="st">"firebrick"</span>, linestyle <span class="op">=</span> <span class="st">"--"</span>, label <span class="op">=</span> <span class="vs">r"</span><span class="dv">$</span><span class="vs">f</span><span class="kw">(</span><span class="vs">x</span><span class="kw">)</span><span class="vs"> = w_0 </span><span class="op">+</span><span class="vs"> w_1 x</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>ax.plot(x.numpy(), signal.numpy(), color <span class="op">=</span> <span class="st">"black"</span>, linestyle <span class="op">=</span> <span class="st">"--"</span>, label <span class="op">=</span> <span class="vs">r"True signal: </span><span class="dv">$</span><span class="vs">f</span><span class="kw">(</span><span class="vs">x</span><span class="kw">)</span><span class="vs"> = 2x </span><span class="op">+</span><span class="vs"> 1</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="vs">r"</span><span class="dv">$</span><span class="vs">x</span><span class="dv">$</span><span class="vs">"</span>, ylabel <span class="op">=</span> <span class="vs">r"</span><span class="dv">$</span><span class="vs">y</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="vs">fr"Best LL: </span><span class="sc">{</span>best_ll<span class="sc">:.2f}</span><span class="vs">   </span><span class="dv">$</span><span class="vs">w_1 = </span><span class="sc">{</span>best_w[<span class="dv">1</span>]<span class="sc">:.2f}</span><span class="vs">, w_0 = </span><span class="sc">{</span>best_w[<span class="dv">0</span>]<span class="sc">:.2f}</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div id="fig-likelihood-fit" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-likelihood-fit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div id="8afa601c" class="cell" data-execution_count="14">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02-signal-noise_files/figure-html/cell-14-output-1.png" class="figure-img" width="503" height="369"></p>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-likelihood-fit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.8: Comparison of the true signal (black dashed line) and the model fit by maximizing the likelihood (red dashed line).
</figcaption>
</figure>
</div>
<p>The model we selected via our maximum likelihood grid-search agrees relatively closely with the true underlying signal.</p>
<p>Soon, we‚Äôll learn how to use the likelihood as a method to learn the function <span class="math inline">\(f\)</span> more systematically from data.</p>


</section>
</section>

<p><br> <br> <span style="color:grey;">¬© Phil Chodrow, 2025</span></p></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../index.html" class="pagination-link" aria-label="Welcome">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Welcome</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/03-maximum-likelihood.html" class="pagination-link" aria-label="Maximum Likelihood Estimation and Gradients">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Maximum Likelihood Estimation and Gradients</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>