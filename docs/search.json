[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Welcome\nThis is a set of notes developed for an undergraduate course in machine learning. The target audience for these notes are undergraduates in computer science who have completed first courses in linear algebra and discrete mathematics. These notes draw on many sources, but are somewhat distinctive in the following ways:\n© Phil Chodrow, 2025",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#pedagogical-features",
    "href": "index.html#pedagogical-features",
    "title": "Machine Learning",
    "section": "Pedagogical Features",
    "text": "Pedagogical Features\nThese notes are explicitly designed for undergraduate instruction in computer science. For this reason:\n\nComputational examples are integrated into the text and shown throughout.\nLive versions of lecture notes are supplied as downloadable Jupyter Notebooks, with certain code components removed. The purpose is to facilitate live-coding in lectures.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#use-and-reuse",
    "href": "index.html#use-and-reuse",
    "title": "Machine Learning",
    "section": "Use and Reuse",
    "text": "Use and Reuse\nThese notes were written by Phil Chodrow for the course CSCI 0451: Machine Learning at Middlebury College. All are welcome to use them for educational purposes. Attribution is appreciated but not expected.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#source-texts",
    "href": "index.html#source-texts",
    "title": "Machine Learning",
    "section": "Source Texts",
    "text": "Source Texts\nThese notes draw on several source texts, most of which are available for free online. These are:\n\nChristopher M. Bishop and Bishop (2023) is the primary influence for the development of technical content.\nHardt and Recht (2022) has been a very helpful guiding influence for content on decision-theory and automated decision systems.\nA Course in Machine Learning by Hal Daumé III is an accessible introduction to many of the topics and serves as a useful source of supplementary readings.\n\nAdditional useful readings:\n\nAbu-Mostafa, Magdon-Ismail, and Lin (2012): Learning From Data: A Short Course\nBarocas, Hardt, and Narayanan (2023) is an advanced text on questions of fairness in automated decision-making for readers who have some background in probability theory.\nChristopher M. Bishop (2006) and Murphy (2022) are advanced texts which are most suitable for advanced readers who have already taken at least one course in probability theory.\n\nDeisenroth, Faisal, and Ong (2020) and Kroese et al. (2020) are useful readings focusing on some of the mathematical fundamentals.\nZhang, Lipton, and Li (2023) tells a helpful story of the fundamentals of deep learning.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Machine Learning",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis site was generated using the Quarto publishing system. It is hosted on GitHub and published via GitHub Pages.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Machine Learning",
    "section": "References",
    "text": "References\n\n\n\n\nAbu-Mostafa, Yaser S., Malik Magdon-Ismail, and Hsuan-Tien Lin. 2012. Learning from Data: A Short Course. S.l. https://amlbook.com/.\n\n\nBarocas, Solon, Moritz Hardt, and Arvind Narayanan. 2023. Fairness and Machine Learning: Limitations and Opportunities. Cambridge, Massachusetts: The MIT Press. https://fairmlbook.org/pdf/fairmlbook.pdf.\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Information Science and Statistics. New York: Springer. https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf.\n\n\nBishop, Christopher M, and Hugh Bishop. 2023. Deep Learning: Foundations and Concepts. Springer Nature.\n\n\nDeisenroth, Marc Peter, A. Aldo Faisal, and Cheng Soon Ong. 2020. Mathematics for Machine Learning. Cambridge, UK New York, NY: Cambridge University Press. https://mml-book.github.io/book/mml-book.pdf.\n\n\nHardt, Moritz, and Benjamin Recht. 2022. Patterns, Predictions, and Actions. Princeton University Press. https://mlstory.org/pdf/patterns.pdf.\n\n\nKroese, Dirk P., Zdravko I. Botev, Thomas Taimre, and Radislav Vaisman. 2020. Data Science and Machine Learning: Mathematical and Statistical Methods. Chapman & Hall/CRC Machine Learning & Pattern Recognition Series. Boca Raton London New York: CRC Press, Taylor & Francis Group.\n\n\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An Introduction. Adaptive Computation and Machine Learning Series. Cambridge, Massachusetts: The MIT Press. https://probml.github.io/pml-book/book1.html.\n\n\nZhang, Aston, Zachary Lipton, and Mu Li. 2023. Dive into Deep Learning. Cambridge, UK: Cambridge University Press.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "chapters/02-signal-noise.html",
    "href": "chapters/02-signal-noise.html",
    "title": "1  Data = Signal + Noise",
    "section": "",
    "text": "Introduction: Data, Signal, and Noise\nOpen the live notebook in Google Colab.\nIn these notes, we’ll expand on the following idea:\nLet’s start by building up a simple data set that contains signal and noise:\nCode\nimport torch \nfrom matplotlib import pyplot as plt\nscatterplot_kwargs = dict(color = \"black\", label = \"data\", facecolors = \"none\", s = 40, alpha = 0.6)\nn_points = 20\nx = torch.linspace(0, 10, n_points)\nsignal = 2.0 * x + 1.0  # underlying pattern (signal)\nnoise = torch.randn(n_points) * 3.0  # random noise\n\ny = signal + noise\nWe can think of this data as consisting of two components: a signal expressed by a relationship \\(y \\approx f(x)\\) (in this case \\(f(x) = 2x + 1\\)), and some noise that partially obscures this relationship. Schematically, we can write this relationship as:\n\\[\n\\begin{aligned}\n    y_i = f(x_i) + \\epsilon_i\\;,\n\\end{aligned}\n\\tag{1.1}\\]\nwhich says that the \\(i\\)th value of the target is equal to some function \\(f(x_i)\\) of the input variable \\(x_i\\), plus some random noise term \\(\\epsilon_i\\).\n© Phil Chodrow, 2025",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data = Signal + Noise</span>"
    ]
  },
  {
    "objectID": "chapters/02-signal-noise.html#introduction-data-signal-and-noise",
    "href": "chapters/02-signal-noise.html#introduction-data-signal-and-noise",
    "title": "1  Data = Signal + Noise",
    "section": "",
    "text": "Machine learning is the science and practice of building algorithms that distinguish between signal and noise in real-world data.\n\n\n\n\n\nCode\n# Plot residual segments\nfig, ax = plt.subplots(figsize = (6, 4))\nax.scatter(x.numpy(), y.numpy(), **scatterplot_kwargs)\nax.plot(x.numpy(), signal.numpy(), color = \"black\", linestyle = \"--\", label = r\"Signal: $f(x_i) = 2x_i + 1$\")\nfor i in range(n_points):\n    if i == 0: \n        ax.plot([x[i].item(), x[i].item()], [signal[i].item(), y[i].item()], color = \"red\", alpha = 0.3, linewidth = 0.8, label = r\"Noise: $\\epsilon_i$\")\n    else: \n        ax.plot([x[i].item(), x[i].item()], [signal[i].item(), y[i].item()], color = \"red\", alpha = 0.3, linewidth = 0.8)\nax.set(xlabel = r\"$x$\", ylabel = r\"$y$\")\nplt.legend()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.1: An illustrative decomposition of a data set (points) into a hypothesized underlying signal (dashed line) and noise (red segments).",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data = Signal + Noise</span>"
    ]
  },
  {
    "objectID": "chapters/02-signal-noise.html#overfitting",
    "href": "chapters/02-signal-noise.html#overfitting",
    "title": "1  Data = Signal + Noise",
    "section": "Overfitting",
    "text": "Overfitting\nIt’s important to emphasize here that the thing we want to learn is not the individual targets \\(y_i\\), but rather the underlying function \\(f(x)\\). To see why, let’s consider an example of what goes wrong if we try to learn the targets \\(y_i\\) exactly. This is called interpolation, and is illustrated by Figure 1.2:\n\nCode\nfrom scipy import interpolate\n# Create an interpolating function\nf_interp = interpolate.interp1d(x.numpy(), y.numpy(), kind='cubic')\nx_dense = torch.linspace(0, 10, 100)\ny_interp = f_interp(x_dense.numpy())\n\nfig, ax = plt.subplots(figsize = (6, 4))\nax.scatter(x.numpy(), y.numpy(), **scatterplot_kwargs)\nax.plot(x_dense.numpy(), y_interp, color = \"red\", linestyle = \"--\", label = \"interpolating fit\", zorder = -10)\nax.set(xlabel = r\"$x$\", ylabel = r\"$y$\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.2: A function which exactly interpolates the data, perfectly fitting both signal and noise.\n\n\n\nThe problem with interpolation is that while we have perfectly fit our training data, we have not learned the underlying signal \\(f(x)\\). If we were to generate new data with the same signal but with different noise, we would likely find that our interpolating function doesn’t actually make very good predictions about that data at all: many of its bends and wiggles don’t have any correspondence to features in the new data.\n\nCode\n# Generate new data\ny_new = 2.0 * x_dense + 1.0 + torch.randn(100) * 3.0\nfig, ax = plt.subplots(figsize = (6, 4))\nax.scatter(x_dense.numpy(), y_new.numpy(), **scatterplot_kwargs)\nax.plot(x_dense.numpy(), y_interp, color = \"red\", linestyle = \"--\", label = \"interpolating fit\", zorder = -10)\nax.set(xlabel = r\"$x$\", ylabel = r\"$y$\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.3: New data plotted alongside the interpolating fit from the previous figure.\n\n\n\nWe observe a number of irrelevant fluctuations in the interpolating fit that do not correspond to the underlying pattern. This phenomenon is called overfitting: by trying to fit the noise in our training data, we have failed to learn the true signal. We’ll learn how to quantify overfitting once we begin to study measures of model quality.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data = Signal + Noise</span>"
    ]
  },
  {
    "objectID": "chapters/02-signal-noise.html#modeling-the-noise",
    "href": "chapters/02-signal-noise.html#modeling-the-noise",
    "title": "1  Data = Signal + Noise",
    "section": "Modeling the Noise",
    "text": "Modeling the Noise\nIf we want to learn the signal \\(f\\) in Equation 1.1, it’s helpful to learn how to talk mathematically about the noise term \\(\\epsilon_i\\). To do this, we need to step into the language of probability theory. Our standing assumption will be that the noise terms \\(\\epsilon_i\\) are random variables drawn independently and identically-distributed from some probability distribution. This means that each time we observe a new data point, we get a different value of \\(\\epsilon_i\\) drawn from the same distribution, without any influence from other values of \\(\\epsilon_j\\) for \\(j \\neq i\\).\nThe noise distribution we will usually consider is the Gaussian distribution:\n\n\n\n\n\n\n\n\n\nFigure 1.4: The probability that a Gaussian random variable lies between two values \\(a\\) and \\(b\\) is given by the area under its PDF between those values.\n\n\n\n\n\nDefinition 1.1 (Gaussian (Normal) Distribution) A random variable \\(\\epsilon\\) has a Gaussian distribution with parameters mean \\(\\mu\\) and standard deviation \\(\\sigma\\) if the probability that \\(\\epsilon\\) has a value between \\(a\\) and \\(b\\) is given by:\n\\[\n\\begin{aligned}\n    \\mathbb{P}(a \\leq \\epsilon \\leq b) = \\int_a^b p_\\epsilon(x;\\mu, \\sigma) \\, dx\\;,\n\\end{aligned}\n\\]\nwhere\n\\[\n\\begin{aligned}\np_\\epsilon(x;\\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\\;.\n\\end{aligned}\n\\]\nThe function \\(p_\\epsilon(x;\\mu, \\sigma)\\) is called the probability density function (PDF) of the Gaussian distribution. We use the shorthand notation \\(\\epsilon \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) to indicate that the random variable \\(\\epsilon\\) has a Gaussian distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\nGaussian distributions are often called normal distributions in the statistics literature.\n\nHere’s a simple vectorized implementation of the Gaussian PDF, which can be evaluated on a PyTorch tensor of inputs:\n\ndef gaussian_pdf(x, mu, sigma):\n    return 1 / (sigma * (2 * torch.pi)**0.5) * torch.exp(-0.5 * ((x - mu) / sigma) ** 2)\n\ngaussian_pdf(torch.tensor([0.0, 1.0, 2.0]), mu=0.0, sigma=1.0)\n\ntensor([0.3989, 0.2420, 0.0540])\n\n\nThe two parameters of the Gaussian distribution describe its “shape:” The mean \\(\\mu\\) indicates the “center” of the distribution, while the standard deviation \\(\\sigma\\) indicates how “spread out” the distribution is.\n\nCode\nfig, ax = plt.subplots(figsize = (6, 4))\n\nfor mu, sigma in [(0, 1), (0, 2), (2, 1)]:\n    x_vals = torch.linspace(mu - 4*sigma, mu + 4*sigma, 100)\n    pdf_vals = gaussian_pdf(x_vals, mu, sigma)\n    ax.plot(x_vals.numpy(), pdf_vals.numpy(), label=f'μ={mu}, σ={sigma}')\n\nax.set_title('Gaussian Distribution PDFs')\nax.set_xlabel('x')\nax.set_ylabel('Probability Density')\nax.legend()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.5: Gaussian distribution PDFs for different parameter values.\n\n\n\n“Gaussian noise” refers to random variables drawn from a Gaussian distribution. We can generate Gaussian noise from a given Gaussian distribution using many functions. We’ll focus on PyTorch’s implementation (given through torch.normal), which allows us to generate tensors of Gaussian noise easily:\n\nmu = 0.0\nsigma = 1.0\nepsilon = torch.normal(mu, sigma, size=(10,))\nepsilon\n\ntensor([ 0.8567, -0.2712, -0.6491,  1.4401,  0.9145, -0.3152,  0.3786, -0.5821,\n         0.2349, -1.6239])\n\n\nIf we take many samples of Gaussian noise and plot a histogram of their values, then we’ll find (via the law of large numbers) that the histogram approximates the PDF of the Gaussian distribution we sampled from, as illustrated in Figure 1.6:\n\n\n\n\n\n\n\n\n\nFigure 1.6: Histogram of samples from a Gaussian distribution compared to its PDF.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data = Signal + Noise</span>"
    ]
  },
  {
    "objectID": "chapters/02-signal-noise.html#properties-of-the-gaussian-distribution",
    "href": "chapters/02-signal-noise.html#properties-of-the-gaussian-distribution",
    "title": "1  Data = Signal + Noise",
    "section": "Properties of the Gaussian Distribution",
    "text": "Properties of the Gaussian Distribution\nThe Gaussian distribution has a number of useful properties for modeling noise in data. Here are a few:\n\nTheorem 1.1 (Translation) If \\(\\epsilon\\) is normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then the random variable \\(\\epsilon + c\\) (where \\(c\\) is a constant) is normally distributed with mean \\(\\mu + c\\) and standard deviation \\(\\sigma\\). In other words, if \\(\\epsilon \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), then \\(\\epsilon + c \\sim \\mathcal{N}(\\mu + c, \\sigma^2)\\).\n\n\nTheorem 1.2 (Scaling) If \\(\\epsilon\\) is normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then the random variable \\(a \\epsilon\\) (where \\(a\\) is a constant) is normally distributed with mean \\(a \\mu\\) and standard deviation \\(|a| \\sigma\\). In other words, if \\(\\epsilon \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), then \\(a \\epsilon \\sim \\mathcal{N}(a \\mu, (a \\sigma)^2)\\).\n\nThe expectation or mean of a continuous-valued random variable \\(\\epsilon\\) with probability density function \\(p_\\epsilon(x)\\) is defined as:\n\\[\n\\begin{aligned}\n    \\mathbb{E}[\\epsilon] = \\int_{-\\infty}^{\\infty} x \\, p_\\epsilon(x) \\, dx\\;.\n\\end{aligned}\n\\]\nThe variance of a continuous-valued random variable \\(\\epsilon\\) with probability density function \\(p_\\epsilon(x)\\) is defined as:\n\\[\n\\begin{aligned}\n    \\text{Var}(\\epsilon) = \\mathbb{E}[(\\epsilon - \\mathbb{E}[\\epsilon])^2] = \\int_{-\\infty}^{\\infty} (x - \\mathbb{E}[\\epsilon])^2 \\, p_\\epsilon(x) \\, dx\\;.\n\\end{aligned}\n\\]\nWe can interpret the variance as a measure of how far away \\(\\epsilon\\) “usually, on average” lies from its mean value.\n\nTheorem 1.3 (Mean and Variance of the Gaussian) For a normally distributed random variable with mean \\(\\mu\\) and standard deviation \\(\\sigma\\):\n\\[\n\\begin{aligned}\n    \\mathbb{E}[\\epsilon] &= \\mu \\\\\n    \\mathrm{Var}[\\epsilon] &= \\sigma^2.\n\\end{aligned}\n\\]\n\nThese two properties can be proven using some integration tricks which are beyond our scope here.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data = Signal + Noise</span>"
    ]
  },
  {
    "objectID": "chapters/02-signal-noise.html#the-standard-gaussian",
    "href": "chapters/02-signal-noise.html#the-standard-gaussian",
    "title": "1  Data = Signal + Noise",
    "section": "The Standard Gaussian",
    "text": "The Standard Gaussian\nThe standard Gaussian distribution is the Gaussian with mean zero and standard deviation one: \\(\\mathcal{N}(0, 1)\\). We often use the symbol \\(Z\\) to Due to the properties above, we can make any Gaussian random variable from a standard Gaussian: if \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), then, using Theorem 1.2 and Theorem 1.1, we can write \\(X\\) as\n\\[\n\\begin{aligned}\n    X \\sim \\sigma Z + \\mu\\;.\n\\end{aligned}\n\\]\nWe can check that this random variable has the correct mean and variance:\n\\[\n\\begin{aligned}\n    \\mathbb{E}[X] = \\mathbb{E}[\\sigma Z + \\mu] = \\sigma \\mathbb{E}[Z] + \\mu = \\sigma \\cdot 0 + \\mu = \\mu\\;,\n\\end{aligned}\n\\]\nwhere we’ve used linearity of expectation and the fact that \\(\\mathbb{E}[Z] = 0\\). To calculate the variance, we use the fact that \\(\\mathbb{E}[Z]= 0\\), again, so that\n\\[\n\\begin{aligned}\n    \\mathrm{Var}[X] = \\mathrm{Var}[\\sigma Z + \\mu] = \\mathrm{Var}[\\sigma Z] = \\sigma^2 \\mathrm{Var}[Z] = \\sigma^2 \\cdot 1 = \\sigma^2\\;.\n\\end{aligned}\n\\]\n\n\nWe’ve used the variance properties\n\\[\n\\begin{aligned}\n    \\mathrm{Var}[aX] = a^2\\mathrm{Var}[X]& \\\\\n    \\mathrm{Var}[X + b] = \\mathrm{Var}[X]&\\;\n\\end{aligned}\n\\]\nfor constants \\(a\\) and \\(b\\).\nThe translation and scaling properties mean that we can actually generate Gaussian noise with arbitrary mean and standard deviation without having to remember how to call torch.normal with the right parameters: we can just generate standard Gaussian noise via torch.randn. For example, we can generate 5 samples of Gaussian noise with mean 3 and standard deviation 2 as follows:\n\nmu = 3.0\nsigma = 2.0\nn_samples = 5\n\nnoise = sigma*torch.randn(n_samples) + mu\n\n# equivalent\nnoise_equiv = torch.normal(mu, sigma, size=(n_samples,))",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data = Signal + Noise</span>"
    ]
  },
  {
    "objectID": "chapters/02-signal-noise.html#gaussian-noise",
    "href": "chapters/02-signal-noise.html#gaussian-noise",
    "title": "1  Data = Signal + Noise",
    "section": "Gaussian Noise",
    "text": "Gaussian Noise\nWith the Gaussian distribution in mind, let’s now return to our signal-plus-noise paradigm:\n\\[\n\\begin{aligned}\n    y_i = f(x_i) + \\epsilon_i\\;,\n\\end{aligned}\n\\]\nIf we assume that \\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\) (i.e., that the noise terms are independent and identically distributed Gaussian random variables with mean zero and standard deviation \\(\\sigma\\)), then we can use the translation property of the Gaussian distribution (Theorem 1.1) to deduce that\n\\[\n\\begin{aligned}\n    y_i &\\sim \\mathcal{N}(f(x_i), \\sigma^2)\\;.\n\\end{aligned}\n\\tag{1.2}\\]\nSo, we are modeling each data point \\(y_i\\) as a Gaussian random variable whose mean is given by the underlying signal \\(f(x_i)\\), and whose standard deviation is given by the noise level \\(\\sigma\\). This modeling approach is important enough to merit a name: Technically, the model described below is an additive Gaussian model, but we won’t worry about non-additive models here and therefore won’t bother repeating “additive.”\n\nDefinition 1.2 (Gaussian Model) The model\n\\[\n\\begin{aligned}\n    y &= f(x_i) + \\epsilon_i \\\\    \n    \\epsilon_i &\\sim \\mathcal{N}(0, \\sigma^2)\n\\end{aligned}\n\\tag{1.3}\\]\nis called a Gaussian data generating model.\n\nOften, as illustrated so far in these notes, we choose \\(f\\) to be a linear function of \\(x_i\\):\n\nDefinition 1.3 (Linear-Gaussian Model) A Gaussian model in which\n\\[\n\\begin{aligned}\n    f(x_i) = w_0 + w_1 x_i   \n\\end{aligned}\n\\tag{1.4}\\]\nfor parameters \\(w_0, w_1 \\in \\mathbb{R}\\) is called a (1-dimensional) linear-Gaussian model. In this context we’ll let \\(\\mathbf{w}= (w_0, w_1)^\\top\\) denote the parameters of the model, and write \\(f_\\mathbf{w}(x_i)\\) to emphasize the dependence of the signal on the parameters.\n\nHere’s a schematic picture of the linear-Gaussian model.\n\nCode\n# from https://stackoverflow.com/questions/47597119/plot-a-vertical-normal-distribution-in-python\ndef draw_gaussian_at(support, sd=1.0, height=1.0, \n        xpos=0.0, ypos=0.0, ax=None, **kwargs):\n    if ax is None:\n        ax = plt.gca()\n    gaussian = torch.exp((-support ** 2.0) / (2 * sd ** 2.0))\n    gaussian /= gaussian.max()\n    gaussian *= height\n    return ax.plot(gaussian + xpos, support + ypos, **kwargs)\n    \nsupport = torch.linspace(-10, 10, 1000)\nfig, ax = plt.subplots()\n\nax.plot(x, signal, color = \"black\", linestyle = \"--\", label = r\"Signal: $f_{\\mathbf{w}}(x_i) = 2x_i + 1$\")\n\nfor each in x:\n    draw_gaussian_at(support, sd=3, height=0.4, xpos=each, ypos=2.0 * each + 1.0, ax=ax, color='C0', alpha=0.4)\n\nax.scatter(x.numpy(), y.numpy(), **scatterplot_kwargs)\n\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$y$\")\nplt.title(\"Data points modeled as Gaussians around the signal\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.7: Illustration of the Gaussian noise model Equation 1.2, showing data points (black circles) as samples from Gaussian distributions (blue curves) centered on the signal (dashed line).\n\n\n\nWe are modeling \\(y_i\\) as a noisy sample from a Gaussian distribution centered at the signal value \\(f_\\mathbf{w}(x_i)\\), with noise level \\(\\sigma\\) controlling how spread out the distribution is. A larger value of \\(\\sigma\\) means that the observed data points \\(y_i\\) will tend to be further away from the signal \\(f_\\mathbf{w}(x_i)\\), while a smaller value of \\(\\sigma\\) means that the data points will tend to be closer to the signal. Larger values of the noise level \\(\\sigma\\) create noisier data sets where it is more difficult to discern the underlying signal.\n\nData Generating Distributions\nOne of the primary reasons to define models like Equation 1.2 is that they give us principled ways for thinking about what our models should learn – the signal – and what they should ignore – the noise. These models are also very helpful as models of where the data comes from – that is, as data generating distributions.\n\nDefinition 1.4 (Data Generating Distribution) A data generating distribution (also called a data generating model) is a probabilistic model that describes how data points (especially targets \\(y\\)) are generated in terms of random variables and their distributions.\n\nThe linear-Gaussian model is a simple example of a data generating model: it describes a recipe to simulate each target \\(y_i\\) by first computing the signal value \\(f_\\mathbf{w}(x_i)\\), then sampling a noise term \\(\\epsilon_i\\) from a Gaussian distribution, and finally adding the two together to get \\(y_i = f_\\mathbf{w}(x_i) + \\epsilon_i\\).\nA very useful feature of data generating models is that they also give us tools to measure how well our learned signal fits observed data, via the concept of a likelihood.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data = Signal + Noise</span>"
    ]
  },
  {
    "objectID": "chapters/02-signal-noise.html#model-likelihood",
    "href": "chapters/02-signal-noise.html#model-likelihood",
    "title": "1  Data = Signal + Noise",
    "section": "Model Likelihood",
    "text": "Model Likelihood\nGiven a data-generating distribution, we can compute the likelihood of the observed data under that model. Recall that the PDF of a single data point \\(y_i\\) under the Gaussian noise model Equation 1.2 with predictor value \\(x_i\\) is given by the formula\n\\[\n\\begin{aligned}\n    p_{y}(y_i; f(x_i), \\sigma^2) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp\\left(-\\frac{(y_i - f(x_i))^2}{2\\sigma^2}\\right)\\;.\n\\end{aligned}\n\\]\nThe likelihood of the complete data set is simply the product of the individual data point PDFs, evaluated at their corresponding observed values. The likelihood is a function of the predictors which we’ll collect into a vector \\(\\mathbf{x}= (x_1,\\ldots,x_n)^T\\), the targets which we’ll collect into a vector \\(\\mathbf{y}= (y_1,\\ldots,y_n)^T\\), and the parameters of the model (in this case, the function \\(f\\) and the noise level \\(\\sigma\\)):\n\nDefinition 1.5 (Gaussian Likelihood, Log-Likelihood) The likelihood of the observed data \\((\\mathbf{x}, \\mathbf{y})\\) under a 1d Gaussian model with function \\(f\\) and noise level \\(\\sigma\\) is given by:\n\\[\n\\begin{aligned}\n    L(\\mathbf{x}, \\mathbf{y}; f) = \\prod_{i = 1}^n p_{y}(y_i; f(x_i), \\sigma^2)\n\\end{aligned}\n\\]\nThe log-likelihood is the logarithm of the likelihood:\n\\[\n\\begin{aligned}\n    \\mathcal{L}(\\mathbf{x}, \\mathbf{y}; f) = \\log L(\\mathbf{x}, \\mathbf{y}; f) = \\sum_{i = 1}^n \\log p_{y}(y_i; f(x_i), \\sigma^2)\\;.\n\\end{aligned}\n\\tag{1.5}\\]\n\nIn this definition we are ignoring the dependence of \\(L\\) and \\(\\mathcal{L}\\) on the noise level \\(\\sigma\\) – that’s something that is typically important in statistics contexts but not in predictive ML contexts.\nThe log-likelihood \\(\\mathcal{L}\\) in Equation 1.5 is almost always the tool we work with in applied contexts – it turns products into sums, which is very useful in computational practice.\n\nA First Look: Likelihood Maximization\nIn the context of the linear-Gaussian model specified by Equation 1.2 and Equation 1.4, the likelihood of the observed data is a function of the parameters \\(\\mathbf{w}= (w_0, w_1)^\\top\\) of the signal function \\(f\\):\n\\[\n\\begin{aligned}\n    L(\\mathbf{x}, \\mathbf{y}; \\mathbf{w}) &= \\prod_{i = 1}^n p_{y}(y_i; f_\\mathbf{w}(x_i), \\sigma^2)\\;. \\\\\n    f_\\mathbf{w}(x_i) &= w_0 + w_1 x_i\\;.\n\\end{aligned}\n\\]\nTo fit the linear-Gaussian model to data, we’ll use the method of maximum likelihood, which says that we should choose the parameters \\(\\mathbf{w}\\) to maximize the likelihood of the observed data under the model.\n\nDefinition 1.6 (Maximum Likelihood Estimation) Given a signal function \\(f_\\mathbf{w}\\) with some parameters \\(\\mathbf{w}\\) (e.g. \\(\\mathbf{w}= (w_0, w_1)\\) in Equation 1.4), the method of maximum likelihood says that we should choose the parameters \\(\\hat{\\mathbf{w}}\\) to maximize the likelihood of the observed data under the model:\n\\[\n\\begin{aligned}\n    \\hat{\\mathbf{w}} = \\mathop{\\mathrm{arg\\,max}}_{\\mathbf{w}} \\; L(\\mathbf{x}, \\mathbf{y}; \\mathbf{w})\\;.\n\\end{aligned}\n\\]\nConventionally (and for computational convenience), this problem is almost always solved by maximizing the log-likelihood, which is equivalent to minimizing the negative log-likelihood:\n\\[\n\\begin{aligned}\n    \\hat{\\mathbf{w}} = \\mathop{\\mathrm{arg\\,max}}_{\\mathbf{w}} \\; \\mathcal{L}(\\mathbf{x}, \\mathbf{y}; \\mathbf{w}) = \\mathop{\\mathrm{arg\\,min}}_{\\mathbf{w}} \\; -\\mathcal{L}(\\mathbf{x}, \\mathbf{y}; \\mathbf{w})\\;.\n\\end{aligned}\n\\]\n\nSo, we need to be able to evaluate \\(-\\mathcal{L}(\\mathbf{x}, \\mathbf{y}; \\mathbf{w})\\) as a function of \\(\\mathbf{w}\\) in order to solve this optimization problem. In case you’re wondering, for the Gaussian likelihood this works out to:\n\\[\n\\begin{aligned}\n    \\mathcal{L}(\\mathbf{x}, \\mathbf{y}; \\mathbf{w}) &= \\sum_{i = 1}^n \\log p_{y}(y_i; f_\\mathbf{w}(x_i), \\sigma^2) \\\\\n    &= \\sum_{i = 1}^n \\log \\left( \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp\\left(-\\frac{(y_i - f_\\mathbf{w}(x_i))^2}{2\\sigma^2}\\right) \\right) \\\\\n    &= \\sum_{i = 1}^n \\left( -\\log(\\sigma \\sqrt{2 \\pi}) - \\frac{(y_i - f_\\mathbf{w}(x_i))^2}{2\\sigma^2} \\right) \\\\\n    &= -n \\log(\\sigma \\sqrt{2 \\pi}) - \\frac{1}{2\\sigma^2} \\sum_{i = 1}^n (y_i - f_\\mathbf{w}(x_i))^2\\;.\n\\end{aligned}\n\\]\nFor this experiment, however, we actually don’t need a designated formula for the log-likelihood – we can just compute it directly from the definition using our gaussian_pdf function. To see how the log-likelihood can give us a tool to assess model fit, we can compute the likelihood of our observed data for different choices of the function \\(f\\). This can be done in a simple grid search over possible values of the parameters \\(w_0, w_1\\) in the linear function \\(f(x) = w_0 + w_1 x\\).\n\nsig = 3.0  # assumed noise level\n1best_ll = -float('inf')\nbest_w = None\n\nfor w0 in torch.linspace(-5, 5, 20):\n    for w1 in torch.linspace(-1, 3, 20):\n2        f = lambda x: w0 + w1 * x\n3        ll = gaussian_pdf(y, f(x), sig).log().sum()\n\n4        if ll &gt; best_ll:\n            best_ll = ll\n            best_w = (w0, w1)\n\n\n1\n\nInitialize the best log-likelihood and best parameters.\n\n2\n\nDefine the predictor function \\(f\\) with current parameters.\n\n3\n\nCompute the data log-likelihood. The gaussian_pdf function computes the PDF values for all data points at once, which we then log and sum to get the log-likelihood.\n\n4\n\nUpdate the best log-likelihood and parameters if the current log-likelihood is better.\n\n\n\n\nLet’s check the predictor function \\(f\\) we learned by heuristically maximizing the log-likelihood against the observed data:\n\nCode\nfig, ax = plt.subplots(figsize = (6, 4))\n\nax.scatter(x.numpy(), y.numpy(), **scatterplot_kwargs)\nax.plot(x.numpy(), (best_w[0] + best_w[1] * x).numpy(), color = \"firebrick\", linestyle = \"--\", label = r\"$f(x) = w_0 + w_1 x$\")\n\nax.plot(x.numpy(), signal.numpy(), color = \"black\", linestyle = \"--\", label = r\"True signal: $f(x) = 2x + 1$\")\nax.set(xlabel = r\"$x$\", ylabel = r\"$y$\")\nplt.legend()\nplt.title(fr\"Best LL: {best_ll:.2f}   $w_1 = {best_w[1]:.2f}, w_0 = {best_w[0]:.2f}$\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.8: Comparison of the true signal (black dashed line) and the model fit by maximizing the likelihood (red dashed line).\n\n\n\nThe model we selected via our maximum likelihood grid-search agrees relatively closely with the true underlying signal.\nSoon, we’ll learn how to use the likelihood as a method to learn the function \\(f\\) more systematically from data.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data = Signal + Noise</span>"
    ]
  },
  {
    "objectID": "chapters/03-maximum-likelihood.html",
    "href": "chapters/03-maximum-likelihood.html",
    "title": "2  Maximum Likelihood Estimation and Gradients",
    "section": "",
    "text": "Recap: Log-Likelihood of the Linear-Gaussian Model\nOpen the live notebook in Google Colab.\nLast time, we introduced the idea of modeling data as signal + noise, studied the Gaussian distribution as a model of noise, and introduced the linear-Gaussian model for prediction in the context of linear trends. We also derived the log-likelihood function for the linear-Gaussian model and introduced the idea that we could learn the signal of the data by maximizing the log-likelihood with respect to the model parameters. In this chapter, we’ll begin our study of how to maximize the likelihood systematically using tools from calculus.\n© Phil Chodrow, 2025",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Maximum Likelihood Estimation and Gradients</span>"
    ]
  },
  {
    "objectID": "chapters/03-maximum-likelihood.html#the-gradient-of-a-multivariate-function",
    "href": "chapters/03-maximum-likelihood.html#the-gradient-of-a-multivariate-function",
    "title": "2  Maximum Likelihood Estimation and Gradients",
    "section": "The Gradient of a Multivariate Function",
    "text": "The Gradient of a Multivariate Function\n\n\nAs you can study in courses dedicated to multivariable calculus, the existence of all of a function’s partial derivatives does not necessarily imply that the function is multivariate differentiable. In this course, we’ll exclusively treat functions which are indeed multivariate differentiable unless otherwise noted, and so this distinction will not be an issue for us.\n\nDefinition 2.1 Let \\(f:\\mathbb{R}^p\\rightarrow \\mathbb{R}\\) be a function which accepts a vector input \\(\\mathbf{w}=(w_1,\\ldots,w_p)^T\\in \\mathbb{R}^p\\) and returns a scalar output \\(f(\\mathbf{w})\\in \\mathbb{R}\\). The partial derivative of \\(f\\) with respect to the \\(j\\)-th coordinate \\(w_j\\) is defined as the limit\n\\[\n\\begin{aligned}\n    \\frac{\\partial f}{\\partial w_i} &= \\lim_{h \\rightarrow 0} \\frac{f(w_1,\\ldots,w_i + h, \\ldots w_p) - f(w_1,\\ldots,w_i, \\ldots w_p)}{h} \\\\\n    &= \\lim_{h \\rightarrow 0} \\frac{f(\\mathbf{w}+ h\\mathbf{e}_i) - f(\\mathbf{w})}{h}\\;,\n\\end{aligned}\n\\]\nwhere \\(\\mathbf{e}_i = (0,0,\\ldots,1,\\ldots,0,0)^T\\) is the \\(i\\)-th standard basis vector in \\(\\mathbb{R}^p\\), i.e., the vector with a 1 in the \\(i\\)-th position and 0’s elsewhere. If this limit does not exist, then the partial derivative is said to be undefined.\n\nJust like in single-variable calculus, it’s not usually convenient to work directly with the limit definition of the partial derivative. Instead we use the following heuristic:\n\nProposition 2.1 To compute \\(\\frac{\\partial f}{\\partial w_i}\\), treat all other variables \\(w_j\\) for \\(j\\neq i\\) as constants, and differentiate \\(f\\) with respect to \\(w_i\\) using the usual rules of single-variable calculus (power rule, product rule, chain rule, etc.).\n\n\nExercise 2.1 (Practice with Partial Derivatives) Let \\(f:\\mathbb{R}^3\\rightarrow \\mathbb{R}\\) be defined by \\(f(x,y,z) = x^2\\sin y + yz + z^3x\\). Compute \\(\\frac{\\partial f}{\\partial x}\\), \\(\\frac{\\partial f}{\\partial y}\\), and \\(\\frac{\\partial f}{\\partial z}\\).\n\n\n\n\n\n\n\nSolSolution for Exercise 2.1\n\n\n\n\n\nTo compute \\(\\frac{\\partial f}{\\partial x}\\), we treat \\(y\\) and \\(z\\) as constants, which yields\n\\[\n\\frac{\\partial f}{\\partial x} = 2x \\sin y + z^3\\;.\n\\]\nSimilarly, we can compute \\(\\frac{\\partial f}{\\partial y}\\) and \\(\\frac{\\partial f}{\\partial z}\\):\n\\[\n\\begin{align}\n    \\frac{\\partial f}{\\partial y} &= x^2 \\cos y + z \\\\\n    \\frac{\\partial f}{\\partial z} &= y + 3z^2 x\\;.    \n\\end{align}\n\\]\n\n\n\n\nDefinition 2.2 Let \\(f:\\mathbb{R}^p\\rightarrow \\mathbb{R}\\) be a differentiable function which accepts a vector input \\(\\mathbf{w}=(w_1,\\ldots,w_p)^T\\in \\mathbb{R}^p\\) and returns a scalar output \\(f(\\mathbf{w})\\in \\mathbb{R}\\). The gradient of \\(f\\) at \\(\\mathbf{w}\\), written \\(\\nabla f(\\mathbf{w}) \\in \\mathbb{R}^p\\), is the vector of partial derivatives\n\\[\n\\begin{align}\n    \\nabla f(\\mathbf{w}) &= \\begin{pmatrix}\n    \\frac{\\partial f}{\\partial w_1} \\\\\n    \\frac{\\partial f}{\\partial w_2} \\\\\n    \\vdots \\\\\n    \\frac{\\partial f}{\\partial w_p}\n    \\end{pmatrix}\\;.\n\\end{align}\n\\]\n\n\nExercise 2.2 (Writing Gradients) Write the gradient of the function in Exercise 2.1.\n\n\n\n\n\n\n\nSolSolution for Exercise 2.2\n\n\n\n\n\nIn the function from Exercise 2.1, the gradient is given by stacking the partial derivatives we computed into a single vector:\n\\[\n\\begin{aligned}\n    \\nabla f(x, y, z) =\n     \\begin{pmatrix}\n    \\frac{\\partial f}{\\partial x} \\\\\n    \\frac{\\partial f}{\\partial y} \\\\\n    \\frac{\\partial f}{\\partial z}\n    \\end{pmatrix} &=\n    \\begin{pmatrix}\n    2x \\sin y + z^3 \\\\\n    x^2 \\cos y + z \\\\\n    y + 3z^2 x  \n    \\end{pmatrix}\n    \\in \\mathbb{R}^3\\;.\n\\end{aligned}\n\\]\n\n\n\n\nExercise 2.3 Consider the mean-squared error function for a simple linear model with parameters \\(w_0\\) and \\(w_1\\):\n\\[\n\\begin{aligned}\n    R(\\mathbf{x}, \\mathbf{y}; w_0, w_1) = \\frac{1}{n} \\sum_{i = 1}^n (y_i - w_1x_i - w_0)^2\\;.\n\\end{aligned}\n\\]\nCompute the gradient \\(\\nabla R(\\mathbf{x}, \\mathbf{y}; w_0, w_1)\\) with respect to the parameters \\(w_0\\) and \\(w_1\\).\nNote: we’ll soon see that this function is closely related to the log-likelihood of the linear-Gaussian model.\n\n\n\n\n\n\n\nSolSolution for Exercise 2.3\n\n\n\n\n\nWe can compute the gradient by computing each partial derivative in turn:\n\\[\n\\begin{aligned}\n    \\frac{\\partial R}{\\partial w_0} &= \\frac{2}{n} (y_i - w_1x_i - w_0) \\\\\n    \\frac{\\partial R}{\\partial w_1} &= \\frac{-2}{n} x_i(y_i - w_1x_i - w_0) \\;,\n\\end{aligned}\n\\] where we’ve used the rules for derivatives. Stacking these into a vector gives\n\\[\n\\begin{aligned}\n    \\nabla R(\\mathbf{x}, \\mathbf{y}; w_0, w_1) = \\frac{2}{n} \\begin{pmatrix}\n    \\sum_{i=1}^n (y_i - w_1x_i - w_0) \\\\\n    \\sum_{i=1}^n x_i (y_i - w_1x_i - w_0)\n    \\end{pmatrix}\n\\end{aligned}\n\\]\n\n\n\n\nChecking Gradients with torch\nThe pytorch package, which we’ll use throughout this course, implements automatic differentiation. Automatic differentiation is an extraordinarily powerful tool which we’ll study later in the course. For now, we’ll just note that it provides a handy way to check calculations of derivatives and gradients. For example, we can use torch to check the gradient we computed in Exercise 2.2 as follows:\n\nimport torch\nx = torch.tensor([1.0, 2.0, 3.0])\n\n# function to differentiate\nf = lambda x: x[0]**2 * torch.sin(x[1]) + x[1]*x[2] + x[2]**3 * x[0]\n\n# compute the gradient by hand using the formula we derived\nour_grad = torch.tensor([\n    2 * x[0] * torch.sin(x[1]) + x[2]**3,\n    x[0]**2 * torch.cos(x[1]) + x[2],\n    x[1] + 3 * x[2]**2 * x[0]\n])\nprint(our_grad)\n\n# compute the gradient using automatic differentiation\n1x.requires_grad_()\ny = f(x)\n2y.backward()\n3print(x.grad)\n\n\n1\n\nFirst, we compute the value of the function we want to differentiate and store the result to a variable (in this case called y).\n\n2\n\nNext, we call the backward() method on y, which computes the gradient of y with respect to its inputs (in this case, the vector x) using automatic differentiation.\n\n3\n\nFinally, we can access the computed gradient via the grad attribute of the input tensor x.\n\n\n\n\ntensor([28.8186,  2.5839, 29.0000])\ntensor([28.8186,  2.5839, 29.0000])\n\n\nThe two approaches agree! As we grow comfortable with the calculus, we’ll begin to rely more on torch’s automatic differentiation capabilities to compute gradients for us.\n\n\nThe Gradient Points In the Direction of Greatest Increase\nAn important feature of the gradient is that it tells us the direction in which a small change in the function inputs \\(\\mathbf{w}\\) could produce the greatest increase in the function output \\(f(\\mathbf{w})\\). Here’s an example using the function from Exercise 2.3. torch makes it very easy to implement this function.\n\ndef MSE(x, y, w0, w1):\n    return -((y - (w1 * x + w0))**2).mean()\n\nWe first plot the function as a function of the parameters \\(w_0\\) and \\(w_1\\) and then we overlay arrows representing the gradients at various points in the \\((w_0, w_1)\\) space, with the gradients calculated via automatic differentiation in torch.\n\nCode\nfrom matplotlib import pyplot as plt\n\n# create the grid of (mu, sigma^2) values and the data\nw0_grid = torch.linspace(-1, 1, 100)\nw1_grid = torch.linspace(0.1, 2, 100)\nW0, W1 = torch.meshgrid(w0_grid, w1_grid, indexing='ij')\n\nx = torch.tensor([0.5, -1.0, 1.0, 0.7, 0.3])  # example data points\ny = torch.tensor([1.0, 0.0, 2.0, 1.5, 0.5])\n\nLL = torch.zeros(W0.shape)\nfor i in range(W0.shape[0]):\n    for j in range(W0.shape[1]):\n        LL[i, j] = MSE(x, y, W0[i, j], W1[i, j])\n\n\n# initialize the figure \nfig, ax = plt.subplots()\n\n# show the log-likelihood as a contour plot\nim = ax.contourf(LL.numpy(), levels=100, cmap='inferno', extent = [w1_grid[0], w1_grid[-1], w0_grid[0], w0_grid[-1]])\nax.contour(LL.numpy(), levels=100, colors='black', linewidths=0.5, extent = [w1_grid[0], w1_grid[-1], w0_grid[0], w0_grid[-1]])\n\nax.set_ylabel(r'$w_1$')\nax.set_xlabel(r'$w_0$')\n\n# compute and plot the gradients at various points\nfor w0, w1 in [( -0.5, 1.5), (0.0, 1.0), (0.5, 1.5), (.25, 0.5), (0.75, 1.0)]:\n    w0_tensor = torch.tensor(w0, requires_grad=True)\n    w1_tensor = torch.tensor(w1, requires_grad=True)\n    \n    ll = MSE(x, y, w0_tensor, w1_tensor)\n    ll.backward()\n    \n    grad_w0 = w0_tensor.grad.item()\n    grad_w1 = w1_tensor.grad.item()\n    \n    ax.quiver(w1, w0, grad_w1, grad_w0, color='black', scale=20, width=0.01)\n    ax.scatter(w1, w0, color='black', s=30)\n\nplt.colorbar(im)\nax.set_title('Gradients of the mean-squared error')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.1: Visualization of the gradients of the mean-squared error function with respect to the parameters \\(w_0\\) and \\(w_1\\). The background color indicates the value of the mean-squared error, with lighter colors representing higher values. Dotted curves give contours along which the function is constant. The black arrows represent the gradients at various points in the \\((w_0, w_1)\\) space, pointing in the direction of greatest increase of the mean-squared error function.\n\n\n\nTwo observations about Figure 2.1 are worth noting:\n\nThe gradient arrows always point uphill and are orthogonal (at right angles with) to the contour lines of the function.\nThe gradient arrows get smaller as we approach the maximum of the log-likelihood function, eventually becoming zero at the maximum itself.\n\nBoth of these features are possible to prove mathematically, although we won’t do so here.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Maximum Likelihood Estimation and Gradients</span>"
    ]
  },
  {
    "objectID": "chapters/03-maximum-likelihood.html#critical-points-and-local-extrema",
    "href": "chapters/03-maximum-likelihood.html#critical-points-and-local-extrema",
    "title": "2  Maximum Likelihood Estimation and Gradients",
    "section": "Critical Points and Local Extrema",
    "text": "Critical Points and Local Extrema\nOne way we can use gradients is by analytically computing the local extrema of a function: solve the equation \\(\\nabla \\mathcal{L}(\\mathbf{w}) = 0\\) for \\(\\mathbf{w}\\) to find critical points of the log-likelihood, and then check which of these points are local maxima.\nA critical point of a multivariate function is a point at which all of its partial derivatives are equal to zero. Critical points are candidates for local maxima or minima of the function, and so they are of interest when performing maximum-likelihood estimation by solving \\(\\nabla \\mathcal{L}(\\mathbf{w}) = 0\\).\n\nDefinition 2.3 (Critical Points of Multivariate Functions) A critical point of a differentiable function \\(f:\\mathbb{R}^p\\rightarrow \\mathbb{R}\\) is a point \\(\\mathbf{w}^* \\in \\mathbb{R}^p\\) such that \\(\\nabla f(\\mathbf{w}^*) = \\mathbf{0}\\) (the zero vector in \\(\\mathbb{R}^p\\)).\n\nAll critical points of a function can be identified by solving the system of equations \\(\\nabla f(\\mathbf{w}) = \\mathbf{0}\\). In a few rare cases, it’s possible to solve this system analytically to find all critical points.\n\n\nThe notation \\(\\lVert \\mathbf{v} \\rVert_2\\) refers to the Euclidean norm of \\(\\mathbf{v}\\), with formula\n\\[\n\\begin{aligned}\n    \\lVert \\mathbf{v} \\rVert_2 = \\sqrt{\\sum_{i = 1}^p v_i^2}\\;.\n\\end{aligned}\n\\]\n\nDefinition 2.4 (Local Minima and Maxima) A local minimum of a differentiable function \\(f:\\mathbb{R}^p\\rightarrow \\mathbb{R}\\) is a point \\(\\mathbf{w}^* \\in \\mathbb{R}^p\\) such that there exists some radius \\(r&gt;0\\) such that for all \\(\\mathbf{w}\\) with \\(\\|\\mathbf{w}- \\mathbf{w}^*\\|_2 &lt; r\\), we have \\(f(\\mathbf{w}) \\geq f(\\mathbf{w}^*)\\). A local maximum is defined similarly, with the inequality reversed: for all \\(\\mathbf{w}\\) with \\(\\|\\mathbf{w}- \\mathbf{w}^*\\|_2 &lt; r\\), we have \\(f(\\mathbf{w}) \\leq f(\\mathbf{w}^*)\\).\n\n\n\nThe Mild Conditions of Theorem 2.1 are that \\(f\\) is continuously differentiable in an open neighborhood around \\(\\mathbf{w}^*\\).\n\nTheorem 2.1 (Local Extrema are Critical Points) Under Mild Conditions*, if \\(\\mathbf{w}^*\\) is a local extremum (minimum or maximum) of a differentiable function \\(f:\\mathbb{R}^p\\rightarrow \\mathbb{R}\\), then \\(\\mathbf{w}^*\\) is a critical point of \\(f\\).\n\n\n\n\n\n\n\nNoteWe Always Minimize\n\n\n\nAlthough our motivating problem is still maximum likelihood estimation, it is conventional in the literature on statistics, machine learning, and optimization to always seek minima of a given function. This works because maximizing \\(\\mathcal{L}(\\mathbf{w})\\) is equivalent to minimizing \\(-\\mathcal{L}(\\mathbf{w})\\). Therefore, in the remainder of this chapter and in subsequent chapters, we will often refer to “minimizing the negative log-likelihood” rather than “maximizing the log-likelihood.” Perhaps confusingly, we’ll still refer to the result as the “maximum likelihood estimate” (MLE).\n\n\nTheorem 2.1 tells us that we can try to find the maximum likelihood estimate of a parameter vector \\(\\mathbf{w}\\) by solving the equation \\(\\nabla \\mathcal{L}(\\mathbf{w}) = \\mathbf{0}\\). In principle, we should then check that the critical points we find are indeed minima of \\(-\\mathcal{L}(\\mathbf{w})\\) rather than maxima or saddle points, which can sometimes be done using the multivariate second-derivative test. In practice, however, this second step is often skipped. Skipping the second-derivative test can be justified if it is known that \\(-\\mathcal{L}\\) is a convex function.\nEquipped with the concept of critical points, we are ready to find maximum likelihood estimates by solving the equation \\(\\nabla \\mathcal{L}(\\mathbf{w}) = \\mathbf{0}\\).\nBy convention, the maximum-likelihood estimate of a parameter is given a “hat” symbol, so we would write the MLE estimators we found above as \\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}^2\\).",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Maximum Likelihood Estimation and Gradients</span>"
    ]
  },
  {
    "objectID": "chapters/03-maximum-likelihood.html#revisiting-the-linear-gaussian-log-likelihood",
    "href": "chapters/03-maximum-likelihood.html#revisiting-the-linear-gaussian-log-likelihood",
    "title": "2  Maximum Likelihood Estimation and Gradients",
    "section": "Revisiting the Linear-Gaussian Log-Likelihood",
    "text": "Revisiting the Linear-Gaussian Log-Likelihood\nLet’s now consider the linear-Gaussian model from last chapter. In this model, we assume that each observed target variable \\(y_i\\) is sampled from a Gaussian distribution with mean equal to a linear function of the corresponding feature vector \\(x_i\\):\n\\[\n\\begin{aligned}\n    y_i &\\sim \\mathcal{N}(w_1 x_i + w_0, \\sigma^2)\\;,\n\\end{aligned}\n\\]\nTo find the maximum-likelihood estimates given a data set of pairs \\((x_i,y_i)\\) for \\(i=1,\\ldots,n\\), we need to compute the log-likelihood function for this model, which as per last chapter is\n\\[\n\\begin{aligned}\n    \\mathcal{L}(\\mathbf{x}, \\mathbf{y}; w_1, w_0) &= \\sum_{i = 1}^n \\log p_y(y_i;w_1x_i + w_0; \\sigma^2) \\\\\n    &= \\sum_{i = 1}^n \\log \\left( \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left( -\\frac{(y_i - (w_1 x_i + w_0))^2}{2\\sigma^2} \\right) \\right) \\\\\n    &= \\sum_{i = 1}^n \\left( -\\frac{1}{2} \\log(2\\pi \\sigma^2) - \\frac{(y_i - (w_1 x_i + w_0))^2}{2\\sigma^2} \\right) \\\\\n    &= \\underbrace{-\\frac{n}{2} \\log(2\\pi \\sigma^2)}_{C} - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - (w_1 x_i + w_0))^2 \\\\\n    &= C - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - w_1 x_i - w_0)^2 \\\\\n    &= C - \\frac{n}{2\\sigma^2} \\cdot \\frac{1}{n} \\sum_{i=1}^n (y_i - w_1 x_i - w_0)^2 \\\\\n    &= C - \\frac{n}{2\\sigma^2} R(\\mathbf{x}, \\mathbf{y}; w_0, w_1)\\;.\n\\end{aligned}\n\\tag{2.1}\\]\nWe’ve collected terms that do not depend on \\(w_0\\) or \\(w_1\\) into a constant term \\(C\\), and noticed that there’s a copy of the mean-squared error function \\(R(\\mathbf{x}, \\mathbf{y}; w_0, w_1) = \\frac{1}{n}\\sum_{i=1}^n (y_i - w_1 x_i - w_0)^2\\) from Exercise 2.3 appearing in the likelihood expression. Indeed, this term is the only that involves the parameters \\(w_0\\) and \\(w_1\\). This means:\n\nTo maximize the likelihood \\(\\mathcal{L}\\) with respect to \\(w_0\\) and \\(w_1\\), we can equivalently minimize the mean-squared error \\(R(\\mathbf{x}, \\mathbf{y}; w_0, w_1)\\).",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Maximum Likelihood Estimation and Gradients</span>"
    ]
  },
  {
    "objectID": "chapters/03-maximum-likelihood.html#a-first-look-gradient-descent-for-maximum-likelihood-estimation",
    "href": "chapters/03-maximum-likelihood.html#a-first-look-gradient-descent-for-maximum-likelihood-estimation",
    "title": "2  Maximum Likelihood Estimation and Gradients",
    "section": "A First Look: Gradient Descent for Maximum Likelihood Estimation",
    "text": "A First Look: Gradient Descent for Maximum Likelihood Estimation\nNow that we have tools to compute gradients, we can use these gradients to find maximum-likelihood estimates numerically using a gradient method. There are many kinds of gradient methods, and they all have in common a simple idea:\n\nDefinition 2.5 (Gradient Methods) A gradient method for optimizing a multivariate function \\(f:\\mathbb{R}^p\\rightarrow \\mathbb{R}\\) is an iterative algorithm which starts from an initial guess \\(\\mathbf{w}^{(0)} \\in \\mathbb{R}^p\\) and produces a sequence of estimates \\(\\mathbf{w}^{(1)}, \\mathbf{w}^{(2)}, \\ldots\\) by repeatedly updating the current estimate \\(\\mathbf{w}^{(t)}\\) in the direction of the negative gradient \\(-\\nabla f(\\mathbf{w}^{(t)})\\), or some approximation thereof.\n\nThe simplest gradient method is gradient descent with fixed learning rate:\n\nDefinition 2.6 (Gradient Descent) Gradient descent is an algorithm that iterates the update\n\\[\n\\begin{aligned}\n    \\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\alpha\\nabla f(\\mathbf{w}^{(t)})\\;,\n\\end{aligned}\n\\]\nwhere \\(\\alpha \\in \\mathbb{R}_{&gt;0}\\) is a fixed hyperparameter called the learning rate.\n\nLet’s use gradient descent to find maximum-likelihood estimates for the parameters of the linear-Gaussian model in a simple example. To visualize gradient descent, we’ll start by implementing a function for the linear-Gaussian log-likelihood in terms of the parameters \\(w_0\\) and \\(w_1\\):\nThen we’ll generate some synthetic data from a linear-Gaussian model with known parameters:\n\n# true parameters\nw0 = torch.tensor(0.0)\nw1 = torch.tensor(2.0)\nsigma2 = torch.tensor(1.0)\n\n# observed data\nx = torch.linspace(-2, 2, 101)\nsignal = w1 * x + w0\nnoise = torch.sqrt(sigma2) * torch.randn_like(x)\ny = signal + noise\n\n\nCode\nfig, ax = plt.subplots(figsize = (6, 4))\nax.scatter(x, y, color='steelblue', s=10)\nax.set_xlabel('Feature (x)')\nax.set_ylabel('Target (y)')\nt = ax.set_title('Observed Data from Linear-Gaussian Model')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.2: Sample data from a linear-Gaussian model with true parameters \\(w_0 = 0\\), \\(w_1 = 2\\), and \\(\\sigma^2 = 1\\).\n\n\n\nOur aim in maximum likelihood estimation is to find estimators \\(\\hat{w}_0\\) and \\(\\hat{w}_1\\) which maximize the log-likelihood of the observed data. As we saw in Equation 2.1, maximizing the log-likelihood is equivalent to minimizing the mean-squared error between the observed targets \\(y_i\\) and the linear predictions \\(w_1 x_i + w_0\\). Let’s go ahead and plot the mean-squared error:\n\nCode\nw0_grid = torch.linspace(-1, 1, 100)\nw1_grid = torch.linspace(1, 3, 100)\n\nW0, W1 = torch.meshgrid(w0_grid, w1_grid, indexing='ij')\n\nLL = torch.zeros(W0.shape)\n\nfor i in range(W0.shape[0]):\n    for j in range(W0.shape[1]):\n        LL[i, j] = -MSE(x, y, W0[i, j], W1[i, j])\n\n# visualize the log-likelihood surface\nfig, ax = plt.subplots(figsize=(6, 5))\n\nim = ax.contourf(LL.numpy(), levels=20, cmap='inferno_r', extent = [w1_grid[0], w1_grid[-1], w0_grid[0], w0_grid[-1]])\nax.contour(LL.numpy(), levels=20, colors='black', linewidths=0.5, extent = [w1_grid[0], w1_grid[-1], w0_grid[0], w0_grid[-1]])\nax.set_ylabel(r'Intercept ($w_0$)')\nax.set_xlabel(r'Slope ($w_1$)')\nplt.colorbar(im, ax=ax)\nax.set_title('Mean-squared error for linear-Gaussian model')\n\nax.scatter([w1], [w0], color='black', s=50, label='True Parameters', facecolors='white')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.3: Mean-squared error \\(R(\\mathbf{x}, \\mathbf{y}; w_0, w_1)\\) for the linear-Gaussian model as a function of the intercept \\(w_0\\) and slope \\(w_1\\) with the data shown in Figure 2.2. The dot indicates the location of the true parameters used to generate the data.\n\n\n\n\n\n\n\n\n\nQuestionQuestion\n\n\n\n\nWhy does this surface have only one minimum, and why is that minimum so close to the true parameters used to generate the data? Seems pretty convenient!\n\nIt is convenient! The uniqueness of the local minimum is due to a property called convexity that we’ll study later in the course. The closeness of the minimum to the true parameters is a consequence of the consistency property of maximum-likelihood estimators, which you can study in courses on statistical inference.\n\n\nThe gradient descent algorithm will start from an initial guess for the parameters \\((w_0, w_1)\\) and iteratively update this guess in the direction of the negative gradient of the negative log-likelihood.\n\nObject-Oriented API for ML Models\nHere we’ll introduce our the object-oriented API for machine learning models which we’ll use throughout this course. For this we need a model class which will represent the linear-Gaussian model and an optimizer class which will implement the gradient descent algorithm.\n\nModel Class\nThe primary responsibility of the model class is to store the weight parameters and to implement a method called forward which computes the model’s predictions (estimate of the signal) given an input \\(x\\). Here’s a simple implementation of a model class for 1d linear regression:\n\nclass LinearRegression1D:\n1    def __init__(self):\n        self.w0 = torch.tensor(1.0)\n        self.w1 = torch.tensor(1.0)\n    \n2    def forward(self, x):\n        return self.w1 * x + self.w0\n\n\n1\n\nInitialize the model with a guess for the parameters \\(w_0\\) and \\(w_1\\). Later, we’ll use just a single instance variable which holds a vector of weights.\n\n2\n\nMethod for computing the model’s predictions (estimate of the signal) given an input \\(x\\).\n\n\n\n\n\n\nOptimizer Class\nThe primary responsibility of the optimizer class is to implement the optimization algorithm of choice in the step method. If we aren’t using automatic differentiation, then the optimizer is also a good place to compute the gradients of the loss function with respect to the model parameters. Here’s a simple implementation of a gradient descent optimizer for 1d linear regression:\n\nclass GradientDescentOptimizer1D: \n\n    def __init__(self, model, lr=0.01):\n        self.model = model\n        self.lr = lr\n\n1    def grad_func(self, x, y):\n        n = x.shape[0]\n        w0_grad = -2/n * torch.sum(y - (self.model.w1 * x + self.model.w0))  \n        w1_grad = -2/n * torch.sum(x * (y - (self.model.w1 * x + self.model.w0)))  \n        return w0_grad, w1_grad\n\n    def step(self, x, y): \n        # compute the gradients\n3        w0_grad, w1_grad = self.grad_func(x, y)\n        # update the parameters\n        self.model.w0 = self.model.w0 - self.lr * w0_grad\n        self.model.w1 = self.model.w1 - self.lr * w1_grad\n\n\n1\n\nMethod for computing the gradients of the negative log-likelihood with respect to the parameters \\(w_0\\) and \\(w_1\\), which we did in Exercise 2.3.\n\n3\n\nUpdate the parameters by taking a step in the direction of the negative gradient, scaled by the learning rate.\n\n\n\n\n\n\nTraining Loop\nOnce we’ve implemented our two classes, the main “training loop” just requires us to repeatedly call the step method of the optimizer, which updates model.w0 and model.w1 at each iteration. We can also keep track of the history of parameter values across iterations to visualize the trajectory of the algorithm on the surface of the mean-squared error function.\n\nmodel = LinearRegression1D()\nopt = GradientDescentOptimizer1D(model, lr=0.05)\n\nw0_history = [model.w0.item()]\nw1_history = [model.w1.item()]\n\nepochs = 100\nfor t in range(epochs): \n    opt.step(x, y)\n    w0_history.append(model.w0.item())\n    w1_history.append(model.w1.item())\n\nFigure 2.4 shows the trajectory of the gradient descent algorithm on the negative log-likelihood surface, ultimately landing near the minimal value of the MSE and close to the true parameters used to generate the data. We also show the fitted linear model corresponding to the final estimates of \\(w_0\\) and \\(w_1\\), which visually agrees well with the data.\n\nCode\nfig, ax = plt.subplots(1, 2, figsize=(8, 3))\n\nim = ax[0].contourf(LL.numpy(), levels=20, cmap='inferno_r', extent = [w1_grid[0], w1_grid[-1], w0_grid[0], w0_grid[-1]])\nax[0].contour(LL.numpy(), levels=20, colors='black', linewidths=0.5, extent = [w1_grid[0], w1_grid[-1], w0_grid[0], w0_grid[-1]])\nax[0].set_ylabel(r'Intercept ($w_0$)')\nax[0].set_xlabel(r'Slope ($w_1$)')\nplt.colorbar(im, ax=ax[0])\nax[0].set_title('Mean-squared error for\\nlinear-Gaussian model')\nax[0].plot(w1_history, w0_history, marker='o', color='black', label='Gradient Descent Path', markersize=3)\nax[0].scatter([w1], [w0], color='black', s=50, label='True Parameters', facecolors='white')\n\nax[1].scatter(x, y, color='steelblue', s=10, label='Observed Data')\nax[1].plot(x, model.w1.detach().item() * x + model.w0.detach().item(), color='black', label='Fitted Line', linestyle='--')\nax[1].legend()\nax[1].set_xlabel('Feature (x)')\nax[1].set_ylabel('Target (y)')\nt = ax[1].set_title('Fitted model')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.4: Trajectory of the gradient descent algorithm (black line with dots) on the negative log-likelihood surface for the linear-Gaussian model from Figure 2.3. The starting point of the algorithm is at the beginning of the line, and each dot represents an iteration of the algorithm. The dot indicates the location of the true parameters used to generate the data.\n\n\n\nWe have just completed a simple implementation of our first machine learning algorithm: gradient descent for 1d linear-Gaussian regression via maximum likelihood estimation. This algorithm appears to successfully learn the linear trend present in the data, as shown in the right panel of Figure 2.4.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Maximum Likelihood Estimation and Gradients</span>"
    ]
  },
  {
    "objectID": "chapters/04-more-gradients.html",
    "href": "chapters/04-more-gradients.html",
    "title": "3  Higher Dimensions",
    "section": "",
    "text": "Recap\nOpen the live notebook in Google Colab.\nLast time, we developed an end-to-end example of maximum-likelihood estimation for the 1-dimensional linear-Gaussian model, which allowed us to fit a regression line to data displaying a linear trend. We saw that we could use the gradient of the log-likelihood function (or in this case, equivalently, the mean-squared error) to iteratively update a guess for the optimal parameters using gradient descent.\nThe model we learned to fit had two parameters, \\(w_1\\) and \\(w_0\\). However, modern models have vastly more parameters, with frontier LLMs having parameter counts nearing the trillions. In order to reason about these models, we need to build fluency in reasoning about high-dimensional spaces of parameters. We turn to this now, with a focus on multivariate linear regression.\n© Phil Chodrow, 2025",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Higher Dimensions</span>"
    ]
  },
  {
    "objectID": "chapters/04-more-gradients.html#multivariate-linear-gaussian-model",
    "href": "chapters/04-more-gradients.html#multivariate-linear-gaussian-model",
    "title": "3  Higher Dimensions",
    "section": "Multivariate Linear-Gaussian Model",
    "text": "Multivariate Linear-Gaussian Model\nRecall that the 1-dimensional linear-Gaussian model assumed that the target variable \\(y\\) was generated from a linear function of a single feature \\(x\\) plus Gaussian noise. We can write this as:\n\\[\n\\begin{aligned}\n    y_i \\sim \\mathcal{N}(w_1 x_i + w_0, \\sigma^2)\n\\end{aligned}\n\\]\nSuppose now that we have more than one predictive feature, which we would like to use in our model. Given \\(p\\) features \\(x_1,\\ldots,x_p\\), we can extend the linear-Gaussian model to incorpoate all of them:\n\\[\n\\begin{aligned}\n    y_i \\sim \\mathcal{N}(w_1 x_{i1} + w_2 x_{i2} + \\cdots + w_p x_{ip} + w_0, \\sigma^2) = \\mathcal{N}\\left(\\sum_{j=1}^p w_j x_{ij} + w_0, \\sigma^2\\right)\n\\end{aligned}\n\\tag{3.1}\\]\nHere’s a visualization of a regression model with two features, \\(x_1\\) and \\(x_2\\), alongside the plane defined by the linear function of these features.\n\n\n\n\n\n\n3D Scatter Plot of Synthetic Data with Regression Plane and Residuals\n\n\n\nIn order to write down Equation 3.1 more compactly, we need to introduce some notation. We’ll collect the features for data point \\(i\\) into a vector:\n\\[\n\\begin{aligned}\n    \\mathbf{x}_i = (x_{i0}, x_{i1},x_{i2},\\ldots,x_{ip})\n\\end{aligned}\n\\]\nwhere we assume that \\(x_{i0} = 1\\) is a constant feature included to capture the intercept term \\(w_0\\). We also collect the parameters into a vector:\n\\[\n\\begin{aligned}\n    \\mathbf{w}= (w_0, w_1, w_2, \\ldots, w_p)\n\\end{aligned}\n\\]\nWith this notation, we can rewrite the sum appearing in the mean of the Gaussian in Equation 3.1 as an inner product between the feature vector \\(\\mathbf{x}_i\\) and the parameter vector \\(\\mathbf{w}\\): An inner product is also often called a dot product. Equation 3.2 can equivalently be written with the notation \\( \\mathbf{x}_i^T \\mathbf{w} = \\mathbf{x}_i \\cdot \\mathbf{w}= \\langle \\mathbf{x}_i,\\mathbf{w} \\rangle\\).\n\\[\n\\begin{aligned}\n     \\mathbf{x}_i^T \\mathbf{w} =  \\sum_{j=0}^p w_j x_{ij} = 1\\cdot w_0 + \\sum_{j=1}^p w_j x_{ij} = 1\\cdot w_0 + \\sum_{j=1}^p w_j x_{ij}\\;.\n\\end{aligned}\n\\tag{3.2}\\]\nThe choice of \\(x_{i0} = 1\\) ensures that the intercept term \\(w_0\\) is included in the inner product. This is so convenient that we’ll assume it from now on.\n\n\n\n\n\n\nNoteAssumption\n\n\n\nFrom this point forwards, we assume that the feature vector \\(\\mathbf{x}_i\\) begins with a constant feature \\(x_{i0} = 1\\).\n\n\nThis notation allows us to compactly rewrite the multivariate linear-Gaussian model as:\n\\[\n\\begin{aligned}\n    y_i \\sim \\mathcal{N}( \\mathbf{x}_i^T \\mathbf{w}, \\sigma^2)\n\\end{aligned}\n\\]\nregardless of the number of features. We can make things even a bit simpler by defining a score \\(s_i\\) associated to each data point \\(i\\):\n\\[\n\\begin{aligned}\n    s_i =  \\mathbf{x}_i^T \\mathbf{w}\n\\end{aligned}\n\\tag{3.3}\\]\nafter which we can write: \\[\n\\begin{aligned}\n    y_i \\sim \\mathcal{N}(s_i, \\sigma^2)\n\\end{aligned}\n\\]\n\nLog-Likelihood and Mean-Squared Error\nThe log-likelihood function of the multivariate linear-Gaussian model is given by\n\\[\n\\begin{aligned}\n    \\mathcal{L}(\\mathbf{w}) &= \\sum_{i=1}^n \\log p_Y(y_i ; s_i \\sigma^2) \\\\\n    &= \\sum_{i=1}^n \\log \\left( \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(y_i - s_i)^2}{2 \\sigma^2}\\right) \\right) \\\\\n    &= -\\frac{n}{2} \\log(2 \\pi \\sigma^2) - \\frac{1}{2 \\sigma^2} \\sum_{i=1}^n (y_i - s_i)^2\\;,\n\\end{aligned}\n\\]\nwhich, like last time, means that our maximum-likelihood estimation problem is equivalent to minimizing the mean-squared error between the observed targets \\(y_i\\) and the scores \\(s_i\\):\n\\[\n\\begin{aligned}\n    R(\\mathbf{x}, \\mathbf{y}; \\mathbf{w}) = \\frac{1}{n} \\sum_{i=1}^n (y_i - s_i)^2 = \\frac{1}{n} \\sum_{i=1}^n (y_i -  \\mathbf{x}_i^T \\mathbf{w})^2\\;.\n\\end{aligned}\n\\]\nIn theory, we’re ready to start taking gradients and minimizing this function. However, it’s helpful to try to first simplify our notation even more, which we can do with the introduction of matrix and norm notation.\n\n\nMatrices and Norms\nRecall that the Euclidean norm of a vector \\(\\mathbf{v}\\in \\mathbb{R}^d\\) is defined by the formula More generally, the \\(\\ell_p\\)-norm of vector \\(\\mathbf{v}\\) is defined by the equation \\[\n\\begin{aligned}\n    \\lVert \\mathbf{v} \\rVert_p = \\sqrt[p]{\\sum_{j = 1}^d v_j^p}\\;.\n\\end{aligned}\n\\] For simplicity, whenever we write \\(\\lVert \\mathbf{v} \\rVert\\) without a subscript, we mean the \\(\\ell_2\\)-norm, i.e. \\(\\lVert \\mathbf{v} \\rVert = \\lVert \\mathbf{v} \\rVert_2\\).\n\\[\n\\begin{aligned}\n    \\lVert \\mathbf{v} \\rVert^2 = \\sum_{j=1}^d v_j^2\\;.\n\\end{aligned}\n\\]\nThe norm notation allows us to eliminate the explicit summation in favor of vector operations:\n\\[\n\\begin{aligned}\n    R(\\mathbf{x}, \\mathbf{y}; \\mathbf{w}) = \\frac{1}{n} \\lVert \\mathbf{y}- \\mathbf{s} \\rVert^2\\;,\n\\end{aligned}\n\\]\nwhere \\(\\mathbf{s}\\) is the vector of scores for all data points, whose \\(i\\)th entry is given by Equation 3.3. Our last step is to give a compact formula for \\(\\mathbf{s}\\). To do this, we collect all of the feature vectors \\(\\mathbf{x}_i\\) into a matrix \\(\\mathbf{X}\\in \\mathbb{R}^{n \\times (p+1)}\\), whose \\(i\\)th row is given by \\(\\mathbf{x}_i^T\\):\n\\[\n\\begin{aligned}\n    \\mathbf{X}= \\left[\\begin{matrix}\n    - & \\mathbf{x}_1^T & - \\\\\n    - & \\mathbf{x}_2^T & - \\\\\n    & \\vdots & \\\\\n    - & \\mathbf{x}_n^T & -\n    \\end{matrix}\\right]\\;.\n\\end{aligned}\n\\]\nNow, if we multiply the matrix \\(\\mathbf{X}\\) by the parameter vector \\(\\mathbf{w}\\), we obtain\n\\[\n\\begin{aligned}\n    \\mathbf{X}\\mathbf{w}= \\left[\\begin{matrix}\n    - & \\mathbf{x}_1^T & - \\\\\n    - & \\mathbf{x}_2^T & - \\\\\n    & \\vdots & \\\\\n    - & \\mathbf{x}_n^T & -\n    \\end{matrix}\\right] \\mathbf{w}= \\left[\\begin{matrix}\n     \\mathbf{x}_1^T \\mathbf{w} \\\\\n     \\mathbf{x}_2^T \\mathbf{w} \\\\\n    \\vdots \\\\\n     \\mathbf{x}_n^T \\mathbf{w}\n    \\end{matrix}\\right] =\n    \\left[\\begin{matrix}\n    s_1 \\\\\n    s_2 \\\\\n    \\vdots \\\\\n    s_n\n    \\end{matrix}\\right] =\n    \\mathbf{s}\\;,\n\\end{aligned}\n\\]\nThis gives us our compact formula for the MSE:\n\\[\n\\begin{aligned}\n    R(\\mathbf{x}, \\mathbf{y}; \\mathbf{w}) = \\frac{1}{n}\\lVert \\mathbf{X}\\mathbf{w}- \\mathbf{y} \\rVert^2\\;.\n\\end{aligned}\n\\tag{3.4}\\]\nThe maximum-likelihood problem for the multivariate linear-Gaussian model can therefore be written Since the \\(\\frac{1}{n}\\) is a positive constant, it does not affect the location of the minimum and can be ignored in the optimization problem.\n\\[\n\\begin{aligned}\n    \\hat{\\mathbf{w}} = \\mathop{\\mathrm{arg\\,min}}_\\mathbf{w}\\lVert \\mathbf{X}\\mathbf{w}- \\mathbf{y} \\rVert^2\\;.\n\\end{aligned}\n\\tag{3.5}\\]",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Higher Dimensions</span>"
    ]
  },
  {
    "objectID": "chapters/04-more-gradients.html#deriving-and-checking-gradient-formulas",
    "href": "chapters/04-more-gradients.html#deriving-and-checking-gradient-formulas",
    "title": "3  Higher Dimensions",
    "section": "Deriving and Checking Gradient Formulas",
    "text": "Deriving and Checking Gradient Formulas\nTo solve the optimization problem in Equation 3.5, we will need the gradient of \\(R\\) with respect to \\(\\mathbf{w}\\). To do so, we’ll highlight two approaches.\n\nEntrywise Derivation\nTo compute the gradient of \\(R\\) entrywise, we start by explicitly rewriting \\(R\\) in summation notation, avoiding matrix and vector notation for the moment:\n\\[\n\\begin{aligned}\n    R(\\mathbf{x}, \\mathbf{y}; \\mathbf{w}) = \\frac{1}{n} \\sum_{i=1}^n (y_i -  \\mathbf{x}_i^T \\mathbf{w})^2 = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\sum_{j=0}^p w_j x_{ij})^2\\;.\n\\end{aligned}\n\\]\nWe now take the derivative with respect to a particular parameter \\(w_k\\):\n\\[\n\\begin{aligned}\n    \\frac{\\partial R}{\\partial w_k} &= \\frac{\\partial}{\\partial w_k} \\left( \\frac{1}{n} \\sum_{i=1}^n (y_i - \\sum_{j=0}^p w_j x_{ij})^2 \\right) \\\\\n    &= \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial}{\\partial w_k} \\left(y_i - \\sum_{j=0}^p w_j x_{ij}\\right)^2 \\\\\n    &= \\frac{1}{n} \\sum_{i=1}^n 2 \\left(y_i - \\sum_{j=0}^p w_j x_{ij}\\right) \\cdot \\left(-x_{ik}\\right) &\\quad \\text{(chain rule)}\\\\\n    &= -\\frac{2}{n} \\sum_{i=1}^n x_{ik} \\left(y_i -  \\mathbf{x}_i^T \\mathbf{w}\\right) \\\\\n    &= \\frac{2}{n} \\sum_{i=1}^n x_{ik} \\left( \\mathbf{x}_i^T \\mathbf{w} - y_i\\right)\\;.\n\\end{aligned}\n\\tag{3.6}\\]\n\n\nVector Derivation\nIt’s more convenient and insightful to compute the gradient in vector form. This requires a few gradient identities, each of which can be derived and verified using entrywise methods like the one above. For our case, we need two identities In both these identities, the gradient is taken with respect to \\(\\mathbf{v}\\).\n\nProposition 3.1 (Gradient of Norm Squared) For any \\(\\mathbf{v}_0 \\in \\mathbb{R}^d\\), if \\(f(\\mathbf{v}) = \\lVert \\mathbf{v} \\rVert^2\\), then \\[\n\\nabla f(\\mathbf{v}_0) = 2 \\mathbf{v}_0\\;.\n\\]\n\n\nProposition 3.2 (Compositions with Linear Maps) Let \\(f: \\mathbb{R}^m \\to \\mathbb{R}\\) be a differentiable function, and let \\(\\mathbf{A}\\in \\mathbb{R}^{m \\times n}\\) be a matrix. Define \\(g: \\mathbb{R}^n \\to \\mathbb{R}\\) by \\(g(\\mathbf{v}) = f(\\mathbf{A}\\mathbf{v})\\). Then, for any \\(\\mathbf{v}_0 \\in \\mathbb{R}^n\\),\n\\[\n\\nabla g(\\mathbf{v}_0) = \\mathbf{A}^T \\nabla f(\\mathbf{A}\\mathbf{v}_0)\\;.\n\\]\n\nIf we apply these identities to Equation 3.4, we obtain\n\\[\n\\begin{aligned}\n    \\nabla R(\\mathbf{x}, \\mathbf{y}; \\mathbf{w}) = \\nabla \\left( \\frac{1}{n} \\lVert \\mathbf{X}\\mathbf{w}- \\mathbf{y} \\rVert^2 \\right) &= \\frac{1}{n} \\nabla \\lVert \\mathbf{X}\\mathbf{w}- \\mathbf{y} \\rVert^2 \\\\\n    &= \\frac{1}{n}\\mathbf{X}^T \\nabla \\lVert \\mathbf{X}\\mathbf{w}- \\mathbf{y} \\rVert^2 &\\quad \\text{(Prop 3.2)}\\\\\n    &= \\frac{1}{n}\\mathbf{X}^T \\cdot 2(\\mathbf{X}\\mathbf{w}- \\mathbf{y}) &\\quad \\text{(Prop 3.1)}\\\\\n    &= \\frac{2}{n} \\mathbf{X}^T (\\mathbf{X}\\mathbf{w}- \\mathbf{y})\\;.\n\\end{aligned}\n\\]\nIt’s possible to check that this vector formula for the gradient matches the entrywise formula from Equation 3.6 by verifying that the \\(k\\)th entry of the vector formula equals the entrywise formula for arbitrary \\(k\\).\n\n\nChecking Vector Identities\nHow do we know that vector gradient identities like the ones we used above are correct? The most direct way is to verify them entrywise. To verify a vector identity entrywise, we just check that the \\(k\\)th entry of the left-hand side equals the \\(k\\)th entry of the right-hand side, for an arbitrary index \\(k\\). For example, let’s verify the first identity above, \\(\\nabla \\lVert \\mathbf{v} \\rVert^2 = 2\\mathbf{v}\\). The \\(k\\)th entry of the left-hand side is given by the partial derivative with respect to entry \\(w_k\\):\n\\[\n\\begin{aligned}\n    \\left(\\nabla \\lVert \\mathbf{v} \\rVert^2\\right)_k = \\frac{\\partial}{\\partial v_k} \\lVert \\mathbf{v} \\rVert^2 = \\frac{\\partial}{\\partial v_k} \\sum_{j=1}^d v_j^2 =  \\sum_{j=1}^d \\frac{\\partial}{\\partial v_k} v_j^2 = \\sum_{j=1}^d 2\\delta_{jk}  v_k = 2 v_k\\;,\n\\end{aligned}\n\\]\nwhere \\(\\delta_{jk}\\) is the Kronecker delta, which equals 1 if \\(j=k\\) and 0 otherwise. The \\(k\\)th entry of the right-hand side is simply \\(2 v_k\\). Since these are equal for arbitrary \\(k\\), the identity holds.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Higher Dimensions</span>"
    ]
  },
  {
    "objectID": "chapters/04-more-gradients.html#implementing-multivariate-regression",
    "href": "chapters/04-more-gradients.html#implementing-multivariate-regression",
    "title": "3  Higher Dimensions",
    "section": "Implementing Multivariate Regression",
    "text": "Implementing Multivariate Regression\nOur implementation for multivariate linear regression is very similar to our implementation for univariate regression from last time, and can actually be more compact in code thanks to our use of matrix operations. torch supplies the syntax X@w for matrix-vector and matrix-matrix multiplication, which we can use to compute the predictions for all data points at once.\n\nObject-Oriented API for Machine Learning Models\nWe are going to implement most of our machine learning models using an object-oriented approach which will generalize nicely once we move on to complex deep learning models.\n\nModel Class\nUnder torch’s standard structure, a model is an instance of a class which holds parameters and implements a forward method which computes predictions given input data. Here’s a minimal implementation for linear regression:\n\nclass LinearRegression:\n    def __init__(self, n_params):\n        self.w = torch.zeros(n_params, 1, requires_grad=True)\n\n    def forward(self, X):\n        return X @ self.w\n\n\n\nLoss Function\nAlongside the model class, we need a loss function. While there are many possibilities, we’ll use our familiar mean-squared error. The loss function should accept predicted and true target values, and return a scalar loss value.\n\ndef mse(y_pred, y):\n    return torch.mean((y_pred - y)**2)\n\n\n\nOptimizer\nThe last thing we need is an algorithm for optimizing the parameters of the model. Gradient descent is one such algorithm. Here, we’ll implement a simple version of gradient descent as a separate class:\n\nclass GradientDescentOptimizer:\n    def __init__(self, model, lr=1e-2):\n        self.model = model\n        self.lr = lr\n\n    # once we begin to rely on automatic differentiation\n    # we won't need to implement this ourselves\n    def grad_func(self, X, y): \n        y_pred = self.model.forward(X)\n        n = y.shape[0]\n        gradient = (2/n) * X.T @ (y_pred - y)\n        return gradient\n\n    def step(self, X, y):\n        y_pred = self.model.forward(X)\n\n        with torch.no_grad():\n            self.model.w -= self.lr * self.grad_func(X, y)\n\nNow we’re ready to run multivariate linear regression. Let’s generate some random synthetic data for testing.\n\n# Generate synthetic data\ntorch.manual_seed(0)\nn_samples = 100\nn_features = 5\nX = torch.randn(n_samples, n_features)\nX = torch.cat([torch.ones(n_samples, 1), X], dim=1) # shape (n_samples, n_features + 1)\ntrue_w = torch.randn(n_features + 1, 1)\n\nsignal = X @ true_w\nnoise = 0.5 * torch.randn(n_samples, 1)\ny = signal + noise\n\nWith this data, we can check that our vector gradient formula matches the entrywise formula we derived earlier, which we can do via torch’s automatic differentiation:\n\nmodel = LinearRegression(n_params=n_features + 1)\nopt = GradientDescentOptimizer(model=model, lr=1e-2)\n\n1grad_manual = opt.grad_func(X, y)\n\ny_pred = model.forward(X)\n2loss = mse(y_pred, y)\n3loss.backward()\n\nprint(torch.allclose(grad_manual, model.w.grad))  # Should print True\n\n\n1\n\nCompute the gradient manually using the optimizer’s function.\n\n2\n\nCompute the loss using the mse function.\n\n3\n\nUse PyTorch’s automatic differentiation to compute the gradient.\n\n\n\n\nTrue\n\n\nLooks fine! Now that we have gradient descent implemented, we can consider the gradient descent algorithm itself. This time, we’ll implement gradient descent as two functions: one which performs a single step of gradient descent, and one which handles the main loop, including storing values of the loss and checking for convergence. There are many ways to check for convergence. Here, we’re just testing whether \\(\\lVert \\mathbf{w}_\\mathrm{new} - \\mathbf{w}_\\mathrm{old} \\rVert\\) is small, which is one way to quantifying the idea that \\(\\mathbf{w}\\) “hasn’t changed much” in a single iteration.\n\n# training loop\n\nloss_history = []\n\nfor _ in range(10000):\n    y_pred = model.forward(X)\n    loss = mse(y_pred, y)\n    loss_history.append(loss.item())\n\n    opt.step(X, y)  # perform a single gradient descent step\n\n    # convergence check\n    if len(loss_history) &gt; 1:\n        if abs(loss_history[-1] - loss_history[-2]) &lt; 1e-6:\n            print(\"Converged\")\n            break\n\nConverged\n\n\n\nInitialize a list to store the loss history.\nCompute and store the current loss.\nPerform a single gradient descent step.\nCheck for convergence by seeing if the change in parameters is small.\n\nNow we’re ready to run gradient descent.\n\nCode\n# Plot loss history\nimport matplotlib.pyplot as plt\nplt.plot(loss_history)\nplt.xlabel('Iteration')\nplt.ylabel('Mean Squared Error')\nplt.title('Gradient Descent Loss History')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.1: Loss vs. iteration during gradient descent for our manual implementation of multivariate linear regression.\n\n\n\nThis time, because we implemented a convergence check, our main loop terminated automatically.\nWe can compare the learned parameters to the true parameters:\n\nprint(\"Learned parameters:\\n\", model.w.flatten())\nprint(\"True parameters:\\n\", true_w.flatten())\n\nLearned parameters:\n tensor([ 0.8640,  0.3088,  0.2475,  1.0593, -0.9756, -1.2738],\n       grad_fn=&lt;ViewBackward0&gt;)\nTrue parameters:\n tensor([ 0.8393,  0.2479,  0.2067,  0.9928, -0.8986, -1.2028])\n\n\nThe learned parameters are relatively close to the true parameters we planted in the synthetic data.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Higher Dimensions</span>"
    ]
  },
  {
    "objectID": "chapters/06-regularization.html",
    "href": "chapters/06-regularization.html",
    "title": "4  Features and Regularization",
    "section": "",
    "text": "Basis Function Expansion\nOpen the live notebook in Google Colab.\nSuppose that we’d like to model a distinctly nonlinear relationship in our data:\nOne flexible candidate model for this kind of data is a nonlinear Gaussian model:\nHere’s a visualization of this model with \\(f(x) = \\sin(2\\pi x)\\) and \\(\\sigma^2 = 0.01\\), which was the setting used to generate the synthetic data above:\nThis is all well and good as a theoretical framework, but how in the world were we supposed to know that \\(f(x) = \\sin(2\\pi x)\\) was the right choice? In practice, of course, we never will.\nSince we don’t know the right functional form for \\(f\\), one common approach is try to approximate\nBefore we look at some examples of basis functions, let’s take a look at the fundamental trick behind basis function expansions: when using a basis function expansion with a nonlinear Gaussian model, the model is linear in the parameters \\(w_j\\), and we can therefore use our previously-developed machinery for linear regression to fit the model. To see this, note that we can write\n\\[\n\\begin{aligned}\n    \\sum_{j=0}^p w_j \\phi_j(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x})\\;,\n\\end{aligned}\n\\]\nwhere we’ve defined the vector of parameters \\(\\mathbf{w}= (w_0, w_1, \\ldots, w_p)^T\\) and the vector of basis functions \\(\\mathbf{\\phi}(\\mathbf{x}) = (\\phi_0(\\mathbf{x}), \\phi_1(\\mathbf{x}), \\ldots, \\phi_p(\\mathbf{x}))^T\\). Let’s give the shorthand \\(\\hat{y}_i = \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x}_i)\\) for the prediction of the model at input \\(\\mathbf{x}_i\\). Then, taken together the nonlinear Gaussian model with basis function expansion says that\n\\[\n\\begin{aligned}\n    y_i \\sim \\mathcal{N}(\\hat{y}_i; \\sigma^2) = \\mathcal{N}(\\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x}_i); \\sigma^2)\\;.\n\\end{aligned}\n\\]\nThis is just like the linear-Gaussian model, but with the input data \\(\\mathbf{x}_i\\) replaced by the transformed data \\(\\mathbf{\\phi}(\\mathbf{x}_i)\\). Therefore, we can use our previous results for maximum likelihood estimation of the parameters \\(\\mathbf{w}\\) in the linear-Gaussian model, simply by replacing each occurrence of \\(\\mathbf{x}_i\\) with \\(\\mathbf{\\phi}(\\mathbf{x}_i)\\). In particular, we can maximize the log-likelihood of the data by minimizing the mean squared error between the predictions \\(\\hat{y}_i\\) and the observed targets \\(y_i\\):\n\\[\n\\begin{aligned}\n    R(\\mathbf{X}, \\mathbf{y}; \\mathbf{w}) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x}_i))^2\\;.\n\\end{aligned}\n\\]\nIf we define\n\\[\n\\begin{aligned}\n    \\mathbf{\\Phi}= \\left[\\begin{matrix}\n        - &\\mathbf{\\phi}(\\mathbf{x}_1)^T & -\\\\\n        - &\\mathbf{\\phi}(\\mathbf{x}_2)^T & -\\\\\n        \\vdots \\\\\n        - &\\mathbf{\\phi}(\\mathbf{x}_n)^T & -    \n    \\end{matrix}\\right]\\;,\n\\end{aligned}\n\\]\nwe can similarly write the mean squared error in matrix form as\n\\[\n\\begin{aligned}\n    R(\\mathbf{\\Phi}, \\mathbf{y}; \\mathbf{w}) = \\frac{1}{n} \\lVert \\mathbf{\\Phi}\\mathbf{w}- \\mathbf{y} \\rVert^2\\;.\n\\end{aligned}\n\\]\nSo, to learn a model with basis function expansion, all we need to do is construct the feature matrix \\(\\mathbf{\\Phi}\\) by applying the basis functions to each data point, and then run linear regression as before.\nLet’s try basis function expansion on our synthetic data. For this, we’ll first bring in the linear regression model that we developed previously:\nclass LinearRegression:\n    def __init__(self, n_params):\n        self.w = torch.zeros(n_params, 1, requires_grad=True)\n\n    def forward(self, X):\n        return X @ self.w\nWe’ll also write a simple training loop, this time using some of PyTorch’s built-in optimization functionality: rather than implement our own GradientDescentOptimizer class today, we’ll instead use the torch.optim.Adam optimizer. This enables faster training, which will help us for the experiments in these notes.\ndef mse(y_pred, y):\n    return torch.mean((y_pred - y) ** 2)\n\ndef train_model(model, X_train, y_train, lr=1e-2, n_epochs=1000, tol=1e-4, regularization = None, verbose = False):\n    opt = torch.optim.Adam(params=[model.w], lr=lr)\n    for epoch in range(n_epochs):\n        y_pred = model.forward(X_train)\n1        loss = mse(y_pred, y_train) + (regularization(model.w) if regularization is not None else 0.0)\n        \n        opt.zero_grad() \n        loss.backward() #&lt;2&gt; automated computation of gradients for Adam optimization\n        opt.step() #&lt;3&gt; perform an optimization step\n        if model.w.grad is not None:\n            if model.w.grad.norm().item() &lt; tol:\n                if verbose: \n                    print(f\"Converged at epoch {epoch}, Loss: {loss.item()}\")\n                break\n        if verbose and epoch % 100 == 0:\n            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n\n\n1\n\nThis function adds a regularization term (introduced below) to the loss if provided.\nIt’s helpful to think about the minimal linear regression model, in which we simply add a constant column to the data, as an example of a basis function expansion. We’ll call this the linear basis function.\ndef linear_basis_function(X):\n    \"\"\"\n    just adds the constant feature\n    \"\"\"\n    n_samples = X.shape[0]\n    Phi = torch.ones(n_samples, 2)  # intercept + linear term\n    Phi[:, 1] = X.flatten()\n    return Phi\nIf we try fitting this model to the sinusoidal data directly, we’ll be disappointed.\nPHI = linear_basis_function(X)\nLR = LinearRegression(n_params=2)  # intercept + slope\ntrain_model(LR, PHI, y, lr=1e-2, n_epochs=1000)\nCode\n# utility function for model visualization\ndef viz_model_predictions(model, X, y, basis_fun, ax, **basis_fun_kwargs): \n\n    x_new = torch.linspace(X.min(), X.max(), 1000).reshape(-1, 1)\n    PHI_new = basis_fun(x_new, **basis_fun_kwargs)\n    y_pred = model.forward(PHI_new)\n    ax.scatter(X.numpy(), y.numpy(), **scatterplot_kwargs)\n    ax.plot(x_new.numpy(), y_pred.detach().numpy(), color='red', label='Prediction')\n    ax.set_xlabel(r\"$x$\")\n    ax.set_ylabel(r\"$y$\")\nHow did we do?\nNow let’s try some nontrivial basis function expansions. If we had reason to believe that our data was periodic, we might try using sine and cosine basis functions. For example, let’s try:\n\\[\n\\begin{aligned}\n    \\phi_0(x) &= 1\\;,\\\\\n    \\phi_1(x) &= \\sin(\\pi x)\\;,\\\\\n    \\phi_2(x) &= \\sin(2 \\pi x)\\;,\\\\\n    \\phi_3(x) &= \\sin(3 \\pi x)\\;,\\\\\n    \\vdots\n\\end{aligned}\n\\]\nThe following function constructs the feature matrix \\(\\mathbf{\\Phi}\\) for this basis function expansion:\ndef sinusoidal_features(X, max_freq=4):\n    n_samples = X.shape[0]\n    Phi = torch.ones(n_samples, max_freq + 1)  \n    for i in range(1, max_freq + 1):\n        Phi[:, i] = torch.sin(i * torch.pi * X).flatten()\n    return Phi\nTo train models and make predictions, all we need to do is call this function to get the feature matrix, and then run linear regression as before.\n# engineer features\nmax_freq = 15\nPHI = sinusoidal_features(X, max_freq=max_freq)\n\n# train the model as usual\nLR = LinearRegression(n_params=PHI.shape[1])\ntrain_model(LR, PHI, y, lr=1e-2, n_epochs=2000)\nLet’s try this for a variety of maximum frequencies.\nAs the number of basis functions increases, the model becomes more flexible and is able to better fit the training data. However, with too many basis functions, the model begins to overfit the data, capturing noise rather than the underlying signal as reflected in the seemingly random high-frequency oscillations in the predictions.\n© Phil Chodrow, 2025",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Features and Regularization</span>"
    ]
  },
  {
    "objectID": "chapters/06-regularization.html#basis-function-expansion",
    "href": "chapters/06-regularization.html#basis-function-expansion",
    "title": "4  Features and Regularization",
    "section": "",
    "text": "\\[\n\\begin{aligned}\n    f(\\mathbf{x}) \\approx w_0\\phi_0(\\mathbf{x}) + w_1 \\phi_1(\\mathbf{x}) + w_2 \\phi_2(\\mathbf{x}) + \\cdots + w_p \\phi_p(\\mathbf{x}) = \\sum_{j=0}^p w_j \\phi_j(\\mathbf{x})\\;,\n\\end{aligned}\n\\] where \\(\\phi_j(\\cdot)\\) are a collection of basis functions that we choose ahead of time. This is called a basis function expansion. Each \\(\\phi_j\\) is often also called a feature map.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\nviz_model_predictions(LR, X, y, basis_fun = linear_basis_function, ax=ax)\nplt.title(\"Linear Regression Fit to Nonlinear Data\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nFigure 4.3: Example of linear regression fit to the sinusoidal data set using the linear (trivial) basis function expansion.\n\n\n\n\n\n\n\n\n\n\nCode\n# visualize\nfig, ax = plt.subplots()\n\nviz_model_predictions(LR, X, y, basis_fun = sinusoidal_features, ax=ax, max_freq=max_freq)\n\nplt.title(f\"Sinusoidal Basis Functions with max_freq={max_freq}\")\nplt.legend()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.4: Example fit to data using the sinusoidal basis function expansion with maximum frequency 15.\n\n\n\n\n\nCode\nfig, axarr = plt.subplots(2, 3, figsize=(8,5))\n\nmax_freqs = [0, 1, 2, 5, 10, 20]\n\nfor i, ax in enumerate(axarr.flatten()):\n    \n    PHI = sinusoidal_features(X, max_freq=max_freqs[i])\n    LR = LinearRegression(n_params=PHI.shape[1])\n    train_model(LR, PHI, y, lr=1e-2, n_epochs=2000)\n   \n    viz_model_predictions(LR, X, y, basis_fun = sinusoidal_features, ax=ax, max_freq=max_freqs[i])\n\n    mse_val = mse(LR.forward(PHI), y).item()\n    ax.set_title(f\"max_freq={max_freqs[i], } | MSE={mse_val:.3f}\")\n\n    if i == 0:\n        ax.legend()\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.5: Example fits to training data using the sinusoidal basis function expansion with varying maximum frequencies.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Features and Regularization</span>"
    ]
  },
  {
    "objectID": "chapters/06-regularization.html#kernel-basis-functions",
    "href": "chapters/06-regularization.html#kernel-basis-functions",
    "title": "4  Features and Regularization",
    "section": "Kernel Basis Functions",
    "text": "Kernel Basis Functions\nAnother common choice of basis functions are kernel basis functions. A kernel is simply a measure of similarity between two inputs. A common choice is the Gaussian radial-basis kernel, which is defined in one dimension by\n\\[\nk(x, c) = \\exp\\left(-(x - c)^2\\right)\\;,\n\\]\nThe Guassian kernel measures similarity between \\(x\\) and a center point \\(c\\), with values close to 1 when \\(x\\) is near \\(c\\) and values close to 0 when \\(x\\) is far from \\(c\\).\nHere’s an implementation in one dimension, where we evenly space choices of \\(c\\) out across the range of the data:\n\ndef kernel_features(X, num_kernels):\n    n_samples = X.shape[0]\n    Phi = torch.ones(n_samples, num_kernels + 1)  \n    centers = torch.linspace(X.min(), X.max(), num_kernels)\n    bandwidth = (X.max() - X.min()) / num_kernels\n    for j in range(num_kernels):\n        Phi[:, j + 1] = torch.exp(-0.5 * ((X.flatten() - centers[j]) / bandwidth) ** 2)\n    return Phi\n\nWe can now run the same experiment as before:\n\nCode\nfig, axarr = plt.subplots(2, 3, figsize=(8,5))\n\nnum_kernels = [0, 3, 5, 10, 20, 30]\n\nfor i, ax in enumerate(axarr.flatten()):\n    \n    PHI = kernel_features(X, num_kernels=num_kernels[i])\n    LR = LinearRegression(n_params=PHI.shape[1])\n    train_model(LR, PHI, y, lr=1e-2, n_epochs=200000, tol = 1e-4)\n\n    viz_model_predictions(LR, X, y, basis_fun = kernel_features, ax=ax, num_kernels=num_kernels[i])\n    \n    mse_val = mse(LR.forward(PHI), y).item()\n    ax.set_title(f\"num_kernels={num_kernels[i]} | MSE={mse_val:.3f}\")\n\n    if i == 0:\n        ax.legend()\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.6: Visualization of fits to training data using the kernel basis function expansion with varying numbers of Gaussian radial basis kernels.\n\n\n\nAs before, we observe that as the number of basis functions increases, the model becomes more flexible and is able to better fit the training data, but eventually begins to overfit.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Features and Regularization</span>"
    ]
  },
  {
    "objectID": "chapters/06-regularization.html#regularization",
    "href": "chapters/06-regularization.html#regularization",
    "title": "4  Features and Regularization",
    "section": "Regularization",
    "text": "Regularization\nWe now find ourselves in a bit of a dilemma – we’d like to use flexible models with many basis functions to capture nonlinear patterns in data, but introducing flexibility raises the risk of overfitting. One approach is to simply restrict which basis functions we’ll use, but this is unsatisfying: how will we know ahead of time which basis functions are best?\nAn alternative approach is to use regularization. Regularization works by encouraging our models to maintain small entries in the parameter vector \\(\\mathbf{w}\\). This effectively allows us to use many basis functions, but penalizes the model for emphasizing any one of them too heavily.\nTypical regularization schemes work by adding a penalty term to the loss function (e.g. the MSE). For example, in ridge regression, we add a penalty proportional to the squared \\(\\ell_2\\) norm of the parameter vector:\n\\[\n\\begin{aligned}\n    \\hat{\\mathbf{w}} = \\mathop{\\mathrm{arg\\,min}}_\\mathbf{w}\\left\\{ \\frac{1}{n} \\lVert \\mathbf{\\Phi}\\mathbf{w}- \\mathbf{y} \\rVert^2 + \\lambda \\lVert \\mathbf{w} \\rVert^2 \\right\\}\\;,\n\\end{aligned}\n\\]\nwhere \\(\\lambda\\) is a hyperparameter that controls the strength of the regularization. Larger values of \\(\\lambda\\) encourage smaller parameter values, while smaller values allow the model to fit the data more closely.\nWe can implement ridge regression by adding an \\(\\ell_2\\) regularization term to our training loop. We’ll first just implement that term itself:\n\ndef ell_2_regularization(w):\n    return torch.mean(w[1:]**2)\n\nThe reason for excluding the first entry of \\(w\\) from the regularization term is that this entry corresponds to the intercept term, which we typically don’t want to penalize. We then pass this function in to the regularization argument of train_model, where flagged line &lt;1&gt; adds the regularization term to the loss. We train again and visualize the results as we vary the regularization strength, this time keeping the maximum frequency of the sinusoidal basis functions fixed at 20:\n\nCode\nfig, axarr = plt.subplots(2, 3, figsize=(8,5))\nreg_strengths = torch.logspace(0, 1.5, 6)\n\nfor i, ax in enumerate(axarr.flatten()):\n    \n    PHI = sinusoidal_features(X, max_freq=20)\n    LR = LinearRegression(n_params=PHI.shape[1])\n    train_model(LR, PHI, y, lr=1e-2, n_epochs=200000, tol = 1e-4, regularization = lambda w: reg_strengths[i]*ell_2_regularization(w))\n    viz_model_predictions(LR, X, y, basis_fun = sinusoidal_features, ax=ax, max_freq=20)\n\n    ax.set_title(f\"reg_strength={reg_strengths[i]:.2f}\")\n    if i == 0:\n        ax.legend()\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.7: Ridge regression models using sinusoidal basis functions with maximum frequency 20, for varying regularization strengths, visualized against training data.\n\n\n\nWe observe that the tendency of \\(\\ell_2\\) regularization in this setting is to “shrink” the coefficients of the basis functions toward zero. This makes the predictions somewhat smoother, but also just makes them smaller, eventually leaving them systematically smaller than the data in magnitude.\nAn alternative regularization is \\(\\ell_1\\) regularization, in which we penalize the absolute values of the parameters rather than their squares. This gives us a version of regression commonly called lasso regression:\n\\[\n\\begin{aligned}\n    \\hat{\\mathbf{w}} = \\mathop{\\mathrm{arg\\,min}}_\\mathbf{w}\\left\\{ \\frac{1}{n} \\lVert \\mathbf{\\Phi}\\mathbf{w}- \\mathbf{y} \\rVert^2 + \\lambda \\sum_{j=1}^p |w_j| \\right\\}\\;.\n\\end{aligned}\n\\]\nWe can implement \\(\\ell_1\\) regularization similarly to before:\n\ndef ell_1_regularization(w):\n    return torch.mean(torch.abs(w[1:]))  # exclude intercept from regularization\n\nNow we can run the same experiment:\n\nCode\nfig, axarr = plt.subplots(2, 3, figsize=(8,5))\nreg_strengths = torch.logspace(0, 1.5, 6)\n\nfor i, ax in enumerate(axarr.flatten()):\n    \n    PHI = sinusoidal_features(X, max_freq=20)\n    LR = LinearRegression(n_params=PHI.shape[1])\n    train_model(LR, PHI, y, lr=1e-2, n_epochs=20000, tol = 1e-4, regularization = lambda w: reg_strengths[i]*ell_1_regularization(w))\n    viz_model_predictions(LR, X, y, basis_fun = sinusoidal_features, ax=ax, max_freq=20)\n    \n    ax.set_title(f\"reg_strength={reg_strengths[i]:.2f}\")\n    if i == 0:\n        ax.legend()\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.8: LASSO regression models using sinusoidal basis functions with maximum frequency 20, for varying regularization strengths, visualized against training data.\n\n\n\nLASSO is somewhat more difficult to fit in computational terms, resulting in longer computation times. We observe that some of the LASSO fits manage to highlight the underlying trend in the data relatively well, with somewhat less systematic underfitting when compared to the ridge regression fits.\nAn important and useful property of LASSO is that it tends to set many of the coefficients exactly to zero, effectively performing feature selection. This can be useful when we have a large number of basis functions, as it allows us to identify which ones are most important for modeling the data. Let’s take a look at the coefficients learned by a LASSO model with moderate regularization strength:Features with coefficients of 0 are effectively thrown away from the model.\n\nCode\nLR = LinearRegression(n_params=PHI.shape[1])\ntrain_model(LR, PHI, y, lr=1e-4, n_epochs=50000, tol = 1e-4, regularization = lambda w: 1.0*ell_1_regularization(w))\n\nfreqs = range(PHI.shape[1]-1)\n\nzeros = torch.abs(LR.w) &lt;= 1e-2\n\nfig, ax = plt.subplots()\nax.scatter(freqs, LR.w.detach().numpy()[:-1], c = zeros.numpy()[:-1], cmap='Greys_r', edgecolors='black')\n\nax.set_xlabel(\"Sinusoidal frequency\")\nax.set_ylabel(\"Coefficient value\")\nt = plt.title(\"Lasso Regression Coefficients (white = zeroed out)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.9: Visualization of the coefficients learned by LASSO regression. Coefficients less than \\(0.01\\) have been highlighted in white. Given a more efficient optimizer, LASSO regression would set these coefficients to exactly 0.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Features and Regularization</span>"
    ]
  },
  {
    "objectID": "chapters/06-regularization.html#model-selection",
    "href": "chapters/06-regularization.html#model-selection",
    "title": "4  Features and Regularization",
    "section": "Model Selection",
    "text": "Model Selection\nIn one way, we’ve just kicked the can down the road – in an environment in which we can’t visually inspect the data or model predictions, how are we supposed to know what features or what regularization strength to use?\nThis is an instance of a problem called model selection, which asks us to make choices between models containing different features or hyperparameters. A very common approach to model selection is to use a validation set. The idea is to split our data into three parts: a training set which we’ll use for actually training our models, a validation set which we’ll use for model selection, and a test set which we’ll use for final evaluation of our chosen model. So, to make choices about the regularization strength, for example, we’ll train separate models with different regularization strengths on the training set, and then evaluate their performance on the validation set. The model with the best validation performance is then selected as the final model.\n\nFeatures In the Wild\nAlthough in the previous examples we engineered our features by hand using basis-function expansions, features are also natural parts of data sets! Often the data set we want to predict already has all the features we need (or more!), and we need to make all the same choices about which features to use and how to regularize. In the case study below, we’ll face some of these same questions without needing to engineer any new features by hand.\n\n\nBike Share Case Study\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom matplotlib import pyplot as plt\n\ndf = pd.read_csv('https://raw.githubusercontent.com/PhilChodrow/ml-notes-update/refs/heads/main/data/bikeshare/hour.csv')\n\nWe’ve downloaded a data set containing hourly bike rental counts in Washington, D.C., along with a variety of features that might be predictive of bike rental demand.  The data contains a variety of features including weather conditions, the year, month, week, and weekday; the hour of the day. It also includes columns for the number of casual and registered users, as well as the total count of bike rentals (cnt). We’ll try to predict the total count of bike rentals using the other features. Let’s take a look:This data set was collected by Fanaee-T and Gama (2013).\n\ndf.head()\n\n\n\n\n\n\n\n\ninstant\ndteday\nseason\nyr\nmnth\nhr\nholiday\nweekday\nworkingday\nweathersit\ntemp\natemp\nhum\nwindspeed\ncasual\nregistered\ncnt\n\n\n\n\n0\n1\n2011-01-01\n1\n0\n1\n0\n0\n6\n0\n1\n0.24\n0.2879\n0.81\n0.0\n3\n13\n16\n\n\n1\n2\n2011-01-01\n1\n0\n1\n1\n0\n6\n0\n1\n0.22\n0.2727\n0.80\n0.0\n8\n32\n40\n\n\n2\n3\n2011-01-01\n1\n0\n1\n2\n0\n6\n0\n1\n0.22\n0.2727\n0.80\n0.0\n5\n27\n32\n\n\n3\n4\n2011-01-01\n1\n0\n1\n3\n0\n6\n0\n1\n0.24\n0.2879\n0.75\n0.0\n3\n10\n13\n\n\n4\n5\n2011-01-01\n1\n0\n1\n4\n0\n6\n0\n1\n0.24\n0.2879\n0.75\n0.0\n0\n1\n1\n\n\n\n\n\n\n\nOur aim is to predict the cnt column using the other features. Let’s visualize the total number of bike rentals over time:\n\nCode\nfig, ax = plt.subplots(figsize = (6, 4))\nridership_by_day = df.groupby(\"dteday\")[\"cnt\"].sum().reset_index()\nridership_by_day[\"dteday\"] = pd.to_datetime(ridership_by_day[\"dteday\"])\n\nax.plot(ridership_by_day[\"dteday\"], ridership_by_day[\"cnt\"], color='steelblue')\n\nplt.xlabel(\"Date\")\nt = plt.ylabel(\"Total Daily Bike Rentals\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.10: Daily ridership in the bikeshare data set.\n\n\n\nPredicting the number of bike rentals on a given day is a very helpful task for bikeshare operators, as this can help them know the urgency of rebalancing (moving bikes between stations) and scheduling maintenance.\nLet’s prepare our data for modeling by dropping some columns and transforming qualitative columns into one-hot encoded features:\n\ndf.drop(columns=['instant', 'dteday', \"workingday\"], inplace=True)\n\ndf = 1.0*pd.get_dummies(df, columns=['season', 'weathersit', 'mnth', 'hr', 'weekday'], drop_first=False)\n\n# ensure that the constant feature is the first column\ndf[\"intercept\"] = 1\ncols = list(df.columns)\ncols.insert(0, cols.pop(cols.index('intercept')))\ndf = df.loc[:,cols]\ndf.columns\n\nIndex(['intercept', 'yr', 'holiday', 'temp', 'atemp', 'hum', 'windspeed',\n       'casual', 'registered', 'cnt', 'season_1', 'season_2', 'season_3',\n       'season_4', 'weathersit_1', 'weathersit_2', 'weathersit_3',\n       'weathersit_4', 'mnth_1', 'mnth_2', 'mnth_3', 'mnth_4', 'mnth_5',\n       'mnth_6', 'mnth_7', 'mnth_8', 'mnth_9', 'mnth_10', 'mnth_11', 'mnth_12',\n       'hr_0', 'hr_1', 'hr_2', 'hr_3', 'hr_4', 'hr_5', 'hr_6', 'hr_7', 'hr_8',\n       'hr_9', 'hr_10', 'hr_11', 'hr_12', 'hr_13', 'hr_14', 'hr_15', 'hr_16',\n       'hr_17', 'hr_18', 'hr_19', 'hr_20', 'hr_21', 'hr_22', 'hr_23',\n       'weekday_0', 'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4',\n       'weekday_5', 'weekday_6'],\n      dtype='object')\n\n\nWe’ll now split the data into features and targets, and then into training, validation, and test sets.\n\nX = 1.0*df.drop(columns=['cnt', 'casual', 'registered'])\ny = df['cnt']\n\ntest_proportion = 0.94\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_proportion, random_state=42)\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=1/2, random_state=42)\n\nX_train = torch.tensor(X_train.values, dtype=torch.float32)\ny_train = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32)\nX_val   = torch.tensor(X_val.values, dtype=torch.float32)\ny_val   = torch.tensor(y_val.values.reshape(-1, 1), dtype=torch.float32)\nX_test  = torch.tensor(X_test.values, dtype=torch.float32)\ny_test  = torch.tensor(y_test.values.reshape(-1, 1), dtype=torch.float32)\n\nWe’ve split the data into three pieces: a small training set (3% of the data), a validation set (3% of the data), and a test set (94% of the data). Given the size of the data set, this split is reasonable – we have enough data to train models effectively, while still leaving a large amount of data for final evaluation.\n\nprint(f\"Training set size: {X_train.shape[0]} samples\")\nprint(f\"Validation set size: {X_val.shape[0]} samples\")\nprint(f\"Test set size: {X_test.shape[0]} samples\")\n\nTraining set size: 521 samples\nValidation set size: 521 samples\nTest set size: 16337 samples\n\n\n\n\n\n\n\n\nNoteWhy so few training samples?\n\n\n\nIssues related to feature engineering and regularization most frequently arise in settings where we many parameters and relatively few data points. Therefore, to illustrate these issues clearly, we’ve deliberately limited the size of the training set. In practice, of course, one would typically want to use as much data as possible for training.\nAlthough this example is a bit artificial, modern neural networks are often trained with vastly more parameters than data points, making these considerations quite relevant in practice.\n\n\nTo contextualize model performance on real data, it’s helpful to compute a base rate.\n\nDefinition 4.2 (Base Rate) A base rate is a measure of performance for a model which uses no features in the data. We typically say that a model has demonstrated success in learning from features if it outperforms the base rate on unseen data.\n\nLet’s check the base rate for the bikeshare data and assess on validation data:\n\ny_mean = y_train.mean()\nbase_rate_mse = mse(y_val, y_mean)\nprint(f\"Base rate MSE on validation data: {base_rate_mse.item():.4f}\")\n\nBase rate MSE on validation data: 33617.3125\n\n\nSo, we are looking for any reasonable candidate model to achieve validation MSE less than the above.\nIn principle, we can already go ahead and fit a model:\n\nLR = LinearRegression(n_params=X_train.shape[1])\ntrain_model(LR, X_train, y_train, lr=1e-2, n_epochs=50000, tol=1e-4)\ny_pred = LR.forward(X_val)\nval_mse = mse(y_pred, y_val)\nprint(f\"Validation MSE without regularization: {val_mse.item():.4f}\")\nprint(f\"Fraction of base rate: {val_mse.item()/base_rate_mse.item():.4f}\")\n\nValidation MSE without regularization: 12257.3740\nFraction of base rate: 0.3646\n\n\nThis simple linear regression model with no regularization already performs considerably better on validation data than the base rate. Can we do better with regularization? To find out, we’ll do a systematic search in which we vary the regularization strength and evaluate performance on validation data, for each of ridge regression and LASSO.\n\nCode\nreg_strengths = torch.logspace(-5, 0, 21)  \n\ntrain_mses = []\nval_mses = []\n\nW = torch.empty(X_train.shape[1], 0)\n\nfor reg in reg_strengths: \n\n    LR = LinearRegression(n_params=X_train.shape[1])\n    train_model(LR, X_train, y_train, lr=1e-1, n_epochs=20000, tol = 1e-2, regularization = lambda w: reg*ell_2_regularization(w))\n\n    y_pred_train = LR.forward(X_train)\n    train_mse = mse(y_pred_train, y_train)\n    y_pred_val = LR.forward(X_val)\n    val_mse = mse(y_pred_val, y_val)\n\n    train_mses.append(train_mse.item())\n    val_mses.append(val_mse.item())\n\n    W = torch.cat((W, LR.w), dim=1)\n\nbest_reg = reg_strengths[val_mses.index(min(val_mses))]\n\nfig, ax = plt.subplots(1, 2, figsize = (8,3))\nax[0].plot(reg_strengths, train_mses, label='Train MSE')\nax[0].plot(reg_strengths, val_mses, label='Validation MSE')\n\nax[0].axvline(best_reg, color='black', linestyle='--', label=fr'Best $\\lambda=${best_reg:.2f}, val MSE {min(val_mses):.1f}')\n\n\nax[0].set_xlabel(\"Regularization Strength\")\nax[0].set_ylabel(\"MSE\")\nax[0].set_xscale(\"log\")\nplt.title(\"Ridge Regression: Train and Validation MSE vs Regularization Strength\")\nax[0].legend()\n\nax[1].plot(reg_strengths, W.detach().numpy().T)\nax[1].axvline(best_reg, color='black', linestyle='--', label=f'Best reg={best_reg:.4f}')\nax[1].set_xlabel(\"Regularization Strength\")\nax[1].set_ylabel(\"Weights\")\nax[1].set_xscale(\"log\")\nplt.title(\"Ridge Regression: Weights vs Regularization Strength\")\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.11: Model selection in ridge regression on the bikeshare data set. (Left): training and validation MSE as a function of regularization strength. (Right): learned weights as a function of regularization strength.\n\n\n\nNow let’s try the same experiment for LASSO.\n\nCode\nreg_strengths = torch.logspace(0, 2, 21)  \n\ntrain_mses = []\nval_mses = []\n\nW = torch.empty(X_train.shape[1], 0)\n\nfor reg in reg_strengths: \n\n    LR = LinearRegression(n_params=X_train.shape[1])\n    train_model(LR, X_train, y_train, lr=1e-2, n_epochs=50000, tol = 1e-2, regularization = lambda w: reg*ell_1_regularization(w))\n\n    y_pred_train = LR.forward(X_train)\n    train_mse = mse(y_pred_train, y_train)\n    y_pred_val = LR.forward(X_val)\n    val_mse = mse(y_pred_val, y_val)\n\n    train_mses.append(train_mse.item())\n    val_mses.append(val_mse.item())\n\n    W = torch.cat((W, LR.w), dim=1)\n\nbest_reg = reg_strengths[val_mses.index(min(val_mses))]\n\nfig, ax = plt.subplots(1, 2, figsize = (8,3))\nax[0].plot(reg_strengths, train_mses, label='Train MSE')\nax[0].plot(reg_strengths, val_mses, label='Validation MSE')\n\nax[0].axvline(best_reg, color='black', linestyle='--', label=fr'Best $\\lambda=${best_reg:.2f}, val MSE {min(val_mses):.1f}')\n\n\nax[0].set_xlabel(\"Regularization Strength\")\nax[0].set_ylabel(\"MSE\")\nax[0].set_xscale(\"log\")\nplt.title(\"LASSO Regression: Train and Validation MSE vs Regularization Strength\")\nax[0].legend()\n\nax[1].plot(reg_strengths, W.detach().numpy().T)\nax[1].axvline(best_reg, color='black', linestyle='--', label=f'Best reg={best_reg:.4f}')\nax[1].set_xlabel(\"Regularization Strength\")\nax[1].set_ylabel(\"Weights\")\nax[1].set_xscale(\"log\")\nplt.title(\"LASSO Regression: Weights vs Regularization Strength\")\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.12: Model selection in LASSO regression on the bikeshare data set. (Left): training and validation MSE as a function of regularization strength. (Right): learned weights as a function of regularization strength.\n\n\n\nWe find that LASSO regression achieves a slightly lower MSE on validation data than ridge regression.\nLet’s train the LASSO model one last time with the best regularization strength and take a look at the learned coefficients:\n\nreg_strength = reg_strengths[val_mses.index(min(val_mses))]\nLR = LinearRegression(n_params=X_train.shape[1])\ntrain_model(LR, X_train, y_train, lr=1e-1, n_epochs=20000, tol = 1e-4, regularization = lambda w: reg_strength*ell_1_regularization(w))\n\nWe can inspect the coefficients to see which features the model found most important:\n\ncoefs = pd.DataFrame({\n    'Feature': X.columns,\n    'Coefficient': LR.w.detach().numpy().flatten()\n}).sort_values(by='Coefficient', key=abs, ascending=False)\ncoefs.head()\n\n\n\n\n\n\n\n\nFeature\nCoefficient\n\n\n\n\n44\nhr_17\n245.015442\n\n\n35\nhr_8\n219.449371\n\n\n3\ntemp\n174.334061\n\n\n45\nhr_18\n144.749573\n\n\n31\nhr_4\n-144.738724\n\n\n\n\n\n\n\nWe see that rush hours and high temperatures are identified by the model as highly predictive of bike rental demand.\nFinally, let’s evaluate our chosen model on the test set to get a final estimate of performance:\n\ny_pred_test = LR.forward(X_test)\ntest_mse = mse(y_pred_test, y_test)\nprint(f\"Test MSE: {test_mse.item():.4f}\")\nprint(f\"Fraction of base rate: {test_mse.item()/base_rate_mse.item():.4f}\")\n\nTest MSE: 11489.1650\nFraction of base rate: 0.3418\n\n\nOur final model achieves a test MSE considerably lower than the base rate, suggesting that it has successfully learned to predict bike rental demand using the available features.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Features and Regularization</span>"
    ]
  },
  {
    "objectID": "chapters/06-regularization.html#references",
    "href": "chapters/06-regularization.html#references",
    "title": "4  Features and Regularization",
    "section": "References",
    "text": "References\n\n\n\n\nFanaee-T, Hadi, and Joao Gama. 2013. “Event Labeling Combining Ensemble Detectors and Background Knowledge.” Progress in Artificial Intelligence, 1–15. https://doi.org/10.1007/s13748-013-0040-3.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Features and Regularization</span>"
    ]
  },
  {
    "objectID": "chapters/07-bias-variance.html",
    "href": "chapters/07-bias-variance.html",
    "title": "5  More on Overfitting",
    "section": "",
    "text": "Introduction\nOpen the live notebook in Google Colab.\nSeveral times in these notes, we’ve seen the topic of overfitting arise. In this set of lecture notes, we’ll define overfitting more formally. We’ll then look at overfitting from two perspectives: the bias-variance decomposition, and the modern phenomenon of double descent.\nIn terms of the signal + noise paradigm of modeling,\n\\[\n\\begin{align}\ny & = f(x) + \\epsilon\n\\end{align}\n\\]\noverfitting occurs when an estimator begins to approximate the noise \\(\\epsilon\\) rather than the signal \\(f(x)\\).\nFor several decades, the received wisdom in the statistics and machine learning communities was that models begin to overfit when they become “too complex.” Model complexity is frequently measured in terms of the number of parameters present in the model. So far in these notes, we’ve exclusively studied models where the number of parameters is equal to the number of features, so another way to think about model complexity is in terms of the total number of features.\n© Phil Chodrow, 2025",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>More on Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/07-bias-variance.html#introduction",
    "href": "chapters/07-bias-variance.html#introduction",
    "title": "5  More on Overfitting",
    "section": "",
    "text": "Definition 5.1 (Overfitting) Overfitting refers to any situation in which increasing the complexity of a model causes the model to improve its performance on training data but worsen its performance on test data.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>More on Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/07-bias-variance.html#bias-variance-decomposition",
    "href": "chapters/07-bias-variance.html#bias-variance-decomposition",
    "title": "5  More on Overfitting",
    "section": "Bias-Variance Decomposition",
    "text": "Bias-Variance Decomposition\nOne theoretical view of the sources of overfitting comes from the bias-variance decomposition.\n\nMany data sets, many models\nIn the bias-variance decomposition, we consider a series of experiments. In each experiment, we pull a data set \\((\\mathbf{X},\\mathbf{y})\\) from some data generating distribution, fit a model to that data, and then make a prediction about an element \\(Y\\) in the test set. Since the data set we pull in each experiment is random, the value \\(Y\\) in the test set is random. Furthermore, since the training data is random, the fitted model \\(\\hat{f}\\) and the resulting prediction \\(\\hat{Y}\\) will also be random.\nLet’s illustrate the setup. For the experiments in this chapter, we need to run a LOT of regression models. For this reason, we’ll use a special, very efficient implementation of linear regression that allows us to fit models in closed form. This is not a general approach to fitting models, but it will allow us to run the experiments in this chapter much more quickly.\n\nimport torch\nclass LinearRegression:\n    def __init__(self, n_params):\n        self.w = torch.zeros(n_params, 1, requires_grad=True)\n \n    def forward(self, X):\n        return X @ self.w\n\nclass ClosedFormOptimizer: \n    def __init__(self, model):\n        self.model = model\n    \n    def step(self, X, y):\n        self.model.w = torch.linalg.lstsq(X, y).solution\n\ndef train(model, X, y): \n    opt = ClosedFormOptimizer(model)\n    opt.step(X, y)\n\n\nThis is a method that obtains the least-squares solution to the linear regression problem using specialized linear algebra methods. It’s very fast and useful for us today, but doesn’t generalize well to other kinds of machine learning problems.\n\nNow we’ll illustrate generating many data sets from the same data generating distribution, fitting a model, and making a prediction \\(\\hat{Y}\\) for some new unseen point \\(x\\) in the test set. For today, we’ll consider the following data generating function, which accepts a fixed set of inputs \\(x\\) and generates a random output \\(y\\) for each input according to the signal + noise paradigm. Unlike in many other models, this particular data generating process gets noisier as \\(x\\) increases, which will allow us to illustrate how the bias and variance of the model can vary across different regions of the input space.\n\ndef generate_data(x, sig, freq):\n    signal = torch.sin(freq * torch.pi * x) + x\n    noise = torch.randn_like(x)*sig*x\n    y = signal + noise\n    return y\n\n\nCode\nfrom matplotlib import pyplot as plt\n\nsig = 0.2\nfreq = 0.8\n\nfig, ax = plt.subplots(1, 3, figsize=(7, 2.5))\n\nn_points = 25\nx = torch.rand(n_points,)\nx = torch.sort(x).values\ntest_ix = 10\n\nfor i in range(3):\n    y = generate_data(x, sig=sig, freq=freq)\n\n    x_test = x[test_ix].unsqueeze(0)\n    y_test = generate_data(x_test, sig=sig, freq=freq)\n    ax[i].scatter(x, y, alpha=0.8, color='grey')\n    ax[i].scatter(x_test, y_test, alpha=0.8, color='k', zorder = 100)\n    ax[i].set_title(f\"Experiment {i+1}\")\n    ax[i].set_xlabel(r\"$x$\")\n    if i == 0:\n        ax[i].set_ylabel(r\"$y$\")\n\n    model = LinearRegression(n_params=2)\n    X = torch.stack([torch.ones_like(x), x], dim=1)\n    train(model, X, y)\n\n    x_viz = torch.linspace(0, 1, 200)\n    X_viz = torch.stack([torch.ones_like(x_viz), x_viz], dim=1)\n    y_pred = model.forward(X_viz).detach().squeeze()\n\n    ax[i].plot(x_viz, y_pred, color='black', label=\"Model Prediction\", linestyle='--')\n\n    X_test = torch.stack([torch.ones_like(x_test), x_test], dim=1)\n    y_pred_test = model.forward(X_test).detach().squeeze()\n    ax[i].plot([x_test.item(), x_test.item()], [y_test.item(), y_pred_test.item()], color='orange', label=\"Prediction at test point\", markersize=10, zorder = 10)\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.1: Illustration of three different experiments in which we pull a random data set from the data generating process, fit a linear regression model, and make a prediction at a random test point. The data points are shown in black, the fitted model is shown in red, and the prediction at the test point is shown with residual in blue.\n\n\n\nNote that, although each plot is somewhat similar, the data points, fitted model, and prediction at the test point are all slightly different across the three experiments. This is because the data we pull in each experiment is random, which causes the fitted model and resulting prediction to also be random.\nLet’s repeat the experiment a large number of times and store the results.\n\nn_reps = 1000\ntargets = torch.zeros(n_reps, len(x))\npredictions = torch.zeros(n_reps, len(x))\n\nfor i in range(n_reps):\n1    y_train = generate_data(x, sig=sig, freq=freq)\n2    X = torch.stack([torch.ones_like(x), x], dim=1)\n    \n3    model = LinearRegression(n_params=2)\n    train(model, X, y_train)\n    predictions[i] = model.forward(X)\n\n4    targets[i] = generate_data(x, sig=sig, freq=freq)\n\n\n1\n\nGenerate the training data.\n\n2\n\nAdd a constant feature to the input data for the intercept term in linear regression.\n\n3\n\nFit the linear regression model to the training data.\n\n4\n\nGenerate the target values for the test data, independently from the training data.\n\n\n\n\nWe now have 1,000 different fitted models and predictions. Let’s visualize the ensemble of fitted models:\n\nCode\nfig, ax = plt.subplots(1, 1, figsize=(6, 4))\n\ntest_point_ix = 10\nfor i in range(n_reps): \n    ax.plot([x[0], x[-1]], [predictions[i,0], predictions[i,-1]], color='black', alpha=0.005, zorder = 0)\n    ax.set(xlabel = r\"$x$\", ylabel = r\"$y$\", title = \"Ensemble of Linear Regression Models\")\n\nax.plot([x[test_point_ix], x[test_point_ix]], [predictions[:, test_point_ix].min(), predictions[:, test_point_ix].max()],  color='red', label=\"Prediction range at test point\", zorder = 200, linewidth = 6, alpha = 0.3)\nax.plot([x[test_point_ix], x[test_point_ix]], [targets[:, test_point_ix].min(), targets[:, test_point_ix].max()],  color='grey', label=\"Range across ensemble of data sets at test point\", zorder = 100,  linestyle='--')\n\nl = ax.legend()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.2: Visualization of the ensemble of fitted linear regression models across 1,000 different experiments. The range of predictions at a particular test point is shown in red, while the range of target values at that test point is shown in grey.\n\n\n\nCollectively, the models capture the general trend, but each varies slightly from the others. The prediction \\(\\hat{Y}\\) at any given point \\(x\\) is random, and the target value \\(Y\\) at that point is also random.\n\n\nDecomposing the Error\nSo, at any given point \\(x\\), we have a random target value \\(Y\\) and a random prediction \\(\\hat{Y}\\). We can measure how close \\(\\hat{Y}\\) is to \\(Y\\) using the expected squared error (this is the theoretical analogue of the MSE):  \\[\n\\begin{align*}\\mathcal{E}& = \\mathbb{E}[(Y - \\hat{Y})^2]\\end{align*}\n\\]Many presentations of the bias-variance decomposition consider the complete expected squared error of the model on all possible inputs. This has the effect of throwing an integral sign in front of the calculation, but does not otherwise change the story.\nImportant: because we are assuming that \\(Y\\) is from the test data set, on which \\(\\hat{Y}\\) is not trained, \\(\\hat{Y}\\) and \\(Y\\) are independent random variables.\nLet’s now study what contributes to the expected testing squared error \\(\\mathcal{E}\\). We can decompose \\(\\mathcal{E}\\) into three terms: the bias of the model, the variance of the model, and the noise in the data. It’s helpful to define \\(\\mu = \\mathbb{E}[Y]\\) and \\(\\hat{\\mu} = \\mathbb{E}[\\hat{Y}]\\) to be the expected value of the target and prediction, respectively. Throughout this calculation, we are using standard algebra rules and the linearity properties of expectation: for any \\(a,b\\in \\mathbb{R}\\) and random variables \\(Z_1\\) and \\(Z_1\\), \\(\\mathbb{E}[aZ_1 + bZ_2] = a\\mathbb{E}[Z_1] + b\\mathbb{E}[Z_2]\\). We’re also at a key point using the independence property: if \\(Z_1\\) and \\(Z_2\\) are independent random variables, then \\(\\mathbb{E}[Z_1Z_2] = \\mathbb{E}[Z_1]\\mathbb{E}[Z_2]\\).\n\\[\n\\begin{aligned}\n    \\mathcal{E}&= \\mathbb{E}[(\\hat{Y} - Y)^2] \\\\\n    &= \\mathbb{E}[\\hat{Y}^2 - 2\\hat{Y}Y + Y^2] \\\\\n    &= \\mathbb{E}[\\hat{Y}^2] - 2\\mathbb{E}[\\hat{Y}]\\mathbb{E}[Y] + \\mathbb{E}[Y^2] \\\\\n    &= \\mathbb{E}[\\hat{Y}^2] - 2\\hat{\\mu}\\mu + \\mathbb{E}[Y^2] &\\quad \\text{(independence of $Y$ and $\\hat{Y}$)} \\\\\n    &= \\mathbb{E}[\\hat{Y}^2] - \\hat{\\mu}^2 + \\hat{\\mu}^2 - 2\\hat{\\mu}\\mu + \\mu^2 + \\mathbb{E}[Y^2] - \\mu^2 \\\\\n    &= (\\mathbb{E}[\\hat{Y}^2] - \\hat{\\mu}^2) + (\\hat{\\mu} - \\mu)^2 + (\\mathbb{E}[Y^2] - \\mu^2) \\\\\n    &= \\underbrace{\\mathrm{Var}(\\hat{Y})}_\\text{model variance} + \\underbrace{(\\hat{\\mu} - \\mu)^2}_\\text{bias} + \\underbrace{\\mathrm{Var}(Y)}_\\text{noise}\n\\end{aligned}\n\\]\nEach of the terms in this expression are importantly interpretable:\nThe model variance \\(\\mathrm{Var}(\\hat{Y})\\) captures how much the prediction \\(\\hat{Y}\\) varies across different data sets and fitted models. If the model is very sensitive to the particular data set it is trained on, then the model variance will be high. Models which are more flexible (e.g. by having more parameters and features) tend to have higher model variance.\nThe bias \\((\\hat{\\mu} - \\mu)^2\\) captures how much the expected prediction \\(\\hat{\\mu}\\) differs from the expected target \\(\\mu\\). If the model is very inflexible and cannot capture the true signal, then the bias will be high. Models which are less flexible (e.g. by having fewer parameters and features) tend to have higher bias.\nFinally, the data noise \\(\\mathrm{Var}(Y)\\) captures how much the target value \\(Y\\) varies across different data sets. This is a property of the data generating process. It is impossible to achieve test error lower than the data noise, since the noise is a property of the data generating process and not the model.\nLet’s compute each of these terms from the linear regression experiment that we did earlier. Since our analysis above was at a specific data point, we are going to compute values at each data point and then compare.\n\nnoise    = targets.var(dim = 0, correction = 0) \nvariance = predictions.var(dim = 0, correction = 0) \nbias     = (targets.mean(dim = 0) - predictions.mean(dim = 0))**2\nmse      = ((targets - predictions)**2).mean(dim = 0)\n\nNow we can get a quantitative description of the sources of error in our model by plotting each one and the total mean-squared error. We’ll show this alongside an example data set and predictor for comparison:\n\nCode\nfig, ax = plt.subplots(1, 2, figsize=(7, 3.5))\n\nax[0].scatter(x, targets[0], alpha = 0.5, color = \"grey\")\ny_hat = predictions.mean(dim = 0)\n\nax[0].plot(x, y_hat, color='black', linestyle='--', label=\"Model Prediction\")\nax[0].set(xlabel = r\"$x$\", ylabel = r\"$y$\", title = \"Example data set and predictor\")\n\nax[1].plot(x, noise, label = \"data noise\", linestyle = \"--\")\nax[1].plot(x, variance, label = \"model variance\", linestyle = \"--\")\nax[1].plot(x, bias, label = \"model bias\", linestyle = \"--\")\nax[1].plot(x, mse, label = \"mse\", color = \"black\")\n\nax[1].set(xlabel = r\"$x$\", ylabel = \"Error\", title = \n\"Bias-Variance Decomposition\")\n\nplt.legend()\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.3: Illustration of the bias-variance tradeoff in the repeated linear regression experiment. The left panel shows an example data set and predictor, while the right panel shows the bias, variance, noise, and total mean-squared error as a function of \\(x\\).\n\n\n\nIn this experiment, the noise and bias are the two primary contributors to the mean-squared error, with the model variance being quite low. The data generating process is deliberately constructed so that the data is noisier in certain regions, and so that the model bias also varies across regions.\n\n\nThe Bias-Variance Tradeoff\nSince the data noise term in the bias-variance decomposition is a property of the data generating process, the only way to reduce test error is to find a model that reduces either the bias or the variance, without increasing the other. We can often tune the bias and variance by adjusting the model complexity. For example, in the linear regression experiment above, we can increase the number of features. This will have the effect of increasing the variance, while reducing the bias. Let’s try this out, generating the same data many times and then evaluating the bias, variance, and noise for different numbers of features.\n\n\nCode\ndef polynomial_features(x, degree):\n    \"\"\"Compute polynomial features for input x up to given degree.\"\"\"\n    features = [torch.ones_like(x)]  # x^0\n    for d in range(1, degree + 1):\n        features.append(x**d)\n    return torch.stack(features, dim=1)  # (n_points, degree + 1)\n\nfeature_counts = torch.arange(0, 10)\n\nmse_list = []\nbias_list = []\nvariance_list = []\nnoise_list = []\n\nfor degree in feature_counts:\n    predictions = torch.zeros(n_reps, len(x))\n    targets = torch.zeros(n_reps, len(x))\n\n    for i in range(n_reps):\n        y_train = generate_data(x, sig=sig, freq=freq)\n        \n        model = LinearRegression(n_params=degree + 1)\n        X = polynomial_features(x, degree=degree)\n        train(model, X, y_train)\n        predictions[i] = model.forward(X).detach().squeeze()\n\n        y_test = generate_data(x, sig=sig, freq=freq)\n        targets[i] = y_test\n\n    noise    = targets.var(dim = 0, correction = 0) \n    variance = predictions.var(dim = 0, correction = 0) \n    bias     = (targets.mean(dim = 0) - predictions.mean(dim = 0))**2\n    mse      = ((targets - predictions)**2).mean(dim = 0)\n\n\n    mse_list.append(mse.mean())\n    bias_list.append(bias.mean())\n    variance_list.append(variance.mean())\n    noise_list.append(noise.mean())\n\n\n\nCode\nfig, ax = plt.subplots(1, 1, figsize=(6, 4))\nax.plot(feature_counts, noise_list, label = \"data noise\", linestyle = \"--\")\nax.plot(feature_counts, variance_list, label = \"model variance\", linestyle = \"--\")\nax.plot(feature_counts, bias_list, label = \"model bias\", linestyle = \"--\")\nax.plot(feature_counts, mse_list, label = \"mse\", color = \"black\")\n\nbest_degree = feature_counts[torch.argmin(torch.tensor(mse_list))]\nax.plot(best_degree, mse_list[best_degree], marker='o', color='k', label=\"Best MSE\")\n\nax.set(xlabel = r\"Model complexity\", ylabel = \"Mean Squared Error\", title = \"Bias-Variance Tradeoff\")\nax.legend()\nax.semilogy()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.4: Illustration of the bias-variance tradeoff. As the model becomes more flexible, the bias decreases while the variance increases. The optimal model complexity balances these two sources of error to minimize the mean-squared error.\n\n\n\nFigures like Figure 5.4 were canon in the machine learning literature for a long time, with a simple message: more model complexity reduces bias but increases variance. Too much complexity will have diminishing rewards for reducing bias but will tend to increase the variance indefinitely, eventually resulting in overfitting. So don’t make your models too complex!",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>More on Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/07-bias-variance.html#the-modern-landscape-interpolation-and-double-descent",
    "href": "chapters/07-bias-variance.html#the-modern-landscape-interpolation-and-double-descent",
    "title": "5  More on Overfitting",
    "section": "The Modern Landscape: Interpolation and Double Descent",
    "text": "The Modern Landscape: Interpolation and Double Descent\nFor a long time in statistics and machine learning, the single-descent curve offered the primary way in which we thought about model complexity. Models with low complexity underfit the data (high bias, low variance), while models with high complexity overfit the data (low bias, high variance). The optimal model complexity balanced these two sources of error to minimize test error. The key was to get enough complexity while avoiding overfitting. The thing you never wanted to do was to interpolate the data, since interpolation is perfect overfitting.\nThis theoretical story was challenged by the advent of deep learning as a practical tool. Modern deep learning models often have parameter counts in the billions or trillions, which in principle is often enough to perfectly interpolate training data. So why is it that these models nevertheless successfully generalize?\nIt turns out that there are both bad and good ways to interpolate your data. Models that have the capacity for “good” interpolation can achieve very low test error even while interpolating the training data. Let’s see an example. This example was inspired by Manuchehr Aminian’s blog post in SIAM News on double descent.\nFor this problem, we’ll suppose that we need to train a model on very few data points: Some aspects of this experiment, including the very small number of data sets and use of a special set of polynomial basis functions are somewhat contrived (i.e. carefully staged). This is because double descent is primarily a phenomenon that takes place for models with very large parameter counts. Since we want to illustrate this phenomenon in a way that’s easy to visualize, we have to set things up to make sure it happens here.\n\nCode\nimport torch \nfrom matplotlib import pyplot as plt\n\ntorch.manual_seed(123)\n\nn_points = 10\nsig = 0.2\nfreq = 5\n\nx = torch.rand(n_points,)\nsignal = torch.sin(freq * torch.pi * x) + x\nnoise = torch.randn(n_points,)*sig\ny = signal + noise\n\nx_test = torch.rand(10*n_points,)\nsignal_test = torch.sin(freq * torch.pi * x_test) + x_test\nnoise_test = torch.randn(10*n_points,)*sig\ny_test = signal_test + noise_test\n\nx_viz = torch.linspace(0, 1, 200)\nsignal_viz = torch.sin(freq * torch.pi * x_viz) + x_viz\n\nfig, ax = plt.subplots(figsize = (8,5))\n\nax.plot(x_viz, signal_viz, color='grey', label=\"True Signal\", zorder = -10, linewidth=2, linestyle='--')\nax.scatter(x, signal + noise, alpha=0.8, label=\"Data\", color = \"k\")\nax.scatter(x_test, signal_test + noise_test, alpha=0.8, label=\"Test Data\", color='k', facecolors='none')\nax.set_xlabel(r\"$x$\")\nax.set_ylabel(r\"$y$\")\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.5: Example training and test data for the double descent experiment. The true signal is shown in grey, training data as black dots, and test data as black circles.\n\n\n\nFor our feature map in this experiment, we are going to make use of a special set of polynomial features called Legendre polynomials. The first few feature maps are:\n\\[\n\\begin{align*}\n\\phi_0(x) & = 1 \\\\\n\\phi_1(x) & = x \\\\\n\\phi_2(x) & = \\frac{1}{2}(3x^2 - 1) \\\\\n\\phi_3(x) & = \\frac{1}{2}(5x^3 - 3x) \\\\\n\\phi_4(x) & = \\frac{1}{8}(35x^4 - 30x^2 + 3) \\\\\n\\vdots\n\\end{align*}\n\\]\nIt’s ok if these coefficients look quite mysterious and random to you – the study of the Legendre polynomials is a beautiful and deep topic but not our primary purpose here.\n\n\n\nCode\ndef legendre_features(x, degree):\n    \"\"\"Compute Legendre polynomial features for input x up to given degree.\"\"\"\n    features = [torch.ones_like(x)]  # P_0(x) = 1\n    if degree &gt;= 1:\n        features.append(x)  # P_1(x) = x\n    for n in range(2, degree + 1):\n        P_n = ((2*n - 1)*x*features[-1] - (n - 1)*features[-2]) / n\n        features.append(P_n)\n    return torch.stack(features, dim=1)  # (n_points, degree + 1)\n\n\n\n\n\n\n\nThe first five Legendre polynomial features on the interval [0, 1].\n\n\n\nWe can fit a model using the Legendre polynomial features to data like this:\n\ndegree = 5\nmodel = LinearRegression(n_params=degree + 1)\nX = legendre_features(x, degree=degree)\ntrain(model, X, y)\n\nx_viz = torch.linspace(0, 1, 200)\nX_viz = legendre_features(x_viz, degree=degree)\ny_pred = model.forward(X_viz)\n\n\n\n\n\n\n\nLegendre polynomial regression (degree 5) on training and test data\n\n\n\nLet’s now run an experiment in which we vary the number of polynomial features over a broad range and track the training and test mean squared error (MSE). Recall that the number of features is equal to the polynomial degree plus one (to account for the constant feature).\n\ndegree_list = list(range(0, 70))\ntrain_mse_list = []\ntest_mse_list = []\nfeature_count = []\n\nfor degree in degree_list:\n    model = LinearRegression(n_params=degree + 1)\n    X = legendre_features(x, degree=degree)\n    train(model, X, y)\n\n    X_test = legendre_features(x_test, degree=degree)\n    y_pred_test = model.forward(X_test).detach().squeeze()\n    train_mse = torch.mean((model.forward(X) - y)**2).item()\n    test_mse = torch.mean( (y_pred_test - y_test)**2).item()\n\n    train_mse_list.append(train_mse)\n    test_mse_list.append(test_mse)\n    feature_count.append(degree + 1)\n\n\nCode\nfig, ax = plt.subplots(figsize = (8,5))\nax.plot(feature_count, train_mse_list, color = \"grey\")\nax.plot(feature_count, test_mse_list, color = \"steelblue\")\nax.annotate(\"Train MSE\", xy=(feature_count[-1]-30, train_mse_list[-1]*10), color='grey')\nax.annotate(\"Test MSE\", xy=(feature_count[-1]-30, test_mse_list[-1]*10), color='steelblue')\n\nax.fill_betweenx([1e-20, 1e9], 0, n_points, color='grey', alpha=0.1)\nax.annotate(\"Classical\\nregime\", xy=(1, 1e-7), color='grey')\n\nax.fill_betweenx([1e-20, 1e9], n_points, 2*n_points, color='firebrick', alpha=0.1)\nax.annotate(\"Classical\\ndanger\\nzone\", xy=(n_points + 1, 1e-3), color='firebrick')\n\nax.fill_betweenx([1e-20, 1e9], 2*n_points, max(feature_count), color='green', alpha=0.1)\nax.annotate(\"Modern\\nregime\", xy=(4*n_points + 1, 1e-5), color='green')\n\n\nax.axvline(x=feature_count[torch.argmin(torch.tensor(test_mse_list))], color='black', linestyle='--')\nax.annotate(\"Best Test MSE\", xy=(feature_count[torch.argmin(torch.tensor(test_mse_list))] -15, max(test_mse_list)/4), color='black')\n\nax.axvline(x=n_points, color='black', linestyle='--')\nax.annotate(\"Interpolation Threshold\", xy=(n_points + 1, max(test_mse_list)*10), color='black')\n\nax.set_xlabel(\"Number of Features (Polynomial Degree + 1)\")\nax.set_ylabel(\"Mean Squared Error\")\nax.set_title(f\"Double descent in Legendre polynomial regression\")\nax.set_ylim(min(min(test_mse_list), min(train_mse_list))/10, max(max(test_mse_list), max(train_mse_list))*1000)\nax.set_xlim(0, max(feature_count))\nax.semilogy()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.6: Training and test MSE as a function of the number of features. The vertical dashed lines indicate the interpolation threshold and the optimal number of features according to the test MSE. The classical, classical danger zone, and modern regimes are indicated with different background colors.\n\n\n\nWe observe that the test MSE increases considerably as we add more features up to the interpolation threshold, and peaks shortly after. However, as we continue to add features, something surprising happens: the test MSE begins to decrease again, and eventually reaches a minimum value beyond the interpolation threshold. This is what is sometimes called the “modern regime” for model complexity in high-dimensional machine learning.\nWhat does this look like in terms of the actual models we fit?\n\nCode\nfig, axarr = plt.subplots(2, 2, figsize=(8, 7))\nbest_num_features = feature_count[torch.argmin(torch.tensor(test_mse_list))]\n\nfor i, num_features in enumerate([2, 5, 10, best_num_features]): \n\n    degree = num_features - 1\n    model = LinearRegression(n_params=degree + 1)\n    X = legendre_features(x, degree=degree)\n    train(model, X, y)\n\n    x_viz = torch.linspace(0, 1, 200)\n    X_viz = legendre_features(x_viz, degree=degree)\n    y_pred = model.forward(X_viz).detach().squeeze()\n\n    ax = axarr.flatten()[i]\n\n    ax.plot(x_viz, signal_viz, color='k', label=\"True Signal\", zorder = -10)\n    ax.scatter(x, signal + noise, alpha=0.5, label=\"Training data\", color = \"k\")\n    ax.scatter(x_test, signal_test + noise_test, alpha=0.5, label=\"Test Data\", color='k', facecolors='none')\n\n    ax.plot(x_viz, y_pred, color='red', label=\"Model Prediction\")\n    ax.set_title(f\"Num Features = {num_features}\")\n    ax.set_ylim(min(y_test.min(), y.min()) - 0.5, max(y_test.max(), y.max()) + 0.5)\n\n    if i == 0:\n        ax.legend(ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.7: Legendre polynomial regression models (red) with different numbers of features, visualized with training and test data as well as the true signal (black).\n\n\n\nWe observe that for small number of features, the model underfits the data, while as we approach the interpolation threshold the model overfits the data in a way that causes the model predictions to swing wildly. However, as we increase the number of features further, the model predictions become much smoother and closer to the true signal, which explains the improved test MSE.\nDouble descent remains an actively explored area of research with major practical implications in the context of deep learning models.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>More on Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/10-intro-classification.html",
    "href": "chapters/10-intro-classification.html",
    "title": "6  Introduction: Binary Labels",
    "section": "",
    "text": "Introduction\nOpen the live notebook in Google Colab.\nIn this set of notes, we’ll begin our investigation into classification. Whereas in regression we aimed to predict a number (like an amount of rainfall, or the price of a house), in classification we aim to predict a category (like whether an email is spam or not, or whether a tumor is malignant or benign).\n© Phil Chodrow, 2025",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction: Binary Labels</span>"
    ]
  },
  {
    "objectID": "chapters/10-intro-classification.html#introduction",
    "href": "chapters/10-intro-classification.html#introduction",
    "title": "6  Introduction: Binary Labels",
    "section": "",
    "text": "Classification and Decision-Making\nClassification is in many contexts a more practically-relevant task than regression, because classification relates directly to decision-making. For example, consider a spam filter. The goal of a spam filter is to classify incoming emails as either “spam” or “not spam”. This classification directly informs the decision of whether to deliver the email to the user’s inbox or to the spam folder.\n\n\nData = Signal + Noise: Classification Edition\nWhen studying regression, we considered a framework in which the data \\(y_i\\) was generated according to a process of the form\n\\[\ny_i = f(\\mathbf{x}_i) + \\epsilon_i\\;,\n\\]\nwhere \\(f\\) was a deterministic function of the input \\(\\mathbf{x}_i\\), and \\(\\epsilon_i\\) was a random noise term. Our goal was to learn the signal \\(f\\) rather than the noise \\(\\epsilon_i\\). For classification we still want to use the “signal + noise” paradigm, but the presence of categorical data means that we need to make some adjustments to the framework. In particular, since \\(y_i\\) is now a category rather than number, we can’t write it as the “sum” of anything, so our idea of “signal + noise” will be a bit metaphorical.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction: Binary Labels</span>"
    ]
  },
  {
    "objectID": "chapters/10-intro-classification.html#binary-classification",
    "href": "chapters/10-intro-classification.html#binary-classification",
    "title": "6  Introduction: Binary Labels",
    "section": "Binary Classification",
    "text": "Binary Classification\nIn binary classification, we have two classes (e.g. “spam” vs “not spam”). We can represent the class labels as \\(y_i \\in \\{0, 1\\}\\), where \\(0\\) represents one class and \\(1\\) represents the other class. Here’s an example of the kind of data we have in mind with a binary classification problem:\n\nCode\nimport torch\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nw = torch.tensor([[0.0], [1.5], [1.5]])\nX = torch.randn(100, 3)\nX[:, 0] = 1.0\n\nq = torch.sigmoid(X @ w)\ny = torch.bernoulli(q)\n\nfig, ax = plt.subplots(figsize=(6, 6))\n\nmarkers = ['o', 's']\nfor i in range(2): \n    idx = (y.flatten() == i)\n    ax.scatter(X[idx, 1], X[idx, 2], c = y.flatten()[idx], label=f'Class {i}', edgecolor='k', marker=markers[i],   cmap = 'BrBG', vmin = -0.5, vmax = 1.5, alpha = 0.7)\n\nax.legend()\nt = ax.set(title='Example Classification Data', xlabel=r'$x_1$', ylabel=r'$x_2$')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.1: Example data for a binary classification problem with two features. The data points are colored and shaped according to their class label.\n\n\n\nWe are going to conceptualize this data as arising according to the following process:\n\n\n\n\n\n\nNoteData Generating Process for Classification\n\n\n\n\nThere is some true underlying signal function \\(q\\) that takes in features \\(\\mathbf{x}_i\\) and outputs a probability of being in class 1.\nFor each data point \\(i\\), we first compute the signal function \\(q(\\mathbf{x}_i)\\), which gives us a probability of being in class 1.\nThen, we flip a biased coin with bias \\(q(\\mathbf{x}_i)\\) to determine the class label \\(y_i\\). The uncertainty of this coin flip is the noise in the data generating process.\n\n\n\nHere’s a visualization of this process:\n\nCode\nfig, axarr = plt.subplots(1, 3, figsize=(8, 3))\nx_1_grid = torch.linspace(X.min(), X.max(), 100)\nx_2_grid = torch.linspace(X.min(), X.max(), 100)\nxx, yy = torch.meshgrid(x_1_grid, x_2_grid, indexing='ij')\ngrid = torch.cat([torch.ones_like(xx).reshape(-1, 1), xx.reshape(-1, 1), yy.reshape(-1, 1)], dim=1)\nwith torch.no_grad():\n    q_grid = torch.sigmoid(grid @ w).reshape(xx.shape)\n\naxarr[0].contourf(xx, yy, q_grid, levels=torch.linspace(0, 1, steps=10), alpha=0.3, cmap='BrBG', extent = (x_1_grid.min(), x_1_grid.max(), x_2_grid.min(), x_2_grid.max()))\n\naxarr[1].contourf(xx, yy, q_grid, levels=torch.linspace(0, 1, steps=10), alpha=0.3, cmap='BrBG')\n\nsns.scatterplot(x=X[:, 1], y=X[:, 2], color = \"black\", ax=axarr[1], legend=False, edgecolor='k', facecolor = \"none\")\n\n\nfor i in range(2): \n    idx = (y.flatten() == i)\n    axarr[2].scatter(X[idx, 1], X[idx, 2], c = y.flatten()[idx], label=f'Class {i}', edgecolor='k', marker=markers[i],   cmap = 'BrBG', vmin = -0.5, vmax = 1.5, alpha = 0.7)\n\nfor i, ax in enumerate(axarr): \n    ax.set(xlim=(x_1_grid.min(), x_1_grid.max()), ylim=(x_2_grid.min(), x_2_grid.max()))\n    ax.set(xlabel=r'$x_1$')\n    if i == 0: \n        ax.set(ylabel=r'$x_2$')\n    else: \n        ax.set(ylabel='')\n\n# plt.colorbar(axarr[1].collections[0], ax=axarr[0], label='Probability of label 1 $q(\\mathbf{x})$')\naxarr[0].set(title='Signal $q$')\naxarr[1].set(title='Data points in\\nfeature space')\naxarr[2].set(title='Data points\\nassigned categories')\n    \nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.2: (Left): example of a signal function \\(q\\) that maps features \\(\\mathbf{x}\\) to a probability of being in class 1. (Middle): the data points in feature space. (Right): the data points with their observed class label after having been assigned categories according to the value of the signal function.\n\n\n\n\nLikelihood Function for Binary Classification\nSuppose that we evaluate the signal function \\(q\\) at some point \\(\\mathbf{x}\\), giving us a probability \\(q(\\mathbf{x})\\) that a data point with features \\(\\mathbf{x}\\) belongs to class 1. We can then write the probability of that data point belonging to class \\(y\\in \\{0,1\\}\\) as\n\\[\n\\begin{align}\n    p_Y(y; q(\\mathbf{x})) = \\begin{cases}\n        q(\\mathbf{x}) & \\text{if } y = 1 \\\\\n        1 - q(\\mathbf{x}) & \\text{if } y = 0\n    \\end{cases}  \n\\end{align}\n\\tag{6.1}\\]\nAlthough it might seem a bit unnecessarily complicated at first, it turns out that a very useful way to write this The notation \\(p_Y(y; q(\\mathbf{x})) = q(\\mathbf{x})^y (1-q(\\mathbf{x}))^{1-y}\\) may look a bit unnecessarily fancy, but convince yourself that we have \\(p_Y(0; q(\\mathbf{x})) = 1-q(\\mathbf{x})\\) and \\(p_Y(1; q(\\mathbf{x})) = q(\\mathbf{x})\\), as in Equation 6.1.\n\\[\n\\begin{aligned}\n    p_Y(y; q(\\mathbf{x})) &= q(\\mathbf{x})^y (1 - q(\\mathbf{x}))^{1 - y}\\;.\n\\end{aligned}\n\\tag{6.2}\\]\nThis is an instance of the Bernoulli distribution with parameter \\(q(\\mathbf{x})\\):\n\nDefinition 6.1 (Bernoulli Distribution) Random variable \\(Y\\) is said to be Bernoulli distributed with parameter \\(q\\) if it takes value \\(1\\) with probability \\(q\\) and value \\(0\\) with probability \\(1-q\\).\nThe probability mass function of a Bernoulli distribution is given by\n\\[\np_Y(y; q) = \\mathbb{P}(Y = y;q) = q^y (1-q)^{1-y}\\;.\n\\]\n\nWe can now write down the likelihood function for some data consisting of a feature matrix \\(\\mathbf{X}\\) and a vector of class labels \\(\\mathbf{y}\\) given a signal function \\(q\\) by multiplying together the probabilities for each data point:\n\\[\n\\begin{aligned}\n    L(\\mathbf{X}, \\mathbf{y}; q) &= \\prod_{i = 1}^n p_Y(y_i; q(\\mathbf{x})) \\\\\n                   &= \\prod_{i = 1}^n q(\\mathbf{x}_i)^{y_i} (1 - q(\\mathbf{x}_i))^{1 - y_i}\\;.\n\\end{aligned}\n\\]\nJust like with regression, it’s usually more convenient to work with the log-likelihood:\n\\[\n\\begin{aligned}\n    \\mathcal{L}(\\mathbf{X}, \\mathbf{y}; q) &= \\log L(\\mathbf{X}, \\mathbf{y}; q) \\\\\n                   &= \\sum_{i = 1}^n \\left[y_i \\log q(\\mathbf{x}_i) + (1 - y_i) \\log (1 - q(\\mathbf{x}_i))\\right]\\;.\n\\end{aligned}\n\\]\nSince we customarily minimize when working with optimization problems, we aim to minimize the negative log-likelihood, which is given by\n\\[\n\\begin{aligned}\n    -\\mathcal{L}(\\mathbf{X}, \\mathbf{y}; q)  \n                   &= -\\sum_{i = 1}^n \\left[y_i \\log q(\\mathbf{x}_i) + (1 - y_i) \\log (1 - q(\\mathbf{x}_i))\\right]\\;.\n\\end{aligned}\n\\tag{6.3}\\]\n\nDefinition 6.2 (Binary Cross-Entropy Loss) The binary cross entropy between a collection of predicted probabilities \\(\\mathbf{q}= (q_1,q_2,\\ldots,q_n) \\in [0,1]^n\\) and true labels \\(\\mathbf{y}= (y_1,y_2,\\ldots,y_n) \\in \\{0,1\\}^n\\) is given by the formula\n\\[\n\\mathrm{CE}(\\mathbf{y}, \\mathbf{q}) = -\\sum_{i = 1}^n \\left[y_i \\log q_i + (1 - y_i) \\log (1 - q_i)\\right]\\;.\n\\tag{6.4}\\]\n\nWhile torch implements some bespoke cross-entropy loss functions optimized for various cases, it’s also a quick formula to implement by hand:\n\ndef binary_cross_entropy(q, y): \n    return -(y * torch.log(q) + (1 - y) * torch.log(1 - q)).mean()\n\nSo, using the binary cross entropy, our loss function for classification in this model is given by\n\\[\n\\begin{aligned}\n    -\\mathcal{L}(\\mathbf{X}, \\mathbf{y}; q) = \\mathrm{CE}(\\mathbf{y}, \\mathbf{q}(\\mathbf{X}))\\;,\n\\end{aligned}\n\\]\nwhere here we are letting \\(\\mathbf{q}(\\mathbf{X}) = (q(\\mathbf{x}_1), q(\\mathbf{x}_2), \\ldots, q(\\mathbf{x}_n))\\) be the vector of predicted probabilities for each data point. We’d like to find a choice of the signal function \\(q\\) that makes this loss as small as possible, which as usual is equivalent to maximizing the log-likelihood.\n\n\nBinary Logistic Regression\nTo complete an algorithm for binary classification, we need to specify the set of possible signal functions \\(q\\) that we are going to search over. In logistic regression, we consider signal functions which consist of applying the logistic sigmoid to a linear function of the features.\n\nDefinition 6.3 (Logistic Sigmoid) The logistic sigmoid \\(\\sigma: \\mathbb{R}\\rightarrow (0, 1)\\) is the function with formula\n\\[\n\\sigma(z) = \\frac{1}{1 + e^{-z}}\\;.\n\\tag{6.5}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nPlot of the logistic sigmoid \\(\\sigma\\).\nHere’s a quick implementation:\n\ndef sigmoid(z): \n    return 1 / (1 + torch.exp(-z))\n\nThe logistic sigmoid sends any numerical input to a value between 0 and 1, which makes it a natural choice for modeling probabilities. In logistic regression, we apply the logistic sigmoid to a linear function of the features, which gives us the following form for the signal function \\(q\\):\n\\[\n\\begin{aligned}\n    q(\\mathbf{x}_i) = \\sigma(\\mathbf{x}_i^\\top \\mathbf{w})\\;.\n\\end{aligned}\n\\tag{6.6}\\]\nIf we insert Equation 6.6 into Equation 6.4, we get the following expression for the cross-entropy of the predictions and data for a given parameter vector \\(\\mathbf{w}\\):\n\\[\n\\begin{aligned}\n    -\\mathcal{L}(\\mathbf{X}, \\mathbf{y}; \\mathbf{w}) &= \\mathrm{CE}(\\mathbf{y}, \\mathbf{q}(\\mathbf{X})) \\\\\n    &= \\mathrm{CE}(\\mathbf{y}, \\sigma(\\mathbf{X}\\mathbf{w})) &\\quad \\text{(sigmoid applied entrywise to $\\mathbf{X}\\mathbf{w}$)} \\\\\n    &=  -\\sum_{i = 1}^n \\left[y_i \\log \\sigma(\\mathbf{x}_i^\\top \\mathbf{w}) + (1 - y_i) \\log (1 - \\sigma(\\mathbf{x}_i^\\top \\mathbf{w}))\\right]\\;.\n\\end{aligned}\n\\]\nMuch like with linear regression, it’s typical to normalize by the number of data points \\(n\\) to get an average log-likelihood per data point, which gives our final formula for the loss function in binary logistic regression: This normalization is why we used .mean() instead of .sum() in the binary_cross_entropy function above.\n\\[\n\\begin{aligned}\n    \\mathrm{Loss} &= \\frac{1}{n} \\sum_{i = 1}^n \\left[y_i \\log \\sigma(\\mathbf{x}_i^\\top \\mathbf{w}) + (1 - y_i) \\log (1 - \\sigma(\\mathbf{x}_i^\\top \\mathbf{w}))\\right]\\;.\n\\end{aligned}\n\\tag{6.7}\\]\n\n\nImplementation of Binary Logistic Regression\nTo implement binary logistic regression, we can use largely the same machinery that we used for linear regression. The forward method will compute the value of the signal function \\(q(\\mathbf{x}_i) = \\sigma(\\mathbf{x}_i^\\top \\mathbf{w})\\) for each data point.\n\nclass BinaryLogisticRegression: \n    def __init__(self, n_features): \n        self.w = torch.zeros(n_features, 1, requires_grad=True)\n\n    def forward(self, X): \n        return sigmoid(X @ self.w)        \n\nNow we’re ready to train the model using gradient descent on the parameters \\(\\mathbf{w}\\). The gradient of a single term in the binary cross-entropy loss is given by the formula\n\\[\n\\begin{aligned}\n    \\nabla \\mathcal{L}(\\mathbf{x}_i, y_i; \\mathbf{w}) = (\\sigma(\\mathbf{x}_i^\\top \\mathbf{w}) - y_i) \\mathbf{x}_i\\;,\n\\end{aligned}\n\\]\nwhich means that the gradient of the full loss is given by\n\\[\n\\begin{aligned}\n    \\nabla \\mathcal{L}(\\mathbf{X}, \\mathbf{y}; \\mathbf{w}) = \\frac{1}{n} \\sum_{i = 1}^n (\\sigma(\\mathbf{x}_i^\\top \\mathbf{w}) - y_i) \\mathbf{x}_i\\;.\n\\end{aligned}\n\\]\nLet’s build in these calculations to a class for performing gradient descent optimization.\n\nclass GradientDescentOptimizer: \n    def __init__(self, model, lr=0.1): \n        self.model = model\n        self.lr = lr\n\n    def grad_func(self, X, y): \n        q = self.model.forward(X)\n        return 1/X.shape[0] * ((q - y).T @ X).T\n        \n\n    def step(self, X, y): \n        grad = self.grad_func(X, y)\n        with torch.no_grad(): \n            self.model.w -= self.lr * grad\n\n\nmodel = BinaryLogisticRegression(n_features=3)\nopt = GradientDescentOptimizer(model, lr=0.1)\n\nlosses = []\nfor epoch in range(100): \n    q = model.forward(X)\n    loss = binary_cross_entropy(q, y)\n    losses.append(loss.item())\n    opt.step(X, y)\n\nLet’s visualize the learned signal function:\n\n\nCode\nx_1_grid = torch.linspace(X.min(), X.max(), 100)\nx_2_grid = torch.linspace(X.min(), X.max(), 100)\nxx, yy = torch.meshgrid(x_1_grid, x_2_grid, indexing='ij')\ngrid = torch.cat([torch.ones_like(xx).reshape(-1, 1), xx.reshape(-1, 1), yy.reshape(-1, 1)], dim=1)\nwith torch.no_grad():\n    q_grid = model.forward(grid).reshape(xx.shape)\n\nfig, axarr = plt.subplots(1, 2, figsize=(8, 4))\n\naxarr[0].plot(losses, color = \"grey\")\naxarr[0].set(title='Loss over training', xlabel='Epoch', ylabel='Binary cross-entropy loss')\naxarr[0].set_ylim(0, 1)\naxarr[1].set(title='Learned signal\\nfunction $q$', xlabel=r'$x_1$', ylabel=r'$x_2$')\n\n\naxarr[1].contourf(xx, yy, q_grid, levels=torch.linspace(0, 1, steps=10), alpha=0.3, cmap='BrBG', extent = (x_1_grid.min(), x_1_grid.max(), x_2_grid.min(), x_2_grid.max()))\naxarr[1].contour(xx, yy, q_grid, levels=[0.5], colors='k', linewidths=1, extent = (x_1_grid.min(), x_1_grid.max(), x_2_grid.min(), x_2_grid.max()), linestyles='--')\n\nfor i in range(2): \n    idx = (y.flatten() == i)\n    axarr[1].scatter(X[idx, 1], X[idx, 2], c = y.flatten()[idx], label=f'Class {i}', edgecolor='k', marker=markers[i],   cmap = 'BrBG', vmin = -0.5, vmax = 1.5, alpha = 0.7)\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nWe can obtain predictions from the model by thresholding the signal function, for example at \\(q^* = 0.5\\) as shown in the contour plot.\n\nq = model.forward(X)\ny_pred = (q &gt;= 0.5).float()\n\n# accuracy\naccuracy = (y_pred == y).float().mean()\nprint(f'Training accuracy: {accuracy:.3f}')\n\nTraining accuracy: 0.820\n\n\nAs usual, to fully assess the classifier we would evaluate the accuracy on a held-out test set.\n\n\nData Case Study\nAs an empirical test of our logistic regression classifier, let’s train a model to predict rainfall based on the previous day’s weather.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/middcs/data-science-notes/refs/heads/main/data/australia-weather/weatherAUS.csv\"\n\ndf = pd.read_csv(url)\ndf.head()\n# df[\"Pressure9am\"] = df[\"Pressure9am\"]/1e3\n# df[\"Humidity3pm\"] = df[\"Humidity3pm\"]/100\n\ndf.dropna(subset=['RainTomorrow', 'Humidity3pm', 'Pressure3pm'], inplace=True)\n\nFor the purposes of this example, we’ll use just two features: the humidity at 3pm and the pressure at 3pm, and use these to predict whether or not rain occurs the next day.\n\nfig, ax = plt.subplots(figsize=(6, 6))\n\nsubset = df.head(1000)\ny = subset['RainTomorrow'].apply(lambda x: 1 if x == 'Yes' else 0).to_numpy().reshape(-1, 1)\nX = subset[['Humidity3pm', 'Pressure3pm']].to_numpy()\n\n\nfor i, label in enumerate([\"No rain tomorrow\", \"Rain tomorrow\"]): \n    idx = y.flatten() == i\n    ax.scatter(X[idx, 0], X[idx, 1], c = y.flatten()[idx], label=f'{label}', edgecolor='k', marker=markers[i],   cmap = 'BrBG', vmin = -0.5, vmax = 1.5, alpha = 0.7)\n\nax.set(xlabel='Humidity3pm', ylabel='Pressure3pm')\n\nplt.legend()\n\nt = plt.title(\"Tomorrow's rain as a function of today's humidity and pressure\")\n\n\n\n\n\n\n\n\nWe’ll start our modeling pipeline by producing a feature matrix and label vector from the data.\n\nX = df[['Humidity3pm', 'Pressure3pm']].to_numpy()\ny = (df['RainTomorrow'] == 'Yes').to_numpy().astype(float).reshape(-1, 1)\n\nNow we’ll perform a train-test split to evaluate the model’s performance on held-out data.\n\ntrain_frac = 0.8\nn = X.shape[0]\nn_train = int(train_frac * n)\n\nX_train = X[:n_train, :]\ny_train = y[:n_train, :]\n\nX_test = X[n_train:, :]\ny_test = y[n_train:, :]\n\nNow we’re ready to train the logistic regression model.\n\nn_features = X_train.shape[1] + 1  # +1 for intercept\nX_train_aug = torch.ones((X_train.shape[0], n_features))\nX_train_aug[:, 1:] = torch.tensor(X_train, dtype=torch.float32)\n\ny_train_tensor = torch.tensor(y_train, dtype=torch.float32)\nmodel = BinaryLogisticRegression(n_features=n_features)\nopt = GradientDescentOptimizer(model, lr=0.00001)\n\nlosses = []\nfor epoch in range(20000): \n\n    q = model.forward(X_train_aug)\n    grad = (q - y_train_tensor).T@X_train_aug / X_train_aug.shape[0]\n    loss = binary_cross_entropy(q, y_train_tensor)\n    \n    losses.append(loss.item())\n    opt.step(X_train_aug, y_train_tensor)\n\n\nCode\nplt.plot(losses, color = \"grey\")\nt = plt.title('Loss over training')\nplt.xlabel('Epoch')\nplt.ylabel('Binary cross-entropy loss')\nplt.ylim(0, 1)\nplt.semilogx()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.3: Visualization of the binary cross-entropy loss over training epochs.\n\n\n\nLet’s plot the learned signal function \\(q\\) in feature space, along with a selection from the test data:\n\nCode\nx1_grid = torch.linspace(X[:,0].min(), X[:,0].max(), 100)\nx2_grid = torch.linspace(X[:,1].min(), X[:,1].max(), 100)\nxx1, xx2 = torch.meshgrid(x1_grid, x2_grid, indexing='ij')\ngrid = torch.cat([torch.ones_like(xx1).reshape(-1, 1), xx1.reshape(-1, 1), xx2.reshape(-1, 1)], dim=1)\nwith torch.no_grad():\n    q_grid = model.forward(grid).reshape(xx1.shape)\n\nfig, ax = plt.subplots(figsize=(6, 6))\n\nax.contour(xx1, xx2, q_grid, levels=[0.5], colors='k', linewidths=1, extent = (x1_grid.min(), x1_grid.max(), x2_grid.min(), x2_grid.max()), linestyles='--')\nax.contourf(xx1, xx2, q_grid, levels=torch.linspace(0, 1, steps=10), alpha=0.3, cmap='BrBG', extent = (x1_grid.min(), x1_grid.max(), x2_grid.min(), x2_grid.max()))\n\n\nax.set(title='Learned decision boundary', xlabel='Humidity3pm', ylabel='Pressure3pm')\n\nfor i, label in enumerate([\"No rain tomorrow\", \"Rain tomorrow\"]): \n    idx = (y_test[:1000, 0] == i)\n    ax.scatter(X_test[:1000, 0][idx], X_test[:1000, 1][idx], c = y_test[:1000, 0][idx], label=f'{label}', edgecolor='k', marker=markers[i],   cmap = 'BrBG', vmin = -0.5, vmax = 1.5, alpha = 0.7)\n\nplt.legend()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.4: Learned decision boundary (dashed line) in feature space, along with 1,000 of the test data points.\n\n\n\nTo evaluate this model on the test set, we need to evaluate the model on the test features and threshold the results to obtain class predictions.\n\n# obtain the probabilities q_test\nX_test_aug = torch.ones((X_test.shape[0], n_features))\nX_test_aug[:, 1:] = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.float32)\nq_test = model.forward(X_test_aug)\n\n# obtain class predictions via thresholding and compute accuracy\ny_test_pred = (q_test &gt;= 0.5).float()\naccuracy_test = (y_test_pred == y_test_tensor).float().mean()\nprint(f'Test accuracy: {accuracy_test:.3f}')\n\nTest accuracy: 0.840\n\n\nThis accuracy is slightly better than the result we would obtain by always guessing that it won’t rain:\n\nbaseline_accuracy = (y_test_tensor == 0).float().mean()\nprint(f'Baseline accuracy (always predict no rain): {baseline_accuracy:.3f}')\n\nBaseline accuracy (always predict no rain): 0.794\n\n\nOur learned classifier does slightly better than baseline in predicting whether or not will rain tomorrow, as measured by accuracy on the test set.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction: Binary Labels</span>"
    ]
  },
  {
    "objectID": "chapters/15-multinomial-classification.html",
    "href": "chapters/15-multinomial-classification.html",
    "title": "7  Multinomial Classification",
    "section": "",
    "text": "Recap\nOpen the live notebook in Google Colab.\nRecently, we developed binary classification (models that learn to predict one of two labels) using the data = signal + noise framework. In this framework, the signal was a function \\(q(\\mathbf{x})\\) which, when evaluated at a feature value \\(\\mathbf{x}\\), gives the probability that a data point with features \\(\\mathbf{x}\\) belongs to class 1. The noise was a Bernoulli random variable that, when sampled, gives us the observed class label for a data point. We trained our model by letting \\(q(\\mathbf{x}) = \\sigma(\\mathbf{w}^\\top \\mathbf{x})\\), where \\(\\sigma\\) is the logistic sigmoid function and \\(\\mathbf{w}\\) is a trainable weight vector. Using gradient descent, we were able to maximize the likelihood of the observed data under this model, which we found to be equivalent to minimizing the binary cross-entropy loss.\nIn this lecture we’ll consider a simple question: how do we extend this framework to the case where we have more than two classes? The fancy vocabulary word for “more than two classes” is multinomial, so we’ll be developing logistic regression for multinomial classification.\n© Phil Chodrow, 2025",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Multinomial Classification</span>"
    ]
  },
  {
    "objectID": "chapters/15-multinomial-classification.html#data-prep",
    "href": "chapters/15-multinomial-classification.html#data-prep",
    "title": "7  Multinomial Classification",
    "section": "Data Prep",
    "text": "Data Prep\n\n\n\nImage source: @allisonhorst\n\n\nOur data set for these notes is Palmer Penguins. This data set contains physiological measurements and species labels for several populations of Adelie, Chinstrap, and Gentoo penguins.\nThe Palmer Penguins data was originally collected by Gorman, Williams, and Fraser (2014) and was nicely packaged and released for use in the data science community by Horst, Hill, and Gorman (2020). You can find a very concise summary of the main workflow using a similar data set in Vanderplas (2016).\nLet’s go ahead and acquire the data.\n\nimport pandas as pd\nimport torch\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/palmer-penguins.csv\"\n\ndf = pd.read_csv(url)\n\n The df variable holds a pandas.DataFrame object. You can think of a data frame as a table of data with a variety of useful behaviors for data manipulation and visualization.You can learn much more about the capabilities of pandas.DataFrame objects in Chapter 3 of Vanderplas (2016)\nLet’s take a look:\n\ndf.head() # first 5 rows\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\nEach row of this data frame contains measurements for a single penguin, and the columns contain different features of the data. We’ll use the Species column as our class labels, and the other columns as features for classification. For illustration, we’ll use only two features: Culmen Length (mm) and Culmen Depth (mm). We’ll separate these into a matrix of features \\(\\mathbf{X}\\) and a vector of class labels \\(\\mathbf{y}\\).\n\n\n\nImage credit: @allisonhorst\n\ndf = df.dropna(subset=['Culmen Length (mm)', 'Culmen Depth (mm)', 'Species'])\ndf = df[['Culmen Length (mm)', 'Culmen Depth (mm)', 'Species']]\ndf[\"Species\"] = df[\"Species\"].str.split().str[0]  # shorten species names to first letter\n\n# features and targets\nX = df[['Culmen Length (mm)', 'Culmen Depth (mm)']].values\nlabels = df['Species'].astype('category')\n\nA look at the species labels in feature space suggests that we should be able to do a pretty good job of classifying the species based on these two features. We’ll define a convenience function for visualizing the data points in feature space, colored by species.\n\ndef scatter_points(X, labels, cmap, ax): \n    if isinstance(labels, torch.Tensor):\n        y = labels\n        unique_labels = torch.unique(y.detach())\n    else:\n        y = labels.cat.codes.values\n        unique_labels = labels.cat.categories\n    \n    for i, label in enumerate(unique_labels):\n        color = cmap(i)\n        marker = ['o', 's', 'D'][i]\n        ax.scatter(X[y == i, 0], \n                   X[y == i, 1], \n                   color=color, \n                   edgecolor='k', \n                   marker=marker, \n                   label=f\"{unique_labels[i]}\", \n                   zorder = 1e3)\n    \n    ax.set(xlabel=r\"$x_1$\", ylabel=r\"$x_2$\")\n    ax.legend().set_zorder(1e7)\n\nNow let’s visualize our penguins:\n\nCode\nfig, ax = plt.subplots(figsize=(6, 6))\ncmap = plt.get_cmap('tab20b', 3)\nscatter_points(X, labels, cmap, ax)\nax.set_title(\"Scatterplot of penguin species in feature space\")\nax.set_xlabel(\"Culmen Length (mm)\")\nt = ax.set_ylabel(\"Culmen Depth (mm)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.1: Scatterplot culmen length and culmen depth in the Palmer Penguins data set, with data points colored by species.\n\n\n\nNow that we have data, we need to extend our logistic regression framework to the multinomial setting.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Multinomial Classification</span>"
    ]
  },
  {
    "objectID": "chapters/15-multinomial-classification.html#implementing-multinomial-logistic-regression",
    "href": "chapters/15-multinomial-classification.html#implementing-multinomial-logistic-regression",
    "title": "7  Multinomial Classification",
    "section": "Implementing Multinomial Logistic Regression",
    "text": "Implementing Multinomial Logistic Regression\n\nOne-Hot Encoding\nTo apply our classification framework to our data, we need to convert the categorical species labels into numerical values. A particularly convenient way to do this is via one-hot encoding.\n\nDefinition 7.1 (One-Hot Encoding) If \\(y\\) is a categorical variable with \\(K\\) categories, then the one-hot encoding of \\(\\mathbf{y}\\) of \\(y\\) is a vector \\(\\mathbf{y}\\in \\{0, 1\\}^K\\) with entries\n\\[\ny_i = \\begin{cases}\n    1 & \\text{if } y \\text{ is in category } i \\\\\n    0 & \\text{otherwise}\n\\end{cases}\\;.\n\\]\n\nFor example, in our case with class labels “Adelie”, “Chinstrap”, and “Gentoo”, a one-hot encoding of a penguin of species “Adelie” would be the vector \\(\\mathbf{y}= (1, 0, 0)\\), a penguin of species “Chinstrap” would be \\(\\mathbf{y}= (0, 1, 0)\\), and a penguin of species “Gentoo” would be \\(\\mathbf{y}= (0, 0, 1)\\).\nThe pandas library provides a convenient way to perform one-hot encoding for data frames via the pd.get_dummies function.\n\ndf = pd.get_dummies(df, columns=['Species'])\ndf.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nSpecies_Adelie\nSpecies_Chinstrap\nSpecies_Gentoo\n\n\n\n\n0\n39.1\n18.7\nTrue\nFalse\nFalse\n\n\n1\n39.5\n17.4\nTrue\nFalse\nFalse\n\n\n2\n40.3\n18.0\nTrue\nFalse\nFalse\n\n\n4\n36.7\n19.3\nTrue\nFalse\nFalse\n\n\n5\n39.3\n20.6\nTrue\nFalse\nFalse\n\n\n\n\n\n\n\nNow we’re ready to split our data into features and targets and to training and test sets.\n\ntrain_ix = df.sample(frac=0.8, random_state=42).index\ntest_ix = df.drop(train_ix).index\n\nfeature_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)']\ntarget_cols = ['Species_Adelie', 'Species_Chinstrap', 'Species_Gentoo']\n\nX_train = torch.tensor(df.loc[train_ix, feature_cols].values, dtype=torch.float32)\ny_train = torch.tensor(df.loc[train_ix, target_cols].values, dtype=torch.float32)\n\nX_test = torch.tensor(df.loc[test_ix, feature_cols].values, dtype=torch.float32)\ny_test = torch.tensor(df.loc[test_ix, target_cols].values, dtype=torch.float32)\n\n\n\nData Model\nLet’s go ahead and write down a model of the data generating process for the multinomial classification problem. We’re going to follow a similar approach as we did in binary classification. Suppose that each data point can be in one of \\(K\\) classes, and suppose that we have a signal function \\(\\mathbf{q}: \\mathbb{R}^d \\rightarrow (0, 1)^K\\) that takes in features \\(\\mathbf{x}_i\\) and outputs a vector of probabilities \\(\\mathbf{q}(\\mathbf{x}_i)\\) for each class. To generate a class label for data point \\(i\\), we draw from a categorical distribution with class probabilities given by \\(\\mathbf{q}(\\mathbf{x}_i)\\). So, the likelihood of a single data point \\(i\\) with features \\(\\mathbf{x}_i\\) and one-hot encoded class label \\(\\mathbf{y}_i\\) is then\n\\[\n\\begin{aligned}\n    p(\\mathbf{y}_i, \\mathbf{x}_i; \\mathbf{q}) = \\begin{cases}\n        q_1(\\mathbf{x}_i) & \\text{if } y_{i1} = 1 \\\\  \n        q_2(\\mathbf{x}_i) & \\text{if } y_{i2} = 1 \\\\\n        \\vdots \\\\\n        q_K(\\mathbf{x}_i) & \\text{if } y_{iK} = 1\\;.\n    \\end{cases}\n\\end{aligned}\n\\]\nIt’s convenient to write this expression in the more compact form \\[\np(\\mathbf{y}_i , \\mathbf{x}_i; \\mathbf{q}) = \\prod_{k=1}^K q_k(\\mathbf{x}_i)^{y_{ik}}\\;.\n\\]\nWe can take the product over all data points to get the likelihood of the full data set. Let \\(\\mathbf{Y}\\) be the \\(n \\times K\\) matrix of one-hot encoded class labels, with \\(i\\)th row \\(\\mathbf{y}_i\\) corresponding to the one-hot encoding of the class label for data point \\(i\\):\n\\[\n\\mathbf{Y}= \\begin{bmatrix}\n    - & \\mathbf{y}_1^\\top & - \\\\\n    - & \\mathbf{y}_2^\\top & - \\\\\n    &\\vdots \\\\\n    - & \\mathbf{y}_n^\\top & -\n\\end{bmatrix}\\;,\n\\]\nThen, the likelihood of the data set under the model is\n\\[\np(\\mathbf{Y}, \\mathbf{X}; \\mathbf{q}) = \\prod_{i=1}^n \\prod_{k=1}^K q_k(\\mathbf{x}_i)^{y_{ik}}\\;.\n\\]\nTaking logs gives us the log-likelihood:\n\\[\n\\begin{aligned}\n    \\mathcal{L}(\\mathbf{Y}, \\mathbf{X}; \\mathbf{q}) = \\sum_{i = 1}^n \\sum_{k = 1}^K y_{ik} \\log q_k(\\mathbf{x}_i)\\;.\n\\end{aligned}\n\\tag{7.1}\\]\nEquation 7.1 is the generalization of the binary cross-entropy to the case of multiple classes.\n\n\nSignal Function for Logistic Regression\nTo complete our specification for logistic regression, we need to specify the functional form of the signal function \\(\\mathbf{q}\\). We’ll use the softmax function, which takes in a vector of real-valued scores and outputs a vector of probabilities.\n\nDefinition 7.2 (Softmax Function) The softmax function \\(\\boldsymbol{\\sigma}: \\mathbb{R}^K \\rightarrow (0, 1)^K\\) is defined as\n\\[\n\\boldsymbol{\\sigma}(\\mathbf{s}) = \\frac{1}{\\sum_{j = 1}^K e^{s_j}} \\begin{pmatrix}\n    e^{s_1} \\\\\n    e^{s_2} \\\\\n    \\vdots \\\\\n    e^{s_K}\n\\end{pmatrix}\\;,\n\\]\nfor \\(k = 1, \\ldots, K\\). The \\(k\\)th entry of \\(\\boldsymbol{\\sigma}(\\mathbf{s})\\) is given by\n\\[\n\\boldsymbol{\\sigma}(\\mathbf{s})_k = \\frac{e^{s_k}}{\\sum_{j = 1}^K e^{s_j}}\\;.\n\\]\n\nHere’s an example of the softmax function in action:\n\ndef softmax(s):\n    exp_s = torch.exp(s)\n    return exp_s / torch.sum(exp_s)\ns = torch.tensor([1.0, 2.0, 3.0])\nprint(softmax(s))\n\ntensor([0.0900, 0.2447, 0.6652])\n\n\nThe largest values of \\(\\mathbf{s}\\) correspond to the largest probabilities in \\(\\boldsymbol{\\sigma}(\\mathbf{s})\\); the entries of \\(\\boldsymbol{\\sigma}(\\mathbf{s})\\) are nonnegative, and sum to one, giving a valid probability vector.\nIn order to use the softmax function as our signal function \\(q\\), we need to specify how to get scores \\(\\mathbf{s}\\in \\mathbb{R}^K\\) from our features \\(\\mathbf{x}\\in \\mathbb{R}^d\\). The simplest way to do this is to use a linear function of the features. To do this, we’ll introduce a \\(d \\times K\\) matrix of trainable parameters \\(\\mathbf{W}\\), and we’ll compute scores as \\(\\mathbf{s}= \\mathbf{x}^\\top \\mathbf{W}\\). Our signal function will then be given by\n\\[\n\\begin{aligned}\n    \\mathbf{q}(\\mathbf{x}) = \\boldsymbol{\\sigma}(\\mathbf{x}^\\top \\mathbf{W})\\;.\n\\end{aligned}\n\\]\nHere, the trainable parameters are contained in the matrix \\(\\mathbf{W}\\). Intuitively, we can think of \\(w_{jk}\\) as the contribution of feature \\(j\\) to the score for class \\(k\\). If \\(w_{jk}\\) is large and positive, then larger values of feature \\(j\\) make it more likely that the data point is in class \\(k\\). If \\(w_{jk}\\) is large and negative, then larger values of feature \\(j\\) make it less likely that the data point is in class \\(k\\).\n\n\nVectorized Computation\nRather than compute the score vector \\(\\mathbf{s}_i\\) for each data point \\(i\\) separately, we can compute the scores for all data points at once using matrix multiplication. Let \\(\\mathbf{X}\\) be the \\(n \\times d\\) matrix of features, with \\(i\\)th row \\(\\mathbf{x}_i^\\top\\) corresponding to the features for data point \\(i\\). Then, we can compute the score matrix \\(\\mathbf{S}\\in \\mathbb{R}^{n \\times K}\\) containing all the scores as\n\\[\n\\begin{aligned}\n    \\mathbf{S}= \\begin{bmatrix}\n    - & \\mathbf{s}_1 & - \\\\\n    - & \\mathbf{s}_2 & - \\\\\n    &\\vdots \\\\\n    - & \\mathbf{s}_n & -\n    \\end{bmatrix} =\n    \\begin{bmatrix}\n    - & \\mathbf{x}_1^\\top \\mathbf{W}& - \\\\\n    - & \\mathbf{x}_2^\\top \\mathbf{W}& - \\\\\n    &\\vdots \\\\\n    - & \\mathbf{x}_n^\\top \\mathbf{W}& -\n    \\end{bmatrix}\n    = \\begin{bmatrix}\n    - & \\mathbf{x}_1^\\top & - \\\\\n    - & \\mathbf{x}_2^\\top & - \\\\\n    &\\vdots \\\\  \n    - & \\mathbf{x}_n^\\top & -\n    \\end{bmatrix} \\mathbf{W}= \\mathbf{X}\\mathbf{W}\\;\n\\end{aligned}\n\\]\nTo then compute the signals \\(\\mathbf{q}(\\mathbf{s}_i)\\), we need to apply the softmax function to each row of the score matrix \\(\\mathbf{S}\\):\n\ndef softmax_rows(S):\n    # equivalent to torch.softmax(S, dim=1)\n    exp_S = torch.exp(S)\n    return exp_S / torch.sum(exp_S, dim=1, keepdim=True)\n\n\n\nTorch Implementation\nWe now have everything we need to illustrate the data generation process and perform inference. Our implementation of logistic regression requires only initializing the weight matrix \\(\\mathbf{W}\\) and defining a forward method that takes in features \\(\\mathbf{X}\\) and outputs the signal function \\(\\mathbf{q}(\\mathbf{X})\\):\n\nclass LogisticRegression: \n    def __init__(self, d_features, k_classes):\n        self.W = torch.randn(d_features, k_classes, requires_grad=True)\n\n    def forward(self, X):\n        S = X @ self.W\n        return softmax_rows(S)\n\nFirst, let’s visualize the data generating process using a random weight matrix \\(\\mathbf{W}\\). We’ll compute the signal \\(\\mathbf{q}(\\mathbf{x})\\) for a grid of points in feature space, and we’ll visualize the predicted class probabilities for each class.\nThe large code block below visualizes the three entries of the function \\(\\mathbf{q}\\) for a logistic regression model with a random weight matrix, and illustrates how randomly sampled data gets classes assigned from these entries.\n\nCode\n# set the random number generator for reproducibility\ntorch.random.manual_seed(1234)\n\n# initialize colors\ncmap = plt.get_cmap('tab20b', 3)\n\n# create a grid of points in feature space and flatten it into a list of points\n# for prediction\nx_grid = torch.linspace(-1, 1, 100)\ny_grid = torch.linspace(-1, 1, 100)\nX_grid, Y_grid = torch.meshgrid(x_grid, y_grid, indexing='xy')\ngrid = torch.stack([X_grid.flatten(), Y_grid.flatten()], dim=1)\n\n# initialize model with random weights\nmodel = LogisticRegression(d_features=2, k_classes=3)\nmodel.W = model.W * 4.0\n\n# compute predicted class probabilities for each point in the grid\nQ = model.forward(grid)\npreds = Q.argmax(dim=1)\n\n# compute opacity as the difference between the max probability and uniform probability, for visualization purposes\nopacities = Q.max(dim=1).values - 1/3\n\n# colors for each data point determined by the predicted class, with opacity determined by confidence\ncolors = cmap(preds) \ncolors[:, -1] = opacities.detach().numpy()\n\n# compute predicted class for a random set of points in feature space\nX_points = 2*torch.rand(50, 2) - 1\nQ_points = model.forward(X_points)\npoint_class = torch.multinomial(Q_points, num_samples=1).squeeze()\n\n# construct the visualization\nfig, axarr = plt.subplots(2, 2, figsize=(7, 7))\n\nfor k in range(3):\n\n    # color for the chosen class\n    color = cmap(k)\n    \n    # colormap for individual panel\n    class_cmap_grad = plt.cm.colors.LinearSegmentedColormap.from_list('white_to_color', ['white', color])\n\n    # plot the predicted class probabilities for class k\n    axarr.ravel()[k].imshow(Q[:, k].reshape(100, 100).detach(), extent=(x_grid.min(), x_grid.max(), y_grid.min(), y_grid.max()), origin='lower', cmap=class_cmap_grad, vmin=0, vmax=1, zorder = 100)\n\n    axarr.ravel()[k].set_title(fr\"$q_{k+1}(\\mathbf{{x}})$ - probability of class {k}\")\n\n    # labels\n    if k == 2: \n        axarr.ravel()[k].set(xlabel=r\"$x_1$\", ylabel=r\"$x_2$\")\n    if k == 0: \n        axarr.ravel()[k].set(ylabel=r\"$x_2$\")\n    \n\n# plot all the predicted class probabilities together, with opacity determined by confidence\naxarr.ravel()[3].imshow(colors.reshape(100, 100, 4), extent=(x_grid.min(), x_grid.max(), y_grid.min(), y_grid.max()), origin='lower', zorder = 100)\n\n# for k in range(3):\n#     color = cmap(k)\n#     marker = ['o', 's', 'D'][k]\n#     axarr.ravel()[3].scatter(X_points[point_class == k, 1], X_points[point_class == k, 0], color=color, edgecolor='k', marker=marker, zorder = 200, label=f\"Class {k+1}\")\n\nscatter_points(X_points, point_class, cmap, axarr.ravel()[3])\n\n# scatter_points(X_points, point_class, cmap, axarr.ravel()[3])\n\n\naxarr.ravel()[3].set_title(\"Predicted class (color) \\nand confidence (opacity)\")\naxarr.ravel()[3].set(xlabel=r\"$x_1$\")\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.2: Illustration of the data generating process for multinomial logistic regression. (Top left): The predicted probability of class 1 (color intensity) across feature space. (Top right and bottom left): The predicted probabilities of classes 2 and 3. (Bottom right): The predicted class (color) and confidence (opacity) across feature space. We’ve included several randomly scattered points, with their assigned class given by asmpling from the predicted class probabilities.\n\n\n\n\n\nModel Training\nNow it’s time to train a model. Our approach is again gradient descent. The addition of multiple classes, reflected in the weight matrix \\(\\mathbf{W}\\in \\mathbb{R}^{d \\times k}\\) (rather than a weight vector \\(\\mathbf{w}\\in \\mathbb{R}^d\\) as in binary classification), doesn’t really change the training procedure but it does make it awkward to compute gradients by hand. We’ll rely on PyTorch’s automatic differentiation to compute the gradients for us. In order to compute the gradients, we first need to implement the cross-entropy loss function from equation {eq-log-likelihood-multinomial}:\n\ndef cross_entropy_loss(q, Y):\n    return -torch.sum(Y * torch.log(q)) / Y.shape[0]\n\n\nmodel = LogisticRegression(d_features=2, k_classes=3)\n\nlosses = []\nfor epoch in range(10000):\n    # compute predicted class probabilities\n    q = model.forward(X_train)\n    # compute loss\n    loss = cross_entropy_loss(q, y_train)\n    losses.append(loss.item())\n    # compute gradients\n    loss.backward()\n    # update weights using gradient descent\n    with torch.no_grad():\n        model.W -= 0.001 * model.W.grad\n        model.W.grad.zero_()\n\n\nCode\nfig, axarr = plt.subplots(1, 2, figsize=(8, 4))\n\nx_grid = torch.linspace(df['Culmen Length (mm)'].min()-2, df['Culmen Length (mm)'].max()+2, 100)\ny_grid = torch.linspace(df['Culmen Depth (mm)'].min()-2, df['Culmen Depth (mm)'].max()+2, 100)\nX_grid, Y_grid = torch.meshgrid(x_grid, y_grid)\ngrid = torch.stack([X_grid.flatten(), Y_grid.flatten()], dim=1)\n\nQ = model.forward(grid)\npreds = Q.argmax(dim=1)\n\ncolors = cmap(preds)\ncolors[:, -1] = Q.max(dim=1).values.detach().numpy()\n\naxarr[0].plot(losses)\naxarr[0].set_title(\"Loss during training\")\naxarr[0].set_xlabel(\"Epoch\")\naxarr[0].set_ylabel(\"Multinomial Cross-Entropy Loss\")\naxarr[0].loglog()\n\n\naxarr[1].imshow(colors.reshape(100, 100, 4).transpose(1, 0, 2), extent=(x_grid.min(), x_grid.max(), y_grid.min(), y_grid.max()), origin='lower', zorder = 100, aspect='auto')\n# sns.scatterplot(data=df, x='Culmen Length (mm)', y='Culmen Depth (mm)', hue='Species', style='Species', edgecolor='k', alpha=0.7, zorder = 200)\naxarr[1].set_title(\"Predicted class (color) \\nand confidence (opacity)\")\n\ny_train_labels = y_train.argmax(dim=1)\npenguin_names = ['Adelie', 'Chinstrap', 'Gentoo']\n\ny_train_labels_named = [penguin_names[i] for i in y_train_labels.numpy()]\ny_train_labels_named = pd.Series(y_train_labels_named, dtype='category')\n\nscatter_points(X_train, y_train_labels_named, cmap, axarr[1])\naxarr[1].set_xlabel(\"Culmen Length (mm)\")\nt = axarr[1].set_ylabel(\"Culmen Depth (mm)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.3: Training our multinomial logistic regression model for classification on the Palmer Penguins data set. (Left): The multinomial cross-entropy loss during training. (Right): The predicted class (color) and confidence (opacity) across feature space after training, with the training data points overlaid.\n\n\n\nLet’s finally evaluate the performance of our model on the test set.\n\n1s_pred = model.forward(X_test)\n2y_test_preds = s_pred.argmax(dim=1)\n3y_test_labels = y_test.argmax(dim=1).int()\n4acc = (1.0*(y_test_preds == y_test_labels)).mean().item()\n\n\n1\n\nCompute the score matrix \\(\\mathbf{S}\\) on the test set.\n\n2\n\nCompute the predicted class labels as the largest score in each row of \\(\\mathbf{S}\\).\n\n3\n\nCompute the true class labels by taking the argmax of the one-hot encoded labels.\n\n4\n\nCompute the accuracy as the fraction of correctly classified data points.\n\n\n\n\n\nCode\nfig, ax = plt.subplots(figsize=(6, 6))\n\nax.imshow(colors.reshape(100, 100, 4).transpose(1, 0, 2), extent=(x_grid.min(), x_grid.max(), y_grid.min(), y_grid.max()), origin='lower', zorder = 100, aspect='auto')\n\ny_test_labels = y_test.argmax(dim=1)\npenguin_names = ['Adelie', 'Chinstrap', 'Gentoo']\n\ny_test_labels_named = [penguin_names[i] for i in y_test_labels.numpy()]\ny_test_labels_named = pd.Series(y_test_labels_named, dtype='category')\n\nscatter_points(X_test, y_test_labels_named, cmap, ax)\n\nax.set_title(f\"Predicted class (color) \\nand confidence (opacity)\\nTest set accuracy: {acc:.3f}\")\n\nax.set_xlabel(\"Culmen Length (mm)\")\nt = ax.set_ylabel(\"Culmen Depth (mm)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.4: Evaluating model performance on the test set, as in Figure 7.3 (right panel), using the test set overlaid.\n\n\n\nAs in the case of binary classification, it may be useful to construct confusion matrices or other measures of error by class when assessing overall model performance.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Multinomial Classification</span>"
    ]
  },
  {
    "objectID": "chapters/15-multinomial-classification.html#references",
    "href": "chapters/15-multinomial-classification.html#references",
    "title": "7  Multinomial Classification",
    "section": "References",
    "text": "References\n\n\n\n\nGorman, Kristen B., Tony D. Williams, and William R. Fraser. 2014. “Ecological Sexual Dimorphism and Environmental Variability Within a Community of Antarctic Penguins (Genus Pygoscelis).” Edited by André Chiaradia. PLoS ONE 9 (3): e90081. https://doi.org/10.1371/journal.pone.0090081.\n\n\nHorst, Allison M, Alison Presmanes Hill, and Kristen B Gorman. 2020. “Allisonhorst/Palmerpenguins: V0.1.0.” Zenodo. https://doi.org/10.5281/ZENODO.3960218.\n\n\nVanderplas, Jacob T. 2016. Python Data Science Handbook: Essential Tools for Working with Data. First edition. Sebastopol, CA: O’Reilly Media, Inc.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Multinomial Classification</span>"
    ]
  }
]