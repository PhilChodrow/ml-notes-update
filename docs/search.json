[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Welcome\nThis is a set of notes developed for an undergraduate course in machine learning. The target audience for these notes are undergraduates in computer science who have completed first courses in linear algebra and discrete mathematics. These notes draw on many sources, but are somewhat distinctive in the following ways:\n© Phil Chodrow, 2025",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#pedagogical-features",
    "href": "index.html#pedagogical-features",
    "title": "Machine Learning",
    "section": "Pedagogical Features",
    "text": "Pedagogical Features\nThese notes are explicitly designed for undergraduate instruction in computer science. For this reason:\n\nComputational examples are integrated into the text and shown throughout.\nLive versions of lecture notes are supplied as downloadable Jupyter Notebooks, with certain code components removed. The purpose is to facilitate live-coding in lectures.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#use-and-reuse",
    "href": "index.html#use-and-reuse",
    "title": "Machine Learning",
    "section": "Use and Reuse",
    "text": "Use and Reuse\nThese notes were written by Phil Chodrow for the course CSCI 0451: Machine Learning at Middlebury College. All are welcome to use them for educational purposes. Attribution is appreciated but not expected.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#source-texts",
    "href": "index.html#source-texts",
    "title": "Machine Learning",
    "section": "Source Texts",
    "text": "Source Texts\nThese notes draw on several source texts, most of which are available for free online. These are:\n\nChristopher M. Bishop and Bishop (2023) is the primary influence for the development of technical content.\nHardt and Recht (2022) has been a very helpful guiding influence for content on decision-theory and automated decision systems.\nA Course in Machine Learning by Hal Daumé III is an accessible introduction to many of the topics and serves as a useful source of supplementary readings.\n\nAdditional useful readings:\n\nAbu-Mostafa, Magdon-Ismail, and Lin (2012): Learning From Data: A Short Course\nBarocas, Hardt, and Narayanan (2023) is an advanced text on questions of fairness in automated decision-making for readers who have some background in probability theory.\nChristopher M. Bishop (2006) and Murphy (2022) are advanced texts which are most suitable for advanced readers who have already taken at least one course in probability theory.\n\nDeisenroth, Faisal, and Ong (2020) and Kroese et al. (2020) are useful readings focusing on some of the mathematical fundamentals.\nZhang, Lipton, and Li (2023) tells a helpful story of the fundamentals of deep learning.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Machine Learning",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis site was generated using the Quarto publishing system. It is hosted on GitHub and published via GitHub Pages.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Machine Learning",
    "section": "References",
    "text": "References\n\n\n\n\nAbu-Mostafa, Yaser S., Malik Magdon-Ismail, and Hsuan-Tien Lin. 2012. Learning from Data: A Short Course. S.l. https://amlbook.com/.\n\n\nBarocas, Solon, Moritz Hardt, and Arvind Narayanan. 2023. Fairness and Machine Learning: Limitations and Opportunities. Cambridge, Massachusetts: The MIT Press. https://fairmlbook.org/pdf/fairmlbook.pdf.\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Information Science and Statistics. New York: Springer. https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf.\n\n\nBishop, Christopher M, and Hugh Bishop. 2023. Deep Learning: Foundations and Concepts. Springer Nature.\n\n\nDeisenroth, Marc Peter, A. Aldo Faisal, and Cheng Soon Ong. 2020. Mathematics for Machine Learning. Cambridge, UK New York, NY: Cambridge University Press. https://mml-book.github.io/book/mml-book.pdf.\n\n\nHardt, Moritz, and Benjamin Recht. 2022. Patterns, Predictions, and Actions. Princeton University Press. https://mlstory.org/pdf/patterns.pdf.\n\n\nKroese, Dirk P., Zdravko I. Botev, Thomas Taimre, and Radislav Vaisman. 2020. Data Science and Machine Learning: Mathematical and Statistical Methods. Chapman & Hall/CRC Machine Learning & Pattern Recognition Series. Boca Raton London New York: CRC Press, Taylor & Francis Group.\n\n\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An Introduction. Adaptive Computation and Machine Learning Series. Cambridge, Massachusetts: The MIT Press. https://probml.github.io/pml-book/book1.html.\n\n\nZhang, Aston, Zachary Lipton, and Mu Li. 2023. Dive into Deep Learning. Cambridge, UK: Cambridge University Press.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "chapters/02-signal-noise.html",
    "href": "chapters/02-signal-noise.html",
    "title": "1  Data = Signal + Noise",
    "section": "",
    "text": "Introduction: Data, Signal, and Noise\nOpen the live notebook in Google Colab or download the live notebook.\nIn these notes, we’ll expand on the following idea:\nConsider the following simple data set:\nWe can think of this data as consisting of two components: a signal expressed by a relationship \\(y \\approx f(x)\\) (in this case \\(f(x) = 2x + 1\\)), and some noise that partially obscures this relationship. Schematically, we can write this relationship as:\n\\[\n\\begin{aligned}\n    y_i = f(x_i) + \\epsilon_i\\;,\n\\end{aligned}\n\\tag{1.1}\\]\nwhich says that the \\(i\\)th value of the target is equal to some function \\(f(x_i)\\) of the input variable \\(x_i\\), plus some random noise term \\(\\epsilon_i\\).\n© Phil Chodrow, 2025",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data = Signal + Noise</span>"
    ]
  },
  {
    "objectID": "chapters/02-signal-noise.html#introduction-data-signal-and-noise",
    "href": "chapters/02-signal-noise.html#introduction-data-signal-and-noise",
    "title": "1  Data = Signal + Noise",
    "section": "",
    "text": "Machine learning is the science and practice of building algorithms that distinguish between signal and noise in real-world data.\n\n\n\nCode\nimport torch \nfrom matplotlib import pyplot as plt\n\nscatterplot_kwargs = dict(color = \"black\", label = \"data\", facecolors = \"none\", s = 40, alpha = 0.6)\n\nn_points = 20\nx = torch.linspace(0, 10, n_points)\nsignal = 2.0 * x + 1.0  # underlying pattern (signal)\nnoise = torch.randn(n_points) * 3.0  # random noise\n\ny = signal + noise\n\n# Plot residual segments\nfig, ax = plt.subplots(figsize = (6, 4))\nax.scatter(x.numpy(), y.numpy(), **scatterplot_kwargs)\nax.plot(x.numpy(), signal.numpy(), color = \"black\", linestyle = \"--\", label = r\"Signal: $f(x_i) = 2x_i + 1$\")\nfor i in range(n_points):\n    if i == 0: \n        ax.plot([x[i].item(), x[i].item()], [signal[i].item(), y[i].item()], color = \"red\", alpha = 0.3, linewidth = 0.8, label = r\"Noise: $\\epsilon_i$\")\n    else: \n        ax.plot([x[i].item(), x[i].item()], [signal[i].item(), y[i].item()], color = \"red\", alpha = 0.3, linewidth = 0.8)\nax.set(xlabel = r\"$x$\", ylabel = r\"$y$\")\nplt.legend()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.1: An illustrative decomposition of a data set (points) into a hypothesized underlying signal (dashed line) and noise (red segments).",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data = Signal + Noise</span>"
    ]
  },
  {
    "objectID": "chapters/02-signal-noise.html#overfitting",
    "href": "chapters/02-signal-noise.html#overfitting",
    "title": "1  Data = Signal + Noise",
    "section": "Overfitting",
    "text": "Overfitting\nIt’s important to emphasize here that the thing we want to learn is not the individual targets \\(y_i\\), but rather the underlying function \\(f(x)\\). To see why, let’s consider an example of what goes wrong if we try to learn the targets \\(y_i\\) exactly. This is called interpolation, and is illustrated by Figure 1.2:\n\nCode\nfrom scipy import interpolate\n# Create an interpolating function\nf_interp = interpolate.interp1d(x.numpy(), y.numpy(), kind='cubic')\nx_dense = torch.linspace(0, 10, 100)\ny_interp = f_interp(x_dense.numpy())\n\nfig, ax = plt.subplots(figsize = (6, 4))\nax.scatter(x.numpy(), y.numpy(), **scatterplot_kwargs)\nax.plot(x_dense.numpy(), y_interp, color = \"red\", linestyle = \"--\", label = \"interpolating fit\", zorder = -10)\nax.set(xlabel = r\"$x$\", ylabel = r\"$y$\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.2: A function which exactly interpolates the data, perfectly fitting both signal and noise.\n\n\n\nThe problem with interpolation is that while we have perfectly fit our training data, we have not learned the underlying signal \\(f(x)\\). If we were to generate new data with the same signal but with different noise, we would likely find that our interpolating function doesn’t actually make very good predictions about that data at all: many of its bends and wiggles don’t have any correspondence to features in the new data.\n\nCode\n# Generate new data\ny_new = 2.0 * x_dense + 1.0 + torch.randn(100) * 3.0\nfig, ax = plt.subplots(figsize = (6, 4))\nax.scatter(x_dense.numpy(), y_new.numpy(), **scatterplot_kwargs)\nax.plot(x_dense.numpy(), y_interp, color = \"red\", linestyle = \"--\", label = \"interpolating fit\", zorder = -10)\nax.set(xlabel = r\"$x$\", ylabel = r\"$y$\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.3: New data plotted alongside the interpolating fit from the previous figure.\n\n\n\nWe observe a number of irrelevant fluctuations in the interpolating fit that do not correspond to the underlying pattern. This phenomenon is called overfitting: by trying to fit the noise in our training data, we have failed to learn the true signal. We’ll learn how to quantify overfitting once we begin to study measures of model quality.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data = Signal + Noise</span>"
    ]
  },
  {
    "objectID": "chapters/02-signal-noise.html#modeling-the-noise",
    "href": "chapters/02-signal-noise.html#modeling-the-noise",
    "title": "1  Data = Signal + Noise",
    "section": "Modeling the Noise",
    "text": "Modeling the Noise\nIf we want to learn the signal \\(f\\) in Equation 1.1, it’s helpful to learn how to talk mathematically about the noise term \\(\\epsilon_i\\). To do this, we need to step into the language of probability theory. Our standing assumption will be that the noise terms \\(\\epsilon_i\\) are random variables drawn independently and identically-distributed from some probability distribution. This means that each time we observe a new data point, we get a different value of \\(\\epsilon_i\\) drawn from the same distribution, without any influence from other values of \\(\\epsilon_j\\) for \\(j \\neq i\\).\nThe noise distribution we will usually consider is the Gaussian distribution, also called the Gaussian distribution.\n\n\n\n\n\n\n\n\n\nFigure 1.4: The probability that a Gaussian random variable lies between two values \\(a\\) and \\(b\\) is given by the area under its PDF between those values.\n\n\n\n\n\nDefinition 1.1 (Gaussian (Normal) Distribution) A random variable \\(\\epsilon\\) has a Gaussian distribution with parameters mean \\(\\mu\\) and standard deviation \\(\\sigma\\) if the probability that \\(\\epsilon\\) has a value between \\(a\\) and \\(b\\) is given by:\n\\[\n\\begin{aligned}\n    \\mathbb{P}(a \\leq \\epsilon \\leq b) = \\int_a^b p_\\epsilon(x;\\mu, \\sigma) \\, dx\\;,\n\\end{aligned}\n\\]\nwhere\n\\[\n\\begin{aligned}\np_\\epsilon(x;\\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\\;.\n\\end{aligned}\n\\]\nThe function \\(p_\\epsilon(x;\\mu, \\sigma)\\) is called the probability density function (PDF) of the Gaussian distribution. We use the shorthand notation \\(\\epsilon \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) to indicate that the random variable \\(\\epsilon\\) has a Gaussian distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\nGaussian distributions are often called normal distributions in the statistics literature.\n\nHere’s a simple vectorized implementation of the Gaussian PDF, which can be evaluated on a PyTorch tensor of inputs:\n\npi = 3.141592653589793\ndef normal_pdf(x, mu, sigma):\n    return 1 / (sigma * (2 * pi)**0.5) * torch.exp(-0.5 * ((x - mu) / sigma) ** 2)\n\nnormal_pdf(torch.tensor([0.0, 1.0, 2.0]), mu=0.0, sigma=1.0)\n\ntensor([0.3989, 0.2420, 0.0540])\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.5: Normal distribution PDFs for different values.\n\n\n\n\nThe two parameters of the Gaussian distribution describe its “shape:” The mean \\(\\mu\\) indicates the “center” of the distribution, while the standard deviation \\(\\sigma\\) indicates how “spread out” the distribution is.\n“Gaussian noise” refers to random variables drawn from a Gaussian distribution. We can generate Gaussian noise from a given Gaussian distribution using many functions. We’ll focus on PyTorch’s implementation, which allows us to generate tensors of Gaussian noise easily:\n\nmu = 0.0\nsigma = 1.0\nepsilon = torch.normal(mu, sigma, size=(10,))\nepsilon\n\ntensor([ 0.4499,  1.8596, -0.5629,  1.0266,  0.2415, -0.7050,  0.2569,  0.1465,\n         0.0287, -2.3102])\n\n\nIf we take many samples of Gaussian noise and plot a histogram of their values, then we’ll find (via the law of large numbers) that the histogram approximates the PDF of the Gaussian distribution we sampled from, as illustrated in Figure 1.6:\n\n\n\n\n\n\n\n\n\nFigure 1.6: Histogram of samples from a Gaussian distribution compared to its PDF.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data = Signal + Noise</span>"
    ]
  },
  {
    "objectID": "chapters/02-signal-noise.html#properties-of-the-gaussian-distribution",
    "href": "chapters/02-signal-noise.html#properties-of-the-gaussian-distribution",
    "title": "1  Data = Signal + Noise",
    "section": "Properties of the Gaussian Distribution",
    "text": "Properties of the Gaussian Distribution\nThe Gaussian distribution has a number of useful properties for modeling noise in data. Here are a few:\n\nTheorem 1.1 (Translation) If \\(\\epsilon\\) is normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then the random variable \\(\\epsilon + c\\) (where \\(c\\) is a constant) is normally distributed with mean \\(\\mu + c\\) and standard deviation \\(\\sigma\\). In other words, if \\(\\epsilon \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), then \\(\\epsilon + c \\sim \\mathcal{N}(\\mu + c, \\sigma^2)\\).\n\n\nTheorem 1.2 (Scaling) If \\(\\epsilon\\) is normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then the random variable \\(a \\epsilon\\) (where \\(a\\) is a constant) is normally distributed with mean \\(a \\mu\\) and standard deviation \\(|a| \\sigma\\). In other words, if \\(\\epsilon \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), then \\(a \\epsilon \\sim \\mathcal{N}(a \\mu, (a \\sigma)^2)\\).\n\nThe expectation or mean of a continuous-valued random variable \\(\\epsilon\\) with probability density function \\(p_\\epsilon(x)\\) is defined as:\n\\[\n\\begin{aligned}\n    \\mathbb{E}[\\epsilon] = \\int_{-\\infty}^{\\infty} x \\, p_\\epsilon(x) \\, dx\\;.\n\\end{aligned}\n\\]\nThe variance of a continuous-valued random variable \\(\\epsilon\\) with probability density function \\(p_\\epsilon(x)\\) is defined as:\n\\[\n\\begin{aligned}\n    \\text{Var}(\\epsilon) = \\mathbb{E}[(\\epsilon - \\mathbb{E}[\\epsilon])^2] = \\int_{-\\infty}^{\\infty} (x - \\mathbb{E}[\\epsilon])^2 \\, p_\\epsilon(x) \\, dx\\;.\n\\end{aligned}\n\\]\nWe can interpret the variance as a measure of how far away \\(\\epsilon\\) “usually, on average” lies from its mean value.\n\nTheorem 1.3 (Mean and Variance of the Gaussian) For a normally distributed random variable with mean \\(\\mu\\) and standard deviation \\(\\sigma\\):\n\\[\n\\begin{aligned}\n    \\mathbb{E}[\\epsilon] &= \\mu \\\\\n    \\mathrm{Var}[\\epsilon] &= \\sigma^2.\n\\end{aligned}\n\\]\n\nThese two properties can be proven using some integration tricks which are beyond our scope here.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data = Signal + Noise</span>"
    ]
  },
  {
    "objectID": "chapters/02-signal-noise.html#the-standard-gaussian",
    "href": "chapters/02-signal-noise.html#the-standard-gaussian",
    "title": "1  Data = Signal + Noise",
    "section": "The Standard Gaussian",
    "text": "The Standard Gaussian\nThe standard Gaussian distribution is the Gaussian with mean zero and standard deviation one: \\(\\mathcal{N}(0, 1)\\). We often use the symbol \\(Z\\) to Due to the properties above, we can make any Gaussian random variable from a standard Gaussian: if \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), then, using Theorem 1.2 and Theorem 1.1, we can write \\(X\\) as\n\\[\n\\begin{aligned}\n    X \\sim \\sigma Z + \\mu\\;.\n\\end{aligned}\n\\]\nWe can check that this random variable has the correct mean and variance:\n\\[\n\\begin{aligned}\n    \\mathbb{E}[X] = \\mathbb{E}[\\sigma Z + \\mu] = \\sigma \\mathbb{E}[Z] + \\mu = \\sigma \\cdot 0 + \\mu = \\mu\\;,\n\\end{aligned}\n\\]\nwhere we’ve used linearity of expectation and the fact that \\(\\mathbb{E}[Z] = 0\\). To calculate the variance, we use the fact that \\(\\mathbb{E}[Z]= 0\\), again, so that\n\\[\n\\begin{aligned}\n    \\mathrm{Var}[X] = \\mathrm{Var}[\\sigma Z + \\mu] = \\mathrm{Var}[\\sigma Z] = \\sigma^2 \\mathrm{Var}[Z] = \\sigma^2 \\cdot 1 = \\sigma^2\\;.\n\\end{aligned}\n\\]\n\n\nWe’ve used the variance properties\n\\[\n\\begin{aligned}\n    \\mathrm{Var}[aX] = a^2\\mathrm{Var}[X]& \\\\\n    \\mathrm{Var}[X + b] = \\mathrm{Var}[X]&\\;\n\\end{aligned}\n\\]\nfor constants \\(a\\) and \\(b\\).",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data = Signal + Noise</span>"
    ]
  },
  {
    "objectID": "chapters/02-signal-noise.html#gaussian-noise",
    "href": "chapters/02-signal-noise.html#gaussian-noise",
    "title": "1  Data = Signal + Noise",
    "section": "Gaussian Noise",
    "text": "Gaussian Noise\nWith the Gaussian distribution in mind, let’s now return to our signal-plus-noise paradigm:\n\\[\n\\begin{aligned}\n    y_i = f(x_i) + \\epsilon_i\\;,\n\\end{aligned}\n\\]\nIf we assume that \\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\) (i.e., that the noise terms are independent and identically distributed Gaussian random variables with mean zero and standard deviation \\(\\sigma\\)), then we can use the translation property of the Gaussian distribution (Theorem 1.1) to deduce that\n\\[\n\\begin{aligned}\n    y_i &\\sim \\mathcal{N}(f(x_i), \\sigma^2)\\;.\n\\end{aligned}\n\\tag{1.2}\\]\nSo, we are modeling each data point \\(y_i\\) as a Gaussian random variable whose mean is given by the underlying signal \\(f(x_i)\\), and whose standard deviation is given by the noise level \\(\\sigma\\). This modeling approach is important enough to merit a name: Technically, the model described below is an additive Gaussian model, but we won’t worry about non-additive models here and therefore won’t bother repeating “additive.”\n\nDefinition 1.2 (Gaussian Model) The model\n\\[\n\\begin{aligned}\n    y &= f(x_i) + \\epsilon_i \\\\    \n    \\epsilon_i &\\sim \\mathcal{N}(0, \\sigma^2)\n\\end{aligned}\n\\tag{1.3}\\]\nis called a Gaussian data generating model.\n\nOften, as illustrated so far in these notes, we choose \\(f\\) to be a linear function of \\(x_i\\):\n\nDefinition 1.3 (Linear-Gaussian Model) A Gaussian model in which\n\\[\n\\begin{aligned}\n    f(x_i) = w_0 + w_1 x_i   \n\\end{aligned}\n\\tag{1.4}\\]\nfor parameters \\(w_0, w_1 \\in \\mathbb{R}\\) is called a (1-dimensional) linear-Gaussian model.\n\nHere’s a schematic picture of the linear-Gaussian model.\n\nCode\n# from https://stackoverflow.com/questions/47597119/plot-a-vertical-normal-distribution-in-python\ndef draw_gaussian_at(support, sd=1.0, height=1.0, \n        xpos=0.0, ypos=0.0, ax=None, **kwargs):\n    if ax is None:\n        ax = plt.gca()\n    gaussian = torch.exp((-support ** 2.0) / (2 * sd ** 2.0))\n    gaussian /= gaussian.max()\n    gaussian *= height\n    return ax.plot(gaussian + xpos, support + ypos, **kwargs)\n    \nsupport = torch.linspace(-10, 10, 1000)\nfig, ax = plt.subplots()\n\nax.plot(x, signal, color = \"black\", linestyle = \"--\", label = r\"Signal: $f(x_i) = 2x_i + 1$\")\n\nfor each in x:\n    draw_gaussian_at(support, sd=3, height=0.4, xpos=each, ypos=2.0 * each + 1.0, ax=ax, color='C0', alpha=0.4)\n\nax.scatter(x.numpy(), y.numpy(), **scatterplot_kwargs)\n\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$y$\")\nplt.title(\"Data points modeled as Gaussians around the signal\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.7: Illustration of the Gaussian noise model Equation 1.2, showing data points (black circles) as samples from Gaussian distributions (blue curves) centered on the signal (dashed line).\n\n\n\nWe are modeling \\(y_i\\) as a noisy sample from a Gaussian distribution centered at the signal value \\(f(x_i)\\), with noise level \\(\\sigma\\) controlling how spread out the distribution is. A larger value of \\(\\sigma\\) means that the observed data points \\(y_i\\) will tend to be further away from the signal \\(f(x_i)\\), while a smaller value of \\(\\sigma\\) means that the data points will tend to be closer to the signal. Larger values of the noise level \\(\\sigma\\) create noisier data sets where it is more difficult to discern the underlying signal.\n\nData Generating Distributions\nOne of the primary reasons to define models like Equation 1.2 is that they give us principled ways for thinking about what our models should learn – the signal – and what they should ignore – the noise. These models are also very helpful as models of where the data comes from – that is, as data generating distributions.\n\nDefinition 1.4 (Data Generating Distribution) A data generating distribution (also called a data generating model) is a probabilistic model that describes how data points (especially targets \\(y\\)) are generated in terms of random variables and their distributions.\n\nThe linear-Gaussian model is a simple example of a data generating model: it describes a recipe to simulate each target \\(y_i\\) by first computing the signal value \\(f(x_i)\\), then sampling a noise term \\(\\epsilon_i\\) from a Gaussian distribution, and finally adding the two together to get \\(y_i = f(x_i) + \\epsilon_i\\).\nA very useful feature of data generating models is that they also give us tools to measure how well our learned signal fits observed data, via the concept of a likelihood.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data = Signal + Noise</span>"
    ]
  },
  {
    "objectID": "chapters/02-signal-noise.html#model-likelihood",
    "href": "chapters/02-signal-noise.html#model-likelihood",
    "title": "1  Data = Signal + Noise",
    "section": "Model Likelihood",
    "text": "Model Likelihood\nGiven a data-generating distribution, we can compute the likelihood of the observed data under that model. Recall that the PDF of a single data point \\(y_i\\) under the Gaussian noise model Equation 1.2 with predictor value \\(x_i\\) is given by the formula\n\\[\n\\begin{aligned}\n    p_{y}(y_i; f(x_i), \\sigma^2) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp\\left(-\\frac{(y_i - f(x_i))^2}{2\\sigma^2}\\right)\\;.\n\\end{aligned}\n\\]\nThe likelihood of the complete data set is simply the product of the individual data point PDFs, evaluated at their corresponding observed values. The likelihood is a function of the predictors which we’ll collect into a vector $= (x_1,,x_n)^T, the targets which we’ll collect into a vector $ \\(\\mathbf{y}= (y_1,\\ldots,y_n)^T\\), and the parameters of the model (in this case, the function \\(f\\) and the noise level \\(\\sigma\\)):\n\nDefinition 1.5 (Gaussian Likelihood, Log-Likelihood) The likelihood of the observed data \\((\\mathbf{x}, \\mathbf{y})\\) under a 1d Gaussian model with function \\(f\\) and noise level \\(\\sigma\\) is given by:\n\\[\n\\begin{aligned}\n    L(\\mathbf{x}, \\mathbf{y}; f, \\sigma) = \\prod_{i = 1}^n p_{y}(y_i; f(x_i), \\sigma^2)\n\\end{aligned}\n\\]\nThe log-likelihood is the logarithm of the likelihood:\n\\[\n\\begin{aligned}\n    \\mathcal{L}(\\mathbf{x}, \\mathbf{y}; f, \\sigma) = \\log L(\\mathbf{x}, \\mathbf{y}; f, \\sigma) = \\sum_{i = 1}^n \\log p_{y}(y_i; f(x_i), \\sigma^2)\\;.\n\\end{aligned}\n\\tag{1.5}\\]\n\nThe log-likelihood \\(\\mathcal{L}\\) in Equation 1.5 is almost always the tool we work with in applied contexts – it turns products into sums, which is very useful in computational practice.\n\nA First Look: Likelihood Maximization\nLet’s now finally fit a machine learning model! We’ll assume that the data is sampled from the linear-Gaussian model specified by Equation 1.3 and Equation 1.4, and try to fit the parameters \\(w_0, w_1\\) of the function \\(f\\) to maximize the likelihood of the observed data. For now, we’ll do this simply by choosing the combination of parameters that achieves the best likelihood from among many candidates:\nTo see how the likelihood can give us a tool to assess model fit, we can compute the likelihood of our observed data for different choices of the function \\(f\\). This can be done in a simple grid search over possible values of the parameters \\(w_0, w_1\\) in the linear function \\(f(x) = w_0 + w_1 x\\).\n\nsig = 3.0  # assumed noise level\n1best_ll = -float('inf')\nbest_w = None\n\nfor w0 in torch.linspace(-5, 5, 20):\n    for w1 in torch.linspace(-1, 3, 20):\n2        f = lambda x: w0 + w1 * x\n        ll = 0.0\n3        ll += normal_pdf(y, f(x), sig).log().sum().item()\n\n4        if ll &gt; best_ll:\n            best_ll = ll\n            best_w = (w0.item(), w1.item())\n\n\n1\n\nInitialize the best log-likelihood and best parameters.\n\n2\n\nDefine the predictor function \\(f\\) with current parameters.\n\n3\n\nCompute the data log-likelihood. The normal_pdf function computes the PDF values for all data points at once, which we then log and sum to get the log-likelihood.\n\n4\n\nUpdate the best log-likelihood and parameters if the current log-likelihood is better.\n\n\n\n\nLet’s check the predictor function \\(f\\) we learned by heuristically maximizing the log-likelihood against the observed data:\n\nCode\nfig, ax = plt.subplots(figsize = (6, 4))\n\nax.scatter(x.numpy(), y.numpy(), **scatterplot_kwargs)\nax.plot(x.numpy(), (best_w[0] + best_w[1] * x).numpy(), color = \"firebrick\", linestyle = \"--\", label = r\"$f(x) = w_0 + w_1 x$\")\n\nax.plot(x.numpy(), signal.numpy(), color = \"black\", linestyle = \"--\", label = r\"True signal: $f(x) = 2x + 1$\")\nax.set(xlabel = r\"$x$\", ylabel = r\"$y$\")\nplt.legend()\nplt.title(fr\"Best LL: {best_ll:.2f}   $w_1 = {best_w[1]:.2f}, w_0 = {best_w[0]:.2f}$\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.8: Comparison of the true signal (black dashed line) and the model fit by maximizing the likelihood (red dashed line).\n\n\n\nThe model we selected via our maximum likelihood grid-search agrees relatively closely with the true underlying signal.\nSoon, we’ll learn how to use the likelihood as a method to learn the function \\(f\\) more systematically from data.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data = Signal + Noise</span>"
    ]
  },
  {
    "objectID": "chapters/03-maximum-likelihood.html",
    "href": "chapters/03-maximum-likelihood.html",
    "title": "2  Maximum Likelihood Estimation and Gradients",
    "section": "",
    "text": "Recap: Log-Likelihood of the Linear-Gaussian Model\nOpen the live notebook in Google Colab or download the live notebook.\nLast time, we introduced the idea of modeling data as signal + noise, studied the Gaussian distribution as a model of noise, and introduced the linear-Gaussian model for prediction in the context of linear trends. We also derived the log-likelihood function for the linear-Gaussian model and introduced the idea that we could learn the signal of the data by maximizing the log-likelihood with respect to the model parameters. In this chapter, we’ll begin our study of how to maximize the likelihood systematically using tools from calculus.\n© Phil Chodrow, 2025",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Maximum Likelihood Estimation and Gradients</span>"
    ]
  },
  {
    "objectID": "chapters/03-maximum-likelihood.html#the-gradient-of-a-multivariate-function",
    "href": "chapters/03-maximum-likelihood.html#the-gradient-of-a-multivariate-function",
    "title": "2  Maximum Likelihood Estimation and Gradients",
    "section": "The Gradient of a Multivariate Function",
    "text": "The Gradient of a Multivariate Function\n\n\nAs you can study in courses dedicated to multivariable calculus, the existence of all of a function’s partial derivatives does not necessarily imply that the function is multivariate differentiable. In this course, we’ll exclusively treat functions which are indeed multivariate differentiable unless otherwise noted, and so this distinction will not be an issue for us.\n\nDefinition 2.1 Let \\(f:\\mathbb{R}^p\\rightarrow \\mathbb{R}\\) be a function which accepts a vector input \\(\\mathbf{w}=(w_1,\\ldots,w_p)^T\\in \\mathbb{R}^p\\) and returns a scalar output \\(f(\\mathbf{w})\\in \\mathbb{R}\\). The partial derivative of \\(f\\) with respect to the \\(j\\)-th coordinate \\(w_j\\) is defined as the limit\n\\[\n\\begin{aligned}\n    \\frac{\\partial f}{\\partial w_i} &= \\lim_{h \\rightarrow 0} \\frac{f(w_1,\\ldots,w_i + h, \\ldots w_p) - f(w_1,\\ldots,w_i, \\ldots w_p)}{h} \\\\\n    &= \\lim_{h \\rightarrow 0} \\frac{f(\\mathbf{w}+ h\\mathbf{e}_i) - f(\\mathbf{w})}{h}\\;,\n\\end{aligned}\n\\]\nwhere \\(\\mathbf{e}_i = (0,0,\\ldots,1,\\ldots,0,0)^T\\) is the \\(i\\)-th standard basis vector in \\(\\mathbb{R}^p\\), i.e., the vector with a 1 in the \\(i\\)-th position and 0’s elsewhere. If this limit does not exist, then the partial derivative is said to be undefined.\n\nJust like in single-variable calculus, it’s not usually convenient to work directly with the limit definition of the partial derivative. Instead we use the following heuristic:\n\nProposition 2.1 To compute \\(\\frac{\\partial f}{\\partial w_i}\\), treat all other variables \\(w_j\\) for \\(j\\neq i\\) as constants, and differentiate \\(f\\) with respect to \\(w_i\\) using the usual rules of single-variable calculus (power rule, product rule, chain rule, etc.).\n\n\nExercise 2.1 (Practice with Partial Derivatives) Let \\(f:\\mathbb{R}^3\\rightarrow \\mathbb{R}\\) be defined by \\(f(x,y,z) = x^2\\sin y + yz + z^3x\\). Compute \\(\\frac{\\partial f}{\\partial x}\\), \\(\\frac{\\partial f}{\\partial y}\\), and \\(\\frac{\\partial f}{\\partial z}\\).\n\n\n\n\n\n\n\nSolSolution for Exercise 2.1\n\n\n\n\n\nTo compute \\(\\frac{\\partial f}{\\partial x}\\), we treat \\(y\\) and \\(z\\) as constants, which yields\n\\[\n\\frac{\\partial f}{\\partial x} = 2x \\sin y + z^3\\;.\n\\]\nSimilarly, we can compute \\(\\frac{\\partial f}{\\partial y}\\) and \\(\\frac{\\partial f}{\\partial z}\\):\n\\[\n\\begin{align}\n    \\frac{\\partial f}{\\partial y} &= x^2 \\cos y + z \\\\\n    \\frac{\\partial f}{\\partial z} &= y + 3z^2 x\\;.    \n\\end{align}\n\\]\n\n\n\n\nExercise 2.2 (Partial Derivative of the Gaussian Log-Likelihood)  \n\n\nDefinition 2.2 Let \\(f:\\mathbb{R}^p\\rightarrow \\mathbb{R}\\) be a differentiable function which accepts a vector input \\(\\mathbf{w}=(w_1,\\ldots,w_p)^T\\in \\mathbb{R}^p\\) and returns a scalar output \\(f(\\mathbf{w})\\in \\mathbb{R}\\). The gradient of \\(f\\) at \\(\\mathbf{w}\\), written \\(\\nabla f(\\mathbf{w}) \\in \\mathbb{R}^p\\), is the vector of partial derivatives\n\\[\n\\begin{align}\n    \\nabla f(\\mathbf{w}) &= \\begin{pmatrix}\n    \\frac{\\partial f}{\\partial w_1} \\\\\n    \\frac{\\partial f}{\\partial w_2} \\\\\n    \\vdots \\\\\n    \\frac{\\partial f}{\\partial w_p}\n    \\end{pmatrix}\\;.\n\\end{align}\n\\]\n\n\nExercise 2.3 (Writing Gradients) Write the gradient of the function in Exercise 2.1.\n\n\n\n\n\n\n\nSolSolution for Exercise 2.3\n\n\n\n\n\nIn the function from Exercise 2.1, the gradient is given by stacking the partial derivatives we computed into a single vector:\n\\[\n\\begin{aligned}\n    \\nabla f(x, y, z) =\n     \\begin{pmatrix}\n    \\frac{\\partial f}{\\partial x} \\\\\n    \\frac{\\partial f}{\\partial y} \\\\\n    \\frac{\\partial f}{\\partial z}\n    \\end{pmatrix} &=\n    \\begin{pmatrix}\n    2x \\sin y + z^3 \\\\\n    x^2 \\cos y + z \\\\\n    y + 3z^2 x  \n    \\end{pmatrix}\n    \\in \\mathbb{R}^3\\;.\n\\end{aligned}\n\\]\n\n\n\n\nExercise 2.4 Consider the mean-squared error function for a simple linear model with parameters \\(w_0\\) and \\(w_1\\):\n\\[\n\\begin{aligned}\n    R(\\mathbf{x}, \\mathbf{y}; w_0, w_1) = \\frac{1}{n} \\sum_{i = 1}^n (y_i - w_1x_i - w_0)^2\\;.\n\\end{aligned}\n\\]\nCompute the gradient \\(\\nabla R(\\mathbf{x}, \\mathbf{y}; w_0, w_1)\\) with respect to the parameters \\(w_0\\) and \\(w_1\\).\nNote: we’ll soon see that this function is closely related to the log-likelihood of the linear-Gaussian model.\n\n\n\n\n\n\n\nSolSolution for Exercise 2.4\n\n\n\n\n\nWe can compute the gradient by computing each partial derivative in turn:\n\\[\n\\begin{aligned}\n    \\frac{\\partial R}{\\partial w_0} &= \\frac{2}{n} (y_i - w_1x_i - w_0) \\\\\n    \\frac{\\partial R}{\\partial w_1} &= \\frac{-2}{n} x_i(y_i - w_1x_i - w_0) \\;,\n\\end{aligned}\n\\] where we’ve used the rules for derivatives. Stacking these into a vector gives\n\\[\n\\begin{aligned}\n    \\nabla R(\\mathbf{x}, \\mathbf{y}; w_0, w_1) = \\frac{2}{n} \\begin{pmatrix}\n    \\sum_{i=1}^n (y_i - w_1x_i - w_0) \\\\\n    \\sum_{i=1}^n x_i (y_i - w_1x_i - w_0)\n    \\end{pmatrix}\n\\end{aligned}\n\\]\n\n\n\n\nChecking Gradients with torch\nThe pytorch package, which we’ll use throughout this course, implements automatic differentiation. Automatic differentiation is an extraordinarily powerful tool which we’ll study later in the course. For now, we’ll just note that it provides a handy way to check calculations of derivatives and gradients. For example, we can use torch to check the gradient we computed in Exercise 2.3 as follows:\n\nimport torch\nx = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n\n# function to differentiate\nf = lambda x: x[0]**2 * torch.sin(x[1]) + x[1]*x[2] + x[2]**3 * x[0]\n\n# compute the gradient by hand using the formula we derived\nour_grad = torch.tensor([\n    2 * x[0] * torch.sin(x[1]) + x[2]**3,\n    x[0]**2 * torch.cos(x[1]) + x[2],\n    x[1] + 3 * x[2]**2 * x[0]\n])\nprint(our_grad)\n\n# compute the gradient using automatic differentiation\n1y = f(x)\n2y.backward()\n3print(x.grad)\n\n\n1\n\nFirst, we compute the value of the function we want to differentiate and store the result to a variable (in this case called y).\n\n2\n\nNext, we call the backward() method on y, which computes the gradient of y with respect to its inputs (in this case, the vector x) using automatic differentiation.\n\n3\n\nFinally, we can access the computed gradient via the grad attribute of the input tensor x.\n\n\n\n\ntensor([28.8186,  2.5839, 29.0000])\ntensor([28.8186,  2.5839, 29.0000])\n\n\nThe two approaches agree! As we grow comfortable with the calculus, we’ll begin to rely more on torch’s automatic differentiation capabilities to compute gradients for us.\n\n\nThe Gradient Points In the Direction of Greatest Increase\nAn important feature of the gradient is that it tells us the direction in which a small change in the function inputs \\(\\mathbf{w}\\) could produce the greatest increase in the function output \\(f(\\mathbf{w})\\). Here’s an example using the function from Exercise 2.4. torch makes it very easy to implement this function.\n\ndef MSE(x, y, w0, w1):\n    return -((y - (w1 * x + w0))**2).mean()\n\nWe first plot the function as a function of the parameters \\(w_0\\) and \\(w_1\\) and then we overlay arrows representing the gradients at various points in the \\((w_0, w_1)\\) space, with the gradients calculated via automatic differentiation in torch.\n\nCode\nfrom matplotlib import pyplot as plt\n\n# create the grid of (mu, sigma^2) values and the data\nw0_grid = torch.linspace(-1, 1, 100)\nw1_grid = torch.linspace(0.1, 2, 100)\nW0, W1 = torch.meshgrid(w0_grid, w1_grid, indexing='ij')\n\nx = torch.tensor([0.5, -1.0, 1.0, 0.7, 0.3])  # example data points\ny = torch.tensor([1.0, 0.0, 2.0, 1.5, 0.5])\n\nLL = torch.zeros(W0.shape)\nfor i in range(W0.shape[0]):\n    for j in range(W0.shape[1]):\n        LL[i, j] = MSE(x, y, W0[i, j], W1[i, j])\n\n\n# initialize the figure \nfig, ax = plt.subplots()\n\n# show the log-likelihood as a contour plot\nim = ax.contourf(LL.numpy(), levels=100, cmap='inferno', extent = [w1_grid[0], w1_grid[-1], w0_grid[0], w0_grid[-1]])\nax.contour(LL.numpy(), levels=100, colors='black', linewidths=0.5, extent = [w1_grid[0], w1_grid[-1], w0_grid[0], w0_grid[-1]])\n\nax.set_ylabel(r'$w_1$')\nax.set_xlabel(r'$w_0$')\n\n# compute and plot the gradients at various points\nfor w0, w1 in [( -0.5, 1.5), (0.0, 1.0), (0.5, 1.5), (.25, 0.5), (0.75, 1.0)]:\n    w0_tensor = torch.tensor(w0, requires_grad=True)\n    w1_tensor = torch.tensor(w1, requires_grad=True)\n    \n    ll = MSE(x, y, w0_tensor, w1_tensor)\n    ll.backward()\n    \n    grad_w0 = w0_tensor.grad.item()\n    grad_w1 = w1_tensor.grad.item()\n    \n    ax.quiver(w1, w0, grad_w1, grad_w0, color='black', scale=20, width=0.01)\n    ax.scatter(w1, w0, color='black', s=30)\n\nplt.colorbar(im)\nax.set_title('Gradients of the mean-squared error')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.1: Visualization of the gradients of the mean-squared error function with respect to the parameters \\(w_0\\) and \\(w_1\\). The background color indicates the value of the mean-squared error, with lighter colors representing higher values. Dotted curves give contours along which the function is constant. The black arrows represent the gradients at various points in the \\((w_0, w_1)\\) space, pointing in the direction of greatest increase of the mean-squared error function.\n\n\n\nTwo observations about Figure 2.1 are worth noting:\n\nThe gradient arrows always point uphill and are orthogonal (at right angles with) to the contour lines of the function.\nThe gradient arrows get smaller as we approach the maximum of the log-likelihood function, eventually becoming zero at the maximum itself.\n\nBoth of these features are possible to prove mathematically, although we won’t do so here.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Maximum Likelihood Estimation and Gradients</span>"
    ]
  },
  {
    "objectID": "chapters/03-maximum-likelihood.html#critical-points-and-local-extrema",
    "href": "chapters/03-maximum-likelihood.html#critical-points-and-local-extrema",
    "title": "2  Maximum Likelihood Estimation and Gradients",
    "section": "Critical Points and Local Extrema",
    "text": "Critical Points and Local Extrema\nOne way we can use gradients is by analytically computing the local extrema of a function: solve the equation \\(\\nabla \\mathcal{L}(\\mathbf{w}) = 0\\) for \\(\\mathbf{w}\\) to find critical points of the log-likelihood, and then check which of these points are local maxima.\nA critical point of a multivariate function is a point at which all of its partial derivatives are equal to zero. Critical points are candidates for local maxima or minima of the function, and so they are of interest when performing maximum-likelihood estimation by solving \\(\\nabla \\mathcal{L}(\\mathbf{w}) = 0\\).\n\nDefinition 2.3 (Critical Points of Multivariate Functions) A critical point of a differentiable function \\(f:\\mathbb{R}^p\\rightarrow \\mathbb{R}\\) is a point \\(\\mathbf{w}^* \\in \\mathbb{R}^p\\) such that \\(\\nabla f(\\mathbf{w}^*) = \\mathbf{0}\\) (the zero vector in \\(\\mathbb{R}^p\\)).\n\nAll critical points of a function can be identified by solving the system of equations \\(\\nabla f(\\mathbf{w}) = \\mathbf{0}\\). In a few rare cases, it’s possible to solve this system analytically to find all critical points.\n\n\nThe notation \\(\\lVert \\mathbf{v} \\rVert_2\\) refers to the Euclidean norm of \\(\\mathbf{v}\\), with formula\n\\[\n\\begin{aligned}\n    \\lVert \\mathbf{v} \\rVert_2 = \\sqrt{\\sum_{i = 1}^p v_i^2}\\;.\n\\end{aligned}\n\\]\n\nDefinition 2.4 (Local Minima and Maxima) A local minimum of a differentiable function \\(f:\\mathbb{R}^p\\rightarrow \\mathbb{R}\\) is a point \\(\\mathbf{w}^* \\in \\mathbb{R}^p\\) such that there exists some radius \\(r&gt;0\\) such that for all \\(\\mathbf{w}\\) with \\(\\|\\mathbf{w}- \\mathbf{w}^*\\|_2 &lt; r\\), we have \\(f(\\mathbf{w}) \\geq f(\\mathbf{w}^*)\\). A local maximum is defined similarly, with the inequality reversed: for all \\(\\mathbf{w}\\) with \\(\\|\\mathbf{w}- \\mathbf{w}^*\\|_2 &lt; r\\), we have \\(f(\\mathbf{w}) \\leq f(\\mathbf{w}^*)\\).\n\n\n\nThe Mild Conditions of Theorem 2.1 are that \\(f\\) is continuously differentiable in an open neighborhood around \\(\\mathbf{w}^*\\).\n\nTheorem 2.1 (Local Extrema are Critical Points) Under Mild Conditions*, if \\(\\mathbf{w}^*\\) is a local extremum (minimum or maximum) of a differentiable function \\(f:\\mathbb{R}^p\\rightarrow \\mathbb{R}\\), then \\(\\mathbf{w}^*\\) is a critical point of \\(f\\).\n\n\n\n\n\n\n\nNoteWe Always Minimize\n\n\n\nAlthough our motivating problem is still maximum likelihood estimation, it is conventional in the literature on statistics, machine learning, and optimization to always seek minima of a given function. This works because maximizing \\(\\mathcal{L}(\\mathbf{w})\\) is equivalent to minimizing \\(-\\mathcal{L}(\\mathbf{w})\\). Therefore, in the remainder of this chapter and in subsequent chapters, we will often refer to “minimizing the negative log-likelihood” rather than “maximizing the log-likelihood.” Perhaps confusingly, we’ll still refer to the result as the “maximum likelihood estimate” (MLE).\n\n\nTheorem 2.1 tells us that we can try to find the maximum likelihood estimate of a parameter vector \\(\\mathbf{w}\\) by solving the equation \\(\\nabla \\mathcal{L}(\\mathbf{w}) = \\mathbf{0}\\). In principle, we should then check that the critical points we find are indeed minima of \\(-\\mathcal{L}(\\mathbf{w})\\) rather than maxima or saddle points, which can sometimes be done using the multivariate second-derivative test. In practice, however, this second step is often skipped. Skipping the second-derivative test can be justified if it is known that \\(-\\mathcal{L}\\) is a convex function.\nEquipped with the concept of critical points, we are ready to find maximum likelihood estimates by solving the equation \\(\\nabla \\mathcal{L}(\\mathbf{w}) = \\mathbf{0}\\).\nBy convention, the maximum-likelihood estimate of a parameter is given a “hat” symbol, so we would write the MLE estimators we found above as \\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}^2\\).",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Maximum Likelihood Estimation and Gradients</span>"
    ]
  },
  {
    "objectID": "chapters/03-maximum-likelihood.html#revisiting-the-linear-gaussian-log-likelihood",
    "href": "chapters/03-maximum-likelihood.html#revisiting-the-linear-gaussian-log-likelihood",
    "title": "2  Maximum Likelihood Estimation and Gradients",
    "section": "Revisiting the Linear-Gaussian Log-Likelihood",
    "text": "Revisiting the Linear-Gaussian Log-Likelihood\nLet’s now consider the linear-Gaussian model from last chapter. In this model, we assume that each observed target variable \\(y_i\\) is sampled from a Gaussian distribution with mean equal to a linear function of the corresponding feature vector \\(x_i\\):\n\\[\n\\begin{aligned}\n    y_i &\\sim \\mathcal{N}(w_1 x_i + w_0, \\sigma^2)\\;,\n\\end{aligned}\n\\]\nTo find the maximum-likelihood estimates given a data set of pairs \\((x_i,y_i)\\) for \\(i=1,\\ldots,n\\), we need to compute the log-likelihood function for this model, which as per last chapter is\n\\[\n\\begin{aligned}\n    \\mathcal{L}(\\mathbf{x}, \\mathbf{y}; w_1, w_0) &= \\sum_{i = 1}^n \\log p_y(y_i;w_1x_i + w_0; \\sigma^2) \\\\\n    &= \\sum_{i = 1}^n \\log \\left( \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left( -\\frac{(y_i - (w_1 x_i + w_0))^2}{2\\sigma^2} \\right) \\right) \\\\\n    &= \\sum_{i = 1}^n \\left( -\\frac{1}{2} \\log(2\\pi \\sigma^2) - \\frac{(y_i - (w_1 x_i + w_0))^2}{2\\sigma^2} \\right) \\\\\n    &= \\underbrace{-\\frac{n}{2} \\log(2\\pi \\sigma^2)}_{C} - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - (w_1 x_i + w_0))^2 \\\\\n    &= C - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - w_1 x_i - w_0)^2 \\\\\n    &= C - \\frac{n}{2\\sigma^2} \\cdot \\frac{1}{n} \\sum_{i=1}^n (y_i - w_1 x_i - w_0)^2 \\\\\n    &= C - \\frac{n}{2\\sigma^2} R(\\mathbf{x}, \\mathbf{y}; w_0, w_1)\\;.\n\\end{aligned}\n\\tag{2.1}\\]\nWe’ve collected terms that do not depend on \\(w_0\\) or \\(w_1\\) into a constant term \\(C\\), and noticed that there’s a copy of the mean-squared error function \\(R(\\mathbf{x}, \\mathbf{y}; w_0, w_1) = \\frac{1}{n}\\sum_{i=1}^n (y_i - w_1 x_i - w_0)^2\\) from Exercise 2.4 appearing in the likelihood expression. Indeed, this term is the only that involves the parameters \\(w_0\\) and \\(w_1\\). This means:\n\nTo maximize the likelihood \\(\\mathcal{L}\\) with respect to \\(w_0\\) and \\(w_1\\), we can equivalently minimize the mean-squared error \\(R(\\mathbf{x}, \\mathbf{y}; w_0, w_1)\\).\n\n\nExercise 2.5 Compute the gradient \\(\\nabla R(\\mathbf{x}, \\mathbf{y}; w_0, w_1)\\) of the sum of squared errors for the linear-Gaussian model with respect to the parameters \\(w_0\\) and \\(w_1\\).\n\n\n\n\n\n\n\nSolSolution for Exercise 2.5\n\n\n\n\n\nWe have\n\\[\n\\begin{aligned}\n    \\frac{\\partial R}{\\partial w_0} = \\frac{1}{n} \\sum_{i=1}^n 2(y_i - w_1 x_i - w_0)(-1) &= -\\frac{2}{n} \\sum_{i=1}^n (y_i - w_1 x_i - w_0) \\\\\n\\end{aligned}\n\\]\nand\n\\[\n\\begin{aligned}\n    \\frac{\\partial R}{\\partial w_1} = \\frac{1}{n} \\sum_{i=1}^n 2(y_i - w_1 x_i - w_0)(-x_i) &= -\\frac{2}{n} \\sum_{i=1}^n x_i (y_i - w_1 x_i - w_0)\\;.\n\\end{aligned}\n\\]",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Maximum Likelihood Estimation and Gradients</span>"
    ]
  },
  {
    "objectID": "chapters/03-maximum-likelihood.html#a-first-look-gradient-descent-for-maximum-likelihood-estimation",
    "href": "chapters/03-maximum-likelihood.html#a-first-look-gradient-descent-for-maximum-likelihood-estimation",
    "title": "2  Maximum Likelihood Estimation and Gradients",
    "section": "A First Look: Gradient Descent for Maximum Likelihood Estimation",
    "text": "A First Look: Gradient Descent for Maximum Likelihood Estimation\nNow that we have tools to compute gradients, we can use these gradients to find maximum-likelihood estimates numerically using a gradient method. There are many kinds of gradient methods, and they all have in common a simple idea:\n\nDefinition 2.5 (Gradient Methods) A gradient method for optimizing a multivariate function \\(f:\\mathbb{R}^p\\rightarrow \\mathbb{R}\\) is an iterative algorithm which starts from an initial guess \\(\\mathbf{w}^{(0)} \\in \\mathbb{R}^p\\) and produces a sequence of estimates \\(\\mathbf{w}^{(1)}, \\mathbf{w}^{(2)}, \\ldots\\) by repeatedly updating the current estimate \\(\\mathbf{w}^{(t)}\\) in the direction of the negative gradient \\(-\\nabla f(\\mathbf{w}^{(t)})\\), or some approximation thereof.\n\nThe simplest gradient method is gradient descent with fixed learning rate:\n\nDefinition 2.6 (Gradient Descent) Gradient descent is an algorithm that iterates the update\n\\[\n\\begin{aligned}\n    \\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\alpha\\nabla f(\\mathbf{w}^{(t)})\\;,\n\\end{aligned}\n\\]\nwhere \\(\\alpha \\in \\mathbb{R}_{&gt;0}\\) is a fixed hyperparameter called the learning rate.\n\nLet’s use gradient descent to find maximum-likelihood estimates for the parameters of the linear-Gaussian model in a simple example. To visualize gradient descent, we’ll start by implementing a function for the linear-Gaussian log-likelihood in terms of the parameters \\(w_0\\) and \\(w_1\\):\nThen we’ll generate some synthetic data from a linear-Gaussian model with known parameters:\n\n# true parameters\nw0 = torch.tensor(0.0)\nw1 = torch.tensor(2.0)\nsigma2 = torch.tensor(1.0)\n\n# observed data\nx = torch.linspace(-2, 2, 101)\ny = w1 * x + w0 + torch.sqrt(sigma2) * torch.randn_like(x)\n\n\nCode\nfig, ax = plt.subplots(figsize = (6, 4))\nax.scatter(x, y, color='steelblue', s=10)\nax.set_xlabel('Feature (x)')\nax.set_ylabel('Target (y)')\nt = ax.set_title('Observed Data from Linear-Gaussian Model')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.2: Sample data from a linear-Gaussian model with true parameters \\(w_0 = 0\\), \\(w_1 = 2\\), and \\(\\sigma^2 = 1\\).\n\n\n\nOur aim in maximum likelihood estimation is to find estimators \\(\\hat{w}_0\\) and \\(\\hat{w}_1\\) which maximize the log-likelihood of the observed data. As we saw in Equation 2.1, maximizing the log-likelihood is equivalent to minimizing the mean-squared error between the observed targets \\(y_i\\) and the linear predictions \\(w_1 x_i + w_0\\). Let’s go ahead and plot the mean-squared error:\n\nCode\nw0_grid = torch.linspace(-1, 1, 100)\nw1_grid = torch.linspace(1, 3, 100)\n\nW0, W1 = torch.meshgrid(w0_grid, w1_grid, indexing='ij')\n\nLL = torch.zeros(W0.shape)\n\nfor i in range(W0.shape[0]):\n    for j in range(W0.shape[1]):\n        LL[i, j] = -MSE(x, y, W0[i, j], W1[i, j])\n\n# visualize the log-likelihood surface\nfig, ax = plt.subplots(figsize=(6, 5))\n\nim = ax.contourf(LL.numpy(), levels=20, cmap='inferno_r', extent = [w1_grid[0], w1_grid[-1], w0_grid[0], w0_grid[-1]])\nax.contour(LL.numpy(), levels=20, colors='black', linewidths=0.5, extent = [w1_grid[0], w1_grid[-1], w0_grid[0], w0_grid[-1]])\nax.set_ylabel(r'Intercept ($w_0$)')\nax.set_xlabel(r'Slope ($w_1$)')\nplt.colorbar(im, ax=ax)\nax.set_title('Mean-squared error for linear-Gaussian model')\n\nax.scatter([w1], [w0], color='black', s=50, label='True Parameters', facecolors='white')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.3: Mean-squared error \\(R(\\mathbf{x}, \\mathbf{y}; w_0, w_1)\\) for the linear-Gaussian model as a function of the intercept \\(w_0\\) and slope \\(w_1\\) with the data shown in Figure 2.2. The dot indicates the location of the true parameters used to generate the data.\n\n\n\n\n\n\n\n\n\nQuestionQuestion\n\n\n\n\nWhy does this surface have only one minimum, and why is that minimum so close to the true parameters used to generate the data? Seems pretty convenient!\n\nIt is convenient! The uniqueness of the local minimum is due to a property called convexity that we’ll study later in the course. The closeness of the minimum to the true parameters is a consequence of the consistency property of maximum-likelihood estimators, which you can study in courses on statistical inference.\n\n\nThe gradient descent algorithm will start from an initial guess for the parameters \\((w_0, w_1)\\) and iteratively update this guess in the direction of the negative gradient of the negative log-likelihood. We’ll implement this with a simple for-loop:\n\n# algorithm parameters\nlearning_rate = 0.05\nnum_iterations = 20\nn = x.shape[0] # number of data points\n\n# initial guesses\nw0_est = torch.tensor(-0.5, requires_grad=True)\nw1_est = torch.tensor(1.5, requires_grad=True)\n\n# store the history of estimates\nw0_history = [w0_est.item()]\nw1_history = [w1_est.item()]\n\n# main loop\nfor t in range(num_iterations):\n\n1    w0_grad = -2/n * torch.sum(y - (w1_est * x + w0_est))\n2    w1_grad = -2/n * torch.sum(x * (y - (w1_est * x + w0_est)))\n\n3    w1_est = w1_est - learning_rate * w1_grad\n4    w0_est = w0_est - learning_rate * w0_grad\n    \n    # store the history of estimates\n    w0_history.append(w0_est.item())\n    w1_history.append(w1_est.item())\n\n\n1\n\nCompute the gradient of the negative log-likelihood with respect to \\(w_0\\) using the formula derived in Exercise 2.5.\n\n2\n\nCompute the gradient of the negative log-likelihood with respect to \\(w_1\\) using the formula derived in Exercise 2.5.\n\n3\n\nUpdate the estimate of \\(w_1\\) by taking a step in the direction of the negative gradient, scaled by the learning rate.\n\n4\n\nUpdate the estimate of \\(w_0\\) by taking a step in the direction of the negative gradient, scaled by the learning rate.\n\n\n\n\nFigure 2.4 shows the trajectory of the gradient descent algorithm on the negative log-likelihood surface, ultimately landing near the minimal value of the MSE and close to the true parameters used to generate the data. We also show the fitted linear model corresponding to the final estimates of \\(w_0\\) and \\(w_1\\), which visually agrees well with the data.\n\nCode\nfig, ax = plt.subplots(1, 2, figsize=(8, 3))\n\nim = ax[0].contourf(LL.numpy(), levels=20, cmap='inferno_r', extent = [w1_grid[0], w1_grid[-1], w0_grid[0], w0_grid[-1]])\nax[0].contour(LL.numpy(), levels=20, colors='black', linewidths=0.5, extent = [w1_grid[0], w1_grid[-1], w0_grid[0], w0_grid[-1]])\nax[0].set_ylabel(r'Intercept ($w_0$)')\nax[0].set_xlabel(r'Slope ($w_1$)')\nplt.colorbar(im, ax=ax[0])\nax[0].set_title('Mean-squared error for\\nlinear-Gaussian model')\nax[0].plot(w1_history, w0_history, marker='o', color='black', label='Gradient Descent Path', markersize=3)\nax[0].scatter([w1], [w0], color='black', s=50, label='True Parameters', facecolors='white')\n\nax[1].scatter(x, y, color='steelblue', s=10, label='Observed Data')\nax[1].plot(x, w1_est.detach().item() * x + w0_est.detach().item(), color='black', label='Fitted Line', linestyle='--')\nax[1].legend()\nax[1].set_xlabel('Feature (x)')\nax[1].set_ylabel('Target (y)')\nt = ax[1].set_title('Fitted model')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.4: Trajectory of the gradient descent algorithm (black line with dots) on the negative log-likelihood surface for the linear-Gaussian model from Figure 2.3. The starting point of the algorithm is at the beginning of the line, and each dot represents an iteration of the algorithm. The dot indicates the location of the true parameters used to generate the data.\n\n\n\nWe have just completed a simple implementation of our first machine learning algorithm: gradient descent for 1d linear-Gaussian regression via maximum likelihood estimation. This algorithm appears to successfully learn the linear trend present in the data, as shown in the right panel of Figure 2.4.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Maximum Likelihood Estimation and Gradients</span>"
    ]
  },
  {
    "objectID": "chapters/04-more-gradients.html",
    "href": "chapters/04-more-gradients.html",
    "title": "3  Higher Dimensions",
    "section": "",
    "text": "Recap\nOpen the live notebook in Google Colab or download the live notebook.\nLast time, we developed an end-to-end example of maximum-likelihood estimation for the 1-dimensional linear-Gaussian model, which allowed us to fit a regression line to data displaying a linear trend. We saw that we could use the gradient of the log-likelihood function (or in this case, equivalently, the mean-squared error) to iteratively update a guess for the optimal parameters using gradient descent.\nThe model we learned to fit had two parameters, \\(w_1\\) and \\(w_0\\). However, modern models have vastly more parameters, with frontier LLMs having parameter counts nearing the trillions. In order to reason about these models, we need to build fluency in reasoning about high-dimensional spaces of parameters. We turn to this now, with a focus on multivariate linear regression.\n© Phil Chodrow, 2025",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Higher Dimensions</span>"
    ]
  },
  {
    "objectID": "chapters/04-more-gradients.html#multivariate-linear-gaussian-model",
    "href": "chapters/04-more-gradients.html#multivariate-linear-gaussian-model",
    "title": "3  Higher Dimensions",
    "section": "Multivariate Linear-Gaussian Model",
    "text": "Multivariate Linear-Gaussian Model\nRecall that the 1-dimensional linear-Gaussian model assumed that the target variable \\(y\\) was generated from a linear function of a single feature \\(x\\) plus Gaussian noise. We can write this as:\n\\[\n\\begin{aligned}\n    y_i \\sim \\mathcal{N}(w_1 x_i + w_0, \\sigma^2)\n\\end{aligned}\n\\]\nSuppose now that we have more than one predictive feature, which we would like to use in our model. Given \\(p\\) features \\(x_1,\\ldots,x_p\\), we can extend the linear-Gaussian model to incorpoate all of them:\n\\[\n\\begin{aligned}\n    y_i \\sim \\mathcal{N}(w_1 x_{i1} + w_2 x_{i2} + \\cdots + w_p x_{ip} + w_0, \\sigma^2) = \\mathcal{N}\\left(\\sum_{j=1}^p w_j x_{ij} + w_0, \\sigma^2\\right)\n\\end{aligned}\n\\tag{3.1}\\]\nHere’s a visualization of a regression model with two features, \\(x_1\\) and \\(x_2\\), alongside the plane defined by the linear function of these features.\n\n\n/Users/philchodrow/opt/anaconda3/envs/cs451/lib/python3.10/site-packages/torch/functional.py:505: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:4383.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n\n\n\n\n\n\n3D Scatter Plot of Synthetic Data with Regression Plane and Residuals\n\n\n\nIn order to write down Equation 3.1 more compactly, we need to introduce some notation. We’ll collect the features for data point \\(i\\) into a vector:\n\\[\n\\begin{aligned}\n    \\mathbf{x}_i = (x_{i0}, x_{i1},x_{i2},\\ldots,x_{ip})\n\\end{aligned}\n\\]\nwhere we assume that \\(x_{i0} = 1\\) is a constant feature included to capture the intercept term \\(w_0\\). We also collect the parameters into a vector:\n\\[\n\\begin{aligned}\n    \\mathbf{w}= (w_0, w_1, w_2, \\ldots, w_p)\n\\end{aligned}\n\\]\nWith this notation, we can rewrite the sum appearing in the mean of the Gaussian in Equation 3.1 as an inner product between the feature vector \\(\\mathbf{x}_i\\) and the parameter vector \\(\\mathbf{w}\\): An inner product is also often called a dot product. Equation 3.2 can equivalently be written with the notation \\( \\mathbf{x}_i^T \\mathbf{w} = \\mathbf{x}_i \\cdot \\mathbf{w}= \\langle \\mathbf{x}_i,\\mathbf{w} \\rangle\\).\n\\[\n\\begin{aligned}\n     \\mathbf{x}_i^T \\mathbf{w} =  \\sum_{j=0}^p w_j x_{ij} = 1\\cdot w_0 + \\sum_{j=1}^p w_j x_{ij} = 1\\cdot w_0 + \\sum_{j=1}^p w_j x_{ij}\\;.\n\\end{aligned}\n\\tag{3.2}\\]\nThe choice of \\(x_{i0} = 1\\) ensures that the intercept term \\(w_0\\) is included in the inner product. This is so convenient that we’ll assume it from now on.\n\n\n\n\n\n\nNoteAssumption\n\n\n\nFrom this point forwards, we assume that the feature vector \\(\\mathbf{x}_i\\) begins with a constant feature \\(x_{i0} = 1\\).\n\n\nThis notation allows us to compactly rewrite the multivariate linear-Gaussian model as:\n\\[\n\\begin{aligned}\n    y_i \\sim \\mathcal{N}( \\mathbf{x}_i^T \\mathbf{w}, \\sigma^2)\n\\end{aligned}\n\\]\nregardless of the number of features. We can make things even a bit simpler by defining a score \\(s_i\\) associated to each data point \\(i\\):\n\\[\n\\begin{aligned}\n    s_i =  \\mathbf{x}_i^T \\mathbf{w}\n\\end{aligned}\n\\tag{3.3}\\]\nafter which we can write: \\[\n\\begin{aligned}\n    y_i \\sim \\mathcal{N}(s_i, \\sigma^2)\n\\end{aligned}\n\\]\n\nLog-Likelihood and Mean-Squared Error\nThe log-likelihood function of the multivariate linear-Gaussian model is given by\n\\[\n\\begin{aligned}\n    \\mathcal{L}(\\mathbf{w}) &= \\sum_{i=1}^n \\log p_Y(y_i ; s_i \\sigma^2) \\\\\n    &= \\sum_{i=1}^n \\log \\left( \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(y_i - s_i)^2}{2 \\sigma^2}\\right) \\right) \\\\\n    &= -\\frac{n}{2} \\log(2 \\pi \\sigma^2) - \\frac{1}{2 \\sigma^2} \\sum_{i=1}^n (y_i - s_i)^2\\;,\n\\end{aligned}\n\\]\nwhich, like last time, means that our maximum-likelihood estimation problem is equivalent to minimizing the mean-squared error between the observed targets \\(y_i\\) and the scores \\(s_i\\):\n\\[\n\\begin{aligned}\n    R(\\mathbf{x}, \\mathbf{y}; \\mathbf{w}) = \\frac{1}{n} \\sum_{i=1}^n (y_i - s_i)^2 = \\frac{1}{n} \\sum_{i=1}^n (y_i -  \\mathbf{x}_i^T \\mathbf{w})^2\\;.\n\\end{aligned}\n\\]\nIn theory, we’re ready to start taking gradients and minimizing this function. However, it’s helpful to try to first simplify our notation even more, which we can do with the introduction of matrix and norm notation.\n\n\nMatrices and Norms\nRecall that the Euclidean norm of a vector \\(\\mathbf{v}\\in \\mathbb{R}^d\\) is defined by the formula More generally, the \\(\\ell_p\\)-norm of vector \\(\\mathbf{v}\\) is defined by the equation \\[\n\\begin{aligned}\n    \\lVert \\mathbf{v} \\rVert_p = \\sqrt[p]{\\sum_{j = 1}^d v_j^p}\\;.\n\\end{aligned}\n\\] For simplicity, whenever we write \\(\\lVert \\mathbf{v} \\rVert\\) without a subscript, we mean the \\(\\ell_2\\)-norm, i.e. \\(\\lVert \\mathbf{v} \\rVert = \\lVert \\mathbf{v} \\rVert_2\\).\n\\[\n\\begin{aligned}\n    \\lVert \\mathbf{v} \\rVert^2 = \\sum_{j=1}^d v_j^2\\;.\n\\end{aligned}\n\\]\nThe norm notation allows us to eliminate the explicit summation in favor of vector operations:\n\\[\n\\begin{aligned}\n    R(\\mathbf{x}, \\mathbf{y}; \\mathbf{w}) = \\frac{1}{n} \\lVert \\mathbf{y}- \\mathbf{s} \\rVert^2\\;,\n\\end{aligned}\n\\]\nwhere \\(\\mathbf{s}\\) is the vector of scores for all data points, whose \\(i\\)th entry is given by Equation 3.3. Our last step is to give a compact formula for \\(\\mathbf{s}\\). To do this, we collect all of the feature vectors \\(\\mathbf{x}_i\\) into a matrix \\(\\mathbf{X}\\in \\mathbb{R}^{n \\times (p+1)}\\), whose \\(i\\)th row is given by \\(\\mathbf{x}_i^T\\):\n\\[\n\\begin{aligned}\n    \\mathbf{X}= \\left[\\begin{matrix}\n    - & \\mathbf{x}_1^T & - \\\\\n    - & \\mathbf{x}_2^T & - \\\\\n    & \\vdots & \\\\\n    - & \\mathbf{x}_n^T & -\n    \\end{matrix}\\right]\\;.\n\\end{aligned}\n\\]\nNow, if we multiply the matrix \\(\\mathbf{X}\\) by the parameter vector \\(\\mathbf{w}\\), we obtain\n\\[\n\\begin{aligned}\n    \\mathbf{X}\\mathbf{w}= \\left[\\begin{matrix}\n    - & \\mathbf{x}_1^T & - \\\\\n    - & \\mathbf{x}_2^T & - \\\\\n    & \\vdots & \\\\\n    - & \\mathbf{x}_n^T & -\n    \\end{matrix}\\right] \\mathbf{w}= \\left[\\begin{matrix}\n     \\mathbf{x}_1^T \\mathbf{w} \\\\\n     \\mathbf{x}_2^T \\mathbf{w} \\\\\n    \\vdots \\\\\n     \\mathbf{x}_n^T \\mathbf{w}\n    \\end{matrix}\\right] =\n    \\left[\\begin{matrix}\n    s_1 \\\\\n    s_2 \\\\\n    \\vdots \\\\\n    s_n\n    \\end{matrix}\\right] =\n    \\mathbf{s}\\;,\n\\end{aligned}\n\\]\nThis gives us our compact formula for the MSE:\n\\[\n\\begin{aligned}\n    R(\\mathbf{x}, \\mathbf{y}; \\mathbf{w}) = \\frac{1}{n}\\lVert \\mathbf{X}\\mathbf{w}- \\mathbf{y} \\rVert^2\\;.\n\\end{aligned}\n\\tag{3.4}\\]\nThe maximum-likelihood problem for the multivariate linear-Gaussian model can therefore be written Since the \\(\\frac{1}{n}\\) is a positive constant, it does not affect the location of the minimum and can be ignored in the optimization problem.\n\\[\n\\begin{aligned}\n    \\hat{\\mathbf{w}} = \\mathop{\\mathrm{arg\\,min}}_\\mathbf{w}\\lVert \\mathbf{X}\\mathbf{w}- \\mathbf{y} \\rVert^2\\;.\n\\end{aligned}\n\\tag{3.5}\\]",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Higher Dimensions</span>"
    ]
  },
  {
    "objectID": "chapters/04-more-gradients.html#deriving-and-checking-gradient-formulas",
    "href": "chapters/04-more-gradients.html#deriving-and-checking-gradient-formulas",
    "title": "3  Higher Dimensions",
    "section": "Deriving and Checking Gradient Formulas",
    "text": "Deriving and Checking Gradient Formulas\nTo solve the optimization problem in Equation 3.5, we will need the gradient of \\(R\\) with respect to \\(\\mathbf{w}\\). To do so, we’ll highlight two approaches.\n\nEntrywise Derivation\nTo compute the gradient of \\(R\\) entrywise, we start by explicitly rewriting \\(R\\) in summation notation, avoiding matrix and vector notation for the moment:\n\\[\n\\begin{aligned}\n    R(\\mathbf{x}, \\mathbf{y}; \\mathbf{w}) = \\frac{1}{n} \\sum_{i=1}^n (y_i -  \\mathbf{x}_i^T \\mathbf{w})^2 = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\sum_{j=0}^p w_j x_{ij})^2\\;.\n\\end{aligned}\n\\]\nWe now take the derivative with respect to a particular parameter \\(w_k\\):\n\\[\n\\begin{aligned}\n    \\frac{\\partial R}{\\partial w_k} &= \\frac{\\partial}{\\partial w_k} \\left( \\frac{1}{n} \\sum_{i=1}^n (y_i - \\sum_{j=0}^p w_j x_{ij})^2 \\right) \\\\\n    &= \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial}{\\partial w_k} \\left(y_i - \\sum_{j=0}^p w_j x_{ij}\\right)^2 \\\\\n    &= \\frac{1}{n} \\sum_{i=1}^n 2 \\left(y_i - \\sum_{j=0}^p w_j x_{ij}\\right) \\cdot \\left(-x_{ik}\\right) &\\quad \\text{(chain rule)}\\\\\n    &= -\\frac{2}{n} \\sum_{i=1}^n x_{ik} \\left(y_i -  \\mathbf{x}_i^T \\mathbf{w}\\right) \\\\\n    &= \\frac{2}{n} \\sum_{i=1}^n x_{ik} \\left( \\mathbf{x}_i^T \\mathbf{w} - y_i\\right)\\;.\n\\end{aligned}\n\\tag{3.6}\\]\n\n\nVector Derivation\nIt’s more convenient and insightful to compute the gradient in vector form. This requires a few gradient identities, each of which can be derived and verified using entrywise methods like the one above. For our case, we need two identities In both these identities, the gradient is taken with respect to \\(\\mathbf{v}\\).\n\nProposition 3.1 (Gradient of Norm Squared) For any \\(\\mathbf{v}\\in \\mathbb{R}^d\\), if \\(f(\\mathbf{v}) = \\lVert \\mathbf{v} \\rVert^2\\), then \\[\n\\nabla f(\\mathbf{v}) = 2 \\mathbf{v}\\;.\n\\]\n\n\nProposition 3.2 (Compositions with Linear Maps) Let \\(f: \\mathbb{R}^m \\to \\mathbb{R}\\) be a differentiable function, and let \\(\\mathbf{A}\\in \\mathbb{R}^{m \\times n}\\) be a matrix. Define \\(g: \\mathbb{R}^n \\to \\mathbb{R}\\) by \\(g(\\mathbf{v}) = f(\\mathbf{A}\\mathbf{v})\\). Then, for any \\(\\mathbf{v}\\in \\mathbb{R}^n\\),\n\\[\n\\nabla g(\\mathbf{v}) = \\mathbf{A}^T \\nabla f(\\mathbf{A}\\mathbf{v})\\;.\n\\]\n\nIf we apply these identities to Equation 3.4, we obtain\n\\[\n\\begin{aligned}\n    \\nabla R(\\mathbf{x}, \\mathbf{y}; \\mathbf{w}) = \\nabla \\left( \\frac{1}{n} \\lVert \\mathbf{X}\\mathbf{w}- \\mathbf{y} \\rVert^2 \\right) &= \\frac{1}{n} \\nabla \\lVert \\mathbf{X}\\mathbf{w}- \\mathbf{y} \\rVert^2 \\\\\n    &= \\frac{1}{n}\\mathbf{X}^T \\nabla \\lVert \\mathbf{X}\\mathbf{w}- \\mathbf{y} \\rVert^2 &\\quad \\text{(Prop 3.2)}\\\\\n    &= \\frac{1}{n}\\mathbf{X}^T \\cdot 2(\\mathbf{X}\\mathbf{w}- \\mathbf{y}) &\\quad \\text{(Prop 3.1)}\\\\\n    &= \\frac{2}{n} \\mathbf{X}^T (\\mathbf{X}\\mathbf{w}- \\mathbf{y})\\;.\n\\end{aligned}\n\\]\nIt’s possible to check that this vector formula for the gradient matches the entrywise formula from Equation 3.6 by verifying that the \\(k\\)th entry of the vector formula equals the entrywise formula for arbitrary \\(k\\).\n\n\nChecking Vector Identities\nHow do we know that vector gradient identities like the ones we used above are correct? The most direct way is to verify them entrywise. To verify a vector identity entrywise, we just check that the \\(k\\)th entry of the left-hand side equals the \\(k\\)th entry of the right-hand side, for an arbitrary index \\(k\\). For example, let’s verify the first identity above, \\(\\nabla \\lVert \\mathbf{v} \\rVert^2 = 2\\mathbf{v}\\). The \\(k\\)th entry of the left-hand side is given by the partial derivative with respect to entry \\(w_k\\):\n\\[\n\\begin{aligned}\n    \\left(\\nabla \\lVert \\mathbf{v} \\rVert^2\\right)_k = \\frac{\\partial}{\\partial v_k} \\lVert \\mathbf{v} \\rVert^2 = \\frac{\\partial}{\\partial v_k} \\sum_{j=1}^d v_j^2 =  \\sum_{j=1}^d \\frac{\\partial}{\\partial v_k} v_j^2 = \\sum_{j=1}^d 2\\delta_{jk}  v_k = 2 v_k\\;,\n\\end{aligned}\n\\]\nwhere \\(\\delta_{jk}\\) is the Kronecker delta, which equals 1 if \\(j=k\\) and 0 otherwise. The \\(k\\)th entry of the right-hand side is simply \\(2 v_k\\). Since these are equal for arbitrary \\(k\\), the identity holds.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Higher Dimensions</span>"
    ]
  },
  {
    "objectID": "chapters/04-more-gradients.html#implementing-multivariate-regression",
    "href": "chapters/04-more-gradients.html#implementing-multivariate-regression",
    "title": "3  Higher Dimensions",
    "section": "Implementing Multivariate Regression",
    "text": "Implementing Multivariate Regression\nOur implementation for multivariate linear regression is very similar to our implementation for univariate regression from last time, and can actually be more compact in code thanks to our use of matrix operations. torch supplies the syntax X@w for matrix-vector and matrix-matrix multiplication, which we can use to compute the predictions for all data points at once. Let’s implement functions for the MSE, its gradient, and a gradient descent step which returns a new parameter vector after one step of gradient descent.\n\nimport torch\n\ndef mse(X, y, w):\n    n = y.shape[0]\n    mse = ((X @ w - y) ** 2).sum() / n\n    return mse\n\ndef grad_mse(X, y, w):\n    n = y.shape[0]\n    gradient = (2/n) * X.T @ (X @ w - y)\n    return gradient\n\nBefore proceeding further, let’s generate some random synthetic data for testing.\n\n# Generate synthetic data\ntorch.manual_seed(0)\nn_samples = 100\nn_features = 5\nX = torch.randn(n_samples, n_features)\nX = torch.cat([torch.ones(n_samples, 1), X], dim=1) # shape (n_samples, n_features + 1)\ntrue_w = torch.randn(n_features + 1)\ny = X @ true_w + 0.5 * torch.randn(n_samples)\n\nWith this data, we can check that our vector gradient formula matches the entrywise formula we derived earlier, which we can do via torch’s automatic differentiation:\n\nw0 = torch.randn(n_features + 1, requires_grad=True)\n\n1grad_manual = grad_mse(X, y, w0)\n2loss = mse(X, y, w0)\n3loss.backward()\n\nprint(torch.allclose(grad_manual, w0.grad))  # Should print True\n\n\n1\n\nCompute the gradient manually using our grad_mse function.\n\n2\n\nCompute the loss using the mse function.\n\n3\n\nUse PyTorch’s automatic differentiation to compute the gradient.\n\n\n\n\nTrue\n\n\nLooks fine! Now that we have gradient descent implemented, we can consider the gradient descent algorithm itself. This time, we’ll implement gradient descent as two functions: one which performs a single step of gradient descent, and one which handles the main loop, including storing values of the loss and checking for convergence. There are many ways to check for convergence. Here, we’re just testing whether \\(\\lVert \\mathbf{w}_\\mathrm{new} - \\mathbf{w}_\\mathrm{old} \\rVert\\) is small, which is one way to quantifying the idea that \\(\\mathbf{w}\\) “hasn’t changed much” in a single iteration.\n\ndef gradient_descent_step(X, y, w, eta):\n    grad = grad_mse(X, y, w)\n    w_new = w - eta * grad\n    return w_new\n\ndef gradient_descent(X, y, w_init, eta, max_iter = int(1e4)):\n1    loss_history = []\n    w_old = w_init\n    for iteration in range(max_iter):\n2        loss_history.append(mse(X, y, w_old).item())\n3        w_new = gradient_descent_step(X, y, w_old, eta)\n4        if ((w_new - w_old)**2).sum() &lt; 1e-8:\n            print(\"Converged after\", iteration, \"iterations\")\n            break\n        w_old = w_new \n    return w_new, loss_history\n\n\n1\n\nInitialize a list to store the loss history.\n\n2\n\nCompute and store the current loss.\n\n3\n\nPerform a single gradient descent step.\n\n4\n\nCheck for convergence by seeing if the change in parameters is small.\n\n\n\n\nNow we’re ready to run gradient descent.\n\nCode\n# Initialize parameters\nw = torch.zeros(n_features + 1)\neta = 0.01\n\n# Run gradient descent!\nw, loss_history = gradient_descent(X, y, w, eta)\n\n# Plot loss history\nimport matplotlib.pyplot as plt\nplt.plot(loss_history)\nplt.xlabel('Iteration')\nplt.ylabel('Mean Squared Error')\nplt.title('Gradient Descent Loss History')\nplt.show()\n\n\n\n\n\n\nConverged after 309 iterations\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.1: Loss vs. iteration during gradient descent for our manual implementation of multivariate linear regression.\n\n\n\nThis time, because we implemented a convergence check, our main loop terminated automatically.\nWe can compare the learned parameters to the true parameters:\n\nprint(\"Learned parameters:\\n\", w)\nprint(\"True parameters:\\n\", true_w)\n\nLearned parameters:\n tensor([ 0.8639,  0.3087,  0.2475,  1.0593, -0.9755, -1.2737])\nTrue parameters:\n tensor([ 0.8393,  0.2479,  0.2067,  0.9928, -0.8986, -1.2028])\n\n\nThe learned parameters are relatively close to the true parameters we planted in the synthetic data.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Higher Dimensions</span>"
    ]
  }
]