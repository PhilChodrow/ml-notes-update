[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Welcome\nThis is a set of notes developed for an undergraduate course in machine learning. The target audience for these notes are undergraduates in computer science who have completed first courses in linear algebra and discrete mathematics. These notes draw on many sources, but are somewhat distinctive in the following ways:\n© Phil Chodrow, 2025",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#pedagogical-features",
    "href": "index.html#pedagogical-features",
    "title": "Machine Learning",
    "section": "Pedagogical Features",
    "text": "Pedagogical Features\nThese notes are explicitly designed for undergraduate instruction in computer science. For this reason:\n\nComputational examples are integrated into the text and shown throughout.\nLive versions of lecture notes are supplied as downloadable Jupyter Notebooks, with certain code components removed. The purpose is to facilitate live-coding in lectures.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#use-and-reuse",
    "href": "index.html#use-and-reuse",
    "title": "Machine Learning",
    "section": "Use and Reuse",
    "text": "Use and Reuse\nThese notes were written by Phil Chodrow for the course CSCI 0451: Machine Learning at Middlebury College. All are welcome to use them for educational purposes. Attribution is appreciated but not expected.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#source-texts",
    "href": "index.html#source-texts",
    "title": "Machine Learning",
    "section": "Source Texts",
    "text": "Source Texts\nThese notes draw on several source texts, most of which are available for free online. These are:\n\nChristopher M. Bishop and Bishop (2023) is the primary influence for the development of technical content.\nHardt and Recht (2022) has been a very helpful guiding influence for content on decision-theory and automated decision systems.\nA Course in Machine Learning by Hal Daumé III is an accessible introduction to many of the topics and serves as a useful source of supplementary readings.\n\nAdditional useful readings:\n\nAbu-Mostafa, Magdon-Ismail, and Lin (2012): Learning From Data: A Short Course\nBarocas, Hardt, and Narayanan (2023) is an advanced text on questions of fairness in automated decision-making for readers who have some background in probability theory.\nChristopher M. Bishop (2006) and Murphy (2022) are advanced texts which are most suitable for advanced readers who have already taken at least one course in probability theory.\n\nDeisenroth, Faisal, and Ong (2020) and Kroese et al. (2020) are useful readings focusing on some of the mathematical fundamentals.\nZhang, Lipton, and Li (2023) tells a helpful story of the fundamentals of deep learning.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Machine Learning",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis site was generated using the Quarto publishing system. It is hosted on GitHub and published via GitHub Pages.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Machine Learning",
    "section": "References",
    "text": "References\n\n\n\n\nAbu-Mostafa, Yaser S., Malik Magdon-Ismail, and Hsuan-Tien Lin. 2012. Learning from Data: A Short Course. S.l. https://amlbook.com/.\n\n\nBarocas, Solon, Moritz Hardt, and Arvind Narayanan. 2023. Fairness and Machine Learning: Limitations and Opportunities. Cambridge, Massachusetts: The MIT Press. https://fairmlbook.org/pdf/fairmlbook.pdf.\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Information Science and Statistics. New York: Springer. https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf.\n\n\nBishop, Christopher M, and Hugh Bishop. 2023. Deep Learning: Foundations and Concepts. Springer Nature.\n\n\nDeisenroth, Marc Peter, A. Aldo Faisal, and Cheng Soon Ong. 2020. Mathematics for Machine Learning. Cambridge, UK New York, NY: Cambridge University Press. https://mml-book.github.io/book/mml-book.pdf.\n\n\nHardt, Moritz, and Benjamin Recht. 2022. Patterns, Predictions, and Actions. Princeton University Press. https://mlstory.org/pdf/patterns.pdf.\n\n\nKroese, Dirk P., Zdravko I. Botev, Thomas Taimre, and Radislav Vaisman. 2020. Data Science and Machine Learning: Mathematical and Statistical Methods. Chapman & Hall/CRC Machine Learning & Pattern Recognition Series. Boca Raton London New York: CRC Press, Taylor & Francis Group.\n\n\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An Introduction. Adaptive Computation and Machine Learning Series. Cambridge, Massachusetts: The MIT Press. https://probml.github.io/pml-book/book1.html.\n\n\nZhang, Aston, Zachary Lipton, and Mu Li. 2023. Dive into Deep Learning. Cambridge, UK: Cambridge University Press.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "chapters/02-signal-noise.html",
    "href": "chapters/02-signal-noise.html",
    "title": "1  Data = Signal + Noise",
    "section": "",
    "text": "Introduction: Data, Signal, and Noise\nOpen the live notebook in Google Colab.\nIn these notes, we’ll expand on the following idea:\nConsider the following simple data set:\nWe can think of this data as consisting of two components: a signal expressed by a relationship \\(y \\approx f(x)\\) (in this case \\(f(x) = 2x + 1\\)), and some noise that partially obscures this relationship. Schematically, we can write this relationship as:\n\\[\n\\begin{aligned}\n    y_i = f(x_i) + \\epsilon_i\\;,\n\\end{aligned}\n\\tag{1.1}\\]\nwhich says that the \\(i\\)th value of the target is equal to some function \\(f(x_i)\\) of the input variable \\(x_i\\), plus some random noise term \\(\\epsilon_i\\).\n© Phil Chodrow, 2025",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data = Signal + Noise</span>"
    ]
  },
  {
    "objectID": "chapters/02-signal-noise.html#introduction-data-signal-and-noise",
    "href": "chapters/02-signal-noise.html#introduction-data-signal-and-noise",
    "title": "1  Data = Signal + Noise",
    "section": "",
    "text": "Machine learning is the science and practice of building algorithms that distinguish between signal and noise in real-world data.\n\n\n\nCode\nimport torch \nfrom matplotlib import pyplot as plt\n\nscatterplot_kwargs = dict(color = \"black\", label = \"data\", facecolors = \"none\", s = 40, alpha = 0.6)\n\nn_points = 20\nx = torch.linspace(0, 10, n_points)\nsignal = 2.0 * x + 1.0  # underlying pattern (signal)\nnoise = torch.randn(n_points) * 3.0  # random noise\n\ny = signal + noise\n\n# Plot residual segments\nfig, ax = plt.subplots(figsize = (6, 4))\nax.scatter(x.numpy(), y.numpy(), **scatterplot_kwargs)\nax.plot(x.numpy(), signal.numpy(), color = \"black\", linestyle = \"--\", label = r\"Signal: $f(x_i) = 2x_i + 1$\")\nfor i in range(n_points):\n    if i == 0: \n        ax.plot([x[i].item(), x[i].item()], [signal[i].item(), y[i].item()], color = \"red\", alpha = 0.3, linewidth = 0.8, label = r\"Noise: $\\epsilon_i$\")\n    else: \n        ax.plot([x[i].item(), x[i].item()], [signal[i].item(), y[i].item()], color = \"red\", alpha = 0.3, linewidth = 0.8)\nax.set(xlabel = r\"$x$\", ylabel = r\"$y$\")\nplt.legend()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.1: An illustrative decomposition of a data set (points) into a hypothesized underlying signal (dashed line) and noise (red segments).",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data = Signal + Noise</span>"
    ]
  },
  {
    "objectID": "chapters/02-signal-noise.html#overfitting",
    "href": "chapters/02-signal-noise.html#overfitting",
    "title": "1  Data = Signal + Noise",
    "section": "Overfitting",
    "text": "Overfitting\nIt’s important to emphasize here that the thing we want to learn is not the individual targets \\(y_i\\), but rather the underlying function \\(f(x)\\). To see why, let’s consider an example of what goes wrong if we try to learn the targets \\(y_i\\) exactly. This is called interpolation, and is illustrated by Figure 1.2:\n\nCode\nfrom scipy import interpolate\n# Create an interpolating function\nf_interp = interpolate.interp1d(x.numpy(), y.numpy(), kind='cubic')\nx_dense = torch.linspace(0, 10, 100)\ny_interp = f_interp(x_dense.numpy())\n\nfig, ax = plt.subplots(figsize = (6, 4))\nax.scatter(x.numpy(), y.numpy(), **scatterplot_kwargs)\nax.plot(x_dense.numpy(), y_interp, color = \"red\", linestyle = \"--\", label = \"interpolating fit\", zorder = -10)\nax.set(xlabel = r\"$x$\", ylabel = r\"$y$\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.2: A function which exactly interpolates the data, perfectly fitting both signal and noise.\n\n\n\nThe problem with interpolation is that while we have perfectly fit our training data, we have not learned the underlying signal \\(f(x)\\). If we were to generate new data with the same signal but with different noise, we would likely find that our interpolating function doesn’t actually make very good predictions about that data at all: many of its bends and wiggles don’t have any correspondence to features in the new data.\n\nCode\n# Generate new data\ny_new = 2.0 * x_dense + 1.0 + torch.randn(100) * 3.0\nfig, ax = plt.subplots(figsize = (6, 4))\nax.scatter(x_dense.numpy(), y_new.numpy(), **scatterplot_kwargs)\nax.plot(x_dense.numpy(), y_interp, color = \"red\", linestyle = \"--\", label = \"interpolating fit\", zorder = -10)\nax.set(xlabel = r\"$x$\", ylabel = r\"$y$\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.3: New data plotted alongside the interpolating fit from the previous figure.\n\n\n\nWe observe a number of irrelevant fluctuations in the interpolating fit that do not correspond to the underlying pattern. This phenomenon is called overfitting: by trying to fit the noise in our training data, we have failed to learn the true signal. We’ll learn how to quantify overfitting once we begin to study measures of model quality.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data = Signal + Noise</span>"
    ]
  },
  {
    "objectID": "chapters/02-signal-noise.html#modeling-the-noise",
    "href": "chapters/02-signal-noise.html#modeling-the-noise",
    "title": "1  Data = Signal + Noise",
    "section": "Modeling the Noise",
    "text": "Modeling the Noise\nIf we want to learn the signal \\(f\\) in Equation 1.1, it’s helpful to learn how to talk mathematically about the noise term \\(\\epsilon_i\\). To do this, we need to step into the language of probability theory. Our standing assumption will be that the noise terms \\(\\epsilon_i\\) are random variables drawn independently and identically-distributed from some probability distribution. This means that each time we observe a new data point, we get a different value of \\(\\epsilon_i\\) drawn from the same distribution, without any influence from other values of \\(\\epsilon_j\\) for \\(j \\neq i\\).\nThe noise distribution we will usually consider is the Gaussian distribution, also called the Gaussian distribution.\n\n\n\n\n\n\n\n\n\nFigure 1.4: The probability that a Gaussian random variable lies between two values \\(a\\) and \\(b\\) is given by the area under its PDF between those values.\n\n\n\n\n\nDefinition 1.1 (Gaussian (Normal) Distribution) A random variable \\(\\epsilon\\) has a Gaussian distribution with parameters mean \\(\\mu\\) and standard deviation \\(\\sigma\\) if the probability that \\(\\epsilon\\) has a value between \\(a\\) and \\(b\\) is given by:\n\\[\n\\begin{aligned}\n    \\mathbb{P}(a \\leq \\epsilon \\leq b) = \\int_a^b p_\\epsilon(x;\\mu, \\sigma) \\, dx\\;,\n\\end{aligned}\n\\]\nwhere\n\\[\n\\begin{aligned}\np_\\epsilon(x;\\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\\;.\n\\end{aligned}\n\\]\nThe function \\(p_\\epsilon(x;\\mu, \\sigma)\\) is called the probability density function (PDF) of the Gaussian distribution. We use the shorthand notation \\(\\epsilon \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) to indicate that the random variable \\(\\epsilon\\) has a Gaussian distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\nGaussian distributions are often called normal distributions in the statistics literature.\n\nHere’s a simple vectorized implementation of the Gaussian PDF, which can be evaluated on a PyTorch tensor of inputs:\n\npi = 3.141592653589793\ndef normal_pdf(x, mu, sigma):\n    return 1 / (sigma * (2 * pi)**0.5) * torch.exp(-0.5 * ((x - mu) / sigma) ** 2)\n\nnormal_pdf(torch.tensor([0.0, 1.0, 2.0]), mu=0.0, sigma=1.0)\n\ntensor([0.3989, 0.2420, 0.0540])\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.5: Normal distribution PDFs for different values.\n\n\n\n\nThe two parameters of the Gaussian distribution describe its “shape:” The mean \\(\\mu\\) indicates the “center” of the distribution, while the standard deviation \\(\\sigma\\) indicates how “spread out” the distribution is.\n“Gaussian noise” refers to random variables drawn from a Gaussian distribution. We can generate Gaussian noise from a given Gaussian distribution using many functions. We’ll focus on PyTorch’s implementation, which allows us to generate tensors of Gaussian noise easily:\n\nmu = 0.0\nsigma = 1.0\nepsilon = torch.normal(mu, sigma, size=(10,))\nepsilon\n\ntensor([ 0.1864,  1.0350,  0.8385,  0.1005, -0.8591,  0.1861, -1.9333, -1.0409,\n        -0.1336,  1.1946])\n\n\nIf we take many samples of Gaussian noise and plot a histogram of their values, then we’ll find (via the law of large numbers) that the histogram approximates the PDF of the Gaussian distribution we sampled from, as illustrated in Figure 1.6:\n\n\n\n\n\n\n\n\n\nFigure 1.6: Histogram of samples from a Gaussian distribution compared to its PDF.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data = Signal + Noise</span>"
    ]
  },
  {
    "objectID": "chapters/02-signal-noise.html#properties-of-the-gaussian-distribution",
    "href": "chapters/02-signal-noise.html#properties-of-the-gaussian-distribution",
    "title": "1  Data = Signal + Noise",
    "section": "Properties of the Gaussian Distribution",
    "text": "Properties of the Gaussian Distribution\nThe Gaussian distribution has a number of useful properties for modeling noise in data. Here are a few:\n\nTheorem 1.1 (Translation) If \\(\\epsilon\\) is normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then the random variable \\(\\epsilon + c\\) (where \\(c\\) is a constant) is normally distributed with mean \\(\\mu + c\\) and standard deviation \\(\\sigma\\). In other words, if \\(\\epsilon \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), then \\(\\epsilon + c \\sim \\mathcal{N}(\\mu + c, \\sigma^2)\\).\n\n\nTheorem 1.2 (Scaling) If \\(\\epsilon\\) is normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then the random variable \\(a \\epsilon\\) (where \\(a\\) is a constant) is normally distributed with mean \\(a \\mu\\) and standard deviation \\(|a| \\sigma\\). In other words, if \\(\\epsilon \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), then \\(a \\epsilon \\sim \\mathcal{N}(a \\mu, (a \\sigma)^2)\\).\n\nThe expectation or mean of a continuous-valued random variable \\(\\epsilon\\) with probability density function \\(p_\\epsilon(x)\\) is defined as:\n\\[\n\\begin{aligned}\n    \\mathbb{E}[\\epsilon] = \\int_{-\\infty}^{\\infty} x \\, p_\\epsilon(x) \\, dx\\;.\n\\end{aligned}\n\\]\nThe variance of a continuous-valued random variable \\(\\epsilon\\) with probability density function \\(p_\\epsilon(x)\\) is defined as:\n\\[\n\\begin{aligned}\n    \\text{Var}(\\epsilon) = \\mathbb{E}[(\\epsilon - \\mathbb{E}[\\epsilon])^2] = \\int_{-\\infty}^{\\infty} (x - \\mathbb{E}[\\epsilon])^2 \\, p_\\epsilon(x) \\, dx\\;.\n\\end{aligned}\n\\]\nWe can interpret the variance as a measure of how far away \\(\\epsilon\\) “usually, on average” lies from its mean value.\n\nTheorem 1.3 (Mean and Variance of the Gaussian) For a normally distributed random variable with mean \\(\\mu\\) and standard deviation \\(\\sigma\\):\n\\[\n\\begin{aligned}\n    \\mathbb{E}[\\epsilon] &= \\mu \\\\\n    \\mathrm{Var}[\\epsilon] &= \\sigma^2.\n\\end{aligned}\n\\]\n\nThese two properties can be proven using some integration tricks which are beyond our scope here.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data = Signal + Noise</span>"
    ]
  },
  {
    "objectID": "chapters/02-signal-noise.html#the-standard-gaussian",
    "href": "chapters/02-signal-noise.html#the-standard-gaussian",
    "title": "1  Data = Signal + Noise",
    "section": "The Standard Gaussian",
    "text": "The Standard Gaussian\nThe standard Gaussian distribution is the Gaussian with mean zero and standard deviation one: \\(\\mathcal{N}(0, 1)\\). We often use the symbol \\(Z\\) to Due to the properties above, we can make any Gaussian random variable from a standard Gaussian: if \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), then, using Theorem 1.2 and Theorem 1.1, we can write \\(X\\) as\n\\[\n\\begin{aligned}\n    X \\sim \\sigma Z + \\mu\\;.\n\\end{aligned}\n\\]\nWe can check that this random variable has the correct mean and variance:\n\\[\n\\begin{aligned}\n    \\mathbb{E}[X] = \\mathbb{E}[\\sigma Z + \\mu] = \\sigma \\mathbb{E}[Z] + \\mu = \\sigma \\cdot 0 + \\mu = \\mu\\;,\n\\end{aligned}\n\\]\nwhere we’ve used linearity of expectation and the fact that \\(\\mathbb{E}[Z] = 0\\). To calculate the variance, we use the fact that \\(\\mathbb{E}[Z]= 0\\), again, so that\n\\[\n\\begin{aligned}\n    \\mathrm{Var}[X] = \\mathrm{Var}[\\sigma Z + \\mu] = \\mathrm{Var}[\\sigma Z] = \\sigma^2 \\mathrm{Var}[Z] = \\sigma^2 \\cdot 1 = \\sigma^2\\;.\n\\end{aligned}\n\\]\n\n\nWe’ve used the variance properties\n\\[\n\\begin{aligned}\n    \\mathrm{Var}[aX] = a^2\\mathrm{Var}[X]& \\\\\n    \\mathrm{Var}[X + b] = \\mathrm{Var}[X]&\\;\n\\end{aligned}\n\\]\nfor constants \\(a\\) and \\(b\\).",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data = Signal + Noise</span>"
    ]
  },
  {
    "objectID": "chapters/02-signal-noise.html#gaussian-noise",
    "href": "chapters/02-signal-noise.html#gaussian-noise",
    "title": "1  Data = Signal + Noise",
    "section": "Gaussian Noise",
    "text": "Gaussian Noise\nWith the Gaussian distribution in mind, let’s now return to our signal-plus-noise paradigm:\n\\[\n\\begin{aligned}\n    y_i = f(x_i) + \\epsilon_i\\;,\n\\end{aligned}\n\\]\nIf we assume that \\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\) (i.e., that the noise terms are independent and identically distributed Gaussian random variables with mean zero and standard deviation \\(\\sigma\\)), then we can use the translation property of the Gaussian distribution (Theorem 1.1) to deduce that\n\\[\n\\begin{aligned}\n    y_i &\\sim \\mathcal{N}(f(x_i), \\sigma^2)\\;.\n\\end{aligned}\n\\tag{1.2}\\]\nSo, we are modeling each data point \\(y_i\\) as a Gaussian random variable whose mean is given by the underlying signal \\(f(x_i)\\), and whose standard deviation is given by the noise level \\(\\sigma\\). This modeling approach is important enough to merit a name: Technically, the model described below is an additive Gaussian model, but we won’t worry about non-additive models here and therefore won’t bother repeating “additive.”\n\nDefinition 1.2 (Gaussian Model) The model\n\\[\n\\begin{aligned}\n    y &= f(x_i) + \\epsilon_i \\\\    \n    \\epsilon_i &\\sim \\mathcal{N}(0, \\sigma^2)\n\\end{aligned}\n\\tag{1.3}\\]\nis called a Gaussian data generating model.\n\nOften, as illustrated so far in these notes, we choose \\(f\\) to be a linear function of \\(x_i\\):\n\nDefinition 1.3 (Linear-Gaussian Model) A Gaussian model in which\n\\[\n\\begin{aligned}\n    f(x_i) = w_0 + w_1 x_i   \n\\end{aligned}\n\\tag{1.4}\\]\nfor parameters \\(w_0, w_1 \\in \\mathbb{R}\\) is called a (1-dimensional) linear-Gaussian model.\n\nHere’s a schematic picture of the linear-Gaussian model.\n\nCode\n# from https://stackoverflow.com/questions/47597119/plot-a-vertical-normal-distribution-in-python\ndef draw_gaussian_at(support, sd=1.0, height=1.0, \n        xpos=0.0, ypos=0.0, ax=None, **kwargs):\n    if ax is None:\n        ax = plt.gca()\n    gaussian = torch.exp((-support ** 2.0) / (2 * sd ** 2.0))\n    gaussian /= gaussian.max()\n    gaussian *= height\n    return ax.plot(gaussian + xpos, support + ypos, **kwargs)\n    \nsupport = torch.linspace(-10, 10, 1000)\nfig, ax = plt.subplots()\n\nax.plot(x, signal, color = \"black\", linestyle = \"--\", label = r\"Signal: $f(x_i) = 2x_i + 1$\")\n\nfor each in x:\n    draw_gaussian_at(support, sd=3, height=0.4, xpos=each, ypos=2.0 * each + 1.0, ax=ax, color='C0', alpha=0.4)\n\nax.scatter(x.numpy(), y.numpy(), **scatterplot_kwargs)\n\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$y$\")\nplt.title(\"Data points modeled as Gaussians around the signal\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.7: Illustration of the Gaussian noise model Equation 1.2, showing data points (black circles) as samples from Gaussian distributions (blue curves) centered on the signal (dashed line).\n\n\n\nWe are modeling \\(y_i\\) as a noisy sample from a Gaussian distribution centered at the signal value \\(f(x_i)\\), with noise level \\(\\sigma\\) controlling how spread out the distribution is. A larger value of \\(\\sigma\\) means that the observed data points \\(y_i\\) will tend to be further away from the signal \\(f(x_i)\\), while a smaller value of \\(\\sigma\\) means that the data points will tend to be closer to the signal. Larger values of the noise level \\(\\sigma\\) create noisier data sets where it is more difficult to discern the underlying signal.\n\nData Generating Distributions\nOne of the primary reasons to define models like Equation 1.2 is that they give us principled ways for thinking about what our models should learn – the signal – and what they should ignore – the noise. These models are also very helpful as models of where the data comes from – that is, as data generating distributions.\n\nDefinition 1.4 (Data Generating Distribution) A data generating distribution (also called a data generating model) is a probabilistic model that describes how data points (especially targets \\(y\\)) are generated in terms of random variables and their distributions.\n\nThe linear-Gaussian model is a simple example of a data generating model: it describes a recipe to simulate each target \\(y_i\\) by first computing the signal value \\(f(x_i)\\), then sampling a noise term \\(\\epsilon_i\\) from a Gaussian distribution, and finally adding the two together to get \\(y_i = f(x_i) + \\epsilon_i\\).\nA very useful feature of data generating models is that they also give us tools to measure how well our learned signal fits observed data, via the concept of a likelihood.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data = Signal + Noise</span>"
    ]
  },
  {
    "objectID": "chapters/02-signal-noise.html#model-likelihood",
    "href": "chapters/02-signal-noise.html#model-likelihood",
    "title": "1  Data = Signal + Noise",
    "section": "Model Likelihood",
    "text": "Model Likelihood\nGiven a data-generating distribution, we can compute the likelihood of the observed data under that model. Recall that the PDF of a single data point \\(y_i\\) under the Gaussian noise model Equation 1.2 with predictor value \\(x_i\\) is given by the formula\n\\[\n\\begin{aligned}\n    p_{y}(y_i; f(x_i), \\sigma^2) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp\\left(-\\frac{(y_i - f(x_i))^2}{2\\sigma^2}\\right)\\;.\n\\end{aligned}\n\\]\nThe likelihood of the complete data set is simply the product of the individual data point PDFs, evaluated at their corresponding observed values. The likelihood is a function of the predictors which we’ll collect into a vector $= (x_1,,x_n)^T, the targets which we’ll collect into a vector $ \\(\\mathbf{y}= (y_1,\\ldots,y_n)^T\\), and the parameters of the model (in this case, the function \\(f\\) and the noise level \\(\\sigma\\)):\n\nDefinition 1.5 (Gaussian Likelihood, Log-Likelihood) The likelihood of the observed data \\((\\mathbf{x}, \\mathbf{y})\\) under a 1d Gaussian model with function \\(f\\) and noise level \\(\\sigma\\) is given by:\n\\[\n\\begin{aligned}\n    L(\\mathbf{x}, \\mathbf{y}; f, \\sigma) = \\prod_{i = 1}^n p_{y}(y_i; f(x_i), \\sigma^2)\n\\end{aligned}\n\\]\nThe log-likelihood is the logarithm of the likelihood:\n\\[\n\\begin{aligned}\n    \\mathcal{L}(\\mathbf{x}, \\mathbf{y}; f, \\sigma) = \\log L(\\mathbf{x}, \\mathbf{y}; f, \\sigma) = \\sum_{i = 1}^n \\log p_{y}(y_i; f(x_i), \\sigma^2)\\;.\n\\end{aligned}\n\\tag{1.5}\\]\n\nThe log-likelihood \\(\\mathcal{L}\\) in Equation 1.5 is almost always the tool we work with in applied contexts – it turns products into sums, which is very useful in computational practice.\n\nA First Look: Likelihood Maximization\nLet’s now finally fit a machine learning model! We’ll assume that the data is sampled from the linear-Gaussian model specified by Equation 1.3 and Equation 1.4, and try to fit the parameters \\(w_0, w_1\\) of the function \\(f\\) to maximize the likelihood of the observed data. For now, we’ll do this simply by choosing the combination of parameters that achieves the best likelihood from among many candidates:\nTo see how the likelihood can give us a tool to assess model fit, we can compute the likelihood of our observed data for different choices of the function \\(f\\). This can be done in a simple grid search over possible values of the parameters \\(w_0, w_1\\) in the linear function \\(f(x) = w_0 + w_1 x\\).\n\nsig = 3.0  # assumed noise level\n1best_ll = -float('inf')\nbest_w = None\n\nfor w0 in torch.linspace(-5, 5, 20):\n    for w1 in torch.linspace(-1, 3, 20):\n2        f = lambda x: w0 + w1 * x\n        ll = 0.0\n3        ll += normal_pdf(y, f(x), sig).log().sum().item()\n\n4        if ll &gt; best_ll:\n            best_ll = ll\n            best_w = (w0.item(), w1.item())\n\n\n1\n\nInitialize the best log-likelihood and best parameters.\n\n2\n\nDefine the predictor function \\(f\\) with current parameters.\n\n3\n\nCompute the data log-likelihood. The normal_pdf function computes the PDF values for all data points at once, which we then log and sum to get the log-likelihood.\n\n4\n\nUpdate the best log-likelihood and parameters if the current log-likelihood is better.\n\n\n\n\nLet’s check the predictor function \\(f\\) we learned by heuristically maximizing the log-likelihood against the observed data:\n\nCode\nfig, ax = plt.subplots(figsize = (6, 4))\n\nax.scatter(x.numpy(), y.numpy(), **scatterplot_kwargs)\nax.plot(x.numpy(), (best_w[0] + best_w[1] * x).numpy(), color = \"firebrick\", linestyle = \"--\", label = r\"$f(x) = w_0 + w_1 x$\")\n\nax.plot(x.numpy(), signal.numpy(), color = \"black\", linestyle = \"--\", label = r\"True signal: $f(x) = 2x + 1$\")\nax.set(xlabel = r\"$x$\", ylabel = r\"$y$\")\nplt.legend()\nplt.title(fr\"Best LL: {best_ll:.2f}   $w_1 = {best_w[1]:.2f}, w_0 = {best_w[0]:.2f}$\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.8: Comparison of the true signal (black dashed line) and the model fit by maximizing the likelihood (red dashed line).\n\n\n\nThe model we selected via our maximum likelihood grid-search agrees relatively closely with the true underlying signal.\nSoon, we’ll learn how to use the likelihood as a method to learn the function \\(f\\) more systematically from data.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data = Signal + Noise</span>"
    ]
  },
  {
    "objectID": "chapters/03-maximum-likelihood.html",
    "href": "chapters/03-maximum-likelihood.html",
    "title": "2  Maximum Likelihood Estimation and Gradients",
    "section": "",
    "text": "Recap: Log-Likelihood of the Linear-Gaussian Model\nOpen the live notebook in Google Colab.\nLast time, we introduced the idea of modeling data as signal + noise, studied the Gaussian distribution as a model of noise, and introduced the linear-Gaussian model for prediction in the context of linear trends. We also derived the log-likelihood function for the linear-Gaussian model and introduced the idea that we could learn the signal of the data by maximizing the log-likelihood with respect to the model parameters. In this chapter, we’ll begin our study of how to maximize the likelihood systematically using tools from calculus.\n© Phil Chodrow, 2025",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Maximum Likelihood Estimation and Gradients</span>"
    ]
  },
  {
    "objectID": "chapters/03-maximum-likelihood.html#the-gradient-of-a-multivariate-function",
    "href": "chapters/03-maximum-likelihood.html#the-gradient-of-a-multivariate-function",
    "title": "2  Maximum Likelihood Estimation and Gradients",
    "section": "The Gradient of a Multivariate Function",
    "text": "The Gradient of a Multivariate Function\n\n\nAs you can study in courses dedicated to multivariable calculus, the existence of all of a function’s partial derivatives does not necessarily imply that the function is multivariate differentiable. In this course, we’ll exclusively treat functions which are indeed multivariate differentiable unless otherwise noted, and so this distinction will not be an issue for us.\n\nDefinition 2.1 Let \\(f:\\mathbb{R}^p\\rightarrow \\mathbb{R}\\) be a function which accepts a vector input \\(\\mathbf{w}=(w_1,\\ldots,w_p)^T\\in \\mathbb{R}^p\\) and returns a scalar output \\(f(\\mathbf{w})\\in \\mathbb{R}\\). The partial derivative of \\(f\\) with respect to the \\(j\\)-th coordinate \\(w_j\\) is defined as the limit\n\\[\n\\begin{aligned}\n    \\frac{\\partial f}{\\partial w_i} &= \\lim_{h \\rightarrow 0} \\frac{f(w_1,\\ldots,w_i + h, \\ldots w_p) - f(w_1,\\ldots,w_i, \\ldots w_p)}{h} \\\\\n    &= \\lim_{h \\rightarrow 0} \\frac{f(\\mathbf{w}+ h\\mathbf{e}_i) - f(\\mathbf{w})}{h}\\;,\n\\end{aligned}\n\\]\nwhere \\(\\mathbf{e}_i = (0,0,\\ldots,1,\\ldots,0,0)^T\\) is the \\(i\\)-th standard basis vector in \\(\\mathbb{R}^p\\), i.e., the vector with a 1 in the \\(i\\)-th position and 0’s elsewhere. If this limit does not exist, then the partial derivative is said to be undefined.\n\nJust like in single-variable calculus, it’s not usually convenient to work directly with the limit definition of the partial derivative. Instead we use the following heuristic:\n\nProposition 2.1 To compute \\(\\frac{\\partial f}{\\partial w_i}\\), treat all other variables \\(w_j\\) for \\(j\\neq i\\) as constants, and differentiate \\(f\\) with respect to \\(w_i\\) using the usual rules of single-variable calculus (power rule, product rule, chain rule, etc.).\n\n\nExercise 2.1 (Practice with Partial Derivatives) Let \\(f:\\mathbb{R}^3\\rightarrow \\mathbb{R}\\) be defined by \\(f(x,y,z) = x^2\\sin y + yz + z^3x\\). Compute \\(\\frac{\\partial f}{\\partial x}\\), \\(\\frac{\\partial f}{\\partial y}\\), and \\(\\frac{\\partial f}{\\partial z}\\).\n\n\n\n\n\n\n\nSolSolution for Exercise 2.1\n\n\n\n\n\nTo compute \\(\\frac{\\partial f}{\\partial x}\\), we treat \\(y\\) and \\(z\\) as constants, which yields\n\\[\n\\frac{\\partial f}{\\partial x} = 2x \\sin y + z^3\\;.\n\\]\nSimilarly, we can compute \\(\\frac{\\partial f}{\\partial y}\\) and \\(\\frac{\\partial f}{\\partial z}\\):\n\\[\n\\begin{align}\n    \\frac{\\partial f}{\\partial y} &= x^2 \\cos y + z \\\\\n    \\frac{\\partial f}{\\partial z} &= y + 3z^2 x\\;.    \n\\end{align}\n\\]\n\n\n\n\nExercise 2.2 (Partial Derivative of the Gaussian Log-Likelihood)  \n\n\nDefinition 2.2 Let \\(f:\\mathbb{R}^p\\rightarrow \\mathbb{R}\\) be a differentiable function which accepts a vector input \\(\\mathbf{w}=(w_1,\\ldots,w_p)^T\\in \\mathbb{R}^p\\) and returns a scalar output \\(f(\\mathbf{w})\\in \\mathbb{R}\\). The gradient of \\(f\\) at \\(\\mathbf{w}\\), written \\(\\nabla f(\\mathbf{w}) \\in \\mathbb{R}^p\\), is the vector of partial derivatives\n\\[\n\\begin{align}\n    \\nabla f(\\mathbf{w}) &= \\begin{pmatrix}\n    \\frac{\\partial f}{\\partial w_1} \\\\\n    \\frac{\\partial f}{\\partial w_2} \\\\\n    \\vdots \\\\\n    \\frac{\\partial f}{\\partial w_p}\n    \\end{pmatrix}\\;.\n\\end{align}\n\\]\n\n\nExercise 2.3 (Writing Gradients) Write the gradient of the function in Exercise 2.1.\n\n\n\n\n\n\n\nSolSolution for Exercise 2.3\n\n\n\n\n\nIn the function from Exercise 2.1, the gradient is given by stacking the partial derivatives we computed into a single vector:\n\\[\n\\begin{aligned}\n    \\nabla f(x, y, z) =\n     \\begin{pmatrix}\n    \\frac{\\partial f}{\\partial x} \\\\\n    \\frac{\\partial f}{\\partial y} \\\\\n    \\frac{\\partial f}{\\partial z}\n    \\end{pmatrix} &=\n    \\begin{pmatrix}\n    2x \\sin y + z^3 \\\\\n    x^2 \\cos y + z \\\\\n    y + 3z^2 x  \n    \\end{pmatrix}\n    \\in \\mathbb{R}^3\\;.\n\\end{aligned}\n\\]\n\n\n\n\nExercise 2.4 Consider the mean-squared error function for a simple linear model with parameters \\(w_0\\) and \\(w_1\\):\n\\[\n\\begin{aligned}\n    R(\\mathbf{x}, \\mathbf{y}; w_0, w_1) = \\frac{1}{n} \\sum_{i = 1}^n (y_i - w_1x_i - w_0)^2\\;.\n\\end{aligned}\n\\]\nCompute the gradient \\(\\nabla R(\\mathbf{x}, \\mathbf{y}; w_0, w_1)\\) with respect to the parameters \\(w_0\\) and \\(w_1\\).\nNote: we’ll soon see that this function is closely related to the log-likelihood of the linear-Gaussian model.\n\n\n\n\n\n\n\nSolSolution for Exercise 2.4\n\n\n\n\n\nWe can compute the gradient by computing each partial derivative in turn:\n\\[\n\\begin{aligned}\n    \\frac{\\partial R}{\\partial w_0} &= \\frac{2}{n} (y_i - w_1x_i - w_0) \\\\\n    \\frac{\\partial R}{\\partial w_1} &= \\frac{-2}{n} x_i(y_i - w_1x_i - w_0) \\;,\n\\end{aligned}\n\\] where we’ve used the rules for derivatives. Stacking these into a vector gives\n\\[\n\\begin{aligned}\n    \\nabla R(\\mathbf{x}, \\mathbf{y}; w_0, w_1) = \\frac{2}{n} \\begin{pmatrix}\n    \\sum_{i=1}^n (y_i - w_1x_i - w_0) \\\\\n    \\sum_{i=1}^n x_i (y_i - w_1x_i - w_0)\n    \\end{pmatrix}\n\\end{aligned}\n\\]\n\n\n\n\nChecking Gradients with torch\nThe pytorch package, which we’ll use throughout this course, implements automatic differentiation. Automatic differentiation is an extraordinarily powerful tool which we’ll study later in the course. For now, we’ll just note that it provides a handy way to check calculations of derivatives and gradients. For example, we can use torch to check the gradient we computed in Exercise 2.3 as follows:\n\nimport torch\nx = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n\n# function to differentiate\nf = lambda x: x[0]**2 * torch.sin(x[1]) + x[1]*x[2] + x[2]**3 * x[0]\n\n# compute the gradient by hand using the formula we derived\nour_grad = torch.tensor([\n    2 * x[0] * torch.sin(x[1]) + x[2]**3,\n    x[0]**2 * torch.cos(x[1]) + x[2],\n    x[1] + 3 * x[2]**2 * x[0]\n])\nprint(our_grad)\n\n# compute the gradient using automatic differentiation\n1y = f(x)\n2y.backward()\n3print(x.grad)\n\n\n1\n\nFirst, we compute the value of the function we want to differentiate and store the result to a variable (in this case called y).\n\n2\n\nNext, we call the backward() method on y, which computes the gradient of y with respect to its inputs (in this case, the vector x) using automatic differentiation.\n\n3\n\nFinally, we can access the computed gradient via the grad attribute of the input tensor x.\n\n\n\n\ntensor([28.8186,  2.5839, 29.0000])\ntensor([28.8186,  2.5839, 29.0000])\n\n\n/var/folders/xn/wvbwvw0d6dx46h9_2bkrknnw0000gn/T/ipykernel_29889/3637606239.py:9: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\nConsider using tensor.detach() first. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:837.)\n  our_grad = torch.tensor([\n\n\nThe two approaches agree! As we grow comfortable with the calculus, we’ll begin to rely more on torch’s automatic differentiation capabilities to compute gradients for us.\n\n\nThe Gradient Points In the Direction of Greatest Increase\nAn important feature of the gradient is that it tells us the direction in which a small change in the function inputs \\(\\mathbf{w}\\) could produce the greatest increase in the function output \\(f(\\mathbf{w})\\). Here’s an example using the function from Exercise 2.4. torch makes it very easy to implement this function.\n\ndef MSE(x, y, w0, w1):\n    return -((y - (w1 * x + w0))**2).mean()\n\nWe first plot the function as a function of the parameters \\(w_0\\) and \\(w_1\\) and then we overlay arrows representing the gradients at various points in the \\((w_0, w_1)\\) space, with the gradients calculated via automatic differentiation in torch.\n\nCode\nfrom matplotlib import pyplot as plt\n\n# create the grid of (mu, sigma^2) values and the data\nw0_grid = torch.linspace(-1, 1, 100)\nw1_grid = torch.linspace(0.1, 2, 100)\nW0, W1 = torch.meshgrid(w0_grid, w1_grid, indexing='ij')\n\nx = torch.tensor([0.5, -1.0, 1.0, 0.7, 0.3])  # example data points\ny = torch.tensor([1.0, 0.0, 2.0, 1.5, 0.5])\n\nLL = torch.zeros(W0.shape)\nfor i in range(W0.shape[0]):\n    for j in range(W0.shape[1]):\n        LL[i, j] = MSE(x, y, W0[i, j], W1[i, j])\n\n\n# initialize the figure \nfig, ax = plt.subplots()\n\n# show the log-likelihood as a contour plot\nim = ax.contourf(LL.numpy(), levels=100, cmap='inferno', extent = [w1_grid[0], w1_grid[-1], w0_grid[0], w0_grid[-1]])\nax.contour(LL.numpy(), levels=100, colors='black', linewidths=0.5, extent = [w1_grid[0], w1_grid[-1], w0_grid[0], w0_grid[-1]])\n\nax.set_ylabel(r'$w_1$')\nax.set_xlabel(r'$w_0$')\n\n# compute and plot the gradients at various points\nfor w0, w1 in [( -0.5, 1.5), (0.0, 1.0), (0.5, 1.5), (.25, 0.5), (0.75, 1.0)]:\n    w0_tensor = torch.tensor(w0, requires_grad=True)\n    w1_tensor = torch.tensor(w1, requires_grad=True)\n    \n    ll = MSE(x, y, w0_tensor, w1_tensor)\n    ll.backward()\n    \n    grad_w0 = w0_tensor.grad.item()\n    grad_w1 = w1_tensor.grad.item()\n    \n    ax.quiver(w1, w0, grad_w1, grad_w0, color='black', scale=20, width=0.01)\n    ax.scatter(w1, w0, color='black', s=30)\n\nplt.colorbar(im)\nax.set_title('Gradients of the mean-squared error')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.1: Visualization of the gradients of the mean-squared error function with respect to the parameters \\(w_0\\) and \\(w_1\\). The background color indicates the value of the mean-squared error, with lighter colors representing higher values. Dotted curves give contours along which the function is constant. The black arrows represent the gradients at various points in the \\((w_0, w_1)\\) space, pointing in the direction of greatest increase of the mean-squared error function.\n\n\n\nTwo observations about Figure 2.1 are worth noting:\n\nThe gradient arrows always point uphill and are orthogonal (at right angles with) to the contour lines of the function.\nThe gradient arrows get smaller as we approach the maximum of the log-likelihood function, eventually becoming zero at the maximum itself.\n\nBoth of these features are possible to prove mathematically, although we won’t do so here.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Maximum Likelihood Estimation and Gradients</span>"
    ]
  },
  {
    "objectID": "chapters/03-maximum-likelihood.html#critical-points-and-local-extrema",
    "href": "chapters/03-maximum-likelihood.html#critical-points-and-local-extrema",
    "title": "2  Maximum Likelihood Estimation and Gradients",
    "section": "Critical Points and Local Extrema",
    "text": "Critical Points and Local Extrema\nOne way we can use gradients is by analytically computing the local extrema of a function: solve the equation \\(\\nabla \\mathcal{L}(\\mathbf{w}) = 0\\) for \\(\\mathbf{w}\\) to find critical points of the log-likelihood, and then check which of these points are local maxima.\nA critical point of a multivariate function is a point at which all of its partial derivatives are equal to zero. Critical points are candidates for local maxima or minima of the function, and so they are of interest when performing maximum-likelihood estimation by solving \\(\\nabla \\mathcal{L}(\\mathbf{w}) = 0\\).\n\nDefinition 2.3 (Critical Points of Multivariate Functions) A critical point of a differentiable function \\(f:\\mathbb{R}^p\\rightarrow \\mathbb{R}\\) is a point \\(\\mathbf{w}^* \\in \\mathbb{R}^p\\) such that \\(\\nabla f(\\mathbf{w}^*) = \\mathbf{0}\\) (the zero vector in \\(\\mathbb{R}^p\\)).\n\nAll critical points of a function can be identified by solving the system of equations \\(\\nabla f(\\mathbf{w}) = \\mathbf{0}\\). In a few rare cases, it’s possible to solve this system analytically to find all critical points.\n\n\nThe notation \\(\\lVert \\mathbf{v} \\rVert_2\\) refers to the Euclidean norm of \\(\\mathbf{v}\\), with formula\n\\[\n\\begin{aligned}\n    \\lVert \\mathbf{v} \\rVert_2 = \\sqrt{\\sum_{i = 1}^p v_i^2}\\;.\n\\end{aligned}\n\\]\n\nDefinition 2.4 (Local Minima and Maxima) A local minimum of a differentiable function \\(f:\\mathbb{R}^p\\rightarrow \\mathbb{R}\\) is a point \\(\\mathbf{w}^* \\in \\mathbb{R}^p\\) such that there exists some radius \\(r&gt;0\\) such that for all \\(\\mathbf{w}\\) with \\(\\|\\mathbf{w}- \\mathbf{w}^*\\|_2 &lt; r\\), we have \\(f(\\mathbf{w}) \\geq f(\\mathbf{w}^*)\\). A local maximum is defined similarly, with the inequality reversed: for all \\(\\mathbf{w}\\) with \\(\\|\\mathbf{w}- \\mathbf{w}^*\\|_2 &lt; r\\), we have \\(f(\\mathbf{w}) \\leq f(\\mathbf{w}^*)\\).\n\n\n\nThe Mild Conditions of Theorem 2.1 are that \\(f\\) is continuously differentiable in an open neighborhood around \\(\\mathbf{w}^*\\).\n\nTheorem 2.1 (Local Extrema are Critical Points) Under Mild Conditions*, if \\(\\mathbf{w}^*\\) is a local extremum (minimum or maximum) of a differentiable function \\(f:\\mathbb{R}^p\\rightarrow \\mathbb{R}\\), then \\(\\mathbf{w}^*\\) is a critical point of \\(f\\).\n\n\n\n\n\n\n\nNoteWe Always Minimize\n\n\n\nAlthough our motivating problem is still maximum likelihood estimation, it is conventional in the literature on statistics, machine learning, and optimization to always seek minima of a given function. This works because maximizing \\(\\mathcal{L}(\\mathbf{w})\\) is equivalent to minimizing \\(-\\mathcal{L}(\\mathbf{w})\\). Therefore, in the remainder of this chapter and in subsequent chapters, we will often refer to “minimizing the negative log-likelihood” rather than “maximizing the log-likelihood.” Perhaps confusingly, we’ll still refer to the result as the “maximum likelihood estimate” (MLE).\n\n\nTheorem 2.1 tells us that we can try to find the maximum likelihood estimate of a parameter vector \\(\\mathbf{w}\\) by solving the equation \\(\\nabla \\mathcal{L}(\\mathbf{w}) = \\mathbf{0}\\). In principle, we should then check that the critical points we find are indeed minima of \\(-\\mathcal{L}(\\mathbf{w})\\) rather than maxima or saddle points, which can sometimes be done using the multivariate second-derivative test. In practice, however, this second step is often skipped. Skipping the second-derivative test can be justified if it is known that \\(-\\mathcal{L}\\) is a convex function.\nEquipped with the concept of critical points, we are ready to find maximum likelihood estimates by solving the equation \\(\\nabla \\mathcal{L}(\\mathbf{w}) = \\mathbf{0}\\).\nBy convention, the maximum-likelihood estimate of a parameter is given a “hat” symbol, so we would write the MLE estimators we found above as \\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}^2\\).",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Maximum Likelihood Estimation and Gradients</span>"
    ]
  },
  {
    "objectID": "chapters/03-maximum-likelihood.html#revisiting-the-linear-gaussian-log-likelihood",
    "href": "chapters/03-maximum-likelihood.html#revisiting-the-linear-gaussian-log-likelihood",
    "title": "2  Maximum Likelihood Estimation and Gradients",
    "section": "Revisiting the Linear-Gaussian Log-Likelihood",
    "text": "Revisiting the Linear-Gaussian Log-Likelihood\nLet’s now consider the linear-Gaussian model from last chapter. In this model, we assume that each observed target variable \\(y_i\\) is sampled from a Gaussian distribution with mean equal to a linear function of the corresponding feature vector \\(x_i\\):\n\\[\n\\begin{aligned}\n    y_i &\\sim \\mathcal{N}(w_1 x_i + w_0, \\sigma^2)\\;,\n\\end{aligned}\n\\]\nTo find the maximum-likelihood estimates given a data set of pairs \\((x_i,y_i)\\) for \\(i=1,\\ldots,n\\), we need to compute the log-likelihood function for this model, which as per last chapter is\n\\[\n\\begin{aligned}\n    \\mathcal{L}(\\mathbf{x}, \\mathbf{y}; w_1, w_0) &= \\sum_{i = 1}^n \\log p_y(y_i;w_1x_i + w_0; \\sigma^2) \\\\\n    &= \\sum_{i = 1}^n \\log \\left( \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left( -\\frac{(y_i - (w_1 x_i + w_0))^2}{2\\sigma^2} \\right) \\right) \\\\\n    &= \\sum_{i = 1}^n \\left( -\\frac{1}{2} \\log(2\\pi \\sigma^2) - \\frac{(y_i - (w_1 x_i + w_0))^2}{2\\sigma^2} \\right) \\\\\n    &= \\underbrace{-\\frac{n}{2} \\log(2\\pi \\sigma^2)}_{C} - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - (w_1 x_i + w_0))^2 \\\\\n    &= C - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - w_1 x_i - w_0)^2 \\\\\n    &= C - \\frac{n}{2\\sigma^2} \\cdot \\frac{1}{n} \\sum_{i=1}^n (y_i - w_1 x_i - w_0)^2 \\\\\n    &= C - \\frac{n}{2\\sigma^2} R(\\mathbf{x}, \\mathbf{y}; w_0, w_1)\\;.\n\\end{aligned}\n\\tag{2.1}\\]\nWe’ve collected terms that do not depend on \\(w_0\\) or \\(w_1\\) into a constant term \\(C\\), and noticed that there’s a copy of the mean-squared error function \\(R(\\mathbf{x}, \\mathbf{y}; w_0, w_1) = \\frac{1}{n}\\sum_{i=1}^n (y_i - w_1 x_i - w_0)^2\\) from Exercise 2.4 appearing in the likelihood expression. Indeed, this term is the only that involves the parameters \\(w_0\\) and \\(w_1\\). This means:\n\nTo maximize the likelihood \\(\\mathcal{L}\\) with respect to \\(w_0\\) and \\(w_1\\), we can equivalently minimize the mean-squared error \\(R(\\mathbf{x}, \\mathbf{y}; w_0, w_1)\\).\n\n\nExercise 2.5 Compute the gradient \\(\\nabla R(\\mathbf{x}, \\mathbf{y}; w_0, w_1)\\) of the sum of squared errors for the linear-Gaussian model with respect to the parameters \\(w_0\\) and \\(w_1\\).\n\n\n\n\n\n\n\nSolSolution for Exercise 2.5\n\n\n\n\n\nWe have\n\\[\n\\begin{aligned}\n    \\frac{\\partial R}{\\partial w_0} = \\frac{1}{n} \\sum_{i=1}^n 2(y_i - w_1 x_i - w_0)(-1) &= -\\frac{2}{n} \\sum_{i=1}^n (y_i - w_1 x_i - w_0) \\\\\n\\end{aligned}\n\\]\nand\n\\[\n\\begin{aligned}\n    \\frac{\\partial R}{\\partial w_1} = \\frac{1}{n} \\sum_{i=1}^n 2(y_i - w_1 x_i - w_0)(-x_i) &= -\\frac{2}{n} \\sum_{i=1}^n x_i (y_i - w_1 x_i - w_0)\\;.\n\\end{aligned}\n\\]",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Maximum Likelihood Estimation and Gradients</span>"
    ]
  },
  {
    "objectID": "chapters/03-maximum-likelihood.html#a-first-look-gradient-descent-for-maximum-likelihood-estimation",
    "href": "chapters/03-maximum-likelihood.html#a-first-look-gradient-descent-for-maximum-likelihood-estimation",
    "title": "2  Maximum Likelihood Estimation and Gradients",
    "section": "A First Look: Gradient Descent for Maximum Likelihood Estimation",
    "text": "A First Look: Gradient Descent for Maximum Likelihood Estimation\nNow that we have tools to compute gradients, we can use these gradients to find maximum-likelihood estimates numerically using a gradient method. There are many kinds of gradient methods, and they all have in common a simple idea:\n\nDefinition 2.5 (Gradient Methods) A gradient method for optimizing a multivariate function \\(f:\\mathbb{R}^p\\rightarrow \\mathbb{R}\\) is an iterative algorithm which starts from an initial guess \\(\\mathbf{w}^{(0)} \\in \\mathbb{R}^p\\) and produces a sequence of estimates \\(\\mathbf{w}^{(1)}, \\mathbf{w}^{(2)}, \\ldots\\) by repeatedly updating the current estimate \\(\\mathbf{w}^{(t)}\\) in the direction of the negative gradient \\(-\\nabla f(\\mathbf{w}^{(t)})\\), or some approximation thereof.\n\nThe simplest gradient method is gradient descent with fixed learning rate:\n\nDefinition 2.6 (Gradient Descent) Gradient descent is an algorithm that iterates the update\n\\[\n\\begin{aligned}\n    \\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\alpha\\nabla f(\\mathbf{w}^{(t)})\\;,\n\\end{aligned}\n\\]\nwhere \\(\\alpha \\in \\mathbb{R}_{&gt;0}\\) is a fixed hyperparameter called the learning rate.\n\nLet’s use gradient descent to find maximum-likelihood estimates for the parameters of the linear-Gaussian model in a simple example. To visualize gradient descent, we’ll start by implementing a function for the linear-Gaussian log-likelihood in terms of the parameters \\(w_0\\) and \\(w_1\\):\nThen we’ll generate some synthetic data from a linear-Gaussian model with known parameters:\n\n# true parameters\nw0 = torch.tensor(0.0)\nw1 = torch.tensor(2.0)\nsigma2 = torch.tensor(1.0)\n\n# observed data\nx = torch.linspace(-2, 2, 101)\ny = w1 * x + w0 + torch.sqrt(sigma2) * torch.randn_like(x)\n\n\nCode\nfig, ax = plt.subplots(figsize = (6, 4))\nax.scatter(x, y, color='steelblue', s=10)\nax.set_xlabel('Feature (x)')\nax.set_ylabel('Target (y)')\nt = ax.set_title('Observed Data from Linear-Gaussian Model')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.2: Sample data from a linear-Gaussian model with true parameters \\(w_0 = 0\\), \\(w_1 = 2\\), and \\(\\sigma^2 = 1\\).\n\n\n\nOur aim in maximum likelihood estimation is to find estimators \\(\\hat{w}_0\\) and \\(\\hat{w}_1\\) which maximize the log-likelihood of the observed data. As we saw in Equation 2.1, maximizing the log-likelihood is equivalent to minimizing the mean-squared error between the observed targets \\(y_i\\) and the linear predictions \\(w_1 x_i + w_0\\). Let’s go ahead and plot the mean-squared error:\n\nCode\nw0_grid = torch.linspace(-1, 1, 100)\nw1_grid = torch.linspace(1, 3, 100)\n\nW0, W1 = torch.meshgrid(w0_grid, w1_grid, indexing='ij')\n\nLL = torch.zeros(W0.shape)\n\nfor i in range(W0.shape[0]):\n    for j in range(W0.shape[1]):\n        LL[i, j] = -MSE(x, y, W0[i, j], W1[i, j])\n\n# visualize the log-likelihood surface\nfig, ax = plt.subplots(figsize=(6, 5))\n\nim = ax.contourf(LL.numpy(), levels=20, cmap='inferno_r', extent = [w1_grid[0], w1_grid[-1], w0_grid[0], w0_grid[-1]])\nax.contour(LL.numpy(), levels=20, colors='black', linewidths=0.5, extent = [w1_grid[0], w1_grid[-1], w0_grid[0], w0_grid[-1]])\nax.set_ylabel(r'Intercept ($w_0$)')\nax.set_xlabel(r'Slope ($w_1$)')\nplt.colorbar(im, ax=ax)\nax.set_title('Mean-squared error for linear-Gaussian model')\n\nax.scatter([w1], [w0], color='black', s=50, label='True Parameters', facecolors='white')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.3: Mean-squared error \\(R(\\mathbf{x}, \\mathbf{y}; w_0, w_1)\\) for the linear-Gaussian model as a function of the intercept \\(w_0\\) and slope \\(w_1\\) with the data shown in Figure 2.2. The dot indicates the location of the true parameters used to generate the data.\n\n\n\n\n\n\n\n\n\nQuestionQuestion\n\n\n\n\nWhy does this surface have only one minimum, and why is that minimum so close to the true parameters used to generate the data? Seems pretty convenient!\n\nIt is convenient! The uniqueness of the local minimum is due to a property called convexity that we’ll study later in the course. The closeness of the minimum to the true parameters is a consequence of the consistency property of maximum-likelihood estimators, which you can study in courses on statistical inference.\n\n\nThe gradient descent algorithm will start from an initial guess for the parameters \\((w_0, w_1)\\) and iteratively update this guess in the direction of the negative gradient of the negative log-likelihood. We’ll implement this with a simple for-loop:\n\n# algorithm parameters\nlearning_rate = 0.05\nnum_iterations = 20\nn = x.shape[0] # number of data points\n\n# initial guesses\nw0_est = torch.tensor(-0.5, requires_grad=True)\nw1_est = torch.tensor(1.5, requires_grad=True)\n\n# store the history of estimates\nw0_history = [w0_est.item()]\nw1_history = [w1_est.item()]\n\n# main loop\nfor t in range(num_iterations):\n\n1    w0_grad = -2/n * torch.sum(y - (w1_est * x + w0_est))\n2    w1_grad = -2/n * torch.sum(x * (y - (w1_est * x + w0_est)))\n\n3    w1_est = w1_est - learning_rate * w1_grad\n4    w0_est = w0_est - learning_rate * w0_grad\n    \n    # store the history of estimates\n    w0_history.append(w0_est.item())\n    w1_history.append(w1_est.item())\n\n\n1\n\nCompute the gradient of the negative log-likelihood with respect to \\(w_0\\) using the formula derived in Exercise 2.5.\n\n2\n\nCompute the gradient of the negative log-likelihood with respect to \\(w_1\\) using the formula derived in Exercise 2.5.\n\n3\n\nUpdate the estimate of \\(w_1\\) by taking a step in the direction of the negative gradient, scaled by the learning rate.\n\n4\n\nUpdate the estimate of \\(w_0\\) by taking a step in the direction of the negative gradient, scaled by the learning rate.\n\n\n\n\nFigure 2.4 shows the trajectory of the gradient descent algorithm on the negative log-likelihood surface, ultimately landing near the minimal value of the MSE and close to the true parameters used to generate the data. We also show the fitted linear model corresponding to the final estimates of \\(w_0\\) and \\(w_1\\), which visually agrees well with the data.\n\nCode\nfig, ax = plt.subplots(1, 2, figsize=(8, 3))\n\nim = ax[0].contourf(LL.numpy(), levels=20, cmap='inferno_r', extent = [w1_grid[0], w1_grid[-1], w0_grid[0], w0_grid[-1]])\nax[0].contour(LL.numpy(), levels=20, colors='black', linewidths=0.5, extent = [w1_grid[0], w1_grid[-1], w0_grid[0], w0_grid[-1]])\nax[0].set_ylabel(r'Intercept ($w_0$)')\nax[0].set_xlabel(r'Slope ($w_1$)')\nplt.colorbar(im, ax=ax[0])\nax[0].set_title('Mean-squared error for\\nlinear-Gaussian model')\nax[0].plot(w1_history, w0_history, marker='o', color='black', label='Gradient Descent Path', markersize=3)\nax[0].scatter([w1], [w0], color='black', s=50, label='True Parameters', facecolors='white')\n\nax[1].scatter(x, y, color='steelblue', s=10, label='Observed Data')\nax[1].plot(x, w1_est.detach().item() * x + w0_est.detach().item(), color='black', label='Fitted Line', linestyle='--')\nax[1].legend()\nax[1].set_xlabel('Feature (x)')\nax[1].set_ylabel('Target (y)')\nt = ax[1].set_title('Fitted model')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.4: Trajectory of the gradient descent algorithm (black line with dots) on the negative log-likelihood surface for the linear-Gaussian model from Figure 2.3. The starting point of the algorithm is at the beginning of the line, and each dot represents an iteration of the algorithm. The dot indicates the location of the true parameters used to generate the data.\n\n\n\nWe have just completed a simple implementation of our first machine learning algorithm: gradient descent for 1d linear-Gaussian regression via maximum likelihood estimation. This algorithm appears to successfully learn the linear trend present in the data, as shown in the right panel of Figure 2.4.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Maximum Likelihood Estimation and Gradients</span>"
    ]
  },
  {
    "objectID": "chapters/04-more-gradients.html",
    "href": "chapters/04-more-gradients.html",
    "title": "3  Higher Dimensions",
    "section": "",
    "text": "Recap\nOpen the live notebook in Google Colab.\nLast time, we developed an end-to-end example of maximum-likelihood estimation for the 1-dimensional linear-Gaussian model, which allowed us to fit a regression line to data displaying a linear trend. We saw that we could use the gradient of the log-likelihood function (or in this case, equivalently, the mean-squared error) to iteratively update a guess for the optimal parameters using gradient descent.\nThe model we learned to fit had two parameters, \\(w_1\\) and \\(w_0\\). However, modern models have vastly more parameters, with frontier LLMs having parameter counts nearing the trillions. In order to reason about these models, we need to build fluency in reasoning about high-dimensional spaces of parameters. We turn to this now, with a focus on multivariate linear regression.\n© Phil Chodrow, 2025",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Higher Dimensions</span>"
    ]
  },
  {
    "objectID": "chapters/04-more-gradients.html#multivariate-linear-gaussian-model",
    "href": "chapters/04-more-gradients.html#multivariate-linear-gaussian-model",
    "title": "3  Higher Dimensions",
    "section": "Multivariate Linear-Gaussian Model",
    "text": "Multivariate Linear-Gaussian Model\nRecall that the 1-dimensional linear-Gaussian model assumed that the target variable \\(y\\) was generated from a linear function of a single feature \\(x\\) plus Gaussian noise. We can write this as:\n\\[\n\\begin{aligned}\n    y_i \\sim \\mathcal{N}(w_1 x_i + w_0, \\sigma^2)\n\\end{aligned}\n\\]\nSuppose now that we have more than one predictive feature, which we would like to use in our model. Given \\(p\\) features \\(x_1,\\ldots,x_p\\), we can extend the linear-Gaussian model to incorpoate all of them:\n\\[\n\\begin{aligned}\n    y_i \\sim \\mathcal{N}(w_1 x_{i1} + w_2 x_{i2} + \\cdots + w_p x_{ip} + w_0, \\sigma^2) = \\mathcal{N}\\left(\\sum_{j=1}^p w_j x_{ij} + w_0, \\sigma^2\\right)\n\\end{aligned}\n\\tag{3.1}\\]\nHere’s a visualization of a regression model with two features, \\(x_1\\) and \\(x_2\\), alongside the plane defined by the linear function of these features.\n\n\n\n\n\n\n3D Scatter Plot of Synthetic Data with Regression Plane and Residuals\n\n\n\nIn order to write down Equation 3.1 more compactly, we need to introduce some notation. We’ll collect the features for data point \\(i\\) into a vector:\n\\[\n\\begin{aligned}\n    \\mathbf{x}_i = (x_{i0}, x_{i1},x_{i2},\\ldots,x_{ip})\n\\end{aligned}\n\\]\nwhere we assume that \\(x_{i0} = 1\\) is a constant feature included to capture the intercept term \\(w_0\\). We also collect the parameters into a vector:\n\\[\n\\begin{aligned}\n    \\mathbf{w}= (w_0, w_1, w_2, \\ldots, w_p)\n\\end{aligned}\n\\]\nWith this notation, we can rewrite the sum appearing in the mean of the Gaussian in Equation 3.1 as an inner product between the feature vector \\(\\mathbf{x}_i\\) and the parameter vector \\(\\mathbf{w}\\): An inner product is also often called a dot product. Equation 3.2 can equivalently be written with the notation \\( \\mathbf{x}_i^T \\mathbf{w} = \\mathbf{x}_i \\cdot \\mathbf{w}= \\langle \\mathbf{x}_i,\\mathbf{w} \\rangle\\).\n\\[\n\\begin{aligned}\n     \\mathbf{x}_i^T \\mathbf{w} =  \\sum_{j=0}^p w_j x_{ij} = 1\\cdot w_0 + \\sum_{j=1}^p w_j x_{ij} = 1\\cdot w_0 + \\sum_{j=1}^p w_j x_{ij}\\;.\n\\end{aligned}\n\\tag{3.2}\\]\nThe choice of \\(x_{i0} = 1\\) ensures that the intercept term \\(w_0\\) is included in the inner product. This is so convenient that we’ll assume it from now on.\n\n\n\n\n\n\nNoteAssumption\n\n\n\nFrom this point forwards, we assume that the feature vector \\(\\mathbf{x}_i\\) begins with a constant feature \\(x_{i0} = 1\\).\n\n\nThis notation allows us to compactly rewrite the multivariate linear-Gaussian model as:\n\\[\n\\begin{aligned}\n    y_i \\sim \\mathcal{N}( \\mathbf{x}_i^T \\mathbf{w}, \\sigma^2)\n\\end{aligned}\n\\]\nregardless of the number of features. We can make things even a bit simpler by defining a score \\(s_i\\) associated to each data point \\(i\\):\n\\[\n\\begin{aligned}\n    s_i =  \\mathbf{x}_i^T \\mathbf{w}\n\\end{aligned}\n\\tag{3.3}\\]\nafter which we can write: \\[\n\\begin{aligned}\n    y_i \\sim \\mathcal{N}(s_i, \\sigma^2)\n\\end{aligned}\n\\]\n\nLog-Likelihood and Mean-Squared Error\nThe log-likelihood function of the multivariate linear-Gaussian model is given by\n\\[\n\\begin{aligned}\n    \\mathcal{L}(\\mathbf{w}) &= \\sum_{i=1}^n \\log p_Y(y_i ; s_i \\sigma^2) \\\\\n    &= \\sum_{i=1}^n \\log \\left( \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(y_i - s_i)^2}{2 \\sigma^2}\\right) \\right) \\\\\n    &= -\\frac{n}{2} \\log(2 \\pi \\sigma^2) - \\frac{1}{2 \\sigma^2} \\sum_{i=1}^n (y_i - s_i)^2\\;,\n\\end{aligned}\n\\]\nwhich, like last time, means that our maximum-likelihood estimation problem is equivalent to minimizing the mean-squared error between the observed targets \\(y_i\\) and the scores \\(s_i\\):\n\\[\n\\begin{aligned}\n    R(\\mathbf{x}, \\mathbf{y}; \\mathbf{w}) = \\frac{1}{n} \\sum_{i=1}^n (y_i - s_i)^2 = \\frac{1}{n} \\sum_{i=1}^n (y_i -  \\mathbf{x}_i^T \\mathbf{w})^2\\;.\n\\end{aligned}\n\\]\nIn theory, we’re ready to start taking gradients and minimizing this function. However, it’s helpful to try to first simplify our notation even more, which we can do with the introduction of matrix and norm notation.\n\n\nMatrices and Norms\nRecall that the Euclidean norm of a vector \\(\\mathbf{v}\\in \\mathbb{R}^d\\) is defined by the formula More generally, the \\(\\ell_p\\)-norm of vector \\(\\mathbf{v}\\) is defined by the equation \\[\n\\begin{aligned}\n    \\lVert \\mathbf{v} \\rVert_p = \\sqrt[p]{\\sum_{j = 1}^d v_j^p}\\;.\n\\end{aligned}\n\\] For simplicity, whenever we write \\(\\lVert \\mathbf{v} \\rVert\\) without a subscript, we mean the \\(\\ell_2\\)-norm, i.e. \\(\\lVert \\mathbf{v} \\rVert = \\lVert \\mathbf{v} \\rVert_2\\).\n\\[\n\\begin{aligned}\n    \\lVert \\mathbf{v} \\rVert^2 = \\sum_{j=1}^d v_j^2\\;.\n\\end{aligned}\n\\]\nThe norm notation allows us to eliminate the explicit summation in favor of vector operations:\n\\[\n\\begin{aligned}\n    R(\\mathbf{x}, \\mathbf{y}; \\mathbf{w}) = \\frac{1}{n} \\lVert \\mathbf{y}- \\mathbf{s} \\rVert^2\\;,\n\\end{aligned}\n\\]\nwhere \\(\\mathbf{s}\\) is the vector of scores for all data points, whose \\(i\\)th entry is given by Equation 3.3. Our last step is to give a compact formula for \\(\\mathbf{s}\\). To do this, we collect all of the feature vectors \\(\\mathbf{x}_i\\) into a matrix \\(\\mathbf{X}\\in \\mathbb{R}^{n \\times (p+1)}\\), whose \\(i\\)th row is given by \\(\\mathbf{x}_i^T\\):\n\\[\n\\begin{aligned}\n    \\mathbf{X}= \\left[\\begin{matrix}\n    - & \\mathbf{x}_1^T & - \\\\\n    - & \\mathbf{x}_2^T & - \\\\\n    & \\vdots & \\\\\n    - & \\mathbf{x}_n^T & -\n    \\end{matrix}\\right]\\;.\n\\end{aligned}\n\\]\nNow, if we multiply the matrix \\(\\mathbf{X}\\) by the parameter vector \\(\\mathbf{w}\\), we obtain\n\\[\n\\begin{aligned}\n    \\mathbf{X}\\mathbf{w}= \\left[\\begin{matrix}\n    - & \\mathbf{x}_1^T & - \\\\\n    - & \\mathbf{x}_2^T & - \\\\\n    & \\vdots & \\\\\n    - & \\mathbf{x}_n^T & -\n    \\end{matrix}\\right] \\mathbf{w}= \\left[\\begin{matrix}\n     \\mathbf{x}_1^T \\mathbf{w} \\\\\n     \\mathbf{x}_2^T \\mathbf{w} \\\\\n    \\vdots \\\\\n     \\mathbf{x}_n^T \\mathbf{w}\n    \\end{matrix}\\right] =\n    \\left[\\begin{matrix}\n    s_1 \\\\\n    s_2 \\\\\n    \\vdots \\\\\n    s_n\n    \\end{matrix}\\right] =\n    \\mathbf{s}\\;,\n\\end{aligned}\n\\]\nThis gives us our compact formula for the MSE:\n\\[\n\\begin{aligned}\n    R(\\mathbf{x}, \\mathbf{y}; \\mathbf{w}) = \\frac{1}{n}\\lVert \\mathbf{X}\\mathbf{w}- \\mathbf{y} \\rVert^2\\;.\n\\end{aligned}\n\\tag{3.4}\\]\nThe maximum-likelihood problem for the multivariate linear-Gaussian model can therefore be written Since the \\(\\frac{1}{n}\\) is a positive constant, it does not affect the location of the minimum and can be ignored in the optimization problem.\n\\[\n\\begin{aligned}\n    \\hat{\\mathbf{w}} = \\mathop{\\mathrm{arg\\,min}}_\\mathbf{w}\\lVert \\mathbf{X}\\mathbf{w}- \\mathbf{y} \\rVert^2\\;.\n\\end{aligned}\n\\tag{3.5}\\]",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Higher Dimensions</span>"
    ]
  },
  {
    "objectID": "chapters/04-more-gradients.html#deriving-and-checking-gradient-formulas",
    "href": "chapters/04-more-gradients.html#deriving-and-checking-gradient-formulas",
    "title": "3  Higher Dimensions",
    "section": "Deriving and Checking Gradient Formulas",
    "text": "Deriving and Checking Gradient Formulas\nTo solve the optimization problem in Equation 3.5, we will need the gradient of \\(R\\) with respect to \\(\\mathbf{w}\\). To do so, we’ll highlight two approaches.\n\nEntrywise Derivation\nTo compute the gradient of \\(R\\) entrywise, we start by explicitly rewriting \\(R\\) in summation notation, avoiding matrix and vector notation for the moment:\n\\[\n\\begin{aligned}\n    R(\\mathbf{x}, \\mathbf{y}; \\mathbf{w}) = \\frac{1}{n} \\sum_{i=1}^n (y_i -  \\mathbf{x}_i^T \\mathbf{w})^2 = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\sum_{j=0}^p w_j x_{ij})^2\\;.\n\\end{aligned}\n\\]\nWe now take the derivative with respect to a particular parameter \\(w_k\\):\n\\[\n\\begin{aligned}\n    \\frac{\\partial R}{\\partial w_k} &= \\frac{\\partial}{\\partial w_k} \\left( \\frac{1}{n} \\sum_{i=1}^n (y_i - \\sum_{j=0}^p w_j x_{ij})^2 \\right) \\\\\n    &= \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial}{\\partial w_k} \\left(y_i - \\sum_{j=0}^p w_j x_{ij}\\right)^2 \\\\\n    &= \\frac{1}{n} \\sum_{i=1}^n 2 \\left(y_i - \\sum_{j=0}^p w_j x_{ij}\\right) \\cdot \\left(-x_{ik}\\right) &\\quad \\text{(chain rule)}\\\\\n    &= -\\frac{2}{n} \\sum_{i=1}^n x_{ik} \\left(y_i -  \\mathbf{x}_i^T \\mathbf{w}\\right) \\\\\n    &= \\frac{2}{n} \\sum_{i=1}^n x_{ik} \\left( \\mathbf{x}_i^T \\mathbf{w} - y_i\\right)\\;.\n\\end{aligned}\n\\tag{3.6}\\]\n\n\nVector Derivation\nIt’s more convenient and insightful to compute the gradient in vector form. This requires a few gradient identities, each of which can be derived and verified using entrywise methods like the one above. For our case, we need two identities In both these identities, the gradient is taken with respect to \\(\\mathbf{v}\\).\n\nProposition 3.1 (Gradient of Norm Squared) For any \\(\\mathbf{v}_0 \\in \\mathbb{R}^d\\), if \\(f(\\mathbf{v}) = \\lVert \\mathbf{v} \\rVert^2\\), then \\[\n\\nabla f(\\mathbf{v}_0) = 2 \\mathbf{v}_0\\;.\n\\]\n\n\nProposition 3.2 (Compositions with Linear Maps) Let \\(f: \\mathbb{R}^m \\to \\mathbb{R}\\) be a differentiable function, and let \\(\\mathbf{A}\\in \\mathbb{R}^{m \\times n}\\) be a matrix. Define \\(g: \\mathbb{R}^n \\to \\mathbb{R}\\) by \\(g(\\mathbf{v}) = f(\\mathbf{A}\\mathbf{v})\\). Then, for any \\(\\mathbf{v}_0 \\in \\mathbb{R}^n\\),\n\\[\n\\nabla g(\\mathbf{v}_0) = \\mathbf{A}^T \\nabla f(\\mathbf{A}\\mathbf{v}_0)\\;.\n\\]\n\nIf we apply these identities to Equation 3.4, we obtain\n\\[\n\\begin{aligned}\n    \\nabla R(\\mathbf{x}, \\mathbf{y}; \\mathbf{w}) = \\nabla \\left( \\frac{1}{n} \\lVert \\mathbf{X}\\mathbf{w}- \\mathbf{y} \\rVert^2 \\right) &= \\frac{1}{n} \\nabla \\lVert \\mathbf{X}\\mathbf{w}- \\mathbf{y} \\rVert^2 \\\\\n    &= \\frac{1}{n}\\mathbf{X}^T \\nabla \\lVert \\mathbf{X}\\mathbf{w}- \\mathbf{y} \\rVert^2 &\\quad \\text{(Prop 3.2)}\\\\\n    &= \\frac{1}{n}\\mathbf{X}^T \\cdot 2(\\mathbf{X}\\mathbf{w}- \\mathbf{y}) &\\quad \\text{(Prop 3.1)}\\\\\n    &= \\frac{2}{n} \\mathbf{X}^T (\\mathbf{X}\\mathbf{w}- \\mathbf{y})\\;.\n\\end{aligned}\n\\]\nIt’s possible to check that this vector formula for the gradient matches the entrywise formula from Equation 3.6 by verifying that the \\(k\\)th entry of the vector formula equals the entrywise formula for arbitrary \\(k\\).\n\n\nChecking Vector Identities\nHow do we know that vector gradient identities like the ones we used above are correct? The most direct way is to verify them entrywise. To verify a vector identity entrywise, we just check that the \\(k\\)th entry of the left-hand side equals the \\(k\\)th entry of the right-hand side, for an arbitrary index \\(k\\). For example, let’s verify the first identity above, \\(\\nabla \\lVert \\mathbf{v} \\rVert^2 = 2\\mathbf{v}\\). The \\(k\\)th entry of the left-hand side is given by the partial derivative with respect to entry \\(w_k\\):\n\\[\n\\begin{aligned}\n    \\left(\\nabla \\lVert \\mathbf{v} \\rVert^2\\right)_k = \\frac{\\partial}{\\partial v_k} \\lVert \\mathbf{v} \\rVert^2 = \\frac{\\partial}{\\partial v_k} \\sum_{j=1}^d v_j^2 =  \\sum_{j=1}^d \\frac{\\partial}{\\partial v_k} v_j^2 = \\sum_{j=1}^d 2\\delta_{jk}  v_k = 2 v_k\\;,\n\\end{aligned}\n\\]\nwhere \\(\\delta_{jk}\\) is the Kronecker delta, which equals 1 if \\(j=k\\) and 0 otherwise. The \\(k\\)th entry of the right-hand side is simply \\(2 v_k\\). Since these are equal for arbitrary \\(k\\), the identity holds.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Higher Dimensions</span>"
    ]
  },
  {
    "objectID": "chapters/04-more-gradients.html#implementing-multivariate-regression",
    "href": "chapters/04-more-gradients.html#implementing-multivariate-regression",
    "title": "3  Higher Dimensions",
    "section": "Implementing Multivariate Regression",
    "text": "Implementing Multivariate Regression\nOur implementation for multivariate linear regression is very similar to our implementation for univariate regression from last time, and can actually be more compact in code thanks to our use of matrix operations. torch supplies the syntax X@w for matrix-vector and matrix-matrix multiplication, which we can use to compute the predictions for all data points at once.\n\nObject-Oriented API for Machine Learning Models\nStarting here, we are going to implement most of our machine learning models using an object-oriented approach which will generalize nicely once we move on to complex deep learning models.\n\nModel Class\nUnder torch’s standard structure, a model is an instance of a class which holds parameters and implements a forward method which computes predictions given input data. Here’s a minimal implementation for linear regression:\n\nclass LinearRegression:\n    def __init__(self, n_params):\n        self.w = torch.zeros(n_params, 1, requires_grad=True)\n\n    def forward(self, X):\n        return X @ self.w\n\n\n\nLoss Function\nAlongside the model class, we need a loss function. While there are many possibilities, we’ll use our familiar mean-squared error. The loss function should accept predicted and true target values, and return a scalar loss value.\n\ndef mse(y_pred, y):\n    return torch.mean((y_pred - y)**2)\n\n\n\nOptimizer\nThe last thing we need is an algorithm for optimizing the parameters of the model. Gradient descent is one such algorithm. Here, we’ll implement a simple version of gradient descent as a separate class:\n\nclass GradientDescentOptimizer:\n    def __init__(self, model, lr=1e-2):\n        self.model = model\n        self.lr = lr\n\n    # once we begin to rely on automatic differentiation\n    # we won't need to implement this ourselves\n    def grad_func(self, X, y): \n        y_pred = self.model.forward(X)\n        n = y.shape[0]\n        gradient = (2/n) * X.T @ (y_pred - y)\n        return gradient\n\n    def step(self, X, y):\n        y_pred = self.model.forward(X)\n\n        with torch.no_grad():\n            self.model.w -= self.lr * self.grad_func(X, y)\n\nNow we’re ready to run multivariate linear regression. Let’s generate some random synthetic data for testing.\n\n# Generate synthetic data\ntorch.manual_seed(0)\nn_samples = 100\nn_features = 5\nX = torch.randn(n_samples, n_features)\nX = torch.cat([torch.ones(n_samples, 1), X], dim=1) # shape (n_samples, n_features + 1)\ntrue_w = torch.randn(n_features + 1, 1)\n\nsignal = X @ true_w\nnoise = 0.5 * torch.randn(n_samples, 1)\ny = signal + noise\n\nWith this data, we can check that our vector gradient formula matches the entrywise formula we derived earlier, which we can do via torch’s automatic differentiation:\n\nmodel = LinearRegression(n_params=n_features + 1)\nopt = GradientDescentOptimizer(model=model, lr=1e-2)\n\n1grad_manual = opt.grad_func(X, y)\n\ny_pred = model.forward(X)\n2loss = mse(y_pred, y)\n3loss.backward()\n\nprint(torch.allclose(grad_manual, model.w.grad))  # Should print True\n\n\n1\n\nCompute the gradient manually using the optimizer’s function.\n\n2\n\nCompute the loss using the mse function.\n\n3\n\nUse PyTorch’s automatic differentiation to compute the gradient.\n\n\n\n\nTrue\n\n\nLooks fine! Now that we have gradient descent implemented, we can consider the gradient descent algorithm itself. This time, we’ll implement gradient descent as two functions: one which performs a single step of gradient descent, and one which handles the main loop, including storing values of the loss and checking for convergence. There are many ways to check for convergence. Here, we’re just testing whether \\(\\lVert \\mathbf{w}_\\mathrm{new} - \\mathbf{w}_\\mathrm{old} \\rVert\\) is small, which is one way to quantifying the idea that \\(\\mathbf{w}\\) “hasn’t changed much” in a single iteration.\n\n# training loop\n\nloss_history = []\n\nfor _ in range(10000):\n    y_pred = model.forward(X)\n    loss = mse(y_pred, y)\n    loss_history.append(loss.item())\n\n    opt.step(X, y)  # perform a single gradient descent step\n\n    # convergence check\n    if len(loss_history) &gt; 1:\n        if abs(loss_history[-1] - loss_history[-2]) &lt; 1e-6:\n            print(\"Converged\")\n            break\n\nConverged\n\n\n\nInitialize a list to store the loss history.\nCompute and store the current loss.\nPerform a single gradient descent step.\nCheck for convergence by seeing if the change in parameters is small.\n\nNow we’re ready to run gradient descent.\n\nCode\n# Plot loss history\nimport matplotlib.pyplot as plt\nplt.plot(loss_history)\nplt.xlabel('Iteration')\nplt.ylabel('Mean Squared Error')\nplt.title('Gradient Descent Loss History')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.1: Loss vs. iteration during gradient descent for our manual implementation of multivariate linear regression.\n\n\n\nThis time, because we implemented a convergence check, our main loop terminated automatically.\nWe can compare the learned parameters to the true parameters:\n\nprint(\"Learned parameters:\\n\", model.w.flatten())\nprint(\"True parameters:\\n\", true_w.flatten())\n\nLearned parameters:\n tensor([ 0.8640,  0.3088,  0.2475,  1.0593, -0.9756, -1.2738],\n       grad_fn=&lt;ViewBackward0&gt;)\nTrue parameters:\n tensor([ 0.8393,  0.2479,  0.2067,  0.9928, -0.8986, -1.2028])\n\n\nThe learned parameters are relatively close to the true parameters we planted in the synthetic data.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Higher Dimensions</span>"
    ]
  },
  {
    "objectID": "chapters/06-regularization.html",
    "href": "chapters/06-regularization.html",
    "title": "4  Features and Regularization",
    "section": "",
    "text": "Basis Function Expansion\nOpen the live notebook in Google Colab.\nSuppose that we’d like to model a distinctly nonlinear relationship in our data:\nOne flexible candidate model for this kind of data is a nonlinear Gaussian model:\nHere’s a visualization of this model with \\(f(x) = \\sin(2\\pi x)\\) and \\(\\sigma^2 = 0.01\\), which was the setting used to generate the synthetic data above:\nThis is all well and good as a theoretical framework, but how in the world were we supposed to know that \\(f(x) = \\sin(2\\pi x)\\) was the right choice? In practice, of course, we never will.\nSince we don’t know the right functional form for \\(f\\), one common approach is try to approximate\nBefore we look at some examples of basis functions, let’s take a look at the fundamental trick behind basis function expansions: when using a basis function expansion with a nonlinear Gaussian model, the model is linear in the parameters \\(w_j\\), and we can therefore use our previously-developed machinery for linear regression to fit the model. To see this, note that we can write\n\\[\n\\begin{aligned}\n    \\sum_{j=0}^p w_j \\phi_j(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x})\\;,\n\\end{aligned}\n\\]\nwhere we’ve defined the vector of parameters \\(\\mathbf{w}= (w_0, w_1, \\ldots, w_p)^T\\) and the vector of basis functions \\(\\mathbf{\\phi}(\\mathbf{x}) = (\\phi_0(\\mathbf{x}), \\phi_1(\\mathbf{x}), \\ldots, \\phi_p(\\mathbf{x}))^T\\). Let’s give the shorthand \\(\\hat{y}_i = \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x}_i)\\) for the prediction of the model at input \\(\\mathbf{x}_i\\). Then, taken together the nonlinear Gaussian model with basis function expansion says that\n\\[\n\\begin{aligned}\n    y_i \\sim \\mathcal{N}(\\hat{y}_i; \\sigma^2) = \\mathcal{N}(\\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x}_i); \\sigma^2)\\;.\n\\end{aligned}\n\\]\nThis is just like the linear-Gaussian model, but with the input data \\(\\mathbf{x}_i\\) replaced by the transformed data \\(\\mathbf{\\phi}(\\mathbf{x}_i)\\). Therefore, we can use our previous results for maximum likelihood estimation of the parameters \\(\\mathbf{w}\\) in the linear-Gaussian model, simply by replacing each occurrence of \\(\\mathbf{x}_i\\) with \\(\\mathbf{\\phi}(\\mathbf{x}_i)\\). In particular, we can maximize the log-likelihood of the data by minimizing the mean squared error between the predictions \\(\\hat{y}_i\\) and the observed targets \\(y_i\\):\n\\[\n\\begin{aligned}\n    R(\\mathbf{X}, \\mathbf{y}; \\mathbf{w}) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x}_i))^2\\;.\n\\end{aligned}\n\\]\nIf we define\n\\[\n\\begin{aligned}\n    \\mathbf{\\Phi}= \\left[\\begin{matrix}\n        - &\\mathbf{\\phi}(\\mathbf{x}_1)^T & -\\\\\n        - &\\mathbf{\\phi}(\\mathbf{x}_2)^T & -\\\\\n        \\vdots \\\\\n        - &\\mathbf{\\phi}(\\mathbf{x}_n)^T & -    \n    \\end{matrix}\\right]\\;,\n\\end{aligned}\n\\]\nwe can similarly write the mean squared error in matrix form as\n\\[\n\\begin{aligned}\n    R(\\mathbf{\\Phi}, \\mathbf{y}; \\mathbf{w}) = \\frac{1}{n} \\lVert \\mathbf{\\Phi}\\mathbf{w}- \\mathbf{y} \\rVert^2\\;.\n\\end{aligned}\n\\]\nSo, to learn a model with basis function expansion, all we need to do is construct the feature matrix \\(\\mathbf{\\Phi}\\) by applying the basis functions to each data point, and then run linear regression as before.\nLet’s try basis function expansion on our synthetic data. For this, we’ll first bring in the linear regression model that we developed previously:\nclass LinearRegression:\n    def __init__(self, n_params):\n        self.w = torch.zeros(n_params, 1, requires_grad=True)\n\n    def forward(self, X):\n        return X @ self.w\nWe’ll also write a simple training loop, this time using some of PyTorch’s built-in optimization functionality:\ndef mse(y_pred, y):\n    return torch.mean((y_pred - y) ** 2)\n\ndef train_model(model, X_train, y_train, lr=1e-2, n_epochs=1000, tol=1e-4, regularization = None, verbose = False):\n    opt = torch.optim.Adam(params=[model.w], lr=lr)\n    for epoch in range(n_epochs):\n        y_pred = model.forward(X_train)\n1        loss = mse(y_pred, y_train) + (regularization(model.w) if regularization is not None else 0.0)\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n        if model.w.grad is not None:\n            if model.w.grad.norm().item() &lt; tol:\n                if verbose: \n                    print(f\"Converged at epoch {epoch}, Loss: {loss.item()}\")\n                break\n        if verbose and epoch % 100 == 0:\n            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n\n\n1\n\nThis function adds a regularization term (introduced below) to the loss if provided.\nIt’s helpful to think about the minimal linear regression model, in which we simply add a constant column to the data, as an example of a basis function expansion. We’ll call this the linear basis function.\ndef linear_basis_function(X):\n    \"\"\"\n    just adds the constant feature\n    \"\"\"\n    n_samples = X.shape[0]\n    Phi = torch.ones(n_samples, 2)  # intercept + linear term\n    Phi[:, 1] = X.flatten()\n    return Phi\nIf we try fitting this model to the sinusoidal data directly, we’ll be disappointed.\nPHI = linear_basis_function(X)\nLR = LinearRegression(n_params=2)  # intercept + slope\ntrain_model(LR, PHI, y, lr=1e-2, n_epochs=1000)\nCode\n# utility function for model visualization\ndef viz_model_predictions(model, X, y, basis_fun, ax, **basis_fun_kwargs): \n\n    x_new = torch.linspace(X.min(), X.max(), 1000).reshape(-1, 1)\n    PHI_new = basis_fun(x_new, **basis_fun_kwargs)\n    y_pred = model.forward(PHI_new)\n    ax.scatter(X.numpy(), y.numpy(), **scatterplot_kwargs)\n    ax.plot(x_new.numpy(), y_pred.detach().numpy(), color='red', label='Prediction')\n    ax.set_xlabel(r\"$x$\")\n    ax.set_ylabel(r\"$y$\")\nHow did we do?\nNow let’s try some nontrivial basis function expansions. If we had reason to believe that our data was periodic, we might try using sine and cosine basis functions. For example, let’s try:\n\\[\n\\begin{aligned}\n    \\phi_0(x) &= 1\\;,\\\\\n    \\phi_1(x) &= \\sin(\\pi x)\\;,\\\\\n    \\phi_2(x) &= \\sin(2 \\pi x)\\;,\\\\\n    \\phi_3(x) &= \\sin(3 \\pi x)\\;,\\\\\n    \\vdots\n\\end{aligned}\n\\]\nThe following function constructs the feature matrix \\(\\mathbf{\\Phi}\\) for this basis function expansion:\ndef sinusoidal_features(X, max_freq=4):\n    n_samples = X.shape[0]\n    Phi = torch.ones(n_samples, max_freq + 1)  \n    for i in range(1, max_freq + 1):\n        Phi[:, i] = torch.sin(i * torch.pi * X).flatten()\n    return Phi\nTo train models and make predictions, all we need to do is call this function to get the feature matrix, and then run linear regression as before.\n# engineer features\nmax_freq = 15\nPHI = sinusoidal_features(X, max_freq=max_freq)\n\n# train the model as usual\nLR = LinearRegression(n_params=PHI.shape[1])\ntrain_model(LR, PHI, y, lr=1e-2, n_epochs=2000)\nLet’s try this for a variety of maximum frequencies.\nAs the number of basis functions increases, the model becomes more flexible and is able to better fit the training data. However, with too many basis functions, the model begins to overfit the data, capturing noise rather than the underlying signal as reflected in the seemingly random high-frequency oscillations in the predictions.\n© Phil Chodrow, 2025",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Features and Regularization</span>"
    ]
  },
  {
    "objectID": "chapters/06-regularization.html#basis-function-expansion",
    "href": "chapters/06-regularization.html#basis-function-expansion",
    "title": "4  Features and Regularization",
    "section": "",
    "text": "\\[\n\\begin{aligned}\n    f(\\mathbf{x}) \\approx w_0\\phi_0(\\mathbf{x}) + w_1 \\phi_1(\\mathbf{x}) + w_2 \\phi_2(\\mathbf{x}) + \\cdots + w_p \\phi_p(\\mathbf{x}) = \\sum_{j=0}^p w_j \\phi_j(\\mathbf{x})\\;,\n\\end{aligned}\n\\] where \\(\\phi_j(\\cdot)\\) are a collection of basis functions that we choose ahead of time. This is called a basis function expansion. Each \\(\\phi_j\\) is often also called a feature map.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\nviz_model_predictions(LR, X, y, basis_fun = linear_basis_function, ax=ax)\nplt.title(\"Linear Regression Fit to Nonlinear Data\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nFigure 4.3: Example of linear regression fit to the sinusoidal data set using the linear (trivial) basis function expansion.\n\n\n\n\n\n\n\n\n\n\nCode\n# visualize\nfig, ax = plt.subplots()\n\nviz_model_predictions(LR, X, y, basis_fun = sinusoidal_features, ax=ax, max_freq=max_freq)\n\nplt.title(f\"Sinusoidal Basis Functions with max_freq={max_freq}\")\nplt.legend()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.4: Example fit to data using the sinusoidal basis function expansion with maximum frequency 15.\n\n\n\n\n\nCode\nfig, axarr = plt.subplots(2, 3, figsize=(8,5))\n\nmax_freqs = [0, 1, 2, 5, 10, 20]\n\nfor i, ax in enumerate(axarr.flatten()):\n    \n    PHI = sinusoidal_features(X, max_freq=max_freqs[i])\n    LR = LinearRegression(n_params=PHI.shape[1])\n    train_model(LR, PHI, y, lr=1e-2, n_epochs=2000)\n   \n    viz_model_predictions(LR, X, y, basis_fun = sinusoidal_features, ax=ax, max_freq=max_freqs[i])\n\n    mse_val = mse(LR.forward(PHI), y).item()\n    ax.set_title(f\"max_freq={max_freqs[i], } | MSE={mse_val:.3f}\")\n\n    if i == 0:\n        ax.legend()\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.5: Example fits to training data using the sinusoidal basis function expansion with varying maximum frequencies.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Features and Regularization</span>"
    ]
  },
  {
    "objectID": "chapters/06-regularization.html#kernel-basis-functions",
    "href": "chapters/06-regularization.html#kernel-basis-functions",
    "title": "4  Features and Regularization",
    "section": "Kernel Basis Functions",
    "text": "Kernel Basis Functions\nAnother common choice of basis functions are kernel basis functions. A kernel is simply a measure of similarity between two inputs. A common choice is the Gaussian radial-basis kernel, which is defined in one dimension by\n\\[\nk(x, c) = \\exp\\left(-(x - c)^2\\right)\\;,\n\\]\nThe Guassian kernel measures similarity between \\(x\\) and a center point \\(c\\), with values close to 1 when \\(x\\) is near \\(c\\) and values close to 0 when \\(x\\) is far from \\(c\\).\nHere’s an implementation in one dimension, where we evenly space choices of \\(c\\) out across the range of the data:\n\ndef kernel_features(X, num_kernels):\n    n_samples = X.shape[0]\n    Phi = torch.ones(n_samples, num_kernels + 1)  \n    centers = torch.linspace(X.min(), X.max(), num_kernels)\n    bandwidth = (X.max() - X.min()) / num_kernels\n    for j in range(num_kernels):\n        Phi[:, j + 1] = torch.exp(-0.5 * ((X.flatten() - centers[j]) / bandwidth) ** 2)\n    return Phi\n\nWe can now run the same experiment as before:\n\nCode\nfig, axarr = plt.subplots(2, 3, figsize=(8,5))\n\nnum_kernels = [0, 3, 5, 10, 20, 30]\n\nfor i, ax in enumerate(axarr.flatten()):\n    \n    PHI = kernel_features(X, num_kernels=num_kernels[i])\n    LR = LinearRegression(n_params=PHI.shape[1])\n    train_model(LR, PHI, y, lr=1e-2, n_epochs=200000, tol = 1e-4)\n\n    viz_model_predictions(LR, X, y, basis_fun = kernel_features, ax=ax, num_kernels=num_kernels[i])\n    \n    mse_val = mse(LR.forward(PHI), y).item()\n    ax.set_title(f\"num_kernels={num_kernels[i]} | MSE={mse_val:.3f}\")\n\n    if i == 0:\n        ax.legend()\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.6: Visualization of fits to training data using the kernel basis function expansion with varying numbers of Gaussian radial basis kernels.\n\n\n\nAs before, we observe that as the number of basis functions increases, the model becomes more flexible and is able to better fit the training data, but eventually begins to overfit.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Features and Regularization</span>"
    ]
  },
  {
    "objectID": "chapters/06-regularization.html#regularization",
    "href": "chapters/06-regularization.html#regularization",
    "title": "4  Features and Regularization",
    "section": "Regularization",
    "text": "Regularization\nWe now find ourselves in a bit of a dilemma – we’d like to use flexible models with many basis functions to capture nonlinear patterns in data, but introducing flexibility raises the risk of overfitting. One approach is to simply restrict which basis functions we’ll use, but this is unsatisfying: how will we know ahead of time which basis functions are best?\nAn alternative approach is to use regularization. Regularization works by encouraging our models to maintain small entries in the parameter vector \\(\\mathbf{w}\\). This effectively allows us to use many basis functions, but penalizes the model for emphasizing any one of them too heavily.\nTypical regularization schemes work by adding a penalty term to the loss function (e.g. the MSE). For example, in ridge regression, we add a penalty proportional to the squared \\(\\ell_2\\) norm of the parameter vector:\n\\[\n\\begin{aligned}\n    \\hat{\\mathbf{w}} = \\mathop{\\mathrm{arg\\,min}}_\\mathbf{w}\\left\\{ \\frac{1}{n} \\lVert \\mathbf{\\Phi}\\mathbf{w}- \\mathbf{y} \\rVert^2 + \\lambda \\lVert \\mathbf{w} \\rVert^2 \\right\\}\\;,\n\\end{aligned}\n\\]\nwhere \\(\\lambda\\) is a hyperparameter that controls the strength of the regularization. Larger values of \\(\\lambda\\) encourage smaller parameter values, while smaller values allow the model to fit the data more closely.\nWe can implement ridge regression by adding an \\(\\ell_2\\) regularization term to our training loop. We’ll first just implement that term itself:\n\ndef ell_2_regularization(w):\n    return torch.mean(w[0:]**2)\n\nThe reason for excluding the first entry of \\(w\\) from the regularization term is that this entry corresponds to the intercept term, which we typically don’t want to penalize. We then pass this function in to the regularization argument of train_model, where flagged line &lt;1&gt; adds the regularization term to the loss. We train again and visualize the results as we vary the regularization strength, this time keeping the maximum frequency of the sinusoidal basis functions fixed at 20:\n\nCode\nfig, axarr = plt.subplots(2, 3, figsize=(8,5))\nreg_strengths = torch.logspace(0, 1.5, 6)\n\nfor i, ax in enumerate(axarr.flatten()):\n    \n    PHI = sinusoidal_features(X, max_freq=20)\n    LR = LinearRegression(n_params=PHI.shape[1])\n    train_model(LR, PHI, y, lr=1e-2, n_epochs=200000, tol = 1e-4, regularization = lambda w: reg_strengths[i]*ell_2_regularization(w))\n    viz_model_predictions(LR, X, y, basis_fun = sinusoidal_features, ax=ax, max_freq=20)\n\n    ax.set_title(f\"reg_strength={reg_strengths[i]:.2f}\")\n    if i == 0:\n        ax.legend()\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.7: Ridge regression models using sinusoidal basis functions with maximum frequency 20, for varying regularization strengths, visualized against training data.\n\n\n\nWe observe that the tendency of \\(\\ell_2\\) regularization in this setting is to “shrink” the coefficients of the basis functions toward zero. This makes the predictions somewhat smoother, but also just makes them smaller, eventually leaving them systematically smaller than the data in magnitude.\nAn alternative regularization is \\(\\ell_1\\) regularization, in which we penalize the absolute values of the parameters rather than their squares. This gives us a version of regression commonly called lasso regression:\n\\[\n\\begin{aligned}\n    \\hat{\\mathbf{w}} = \\mathop{\\mathrm{arg\\,min}}_\\mathbf{w}\\left\\{ \\frac{1}{n} \\lVert \\mathbf{\\Phi}\\mathbf{w}- \\mathbf{y} \\rVert^2 + \\lambda \\sum_{j=1}^p |w_j| \\right\\}\\;.\n\\end{aligned}\n\\]\nWe can implement \\(\\ell_1\\) regularization similarly to before:\n\ndef ell_1_regularization(w):\n    return torch.mean(torch.abs(w[:-1]))  # exclude intercept from regularization\n\nNow we can run the same experiment:\n\nCode\nfig, axarr = plt.subplots(2, 3, figsize=(8,5))\nreg_strengths = torch.logspace(0, 1.5, 6)\n\nfor i, ax in enumerate(axarr.flatten()):\n    \n    PHI = sinusoidal_features(X, max_freq=20)\n    LR = LinearRegression(n_params=PHI.shape[1])\n    train_model(LR, PHI, y, lr=1e-2, n_epochs=20000, tol = 1e-4, regularization = lambda w: reg_strengths[i]*ell_1_regularization(w))\n    viz_model_predictions(LR, X, y, basis_fun = sinusoidal_features, ax=ax, max_freq=20)\n    \n    ax.set_title(f\"reg_strength={reg_strengths[i]:.2f}\")\n    if i == 0:\n        ax.legend()\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.8: LASSO regression models using sinusoidal basis functions with maximum frequency 20, for varying regularization strengths, visualized against training data.\n\n\n\nLASSO is somewhat more difficult to fit in computational terms, resulting in longer computation times. We observe that some of the LASSO fits manage to highlight the underlying trend in the data relatively well, with somewhat less systematic underfitting when compared to the ridge regression fits.\nAn important and useful property of LASSO is that it tends to set many of the coefficients exactly to zero, effectively performing feature selection. This can be useful when we have a large number of basis functions, as it allows us to identify which ones are most important for modeling the data. Let’s take a look at the coefficients learned by a LASSO model with moderate regularization strength:Features with coefficients of 0 are effectively thrown away from the model.\n\nCode\nLR = LinearRegression(n_params=PHI.shape[1])\ntrain_model(LR, PHI, y, lr=1e-4, n_epochs=50000, tol = 1e-4, regularization = lambda w: 1.0*ell_1_regularization(w))\n\nfreqs = range(PHI.shape[1]-1)\n\nzeros = torch.abs(LR.w) &lt;= 1e-2\n\nfig, ax = plt.subplots()\nax.scatter(freqs, LR.w.detach().numpy()[:-1], c = zeros.numpy()[:-1], cmap='Greys_r', edgecolors='black')\n\nax.set_xlabel(\"Sinusoidal frequency\")\nax.set_ylabel(\"Coefficient value\")\nplt.title(\"Lasso Regression Coefficients (white = zeroed out)\")\n\n\n\n\n\n\nText(0.5, 1.0, 'Lasso Regression Coefficients (white = zeroed out)')\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.9: Visualization of the coefficients learned by LASSO regression. Coefficients less than \\(0.01\\) have been highlighted in white. Given a more efficient optimizer, LASSO regression would set these coefficients to exactly 0.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Features and Regularization</span>"
    ]
  },
  {
    "objectID": "chapters/06-regularization.html#model-selection",
    "href": "chapters/06-regularization.html#model-selection",
    "title": "4  Features and Regularization",
    "section": "Model Selection",
    "text": "Model Selection\nIn one way, we’ve just kicked the can down the road – in an environment in which we can’t visually inspect the data or model predictions, how are we supposed to know what features or what regularization strength to use?\nThis is an instance of a problem called model selection, which asks us to make choices between models containing different features or hyperparameters. A very common approach to model selection is to use a validation set. The idea is to split our data into three parts: a training set which we’ll use for actually training our models, a validation set which we’ll use for model selection, and a test set which we’ll use for final evaluation of our chosen model. So, to make choices about the regularization strength, for example, we’ll train separate models with different regularization strengths on the training set, and then evaluate their performance on the validation set. The model with the best validation performance is then selected as the final model.\n\nFeatures In the Wild\nAlthough in the previous examples we engineered our features by hand using basis-function expansions, features are also natural parts of data sets! Often the data set we want to predict already has all the features we need (or more!), and we need to make all the same choices about which features to use and how to regularize. In the case study below, we’ll face some of these same questions without needing to engineer any new features by hand.\n\n\nBike Share Case Study\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom matplotlib import pyplot as plt\n\ndf = pd.read_csv('https://raw.githubusercontent.com/PhilChodrow/ml-notes-update/refs/heads/main/data/bikeshare/hour.csv')\n\nWe’ve downloaded a data set containing hourly bike rental counts in Washington, D.C., along with a variety of features that might be predictive of bike rental demand.  The data contains a variety of features including weather conditions, the year, month, week, and weekday; the hour of the day. It also includes columns for the number of casual and registered users, as well as the total count of bike rentals (cnt). We’ll try to predict the total count of bike rentals using the other features. Let’s take a look:This data set was collected by Fanaee-T and Gama (2013).\n\ndf.head()\n\n\n\n\n\n\n\n\ninstant\ndteday\nseason\nyr\nmnth\nhr\nholiday\nweekday\nworkingday\nweathersit\ntemp\natemp\nhum\nwindspeed\ncasual\nregistered\ncnt\n\n\n\n\n0\n1\n2011-01-01\n1\n0\n1\n0\n0\n6\n0\n1\n0.24\n0.2879\n0.81\n0.0\n3\n13\n16\n\n\n1\n2\n2011-01-01\n1\n0\n1\n1\n0\n6\n0\n1\n0.22\n0.2727\n0.80\n0.0\n8\n32\n40\n\n\n2\n3\n2011-01-01\n1\n0\n1\n2\n0\n6\n0\n1\n0.22\n0.2727\n0.80\n0.0\n5\n27\n32\n\n\n3\n4\n2011-01-01\n1\n0\n1\n3\n0\n6\n0\n1\n0.24\n0.2879\n0.75\n0.0\n3\n10\n13\n\n\n4\n5\n2011-01-01\n1\n0\n1\n4\n0\n6\n0\n1\n0.24\n0.2879\n0.75\n0.0\n0\n1\n1\n\n\n\n\n\n\n\nOur aim is to predict the cnt column using the other features. Let’s visualize the total number of bike rentals over time:\n\nCode\nfig, ax = plt.subplots(figsize = (6, 4))\nridership_by_day = df.groupby(\"dteday\")[\"cnt\"].sum().reset_index()\nridership_by_day[\"dteday\"] = pd.to_datetime(ridership_by_day[\"dteday\"])\n\nax.plot(ridership_by_day[\"dteday\"], ridership_by_day[\"cnt\"], color='steelblue')\n\nplt.xlabel(\"Date\")\nplt.ylabel(\"Total Daily Bike Rentals\")\n\n\n\n\n\n\nText(0, 0.5, 'Total Daily Bike Rentals')\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.10: Daily ridership in the bikeshare data set.\n\n\n\nPredicting the number of bike rentals on a given day is a very helpful task for bikeshare operators, as this can help them know the urgency of rebalancing (moving bikes between stations) and scheduling maintenance.\nLet’s prepare our data for modeling by dropping some columns and transforming qualitative columns into one-hot encoded features:\n\ndf.drop(columns=['instant', 'dteday', \"workingday\"], inplace=True)\n\ndf = 1.0*pd.get_dummies(df, columns=['season', 'weathersit', 'mnth', 'hr', 'weekday'], drop_first=False)\ndf[\"intercept\"] = 1\n\nWe’ll now split the data into features and targets, and then into training, validation, and test sets.\n\nX = 1.0*df.drop(columns=['cnt', 'casual', 'registered'])\ny = df['cnt']\n\ntest_proportion = 0.94\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_proportion, random_state=42)\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=1/2, random_state=42)\n\nX_train = torch.tensor(X_train.values, dtype=torch.float32)\ny_train = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32)\nX_val   = torch.tensor(X_val.values, dtype=torch.float32)\ny_val   = torch.tensor(y_val.values.reshape(-1, 1), dtype=torch.float32)\nX_test  = torch.tensor(X_test.values, dtype=torch.float32)\ny_test  = torch.tensor(y_test.values.reshape(-1, 1), dtype=torch.float32)\n\nWe’ve split the data into three pieces: a small training set (3% of the data), a validation set (3% of the data), and a test set (94% of the data). Given the size of the data set, this split is reasonable – we have enough data to train models effectively, while still leaving a large amount of data for final evaluation.\n\nprint(f\"Training set size: {X_train.shape[0]} samples\")\nprint(f\"Validation set size: {X_val.shape[0]} samples\")\nprint(f\"Test set size: {X_test.shape[0]} samples\")\n\nTraining set size: 521 samples\nValidation set size: 521 samples\nTest set size: 16337 samples\n\n\n\n\n\n\n\n\nNoteWhy so few training samples?\n\n\n\nIssues related to feature engineering and regularization most frequently arise in settings where we many parameters and relatively few data points. Therefore, to illustrate these issues clearly, we’ve deliberately limited the size of the training set. In practice, of course, one would typically want to use as much data as possible for training.\nAlthough this example is a bit artificial, modern neural networks are often trained with vastly more parameters than data points, making these considerations quite relevant in practice.\n\n\nTo contextualize model performance on real data, it’s helpful to compute a base rate.\n\nDefinition 4.2 (Base Rate) A base rate is a measure of performance for a model which uses no features in the data.\nWe typically say that a model has demonstrated success in learning from features if it outperforms the base rate on unseen data.\n\nLet’s check the base rate for the bikeshare data and assess on validation data:\n\ny_mean = y_train.mean()\nbase_rate_mse = mse(y_val, y_mean)\nprint(f\"Base rate MSE on validation data: {base_rate_mse.item():.4f}\")\n\nBase rate MSE on validation data: 33617.3125\n\n\nSo, we are looking for any reasonable candidate model to achieve validation MSE less than the above.\nIn principle, we can already go ahead and fit a model:\n\nLR = LinearRegression(n_params=X_train.shape[1])\ntrain_model(LR, X_train, y_train, lr=1e-2, n_epochs=50000, tol=1e-4)\ny_pred = LR.forward(X_val)\nval_mse = mse(y_pred, y_val)\nprint(f\"Validation MSE without regularization: {val_mse.item():.4f}\")\nprint(f\"Fraction of base rate: {val_mse.item()/base_rate_mse.item():.4f}\")\n\nValidation MSE without regularization: 12257.3779\nFraction of base rate: 0.3646\n\n\nThis simple linear regression model with no regularization already performs considerably better on validation data than the base rate. Can we do better with regularization? To find out, we’ll do a systematic search in which we vary the regularization strength and evaluate performance on validation data, for each of ridge regression and LASSO.\n\nCode\nreg_strengths = torch.logspace(-5, 0, 21)  \n\ntrain_mses = []\nval_mses = []\n\nW = torch.empty(X_train.shape[1], 0)\n\nfor reg in reg_strengths: \n\n    LR = LinearRegression(n_params=X_train.shape[1])\n    train_model(LR, X_train, y_train, lr=1e-1, n_epochs=20000, tol = 1e-2, regularization = lambda w: reg*ell_2_regularization(w))\n\n    y_pred_train = LR.forward(X_train)\n    train_mse = mse(y_pred_train, y_train)\n    y_pred_val = LR.forward(X_val)\n    val_mse = mse(y_pred_val, y_val)\n\n    train_mses.append(train_mse.item())\n    val_mses.append(val_mse.item())\n\n    W = torch.cat((W, LR.w), dim=1)\n\nbest_reg = reg_strengths[val_mses.index(min(val_mses))]\n\nfig, ax = plt.subplots(1, 2, figsize = (8,3))\nax[0].plot(reg_strengths, train_mses, label='Train MSE')\nax[0].plot(reg_strengths, val_mses, label='Validation MSE')\n\nax[0].axvline(best_reg, color='black', linestyle='--', label=fr'Best $\\lambda=${best_reg:.2f}, val MSE {min(val_mses):.1f}')\n\n\nax[0].set_xlabel(\"Regularization Strength\")\nax[0].set_ylabel(\"MSE\")\nax[0].set_xscale(\"log\")\nplt.title(\"Ridge Regression: Train and Validation MSE vs Regularization Strength\")\nax[0].legend()\n\nax[1].plot(reg_strengths, W.detach().numpy().T)\nax[1].axvline(best_reg, color='black', linestyle='--', label=f'Best reg={best_reg:.4f}')\nax[1].set_xlabel(\"Regularization Strength\")\nax[1].set_ylabel(\"Weights\")\nax[1].set_xscale(\"log\")\nplt.title(\"Ridge Regression: Weights vs Regularization Strength\")\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.11: Model selection in ridge regression on the bikeshare data set. (Left): training and validation MSE as a function of regularization strength. (Right): learned weights as a function of regularization strength.\n\n\n\nNow let’s try the same experiment for LASSO.\n\nCode\nreg_strengths = torch.logspace(0, 2, 21)  \n\ntrain_mses = []\nval_mses = []\n\nW = torch.empty(X_train.shape[1], 0)\n\nfor reg in reg_strengths: \n\n    LR = LinearRegression(n_params=X_train.shape[1])\n    train_model(LR, X_train, y_train, lr=1e-2, n_epochs=50000, tol = 1e-2, regularization = lambda w: reg*ell_1_regularization(w), verbose = True)\n\n    y_pred_train = LR.forward(X_train)\n    train_mse = mse(y_pred_train, y_train)\n    y_pred_val = LR.forward(X_val)\n    val_mse = mse(y_pred_val, y_val)\n\n    train_mses.append(train_mse.item())\n    val_mses.append(val_mse.item())\n\n    W = torch.cat((W, LR.w), dim=1)\n\nbest_reg = reg_strengths[val_mses.index(min(val_mses))]\n\nfig, ax = plt.subplots(1, 2, figsize = (8,3))\nax[0].plot(reg_strengths, train_mses, label='Train MSE')\nax[0].plot(reg_strengths, val_mses, label='Validation MSE')\n\nax[0].axvline(best_reg, color='black', linestyle='--', label=fr'Best $\\lambda=${best_reg:.2f}, val MSE {min(val_mses):.1f}')\n\n\nax[0].set_xlabel(\"Regularization Strength\")\nax[0].set_ylabel(\"MSE\")\nax[0].set_xscale(\"log\")\nplt.title(\"LASSO Regression: Train and Validation MSE vs Regularization Strength\")\nax[0].legend()\n\nax[1].plot(reg_strengths, W.detach().numpy().T)\nax[1].axvline(best_reg, color='black', linestyle='--', label=f'Best reg={best_reg:.4f}')\nax[1].set_xlabel(\"Regularization Strength\")\nax[1].set_ylabel(\"Weights\")\nax[1].set_xscale(\"log\")\nplt.title(\"LASSO Regression: Weights vs Regularization Strength\")\n\nplt.tight_layout()\n\n\n\n\n\n\nEpoch 0, Loss: 63828.96875\nEpoch 100, Loss: 60922.2421875\nEpoch 200, Loss: 58204.4296875\nEpoch 300, Loss: 55661.859375\nEpoch 400, Loss: 53281.234375\nEpoch 500, Loss: 51051.63671875\nEpoch 600, Loss: 48963.96484375\nEpoch 700, Loss: 47009.9609375\nEpoch 800, Loss: 45181.90234375\nEpoch 900, Loss: 43472.578125\nEpoch 1000, Loss: 41875.12109375\nEpoch 1100, Loss: 40382.87890625\nEpoch 1200, Loss: 38989.4453125\nEpoch 1300, Loss: 37688.6796875\nEpoch 1400, Loss: 36474.65234375\nEpoch 1500, Loss: 35341.7578125\nEpoch 1600, Loss: 34284.3984375\nEpoch 1700, Loss: 33297.328125\nEpoch 1800, Loss: 32375.54296875\nEpoch 1900, Loss: 31514.36328125\nEpoch 2000, Loss: 30709.40625\nEpoch 2100, Loss: 29956.591796875\nEpoch 2200, Loss: 29252.201171875\nEpoch 2300, Loss: 28592.66796875\nEpoch 2400, Loss: 27974.79296875\nEpoch 2500, Loss: 27395.48828125\nEpoch 2600, Loss: 26851.85546875\nEpoch 2700, Loss: 26341.259765625\nEpoch 2800, Loss: 25861.25\nEpoch 2900, Loss: 25409.55078125\nEpoch 3000, Loss: 24984.01953125\nEpoch 3100, Loss: 24582.662109375\nEpoch 3200, Loss: 24203.607421875\nEpoch 3300, Loss: 23845.119140625\nEpoch 3400, Loss: 23505.544921875\nEpoch 3500, Loss: 23183.3515625\nEpoch 3600, Loss: 22877.05859375\nEpoch 3700, Loss: 22585.251953125\nEpoch 3800, Loss: 22306.6328125\nEpoch 3900, Loss: 22040.013671875\nEpoch 4000, Loss: 21784.294921875\nEpoch 4100, Loss: 21538.505859375\nEpoch 4200, Loss: 21301.740234375\nEpoch 4300, Loss: 21073.12890625\nEpoch 4400, Loss: 20851.94140625\nEpoch 4500, Loss: 20637.576171875\nEpoch 4600, Loss: 20429.40625\nEpoch 4700, Loss: 20226.927734375\nEpoch 4800, Loss: 20029.701171875\nEpoch 4900, Loss: 19837.341796875\nEpoch 5000, Loss: 19649.52734375\nEpoch 5100, Loss: 19465.970703125\nEpoch 5200, Loss: 19286.419921875\nEpoch 5300, Loss: 19110.658203125\nEpoch 5400, Loss: 18938.4921875\nEpoch 5500, Loss: 18769.751953125\nEpoch 5600, Loss: 18604.2890625\nEpoch 5700, Loss: 18441.96484375\nEpoch 5800, Loss: 18282.66015625\nEpoch 5900, Loss: 18126.255859375\nEpoch 6000, Loss: 17972.66015625\nEpoch 6100, Loss: 17821.82421875\nEpoch 6200, Loss: 17673.611328125\nEpoch 6300, Loss: 17527.947265625\nEpoch 6400, Loss: 17384.759765625\nEpoch 6500, Loss: 17243.978515625\nEpoch 6600, Loss: 17105.548828125\nEpoch 6700, Loss: 16969.412109375\nEpoch 6800, Loss: 16835.509765625\nEpoch 6900, Loss: 16703.798828125\nEpoch 7000, Loss: 16574.224609375\nEpoch 7100, Loss: 16446.74609375\nEpoch 7200, Loss: 16321.3388671875\nEpoch 7300, Loss: 16197.951171875\nEpoch 7400, Loss: 16076.5908203125\nEpoch 7500, Loss: 15957.162109375\nEpoch 7600, Loss: 15839.603515625\nEpoch 7700, Loss: 15723.8837890625\nEpoch 7800, Loss: 15609.9521484375\nEpoch 7900, Loss: 15497.767578125\nEpoch 8000, Loss: 15387.3076171875\nEpoch 8100, Loss: 15278.51953125\nEpoch 8200, Loss: 15171.3857421875\nEpoch 8300, Loss: 15065.8623046875\nEpoch 8400, Loss: 14961.9150390625\nEpoch 8500, Loss: 14859.517578125\nEpoch 8600, Loss: 14758.6416015625\nEpoch 8700, Loss: 14659.255859375\nEpoch 8800, Loss: 14561.3486328125\nEpoch 8900, Loss: 14464.8798828125\nEpoch 9000, Loss: 14369.8427734375\nEpoch 9100, Loss: 14276.2119140625\nEpoch 9200, Loss: 14183.96875\nEpoch 9300, Loss: 14093.1240234375\nEpoch 9400, Loss: 14003.68359375\nEpoch 9500, Loss: 13915.5869140625\nEpoch 9600, Loss: 13828.826171875\nEpoch 9700, Loss: 13743.3935546875\nEpoch 9800, Loss: 13659.275390625\nEpoch 9900, Loss: 13576.46484375\nEpoch 10000, Loss: 13494.9541015625\nEpoch 10100, Loss: 13414.740234375\nEpoch 10200, Loss: 13335.8125\nEpoch 10300, Loss: 13258.1669921875\nEpoch 10400, Loss: 13181.798828125\nEpoch 10500, Loss: 13106.7001953125\nEpoch 10600, Loss: 13032.87109375\nEpoch 10700, Loss: 12960.2998046875\nEpoch 10800, Loss: 12888.982421875\nEpoch 10900, Loss: 12818.9052734375\nEpoch 11000, Loss: 12750.0673828125\nEpoch 11100, Loss: 12682.4560546875\nEpoch 11200, Loss: 12616.060546875\nEpoch 11300, Loss: 12550.865234375\nEpoch 11400, Loss: 12486.861328125\nEpoch 11500, Loss: 12424.03515625\nEpoch 11600, Loss: 12362.373046875\nEpoch 11700, Loss: 12301.869140625\nEpoch 11800, Loss: 12242.4921875\nEpoch 11900, Loss: 12184.224609375\nEpoch 12000, Loss: 12127.048828125\nEpoch 12100, Loss: 12070.9453125\nEpoch 12200, Loss: 12015.890625\nEpoch 12300, Loss: 11961.8984375\nEpoch 12400, Loss: 11908.9599609375\nEpoch 12500, Loss: 11857.015625\nEpoch 12600, Loss: 11806.05078125\nEpoch 12700, Loss: 11756.0439453125\nEpoch 12800, Loss: 11706.978515625\nEpoch 12900, Loss: 11658.841796875\nEpoch 13000, Loss: 11611.6181640625\nEpoch 13100, Loss: 11565.2919921875\nEpoch 13200, Loss: 11519.8427734375\nEpoch 13300, Loss: 11475.2724609375\nEpoch 13400, Loss: 11431.556640625\nEpoch 13500, Loss: 11388.6845703125\nEpoch 13600, Loss: 11346.64453125\nEpoch 13700, Loss: 11305.435546875\nEpoch 13800, Loss: 11265.0322265625\nEpoch 13900, Loss: 11225.4306640625\nEpoch 14000, Loss: 11186.6181640625\nEpoch 14100, Loss: 11148.5888671875\nEpoch 14200, Loss: 11111.330078125\nEpoch 14300, Loss: 11074.8603515625\nEpoch 14400, Loss: 11039.201171875\nEpoch 14500, Loss: 11004.291015625\nEpoch 14600, Loss: 10970.119140625\nEpoch 14700, Loss: 10936.6748046875\nEpoch 14800, Loss: 10903.982421875\nEpoch 14900, Loss: 10872.0234375\nEpoch 15000, Loss: 10840.7724609375\nEpoch 15100, Loss: 10810.22265625\nEpoch 15200, Loss: 10780.3701171875\nEpoch 15300, Loss: 10751.2080078125\nEpoch 15400, Loss: 10722.7314453125\nEpoch 15500, Loss: 10694.9521484375\nEpoch 15600, Loss: 10667.8515625\nEpoch 15700, Loss: 10641.4111328125\nEpoch 15800, Loss: 10615.6279296875\nEpoch 15900, Loss: 10590.490234375\nEpoch 16000, Loss: 10565.998046875\nEpoch 16100, Loss: 10542.1630859375\nEpoch 16200, Loss: 10518.9453125\nEpoch 16300, Loss: 10496.3505859375\nEpoch 16400, Loss: 10474.3701171875\nEpoch 16500, Loss: 10452.982421875\nEpoch 16600, Loss: 10432.1748046875\nEpoch 16700, Loss: 10411.9404296875\nEpoch 16800, Loss: 10392.2666015625\nEpoch 16900, Loss: 10373.146484375\nEpoch 17000, Loss: 10354.5712890625\nEpoch 17100, Loss: 10336.5263671875\nEpoch 17200, Loss: 10319.00390625\nEpoch 17300, Loss: 10301.99609375\nEpoch 17400, Loss: 10285.4921875\nEpoch 17500, Loss: 10269.486328125\nEpoch 17600, Loss: 10253.96875\nEpoch 17700, Loss: 10238.9326171875\nEpoch 17800, Loss: 10224.3720703125\nEpoch 17900, Loss: 10210.28125\nEpoch 18000, Loss: 10196.65625\nEpoch 18100, Loss: 10183.48828125\nEpoch 18200, Loss: 10170.7744140625\nEpoch 18300, Loss: 10158.5068359375\nEpoch 18400, Loss: 10146.69140625\nEpoch 18500, Loss: 10135.3330078125\nEpoch 18600, Loss: 10124.412109375\nEpoch 18700, Loss: 10113.9169921875\nEpoch 18800, Loss: 10103.8427734375\nEpoch 18900, Loss: 10094.1826171875\nEpoch 19000, Loss: 10084.9296875\nEpoch 19100, Loss: 10076.080078125\nEpoch 19200, Loss: 10067.6240234375\nEpoch 19300, Loss: 10059.5869140625\nEpoch 19400, Loss: 10051.9345703125\nEpoch 19500, Loss: 10044.658203125\nEpoch 19600, Loss: 10037.748046875\nEpoch 19700, Loss: 10031.197265625\nEpoch 19800, Loss: 10024.99609375\nEpoch 19900, Loss: 10019.134765625\nEpoch 20000, Loss: 10013.6015625\nEpoch 20100, Loss: 10008.400390625\nEpoch 20200, Loss: 10003.521484375\nEpoch 20300, Loss: 9998.9501953125\nEpoch 20400, Loss: 9994.6650390625\nEpoch 20500, Loss: 9990.662109375\nEpoch 20600, Loss: 9986.92578125\nEpoch 20700, Loss: 9983.4599609375\nEpoch 20800, Loss: 9980.2392578125\nEpoch 20900, Loss: 9977.2548828125\nEpoch 21000, Loss: 9974.4931640625\nEpoch 21100, Loss: 9971.9482421875\nEpoch 21200, Loss: 9969.6015625\nEpoch 21300, Loss: 9967.4453125\nEpoch 21400, Loss: 9965.4658203125\nEpoch 21500, Loss: 9963.65234375\nEpoch 21600, Loss: 9961.9931640625\nEpoch 21700, Loss: 9960.4794921875\nEpoch 21800, Loss: 9959.09765625\nEpoch 21900, Loss: 9957.841796875\nEpoch 22000, Loss: 9956.69921875\nEpoch 22100, Loss: 9955.6630859375\nEpoch 22200, Loss: 9954.72265625\nEpoch 22300, Loss: 9953.8720703125\nEpoch 22400, Loss: 9953.103515625\nEpoch 22500, Loss: 9952.4111328125\nEpoch 22600, Loss: 9951.7861328125\nEpoch 22700, Loss: 9951.2236328125\nEpoch 22800, Loss: 9950.71875\nEpoch 22900, Loss: 9950.267578125\nEpoch 23000, Loss: 9949.86328125\nEpoch 23100, Loss: 9949.50390625\nEpoch 23200, Loss: 9949.1845703125\nEpoch 23300, Loss: 9948.8994140625\nEpoch 23400, Loss: 9948.6484375\nEpoch 23500, Loss: 9948.431640625\nEpoch 23600, Loss: 9948.2392578125\nEpoch 23700, Loss: 9948.072265625\nEpoch 23800, Loss: 9947.9248046875\nEpoch 23900, Loss: 9947.7939453125\nEpoch 24000, Loss: 9947.6796875\nEpoch 24100, Loss: 9947.576171875\nEpoch 24200, Loss: 9947.4833984375\nEpoch 24300, Loss: 9947.3994140625\nEpoch 24400, Loss: 9947.322265625\nEpoch 24500, Loss: 9947.25\nEpoch 24600, Loss: 9947.181640625\nEpoch 24700, Loss: 9947.1171875\nEpoch 24800, Loss: 9947.0517578125\nEpoch 24900, Loss: 9946.9892578125\nEpoch 25000, Loss: 9946.92578125\nEpoch 25100, Loss: 9946.8623046875\nEpoch 25200, Loss: 9946.7978515625\nEpoch 25300, Loss: 9946.732421875\nEpoch 25400, Loss: 9946.6650390625\nEpoch 25500, Loss: 9946.59375\nEpoch 25600, Loss: 9946.5244140625\nEpoch 25700, Loss: 9946.4521484375\nEpoch 25800, Loss: 9946.3779296875\nEpoch 25900, Loss: 9946.3037109375\nEpoch 26000, Loss: 9946.2255859375\nEpoch 26100, Loss: 9946.142578125\nEpoch 26200, Loss: 9946.05859375\nEpoch 26300, Loss: 9945.970703125\nEpoch 26400, Loss: 9945.880859375\nEpoch 26500, Loss: 9945.78515625\nEpoch 26600, Loss: 9945.6884765625\nEpoch 26700, Loss: 9945.5869140625\nEpoch 26800, Loss: 9945.482421875\nEpoch 26900, Loss: 9945.3740234375\nEpoch 27000, Loss: 9945.2626953125\nEpoch 27100, Loss: 9945.146484375\nEpoch 27200, Loss: 9945.0263671875\nEpoch 27300, Loss: 9944.9033203125\nEpoch 27400, Loss: 9944.77734375\nEpoch 27500, Loss: 9944.6455078125\nEpoch 27600, Loss: 9944.5107421875\nEpoch 27700, Loss: 9944.37109375\nEpoch 27800, Loss: 9944.228515625\nEpoch 27900, Loss: 9944.08203125\nEpoch 28000, Loss: 9943.931640625\nEpoch 28100, Loss: 9943.7763671875\nEpoch 28200, Loss: 9943.6171875\nEpoch 28300, Loss: 9943.455078125\nEpoch 28400, Loss: 9943.2900390625\nEpoch 28500, Loss: 9943.13671875\nEpoch 28600, Loss: 9942.9892578125\nEpoch 28700, Loss: 9942.86328125\nEpoch 28800, Loss: 9942.751953125\nEpoch 28900, Loss: 9942.640625\nEpoch 29000, Loss: 9942.525390625\nEpoch 29100, Loss: 9942.4072265625\nEpoch 29200, Loss: 9942.287109375\nEpoch 29300, Loss: 9942.1611328125\nEpoch 29400, Loss: 9942.03515625\nEpoch 29500, Loss: 9941.9033203125\nEpoch 29600, Loss: 9941.76953125\nEpoch 29700, Loss: 9941.6328125\nEpoch 29800, Loss: 9941.49609375\nEpoch 29900, Loss: 9941.3544921875\nEpoch 30000, Loss: 9941.2109375\nEpoch 30100, Loss: 9941.0673828125\nEpoch 30200, Loss: 9940.955078125\nEpoch 30300, Loss: 9940.857421875\nEpoch 30400, Loss: 9940.7568359375\nEpoch 30500, Loss: 9940.677734375\nEpoch 30600, Loss: 9940.6171875\nEpoch 30700, Loss: 9940.5556640625\nEpoch 30800, Loss: 9940.4912109375\nEpoch 30900, Loss: 9940.4267578125\nEpoch 31000, Loss: 9940.359375\nEpoch 31100, Loss: 9940.291015625\nEpoch 31200, Loss: 9940.220703125\nEpoch 31300, Loss: 9940.1474609375\nEpoch 31400, Loss: 9940.076171875\nEpoch 31500, Loss: 9939.9990234375\nEpoch 31600, Loss: 9939.9228515625\nEpoch 31700, Loss: 9939.8447265625\nEpoch 31800, Loss: 9939.763671875\nEpoch 31900, Loss: 9939.6826171875\nEpoch 32000, Loss: 9939.5986328125\nEpoch 32100, Loss: 9939.529296875\nEpoch 32200, Loss: 9939.4912109375\nEpoch 32300, Loss: 9939.451171875\nEpoch 32400, Loss: 9939.4111328125\nEpoch 32500, Loss: 9939.369140625\nEpoch 32600, Loss: 9939.3427734375\nEpoch 32700, Loss: 9939.3310546875\nEpoch 32800, Loss: 9939.3193359375\nEpoch 32900, Loss: 9939.30859375\nEpoch 33000, Loss: 9939.3056640625\nEpoch 33100, Loss: 9939.302734375\nEpoch 33200, Loss: 9939.3017578125\nEpoch 33300, Loss: 9939.3017578125\nEpoch 33400, Loss: 9939.3017578125\nEpoch 33500, Loss: 9939.3017578125\nEpoch 33600, Loss: 9939.30078125\nEpoch 33700, Loss: 9939.3017578125\nEpoch 33800, Loss: 9939.30078125\nEpoch 33900, Loss: 9939.30078125\nEpoch 34000, Loss: 9939.3017578125\nEpoch 34100, Loss: 9939.3017578125\nEpoch 34200, Loss: 9939.30078125\nEpoch 34300, Loss: 9939.3017578125\nEpoch 34400, Loss: 9939.3017578125\nEpoch 34500, Loss: 9939.30078125\nEpoch 34600, Loss: 9939.30078125\nEpoch 34700, Loss: 9939.3017578125\nEpoch 34800, Loss: 9939.3017578125\nEpoch 34900, Loss: 9939.3017578125\nEpoch 35000, Loss: 9939.30078125\nEpoch 35100, Loss: 9939.30078125\nEpoch 35200, Loss: 9939.30078125\nEpoch 35300, Loss: 9939.30078125\nEpoch 35400, Loss: 9939.3017578125\nEpoch 35500, Loss: 9939.30078125\nEpoch 35600, Loss: 9939.30078125\nEpoch 35700, Loss: 9939.30078125\nEpoch 35800, Loss: 9939.30078125\nEpoch 35900, Loss: 9939.30078125\nEpoch 36000, Loss: 9939.30078125\nEpoch 36100, Loss: 9939.30078125\nEpoch 36200, Loss: 9939.30078125\nEpoch 36300, Loss: 9939.30078125\nEpoch 36400, Loss: 9939.30078125\nEpoch 36500, Loss: 9939.30078125\nEpoch 36600, Loss: 9939.30078125\nEpoch 36700, Loss: 9939.30078125\nEpoch 36800, Loss: 9939.3017578125\nEpoch 36900, Loss: 9939.30078125\nEpoch 37000, Loss: 9939.30078125\nEpoch 37100, Loss: 9939.30078125\nEpoch 37200, Loss: 9939.3017578125\nEpoch 37300, Loss: 9939.30078125\nEpoch 37400, Loss: 9939.30078125\nEpoch 37500, Loss: 9939.30078125\nEpoch 37600, Loss: 9939.30078125\nEpoch 37700, Loss: 9939.30078125\nEpoch 37800, Loss: 9939.30078125\nEpoch 37900, Loss: 9939.30078125\nEpoch 38000, Loss: 9939.30078125\nEpoch 38100, Loss: 9939.3017578125\nEpoch 38200, Loss: 9939.30078125\nEpoch 38300, Loss: 9939.30078125\nEpoch 38400, Loss: 9939.30078125\nEpoch 38500, Loss: 9939.30078125\nEpoch 38600, Loss: 9939.30078125\nEpoch 38700, Loss: 9939.30078125\nEpoch 38800, Loss: 9939.30078125\nEpoch 38900, Loss: 9939.30078125\nEpoch 39000, Loss: 9939.30078125\nEpoch 39100, Loss: 9939.30078125\nEpoch 39200, Loss: 9939.3017578125\nEpoch 39300, Loss: 9939.30078125\nEpoch 39400, Loss: 9939.30078125\nEpoch 39500, Loss: 9939.30078125\nEpoch 39600, Loss: 9939.30078125\nEpoch 39700, Loss: 9939.30078125\nEpoch 39800, Loss: 9939.30078125\nEpoch 39900, Loss: 9939.30078125\nEpoch 40000, Loss: 9939.30078125\nEpoch 40100, Loss: 9939.30078125\nEpoch 40200, Loss: 9939.30078125\nEpoch 40300, Loss: 9939.3017578125\nEpoch 40400, Loss: 9939.30078125\nEpoch 40500, Loss: 9939.30078125\nEpoch 40600, Loss: 9939.30078125\nEpoch 40700, Loss: 9939.30078125\nEpoch 40800, Loss: 9939.3017578125\nEpoch 40900, Loss: 9939.30078125\nEpoch 41000, Loss: 9939.30078125\nEpoch 41100, Loss: 9939.30078125\nEpoch 41200, Loss: 9939.30078125\nEpoch 41300, Loss: 9939.30078125\nEpoch 41400, Loss: 9939.30078125\nEpoch 41500, Loss: 9939.30078125\nEpoch 41600, Loss: 9939.30078125\nEpoch 41700, Loss: 9939.30078125\nEpoch 41800, Loss: 9939.30078125\nEpoch 41900, Loss: 9939.30078125\nEpoch 42000, Loss: 9939.3017578125\nEpoch 42100, Loss: 9939.30078125\nEpoch 42200, Loss: 9939.30078125\nEpoch 42300, Loss: 9939.30078125\nEpoch 42400, Loss: 9939.30078125\nEpoch 42500, Loss: 9939.30078125\nEpoch 42600, Loss: 9939.30078125\nEpoch 42700, Loss: 9939.30078125\nEpoch 42800, Loss: 9939.30078125\nEpoch 42900, Loss: 9939.30078125\nEpoch 43000, Loss: 9939.30078125\nEpoch 43100, Loss: 9939.30078125\nEpoch 43200, Loss: 9939.30078125\nEpoch 43300, Loss: 9939.3017578125\nEpoch 43400, Loss: 9939.30078125\nEpoch 43500, Loss: 9939.30078125\nEpoch 43600, Loss: 9939.30078125\nEpoch 43700, Loss: 9939.30078125\nEpoch 43800, Loss: 9939.30078125\nEpoch 43900, Loss: 9939.30078125\nEpoch 44000, Loss: 9939.30078125\nEpoch 44100, Loss: 9939.30078125\nEpoch 44200, Loss: 9939.30078125\nEpoch 44300, Loss: 9939.30078125\nEpoch 44400, Loss: 9939.30078125\nEpoch 44500, Loss: 9939.30078125\nEpoch 44600, Loss: 9939.30078125\nEpoch 44700, Loss: 9939.30078125\nEpoch 44800, Loss: 9939.3017578125\nEpoch 44900, Loss: 9939.30078125\nEpoch 45000, Loss: 9939.3017578125\nEpoch 45100, Loss: 9939.30078125\nEpoch 45200, Loss: 9939.30078125\nEpoch 45300, Loss: 9939.3017578125\nEpoch 45400, Loss: 9939.30078125\nEpoch 45500, Loss: 9939.30078125\nEpoch 45600, Loss: 9939.30078125\nEpoch 45700, Loss: 9939.30078125\nEpoch 45800, Loss: 9939.30078125\nEpoch 45900, Loss: 9939.30078125\nEpoch 46000, Loss: 9939.30078125\nEpoch 46100, Loss: 9939.30078125\nEpoch 46200, Loss: 9939.30078125\nEpoch 46300, Loss: 9939.30078125\nEpoch 46400, Loss: 9939.30078125\nEpoch 46500, Loss: 9939.30078125\nEpoch 46600, Loss: 9939.30078125\nEpoch 46700, Loss: 9939.30078125\nEpoch 46800, Loss: 9939.30078125\nEpoch 46900, Loss: 9939.30078125\nEpoch 47000, Loss: 9939.30078125\nEpoch 47100, Loss: 9939.3017578125\nEpoch 47200, Loss: 9939.30078125\nEpoch 47300, Loss: 9939.3017578125\nEpoch 47400, Loss: 9939.2998046875\nEpoch 47500, Loss: 9939.30078125\nEpoch 47600, Loss: 9939.30078125\nEpoch 47700, Loss: 9939.30078125\nEpoch 47800, Loss: 9939.30078125\nEpoch 47900, Loss: 9939.30078125\nEpoch 48000, Loss: 9939.30078125\nEpoch 48100, Loss: 9939.3017578125\nEpoch 48200, Loss: 9939.30078125\nEpoch 48300, Loss: 9939.30078125\nEpoch 48400, Loss: 9939.30078125\nEpoch 48500, Loss: 9939.30078125\nEpoch 48600, Loss: 9939.30078125\nEpoch 48700, Loss: 9939.30078125\nEpoch 48800, Loss: 9939.30078125\nEpoch 48900, Loss: 9939.30078125\nEpoch 49000, Loss: 9939.30078125\nEpoch 49100, Loss: 9939.30078125\nEpoch 49200, Loss: 9939.30078125\nEpoch 49300, Loss: 9939.30078125\nEpoch 49400, Loss: 9939.30078125\nEpoch 49500, Loss: 9939.30078125\nEpoch 49600, Loss: 9939.30078125\nEpoch 49700, Loss: 9939.30078125\nEpoch 49800, Loss: 9939.30078125\nEpoch 49900, Loss: 9939.3017578125\nEpoch 0, Loss: 63828.96875\nEpoch 100, Loss: 60922.49609375\nEpoch 200, Loss: 58204.91015625\nEpoch 300, Loss: 55662.53515625\nEpoch 400, Loss: 53282.11328125\nEpoch 500, Loss: 51052.69921875\nEpoch 600, Loss: 48965.23046875\nEpoch 700, Loss: 47011.41015625\nEpoch 800, Loss: 45183.52734375\nEpoch 900, Loss: 43474.3671875\nEpoch 1000, Loss: 41877.0859375\nEpoch 1100, Loss: 40385.0\nEpoch 1200, Loss: 38991.71875\nEpoch 1300, Loss: 37691.0859375\nEpoch 1400, Loss: 36477.1875\nEpoch 1500, Loss: 35344.421875\nEpoch 1600, Loss: 34287.1953125\nEpoch 1700, Loss: 33300.23046875\nEpoch 1800, Loss: 32378.560546875\nEpoch 1900, Loss: 31517.4765625\nEpoch 2000, Loss: 30712.607421875\nEpoch 2100, Loss: 29959.875\nEpoch 2200, Loss: 29255.583984375\nEpoch 2300, Loss: 28596.146484375\nEpoch 2400, Loss: 27978.390625\nEpoch 2500, Loss: 27399.212890625\nEpoch 2600, Loss: 26855.701171875\nEpoch 2700, Loss: 26345.21875\nEpoch 2800, Loss: 25865.326171875\nEpoch 2900, Loss: 25413.72265625\nEpoch 3000, Loss: 24988.291015625\nEpoch 3100, Loss: 24587.03515625\nEpoch 3200, Loss: 24208.072265625\nEpoch 3300, Loss: 23849.681640625\nEpoch 3400, Loss: 23510.21484375\nEpoch 3500, Loss: 23188.13671875\nEpoch 3600, Loss: 22881.97265625\nEpoch 3700, Loss: 22590.291015625\nEpoch 3800, Loss: 22311.794921875\nEpoch 3900, Loss: 22045.296875\nEpoch 4000, Loss: 21789.693359375\nEpoch 4100, Loss: 21544.029296875\nEpoch 4200, Loss: 21307.38671875\nEpoch 4300, Loss: 21078.900390625\nEpoch 4400, Loss: 20857.83203125\nEpoch 4500, Loss: 20643.595703125\nEpoch 4600, Loss: 20435.5546875\nEpoch 4700, Loss: 20233.201171875\nEpoch 4800, Loss: 20036.091796875\nEpoch 4900, Loss: 19843.853515625\nEpoch 5000, Loss: 19656.154296875\nEpoch 5100, Loss: 19472.708984375\nEpoch 5200, Loss: 19293.265625\nEpoch 5300, Loss: 19117.611328125\nEpoch 5400, Loss: 18945.548828125\nEpoch 5500, Loss: 18776.9140625\nEpoch 5600, Loss: 18611.552734375\nEpoch 5700, Loss: 18449.326171875\nEpoch 5800, Loss: 18290.11328125\nEpoch 5900, Loss: 18133.806640625\nEpoch 6000, Loss: 17980.30078125\nEpoch 6100, Loss: 17829.568359375\nEpoch 6200, Loss: 17681.45703125\nEpoch 6300, Loss: 17535.88671875\nEpoch 6400, Loss: 17392.79296875\nEpoch 6500, Loss: 17252.107421875\nEpoch 6600, Loss: 17113.763671875\nEpoch 6700, Loss: 16977.71484375\nEpoch 6800, Loss: 16843.900390625\nEpoch 6900, Loss: 16712.267578125\nEpoch 7000, Loss: 16582.779296875\nEpoch 7100, Loss: 16455.376953125\nEpoch 7200, Loss: 16330.0556640625\nEpoch 7300, Loss: 16206.7587890625\nEpoch 7400, Loss: 16085.5\nEpoch 7500, Loss: 15966.169921875\nEpoch 7600, Loss: 15848.7109375\nEpoch 7700, Loss: 15733.0830078125\nEpoch 7800, Loss: 15619.2421875\nEpoch 7900, Loss: 15507.146484375\nEpoch 8000, Loss: 15396.767578125\nEpoch 8100, Loss: 15288.0615234375\nEpoch 8200, Loss: 15181.0068359375\nEpoch 8300, Loss: 15075.560546875\nEpoch 8400, Loss: 14971.6845703125\nEpoch 8500, Loss: 14869.3583984375\nEpoch 8600, Loss: 14768.552734375\nEpoch 8700, Loss: 14669.2333984375\nEpoch 8800, Loss: 14571.388671875\nEpoch 8900, Loss: 14474.9833984375\nEpoch 9000, Loss: 14380.0078125\nEpoch 9100, Loss: 14286.4384765625\nEpoch 9200, Loss: 14194.2509765625\nEpoch 9300, Loss: 14103.4755859375\nEpoch 9400, Loss: 14014.1064453125\nEpoch 9500, Loss: 13926.0791015625\nEpoch 9600, Loss: 13839.384765625\nEpoch 9700, Loss: 13754.017578125\nEpoch 9800, Loss: 13669.9638671875\nEpoch 9900, Loss: 13587.2177734375\nEpoch 10000, Loss: 13505.76953125\nEpoch 10100, Loss: 13425.615234375\nEpoch 10200, Loss: 13346.74609375\nEpoch 10300, Loss: 13269.16015625\nEpoch 10400, Loss: 13192.8505859375\nEpoch 10500, Loss: 13117.814453125\nEpoch 10600, Loss: 13044.0380859375\nEpoch 10700, Loss: 12971.5234375\nEpoch 10800, Loss: 12900.2578125\nEpoch 10900, Loss: 12830.2353515625\nEpoch 11000, Loss: 12761.4501953125\nEpoch 11100, Loss: 12693.8876953125\nEpoch 11200, Loss: 12627.541015625\nEpoch 11300, Loss: 12562.396484375\nEpoch 11400, Loss: 12498.439453125\nEpoch 11500, Loss: 12435.658203125\nEpoch 11600, Loss: 12374.0478515625\nEpoch 11700, Loss: 12313.58984375\nEpoch 11800, Loss: 12254.2607421875\nEpoch 11900, Loss: 12196.0380859375\nEpoch 12000, Loss: 12138.9072265625\nEpoch 12100, Loss: 12082.8447265625\nEpoch 12200, Loss: 12027.8330078125\nEpoch 12300, Loss: 11973.8916015625\nEpoch 12400, Loss: 11921.0068359375\nEpoch 12500, Loss: 11869.11328125\nEpoch 12600, Loss: 11818.1962890625\nEpoch 12700, Loss: 11768.2373046875\nEpoch 12800, Loss: 11719.21875\nEpoch 12900, Loss: 11671.1279296875\nEpoch 13000, Loss: 11623.94921875\nEpoch 13100, Loss: 11577.6630859375\nEpoch 13200, Loss: 11532.2578125\nEpoch 13300, Loss: 11487.7265625\nEpoch 13400, Loss: 11444.048828125\nEpoch 13500, Loss: 11401.21484375\nEpoch 13600, Loss: 11359.21484375\nEpoch 13700, Loss: 11318.041015625\nEpoch 13800, Loss: 11277.671875\nEpoch 13900, Loss: 11238.1064453125\nEpoch 14000, Loss: 11199.3271484375\nEpoch 14100, Loss: 11161.330078125\nEpoch 14200, Loss: 11124.1015625\nEpoch 14300, Loss: 11087.6806640625\nEpoch 14400, Loss: 11052.072265625\nEpoch 14500, Loss: 11017.2109375\nEpoch 14600, Loss: 10983.0859375\nEpoch 14700, Loss: 10949.6884765625\nEpoch 14800, Loss: 10917.052734375\nEpoch 14900, Loss: 10885.146484375\nEpoch 15000, Loss: 10853.947265625\nEpoch 15100, Loss: 10823.4482421875\nEpoch 15200, Loss: 10793.64453125\nEpoch 15300, Loss: 10764.525390625\nEpoch 15400, Loss: 10736.09375\nEpoch 15500, Loss: 10708.357421875\nEpoch 15600, Loss: 10681.3046875\nEpoch 15700, Loss: 10654.91015625\nEpoch 15800, Loss: 10629.16796875\nEpoch 15900, Loss: 10604.08984375\nEpoch 16000, Loss: 10579.6708984375\nEpoch 16100, Loss: 10555.912109375\nEpoch 16200, Loss: 10532.7734375\nEpoch 16300, Loss: 10510.244140625\nEpoch 16400, Loss: 10488.3134765625\nEpoch 16500, Loss: 10466.970703125\nEpoch 16600, Loss: 10446.2080078125\nEpoch 16700, Loss: 10426.015625\nEpoch 16800, Loss: 10406.384765625\nEpoch 16900, Loss: 10387.3046875\nEpoch 17000, Loss: 10368.765625\nEpoch 17100, Loss: 10350.7568359375\nEpoch 17200, Loss: 10333.26953125\nEpoch 17300, Loss: 10316.29296875\nEpoch 17400, Loss: 10299.8193359375\nEpoch 17500, Loss: 10283.8408203125\nEpoch 17600, Loss: 10268.3505859375\nEpoch 17700, Loss: 10253.33984375\nEpoch 17800, Loss: 10238.8046875\nEpoch 17900, Loss: 10224.7353515625\nEpoch 18000, Loss: 10211.1318359375\nEpoch 18100, Loss: 10197.9833984375\nEpoch 18200, Loss: 10185.2880859375\nEpoch 18300, Loss: 10173.041015625\nEpoch 18400, Loss: 10161.23828125\nEpoch 18500, Loss: 10149.9013671875\nEpoch 18600, Loss: 10139.001953125\nEpoch 18700, Loss: 10128.5537109375\nEpoch 18800, Loss: 10118.5400390625\nEpoch 18900, Loss: 10108.9423828125\nEpoch 19000, Loss: 10099.7529296875\nEpoch 19100, Loss: 10090.96484375\nEpoch 19200, Loss: 10082.5732421875\nEpoch 19300, Loss: 10074.56640625\nEpoch 19400, Loss: 10066.9423828125\nEpoch 19500, Loss: 10059.6904296875\nEpoch 19600, Loss: 10052.8046875\nEpoch 19700, Loss: 10046.2763671875\nEpoch 19800, Loss: 10040.0966796875\nEpoch 19900, Loss: 10034.25390625\nEpoch 20000, Loss: 10028.7421875\nEpoch 20100, Loss: 10023.5791015625\nEpoch 20200, Loss: 10018.7451171875\nEpoch 20300, Loss: 10014.2138671875\nEpoch 20400, Loss: 10009.9755859375\nEpoch 20500, Loss: 10006.013671875\nEpoch 20600, Loss: 10002.3212890625\nEpoch 20700, Loss: 9998.8896484375\nEpoch 20800, Loss: 9995.703125\nEpoch 20900, Loss: 9992.7490234375\nEpoch 21000, Loss: 9990.015625\nEpoch 21100, Loss: 9987.4931640625\nEpoch 21200, Loss: 9985.1669921875\nEpoch 21300, Loss: 9983.0322265625\nEpoch 21400, Loss: 9981.0693359375\nEpoch 21500, Loss: 9979.2724609375\nEpoch 21600, Loss: 9977.62890625\nEpoch 21700, Loss: 9976.1279296875\nEpoch 21800, Loss: 9974.7587890625\nEpoch 21900, Loss: 9973.513671875\nEpoch 22000, Loss: 9972.380859375\nEpoch 22100, Loss: 9971.3525390625\nEpoch 22200, Loss: 9970.4189453125\nEpoch 22300, Loss: 9969.5751953125\nEpoch 22400, Loss: 9968.810546875\nEpoch 22500, Loss: 9968.1181640625\nEpoch 22600, Loss: 9967.494140625\nEpoch 22700, Loss: 9966.9306640625\nEpoch 22800, Loss: 9966.43359375\nEpoch 22900, Loss: 9965.990234375\nEpoch 23000, Loss: 9965.5927734375\nEpoch 23100, Loss: 9965.23828125\nEpoch 23200, Loss: 9964.9228515625\nEpoch 23300, Loss: 9964.640625\nEpoch 23400, Loss: 9964.390625\nEpoch 23500, Loss: 9964.166015625\nEpoch 23600, Loss: 9963.9677734375\nEpoch 23700, Loss: 9963.791015625\nEpoch 23800, Loss: 9963.6318359375\nEpoch 23900, Loss: 9963.490234375\nEpoch 24000, Loss: 9963.3603515625\nEpoch 24100, Loss: 9963.2421875\nEpoch 24200, Loss: 9963.1328125\nEpoch 24300, Loss: 9963.0302734375\nEpoch 24400, Loss: 9962.9326171875\nEpoch 24500, Loss: 9962.8388671875\nEpoch 24600, Loss: 9962.7490234375\nEpoch 24700, Loss: 9962.66015625\nEpoch 24800, Loss: 9962.572265625\nEpoch 24900, Loss: 9962.482421875\nEpoch 25000, Loss: 9962.3935546875\nEpoch 25100, Loss: 9962.30078125\nEpoch 25200, Loss: 9962.2080078125\nEpoch 25300, Loss: 9962.11328125\nEpoch 25400, Loss: 9962.0146484375\nEpoch 25500, Loss: 9961.916015625\nEpoch 25600, Loss: 9961.8125\nEpoch 25700, Loss: 9961.708984375\nEpoch 25800, Loss: 9961.5986328125\nEpoch 25900, Loss: 9961.4853515625\nEpoch 26000, Loss: 9961.3681640625\nEpoch 26100, Loss: 9961.248046875\nEpoch 26200, Loss: 9961.12109375\nEpoch 26300, Loss: 9960.9921875\nEpoch 26400, Loss: 9960.8564453125\nEpoch 26500, Loss: 9960.716796875\nEpoch 26600, Loss: 9960.5732421875\nEpoch 26700, Loss: 9960.423828125\nEpoch 26800, Loss: 9960.2705078125\nEpoch 26900, Loss: 9960.111328125\nEpoch 27000, Loss: 9959.9482421875\nEpoch 27100, Loss: 9959.7783203125\nEpoch 27200, Loss: 9959.60546875\nEpoch 27300, Loss: 9959.42578125\nEpoch 27400, Loss: 9959.2421875\nEpoch 27500, Loss: 9959.052734375\nEpoch 27600, Loss: 9958.859375\nEpoch 27700, Loss: 9958.6591796875\nEpoch 27800, Loss: 9958.484375\nEpoch 27900, Loss: 9958.3115234375\nEpoch 28000, Loss: 9958.1357421875\nEpoch 28100, Loss: 9957.955078125\nEpoch 28200, Loss: 9957.7939453125\nEpoch 28300, Loss: 9957.654296875\nEpoch 28400, Loss: 9957.5166015625\nEpoch 28500, Loss: 9957.375\nEpoch 28600, Loss: 9957.228515625\nEpoch 28700, Loss: 9957.078125\nEpoch 28800, Loss: 9956.923828125\nEpoch 28900, Loss: 9956.765625\nEpoch 29000, Loss: 9956.603515625\nEpoch 29100, Loss: 9956.4384765625\nEpoch 29200, Loss: 9956.2685546875\nEpoch 29300, Loss: 9956.095703125\nEpoch 29400, Loss: 9955.9189453125\nEpoch 29500, Loss: 9955.7392578125\nEpoch 29600, Loss: 9955.55859375\nEpoch 29700, Loss: 9955.4072265625\nEpoch 29800, Loss: 9955.2822265625\nEpoch 29900, Loss: 9955.1572265625\nEpoch 30000, Loss: 9955.048828125\nEpoch 30100, Loss: 9954.9697265625\nEpoch 30200, Loss: 9954.892578125\nEpoch 30300, Loss: 9954.8134765625\nEpoch 30400, Loss: 9954.73046875\nEpoch 30500, Loss: 9954.6484375\nEpoch 30600, Loss: 9954.5615234375\nEpoch 30700, Loss: 9954.474609375\nEpoch 30800, Loss: 9954.3828125\nEpoch 30900, Loss: 9954.2900390625\nEpoch 31000, Loss: 9954.1962890625\nEpoch 31100, Loss: 9954.09765625\nEpoch 31200, Loss: 9953.9990234375\nEpoch 31300, Loss: 9953.8974609375\nEpoch 31400, Loss: 9953.794921875\nEpoch 31500, Loss: 9953.6904296875\nEpoch 31600, Loss: 9953.5908203125\nEpoch 31700, Loss: 9953.5361328125\nEpoch 31800, Loss: 9953.486328125\nEpoch 31900, Loss: 9953.435546875\nEpoch 32000, Loss: 9953.3818359375\nEpoch 32100, Loss: 9953.33984375\nEpoch 32200, Loss: 9953.3203125\nEpoch 32300, Loss: 9953.3056640625\nEpoch 32400, Loss: 9953.2919921875\nEpoch 32500, Loss: 9953.28125\nEpoch 32600, Loss: 9953.27734375\nEpoch 32700, Loss: 9953.2744140625\nEpoch 32800, Loss: 9953.2744140625\nEpoch 32900, Loss: 9953.2744140625\nEpoch 33000, Loss: 9953.2734375\nEpoch 33100, Loss: 9953.2734375\nEpoch 33200, Loss: 9953.2734375\nEpoch 33300, Loss: 9953.2724609375\nEpoch 33400, Loss: 9953.2724609375\nEpoch 33500, Loss: 9953.2724609375\nEpoch 33600, Loss: 9953.2724609375\nEpoch 33700, Loss: 9953.2724609375\nEpoch 33800, Loss: 9953.2724609375\nEpoch 33900, Loss: 9953.2724609375\nEpoch 34000, Loss: 9953.271484375\nEpoch 34100, Loss: 9953.271484375\nEpoch 34200, Loss: 9953.271484375\nEpoch 34300, Loss: 9953.271484375\nEpoch 34400, Loss: 9953.271484375\nEpoch 34500, Loss: 9953.2724609375\nEpoch 34600, Loss: 9953.271484375\nEpoch 34700, Loss: 9953.2724609375\nEpoch 34800, Loss: 9953.2724609375\nEpoch 34900, Loss: 9953.2724609375\nEpoch 35000, Loss: 9953.2724609375\nEpoch 35100, Loss: 9953.2724609375\nEpoch 35200, Loss: 9953.2724609375\nEpoch 35300, Loss: 9953.2724609375\nEpoch 35400, Loss: 9953.2724609375\nEpoch 35500, Loss: 9953.2724609375\nEpoch 35600, Loss: 9953.2724609375\nEpoch 35700, Loss: 9953.2724609375\nEpoch 35800, Loss: 9953.2724609375\nEpoch 35900, Loss: 9953.2724609375\nEpoch 36000, Loss: 9953.2724609375\nEpoch 36100, Loss: 9953.2724609375\nEpoch 36200, Loss: 9953.2724609375\nEpoch 36300, Loss: 9953.2724609375\nEpoch 36400, Loss: 9953.2724609375\nEpoch 36500, Loss: 9953.2724609375\nEpoch 36600, Loss: 9953.2724609375\nEpoch 36700, Loss: 9953.2724609375\nEpoch 36800, Loss: 9953.2724609375\nEpoch 36900, Loss: 9953.2724609375\nEpoch 37000, Loss: 9953.2724609375\nEpoch 37100, Loss: 9953.2724609375\nEpoch 37200, Loss: 9953.2724609375\nEpoch 37300, Loss: 9953.2724609375\nEpoch 37400, Loss: 9953.2724609375\nEpoch 37500, Loss: 9953.2724609375\nEpoch 37600, Loss: 9953.2724609375\nEpoch 37700, Loss: 9953.2724609375\nEpoch 37800, Loss: 9953.2724609375\nEpoch 37900, Loss: 9953.2724609375\nEpoch 38000, Loss: 9953.2724609375\nEpoch 38100, Loss: 9953.2724609375\nEpoch 38200, Loss: 9953.2724609375\nEpoch 38300, Loss: 9953.2724609375\nEpoch 38400, Loss: 9953.2724609375\nEpoch 38500, Loss: 9953.2724609375\nEpoch 38600, Loss: 9953.2724609375\nEpoch 38700, Loss: 9953.2724609375\nEpoch 38800, Loss: 9953.2724609375\nEpoch 38900, Loss: 9953.2724609375\nEpoch 39000, Loss: 9953.2724609375\nEpoch 39100, Loss: 9953.2724609375\nEpoch 39200, Loss: 9953.2724609375\nEpoch 39300, Loss: 9953.2724609375\nEpoch 39400, Loss: 9953.2724609375\nEpoch 39500, Loss: 9953.2724609375\nEpoch 39600, Loss: 9953.2724609375\nEpoch 39700, Loss: 9953.2724609375\nEpoch 39800, Loss: 9953.2724609375\nEpoch 39900, Loss: 9953.2724609375\nEpoch 40000, Loss: 9953.2724609375\nEpoch 40100, Loss: 9953.2724609375\nEpoch 40200, Loss: 9953.2724609375\nEpoch 40300, Loss: 9953.2724609375\nEpoch 40400, Loss: 9953.2724609375\nEpoch 40500, Loss: 9953.2724609375\nEpoch 40600, Loss: 9953.2724609375\nEpoch 40700, Loss: 9953.2724609375\nEpoch 40800, Loss: 9953.271484375\nEpoch 40900, Loss: 9953.2724609375\nEpoch 41000, Loss: 9953.2724609375\nEpoch 41100, Loss: 9953.2724609375\nEpoch 41200, Loss: 9953.271484375\nEpoch 41300, Loss: 9953.2734375\nEpoch 41400, Loss: 9953.2724609375\nEpoch 41500, Loss: 9953.2724609375\nEpoch 41600, Loss: 9953.2724609375\nEpoch 41700, Loss: 9953.2724609375\nEpoch 41800, Loss: 9953.2724609375\nEpoch 41900, Loss: 9953.2724609375\nEpoch 42000, Loss: 9953.2724609375\nEpoch 42100, Loss: 9953.2724609375\nEpoch 42200, Loss: 9953.2724609375\nEpoch 42300, Loss: 9953.2724609375\nEpoch 42400, Loss: 9953.2724609375\nEpoch 42500, Loss: 9953.2724609375\nEpoch 42600, Loss: 9953.2734375\nEpoch 42700, Loss: 9953.2724609375\nEpoch 42800, Loss: 9953.271484375\nEpoch 42900, Loss: 9953.2724609375\nEpoch 43000, Loss: 9953.2724609375\nEpoch 43100, Loss: 9953.2724609375\nEpoch 43200, Loss: 9953.2724609375\nEpoch 43300, Loss: 9953.2724609375\nEpoch 43400, Loss: 9953.2724609375\nEpoch 43500, Loss: 9953.2724609375\nEpoch 43600, Loss: 9953.2724609375\nEpoch 43700, Loss: 9953.2724609375\nEpoch 43800, Loss: 9953.2724609375\nEpoch 43900, Loss: 9953.2724609375\nEpoch 44000, Loss: 9953.2724609375\nEpoch 44100, Loss: 9953.2724609375\nEpoch 44200, Loss: 9953.2724609375\nEpoch 44300, Loss: 9953.2724609375\nEpoch 44400, Loss: 9953.2724609375\nEpoch 44500, Loss: 9953.271484375\nEpoch 44600, Loss: 9953.2724609375\nEpoch 44700, Loss: 9953.271484375\nEpoch 44800, Loss: 9953.2724609375\nEpoch 44900, Loss: 9953.2724609375\nEpoch 45000, Loss: 9953.2724609375\nEpoch 45100, Loss: 9953.2724609375\nEpoch 45200, Loss: 9953.2724609375\nEpoch 45300, Loss: 9953.2734375\nEpoch 45400, Loss: 9953.2724609375\nEpoch 45500, Loss: 9953.2724609375\nEpoch 45600, Loss: 9953.2724609375\nEpoch 45700, Loss: 9953.2724609375\nEpoch 45800, Loss: 9953.2724609375\nEpoch 45900, Loss: 9953.2724609375\nEpoch 46000, Loss: 9953.2724609375\nEpoch 46100, Loss: 9953.2724609375\nEpoch 46200, Loss: 9953.2724609375\nEpoch 46300, Loss: 9953.2724609375\nEpoch 46400, Loss: 9953.2724609375\nEpoch 46500, Loss: 9953.2724609375\nEpoch 46600, Loss: 9953.2724609375\nEpoch 46700, Loss: 9953.2724609375\nEpoch 46800, Loss: 9953.2724609375\nEpoch 46900, Loss: 9953.2724609375\nEpoch 47000, Loss: 9953.2724609375\nEpoch 47100, Loss: 9953.271484375\nEpoch 47200, Loss: 9953.2724609375\nEpoch 47300, Loss: 9953.271484375\nEpoch 47400, Loss: 9953.2724609375\nEpoch 47500, Loss: 9953.271484375\nEpoch 47600, Loss: 9953.2724609375\nEpoch 47700, Loss: 9953.2724609375\nEpoch 47800, Loss: 9953.2724609375\nEpoch 47900, Loss: 9953.2724609375\nEpoch 48000, Loss: 9953.2724609375\nEpoch 48100, Loss: 9953.2724609375\nEpoch 48200, Loss: 9953.2724609375\nEpoch 48300, Loss: 9953.2724609375\nEpoch 48400, Loss: 9953.2724609375\nEpoch 48500, Loss: 9953.2724609375\nEpoch 48600, Loss: 9953.2724609375\nEpoch 48700, Loss: 9953.2724609375\nEpoch 48800, Loss: 9953.2724609375\nEpoch 48900, Loss: 9953.2724609375\nEpoch 49000, Loss: 9953.2724609375\nEpoch 49100, Loss: 9953.2724609375\nEpoch 49200, Loss: 9953.2724609375\nEpoch 49300, Loss: 9953.2724609375\nEpoch 49400, Loss: 9953.2734375\nEpoch 49500, Loss: 9953.2724609375\nEpoch 49600, Loss: 9953.2724609375\nEpoch 49700, Loss: 9953.2724609375\nEpoch 49800, Loss: 9953.2724609375\nEpoch 49900, Loss: 9953.2724609375\nEpoch 0, Loss: 63828.96875\nEpoch 100, Loss: 60922.81640625\nEpoch 200, Loss: 58205.51953125\nEpoch 300, Loss: 55663.38671875\nEpoch 400, Loss: 53283.2109375\nEpoch 500, Loss: 51054.04296875\nEpoch 600, Loss: 48966.82421875\nEpoch 700, Loss: 47013.23828125\nEpoch 800, Loss: 45185.5703125\nEpoch 900, Loss: 43476.62109375\nEpoch 1000, Loss: 41879.5625\nEpoch 1100, Loss: 40387.6796875\nEpoch 1200, Loss: 38994.58203125\nEpoch 1300, Loss: 37694.12109375\nEpoch 1400, Loss: 36480.37890625\nEpoch 1500, Loss: 35347.78515625\nEpoch 1600, Loss: 34290.7109375\nEpoch 1700, Loss: 33303.8984375\nEpoch 1800, Loss: 32382.353515625\nEpoch 1900, Loss: 31521.38671875\nEpoch 2000, Loss: 30716.630859375\nEpoch 2100, Loss: 29964.00390625\nEpoch 2200, Loss: 29259.83203125\nEpoch 2300, Loss: 28600.513671875\nEpoch 2400, Loss: 27982.912109375\nEpoch 2500, Loss: 27403.896484375\nEpoch 2600, Loss: 26860.5390625\nEpoch 2700, Loss: 26350.205078125\nEpoch 2800, Loss: 25870.447265625\nEpoch 2900, Loss: 25418.978515625\nEpoch 3000, Loss: 24993.671875\nEpoch 3100, Loss: 24592.53515625\nEpoch 3200, Loss: 24213.6953125\nEpoch 3300, Loss: 23855.423828125\nEpoch 3400, Loss: 23516.087890625\nEpoch 3500, Loss: 23194.16015625\nEpoch 3600, Loss: 22888.16015625\nEpoch 3700, Loss: 22596.63671875\nEpoch 3800, Loss: 22318.294921875\nEpoch 3900, Loss: 22051.943359375\nEpoch 4000, Loss: 21796.484375\nEpoch 4100, Loss: 21550.9765625\nEpoch 4200, Loss: 21314.494140625\nEpoch 4300, Loss: 21086.16015625\nEpoch 4400, Loss: 20865.244140625\nEpoch 4500, Loss: 20651.171875\nEpoch 4600, Loss: 20443.29296875\nEpoch 4700, Loss: 20241.09375\nEpoch 4800, Loss: 20044.138671875\nEpoch 4900, Loss: 19852.048828125\nEpoch 5000, Loss: 19664.4921875\nEpoch 5100, Loss: 19481.1875\nEpoch 5200, Loss: 19301.8828125\nEpoch 5300, Loss: 19126.361328125\nEpoch 5400, Loss: 18954.431640625\nEpoch 5500, Loss: 18785.921875\nEpoch 5600, Loss: 18620.68359375\nEpoch 5700, Loss: 18458.58203125\nEpoch 5800, Loss: 18299.490234375\nEpoch 5900, Loss: 18143.298828125\nEpoch 6000, Loss: 17989.91796875\nEpoch 6100, Loss: 17839.310546875\nEpoch 6200, Loss: 17691.32421875\nEpoch 6300, Loss: 17545.87890625\nEpoch 6400, Loss: 17402.90234375\nEpoch 6500, Loss: 17262.330078125\nEpoch 6600, Loss: 17124.10546875\nEpoch 6700, Loss: 16988.162109375\nEpoch 6800, Loss: 16854.455078125\nEpoch 6900, Loss: 16722.927734375\nEpoch 7000, Loss: 16593.537109375\nEpoch 7100, Loss: 16466.232421875\nEpoch 7200, Loss: 16341.0205078125\nEpoch 7300, Loss: 16217.837890625\nEpoch 7400, Loss: 16096.705078125\nEpoch 7500, Loss: 15977.5029296875\nEpoch 7600, Loss: 15860.1689453125\nEpoch 7700, Loss: 15744.6572265625\nEpoch 7800, Loss: 15630.9296875\nEpoch 7900, Loss: 15518.9443359375\nEpoch 8000, Loss: 15408.66796875\nEpoch 8100, Loss: 15300.0703125\nEpoch 8200, Loss: 15193.1103515625\nEpoch 8300, Loss: 15087.7587890625\nEpoch 8400, Loss: 14983.974609375\nEpoch 8500, Loss: 14881.7392578125\nEpoch 8600, Loss: 14781.0166015625\nEpoch 8700, Loss: 14681.783203125\nEpoch 8800, Loss: 14584.017578125\nEpoch 8900, Loss: 14487.6943359375\nEpoch 9000, Loss: 14392.7958984375\nEpoch 9100, Loss: 14299.2998046875\nEpoch 9200, Loss: 14207.1865234375\nEpoch 9300, Loss: 14116.4990234375\nEpoch 9400, Loss: 14027.216796875\nEpoch 9500, Loss: 13939.275390625\nEpoch 9600, Loss: 13852.6669921875\nEpoch 9700, Loss: 13767.3818359375\nEpoch 9800, Loss: 13683.408203125\nEpoch 9900, Loss: 13600.7412109375\nEpoch 10000, Loss: 13519.3701171875\nEpoch 10100, Loss: 13439.291015625\nEpoch 10200, Loss: 13360.4990234375\nEpoch 10300, Loss: 13282.9912109375\nEpoch 10400, Loss: 13206.75390625\nEpoch 10500, Loss: 13131.7880859375\nEpoch 10600, Loss: 13058.08203125\nEpoch 10700, Loss: 12985.6337890625\nEpoch 10800, Loss: 12914.4345703125\nEpoch 10900, Loss: 12844.4794921875\nEpoch 11000, Loss: 12775.7578125\nEpoch 11100, Loss: 12708.2587890625\nEpoch 11200, Loss: 12641.97265625\nEpoch 11300, Loss: 12576.88671875\nEpoch 11400, Loss: 12512.990234375\nEpoch 11500, Loss: 12450.2685546875\nEpoch 11600, Loss: 12388.7177734375\nEpoch 11700, Loss: 12328.3203125\nEpoch 11800, Loss: 12269.046875\nEpoch 11900, Loss: 12210.8818359375\nEpoch 12000, Loss: 12153.8037109375\nEpoch 12100, Loss: 12097.7919921875\nEpoch 12200, Loss: 12042.833984375\nEpoch 12300, Loss: 11988.9580078125\nEpoch 12400, Loss: 11936.1376953125\nEpoch 12500, Loss: 11884.30859375\nEpoch 12600, Loss: 11833.4541015625\nEpoch 12700, Loss: 11783.556640625\nEpoch 12800, Loss: 11734.595703125\nEpoch 12900, Loss: 11686.5595703125\nEpoch 13000, Loss: 11639.43359375\nEpoch 13100, Loss: 11593.19921875\nEpoch 13200, Loss: 11547.8447265625\nEpoch 13300, Loss: 11503.36328125\nEpoch 13400, Loss: 11459.7294921875\nEpoch 13500, Loss: 11416.9423828125\nEpoch 13600, Loss: 11374.9921875\nEpoch 13700, Loss: 11333.859375\nEpoch 13800, Loss: 11293.53125\nEpoch 13900, Loss: 11254.0048828125\nEpoch 14000, Loss: 11215.267578125\nEpoch 14100, Loss: 11177.3076171875\nEpoch 14200, Loss: 11140.115234375\nEpoch 14300, Loss: 11103.7548828125\nEpoch 14400, Loss: 11068.2060546875\nEpoch 14500, Loss: 11033.404296875\nEpoch 14600, Loss: 10999.333984375\nEpoch 14700, Loss: 10965.9892578125\nEpoch 14800, Loss: 10933.4228515625\nEpoch 14900, Loss: 10901.576171875\nEpoch 15000, Loss: 10870.439453125\nEpoch 15100, Loss: 10840.0009765625\nEpoch 15200, Loss: 10810.2490234375\nEpoch 15300, Loss: 10781.1826171875\nEpoch 15400, Loss: 10752.8056640625\nEpoch 15500, Loss: 10725.158203125\nEpoch 15600, Loss: 10698.1982421875\nEpoch 15700, Loss: 10671.896484375\nEpoch 15800, Loss: 10646.2470703125\nEpoch 15900, Loss: 10621.240234375\nEpoch 16000, Loss: 10596.880859375\nEpoch 16100, Loss: 10573.185546875\nEpoch 16200, Loss: 10550.111328125\nEpoch 16300, Loss: 10527.640625\nEpoch 16400, Loss: 10505.7685546875\nEpoch 16500, Loss: 10484.4814453125\nEpoch 16600, Loss: 10463.7724609375\nEpoch 16700, Loss: 10443.6298828125\nEpoch 16800, Loss: 10424.046875\nEpoch 16900, Loss: 10405.013671875\nEpoch 17000, Loss: 10386.5185546875\nEpoch 17100, Loss: 10368.548828125\nEpoch 17200, Loss: 10351.099609375\nEpoch 17300, Loss: 10334.1572265625\nEpoch 17400, Loss: 10317.7177734375\nEpoch 17500, Loss: 10301.7705078125\nEpoch 17600, Loss: 10286.3095703125\nEpoch 17700, Loss: 10271.3291015625\nEpoch 17800, Loss: 10256.818359375\nEpoch 17900, Loss: 10242.7763671875\nEpoch 18000, Loss: 10229.1943359375\nEpoch 18100, Loss: 10216.083984375\nEpoch 18200, Loss: 10203.4580078125\nEpoch 18300, Loss: 10191.2822265625\nEpoch 18400, Loss: 10179.552734375\nEpoch 18500, Loss: 10168.2900390625\nEpoch 18600, Loss: 10157.4736328125\nEpoch 18700, Loss: 10147.0849609375\nEpoch 18800, Loss: 10137.1142578125\nEpoch 18900, Loss: 10127.5556640625\nEpoch 19000, Loss: 10118.404296875\nEpoch 19100, Loss: 10109.6513671875\nEpoch 19200, Loss: 10101.29296875\nEpoch 19300, Loss: 10093.3212890625\nEpoch 19400, Loss: 10085.7255859375\nEpoch 19500, Loss: 10078.5029296875\nEpoch 19600, Loss: 10071.6630859375\nEpoch 19700, Loss: 10065.1865234375\nEpoch 19800, Loss: 10059.0576171875\nEpoch 19900, Loss: 10053.265625\nEpoch 20000, Loss: 10047.8134765625\nEpoch 20100, Loss: 10042.703125\nEpoch 20200, Loss: 10037.9140625\nEpoch 20300, Loss: 10033.4326171875\nEpoch 20400, Loss: 10029.23828125\nEpoch 20500, Loss: 10025.322265625\nEpoch 20600, Loss: 10021.6689453125\nEpoch 20700, Loss: 10018.26953125\nEpoch 20800, Loss: 10015.11328125\nEpoch 20900, Loss: 10012.1875\nEpoch 21000, Loss: 10009.48046875\nEpoch 21100, Loss: 10006.982421875\nEpoch 21200, Loss: 10004.6796875\nEpoch 21300, Loss: 10002.5634765625\nEpoch 21400, Loss: 10000.62109375\nEpoch 21500, Loss: 9998.8408203125\nEpoch 21600, Loss: 9997.2119140625\nEpoch 21700, Loss: 9995.724609375\nEpoch 21800, Loss: 9994.3701171875\nEpoch 21900, Loss: 9993.1337890625\nEpoch 22000, Loss: 9992.009765625\nEpoch 22100, Loss: 9990.98828125\nEpoch 22200, Loss: 9990.0693359375\nEpoch 22300, Loss: 9989.24609375\nEpoch 22400, Loss: 9988.4990234375\nEpoch 22500, Loss: 9987.8251953125\nEpoch 22600, Loss: 9987.2177734375\nEpoch 22700, Loss: 9986.66796875\nEpoch 22800, Loss: 9986.1728515625\nEpoch 22900, Loss: 9985.7275390625\nEpoch 23000, Loss: 9985.3271484375\nEpoch 23100, Loss: 9984.9677734375\nEpoch 23200, Loss: 9984.64453125\nEpoch 23300, Loss: 9984.3544921875\nEpoch 23400, Loss: 9984.0908203125\nEpoch 23500, Loss: 9983.857421875\nEpoch 23600, Loss: 9983.64453125\nEpoch 23700, Loss: 9983.44921875\nEpoch 23800, Loss: 9983.271484375\nEpoch 23900, Loss: 9983.1083984375\nEpoch 24000, Loss: 9982.9560546875\nEpoch 24100, Loss: 9982.8134765625\nEpoch 24200, Loss: 9982.677734375\nEpoch 24300, Loss: 9982.5478515625\nEpoch 24400, Loss: 9982.421875\nEpoch 24500, Loss: 9982.298828125\nEpoch 24600, Loss: 9982.1748046875\nEpoch 24700, Loss: 9982.0517578125\nEpoch 24800, Loss: 9981.927734375\nEpoch 24900, Loss: 9981.80078125\nEpoch 25000, Loss: 9981.673828125\nEpoch 25100, Loss: 9981.544921875\nEpoch 25200, Loss: 9981.412109375\nEpoch 25300, Loss: 9981.275390625\nEpoch 25400, Loss: 9981.1357421875\nEpoch 25500, Loss: 9980.990234375\nEpoch 25600, Loss: 9980.8408203125\nEpoch 25700, Loss: 9980.6845703125\nEpoch 25800, Loss: 9980.5234375\nEpoch 25900, Loss: 9980.3583984375\nEpoch 26000, Loss: 9980.1865234375\nEpoch 26100, Loss: 9980.0078125\nEpoch 26200, Loss: 9979.8251953125\nEpoch 26300, Loss: 9979.634765625\nEpoch 26400, Loss: 9979.4384765625\nEpoch 26500, Loss: 9979.2373046875\nEpoch 26600, Loss: 9979.02734375\nEpoch 26700, Loss: 9978.8115234375\nEpoch 26800, Loss: 9978.58984375\nEpoch 26900, Loss: 9978.3623046875\nEpoch 27000, Loss: 9978.126953125\nEpoch 27100, Loss: 9977.8857421875\nEpoch 27200, Loss: 9977.64453125\nEpoch 27300, Loss: 9977.4326171875\nEpoch 27400, Loss: 9977.21875\nEpoch 27500, Loss: 9977.001953125\nEpoch 27600, Loss: 9976.77734375\nEpoch 27700, Loss: 9976.5751953125\nEpoch 27800, Loss: 9976.3984375\nEpoch 27900, Loss: 9976.228515625\nEpoch 28000, Loss: 9976.0537109375\nEpoch 28100, Loss: 9975.8720703125\nEpoch 28200, Loss: 9975.6865234375\nEpoch 28300, Loss: 9975.494140625\nEpoch 28400, Loss: 9975.2998046875\nEpoch 28500, Loss: 9975.099609375\nEpoch 28600, Loss: 9974.8935546875\nEpoch 28700, Loss: 9974.6806640625\nEpoch 28800, Loss: 9974.4658203125\nEpoch 28900, Loss: 9974.24609375\nEpoch 29000, Loss: 9974.0234375\nEpoch 29100, Loss: 9973.7958984375\nEpoch 29200, Loss: 9973.595703125\nEpoch 29300, Loss: 9973.43359375\nEpoch 29400, Loss: 9973.2783203125\nEpoch 29500, Loss: 9973.130859375\nEpoch 29600, Loss: 9973.0263671875\nEpoch 29700, Loss: 9972.9306640625\nEpoch 29800, Loss: 9972.8310546875\nEpoch 29900, Loss: 9972.728515625\nEpoch 30000, Loss: 9972.6259765625\nEpoch 30100, Loss: 9972.5185546875\nEpoch 30200, Loss: 9972.4072265625\nEpoch 30300, Loss: 9972.2939453125\nEpoch 30400, Loss: 9972.177734375\nEpoch 30500, Loss: 9972.0595703125\nEpoch 30600, Loss: 9971.9384765625\nEpoch 30700, Loss: 9971.8134765625\nEpoch 30800, Loss: 9971.6865234375\nEpoch 30900, Loss: 9971.5576171875\nEpoch 31000, Loss: 9971.427734375\nEpoch 31100, Loss: 9971.2958984375\nEpoch 31200, Loss: 9971.2060546875\nEpoch 31300, Loss: 9971.142578125\nEpoch 31400, Loss: 9971.078125\nEpoch 31500, Loss: 9971.0126953125\nEpoch 31600, Loss: 9970.9521484375\nEpoch 31700, Loss: 9970.916015625\nEpoch 31800, Loss: 9970.8955078125\nEpoch 31900, Loss: 9970.8779296875\nEpoch 32000, Loss: 9970.861328125\nEpoch 32100, Loss: 9970.853515625\nEpoch 32200, Loss: 9970.84765625\nEpoch 32300, Loss: 9970.845703125\nEpoch 32400, Loss: 9970.8427734375\nEpoch 32500, Loss: 9970.841796875\nEpoch 32600, Loss: 9970.841796875\nEpoch 32700, Loss: 9970.8408203125\nEpoch 32800, Loss: 9970.841796875\nEpoch 32900, Loss: 9970.8408203125\nEpoch 33000, Loss: 9970.8408203125\nEpoch 33100, Loss: 9970.83984375\nEpoch 33200, Loss: 9970.8388671875\nEpoch 33300, Loss: 9970.8408203125\nEpoch 33400, Loss: 9970.83984375\nEpoch 33500, Loss: 9970.83984375\nEpoch 33600, Loss: 9970.8388671875\nEpoch 33700, Loss: 9970.83984375\nEpoch 33800, Loss: 9970.8388671875\nEpoch 33900, Loss: 9970.83984375\nEpoch 34000, Loss: 9970.8388671875\nEpoch 34100, Loss: 9970.83984375\nEpoch 34200, Loss: 9970.8388671875\nEpoch 34300, Loss: 9970.8388671875\nEpoch 34400, Loss: 9970.83984375\nEpoch 34500, Loss: 9970.83984375\nEpoch 34600, Loss: 9970.83984375\nEpoch 34700, Loss: 9970.83984375\nEpoch 34800, Loss: 9970.8388671875\nEpoch 34900, Loss: 9970.83984375\nEpoch 35000, Loss: 9970.83984375\nEpoch 35100, Loss: 9970.8388671875\nEpoch 35200, Loss: 9970.83984375\nEpoch 35300, Loss: 9970.8388671875\nEpoch 35400, Loss: 9970.8388671875\nEpoch 35500, Loss: 9970.83984375\nEpoch 35600, Loss: 9970.8388671875\nEpoch 35700, Loss: 9970.83984375\nEpoch 35800, Loss: 9970.83984375\nEpoch 35900, Loss: 9970.83984375\nEpoch 36000, Loss: 9970.837890625\nEpoch 36100, Loss: 9970.8388671875\nEpoch 36200, Loss: 9970.8388671875\nEpoch 36300, Loss: 9970.8388671875\nEpoch 36400, Loss: 9970.8388671875\nEpoch 36500, Loss: 9970.83984375\nEpoch 36600, Loss: 9970.83984375\nEpoch 36700, Loss: 9970.8388671875\nEpoch 36800, Loss: 9970.8388671875\nEpoch 36900, Loss: 9970.83984375\nEpoch 37000, Loss: 9970.8388671875\nEpoch 37100, Loss: 9970.8388671875\nEpoch 37200, Loss: 9970.8388671875\nEpoch 37300, Loss: 9970.83984375\nEpoch 37400, Loss: 9970.83984375\nEpoch 37500, Loss: 9970.83984375\nEpoch 37600, Loss: 9970.8388671875\nEpoch 37700, Loss: 9970.83984375\nEpoch 37800, Loss: 9970.83984375\nEpoch 37900, Loss: 9970.8388671875\nEpoch 38000, Loss: 9970.8388671875\nEpoch 38100, Loss: 9970.8388671875\nEpoch 38200, Loss: 9970.83984375\nEpoch 38300, Loss: 9970.8388671875\nEpoch 38400, Loss: 9970.8388671875\nEpoch 38500, Loss: 9970.83984375\nEpoch 38600, Loss: 9970.8388671875\nEpoch 38700, Loss: 9970.8388671875\nEpoch 38800, Loss: 9970.83984375\nEpoch 38900, Loss: 9970.8408203125\nEpoch 39000, Loss: 9970.8388671875\nEpoch 39100, Loss: 9970.83984375\nEpoch 39200, Loss: 9970.837890625\nEpoch 39300, Loss: 9970.8388671875\nEpoch 39400, Loss: 9970.8388671875\nEpoch 39500, Loss: 9970.8388671875\nEpoch 39600, Loss: 9970.83984375\nEpoch 39700, Loss: 9970.8388671875\nEpoch 39800, Loss: 9970.8388671875\nEpoch 39900, Loss: 9970.8388671875\nEpoch 40000, Loss: 9970.8388671875\nEpoch 40100, Loss: 9970.8388671875\nEpoch 40200, Loss: 9970.8388671875\nEpoch 40300, Loss: 9970.83984375\nEpoch 40400, Loss: 9970.83984375\nEpoch 40500, Loss: 9970.83984375\nEpoch 40600, Loss: 9970.8388671875\nEpoch 40700, Loss: 9970.8388671875\nEpoch 40800, Loss: 9970.8388671875\nEpoch 40900, Loss: 9970.8388671875\nEpoch 41000, Loss: 9970.8388671875\nEpoch 41100, Loss: 9970.83984375\nEpoch 41200, Loss: 9970.83984375\nEpoch 41300, Loss: 9970.83984375\nEpoch 41400, Loss: 9970.8388671875\nEpoch 41500, Loss: 9970.8388671875\nEpoch 41600, Loss: 9970.837890625\nEpoch 41700, Loss: 9970.83984375\nEpoch 41800, Loss: 9970.8388671875\nEpoch 41900, Loss: 9970.8388671875\nEpoch 42000, Loss: 9970.8388671875\nEpoch 42100, Loss: 9970.83984375\nEpoch 42200, Loss: 9970.8388671875\nEpoch 42300, Loss: 9970.83984375\nEpoch 42400, Loss: 9970.8388671875\nEpoch 42500, Loss: 9970.8388671875\nEpoch 42600, Loss: 9970.8388671875\nEpoch 42700, Loss: 9970.83984375\nEpoch 42800, Loss: 9970.83984375\nEpoch 42900, Loss: 9970.8388671875\nEpoch 43000, Loss: 9970.83984375\nEpoch 43100, Loss: 9970.8388671875\nEpoch 43200, Loss: 9970.8388671875\nEpoch 43300, Loss: 9970.837890625\nEpoch 43400, Loss: 9970.837890625\nEpoch 43500, Loss: 9970.83984375\nEpoch 43600, Loss: 9970.8388671875\nEpoch 43700, Loss: 9970.8388671875\nEpoch 43800, Loss: 9970.8388671875\nEpoch 43900, Loss: 9970.83984375\nEpoch 44000, Loss: 9970.8388671875\nEpoch 44100, Loss: 9970.83984375\nEpoch 44200, Loss: 9970.8388671875\nEpoch 44300, Loss: 9970.8388671875\nEpoch 44400, Loss: 9970.83984375\nEpoch 44500, Loss: 9970.8388671875\nEpoch 44600, Loss: 9970.83984375\nEpoch 44700, Loss: 9970.83984375\nEpoch 44800, Loss: 9970.8388671875\nEpoch 44900, Loss: 9970.8388671875\nEpoch 45000, Loss: 9970.83984375\nEpoch 45100, Loss: 9970.8388671875\nEpoch 45200, Loss: 9970.83984375\nEpoch 45300, Loss: 9970.83984375\nEpoch 45400, Loss: 9970.83984375\nEpoch 45500, Loss: 9970.8388671875\nEpoch 45600, Loss: 9970.8388671875\nEpoch 45700, Loss: 9970.8388671875\nEpoch 45800, Loss: 9970.8388671875\nEpoch 45900, Loss: 9970.8388671875\nEpoch 46000, Loss: 9970.8388671875\nEpoch 46100, Loss: 9970.83984375\nEpoch 46200, Loss: 9970.83984375\nEpoch 46300, Loss: 9970.83984375\nEpoch 46400, Loss: 9970.83984375\nEpoch 46500, Loss: 9970.83984375\nEpoch 46600, Loss: 9970.83984375\nEpoch 46700, Loss: 9970.83984375\nEpoch 46800, Loss: 9970.83984375\nEpoch 46900, Loss: 9970.8388671875\nEpoch 47000, Loss: 9970.8388671875\nEpoch 47100, Loss: 9970.8388671875\nEpoch 47200, Loss: 9970.83984375\nEpoch 47300, Loss: 9970.83984375\nEpoch 47400, Loss: 9970.8388671875\nEpoch 47500, Loss: 9970.8388671875\nEpoch 47600, Loss: 9970.83984375\nEpoch 47700, Loss: 9970.8388671875\nEpoch 47800, Loss: 9970.83984375\nEpoch 47900, Loss: 9970.8388671875\nEpoch 48000, Loss: 9970.83984375\nEpoch 48100, Loss: 9970.837890625\nEpoch 48200, Loss: 9970.8388671875\nEpoch 48300, Loss: 9970.8388671875\nEpoch 48400, Loss: 9970.8388671875\nEpoch 48500, Loss: 9970.8388671875\nEpoch 48600, Loss: 9970.8388671875\nEpoch 48700, Loss: 9970.83984375\nEpoch 48800, Loss: 9970.8388671875\nEpoch 48900, Loss: 9970.8388671875\nEpoch 49000, Loss: 9970.8388671875\nEpoch 49100, Loss: 9970.83984375\nEpoch 49200, Loss: 9970.8388671875\nEpoch 49300, Loss: 9970.8388671875\nEpoch 49400, Loss: 9970.8388671875\nEpoch 49500, Loss: 9970.8388671875\nEpoch 49600, Loss: 9970.83984375\nEpoch 49700, Loss: 9970.83984375\nEpoch 49800, Loss: 9970.8388671875\nEpoch 49900, Loss: 9970.8388671875\nEpoch 0, Loss: 63828.96875\nEpoch 100, Loss: 60923.21484375\nEpoch 200, Loss: 58206.27734375\nEpoch 300, Loss: 55664.4609375\nEpoch 400, Loss: 53284.59765625\nEpoch 500, Loss: 51055.73046875\nEpoch 600, Loss: 48968.84375\nEpoch 700, Loss: 47015.55078125\nEpoch 800, Loss: 45188.1484375\nEpoch 900, Loss: 43479.48046875\nEpoch 1000, Loss: 41882.6875\nEpoch 1100, Loss: 40391.0546875\nEpoch 1200, Loss: 38998.1875\nEpoch 1300, Loss: 37697.9453125\nEpoch 1400, Loss: 36484.40234375\nEpoch 1500, Loss: 35352.0234375\nEpoch 1600, Loss: 34295.15234375\nEpoch 1700, Loss: 33308.515625\nEpoch 1800, Loss: 32387.134765625\nEpoch 1900, Loss: 31526.32421875\nEpoch 2000, Loss: 30721.70703125\nEpoch 2100, Loss: 29969.205078125\nEpoch 2200, Loss: 29265.19140625\nEpoch 2300, Loss: 28606.029296875\nEpoch 2400, Loss: 27988.615234375\nEpoch 2500, Loss: 27409.80078125\nEpoch 2600, Loss: 26866.63671875\nEpoch 2700, Loss: 26356.484375\nEpoch 2800, Loss: 25876.900390625\nEpoch 2900, Loss: 25425.603515625\nEpoch 3000, Loss: 25000.458984375\nEpoch 3100, Loss: 24599.470703125\nEpoch 3200, Loss: 24220.77734375\nEpoch 3300, Loss: 23862.662109375\nEpoch 3400, Loss: 23523.490234375\nEpoch 3500, Loss: 23201.748046875\nEpoch 3600, Loss: 22895.955078125\nEpoch 3700, Loss: 22604.6328125\nEpoch 3800, Loss: 22326.486328125\nEpoch 3900, Loss: 22060.318359375\nEpoch 4000, Loss: 21805.04296875\nEpoch 4100, Loss: 21559.73046875\nEpoch 4200, Loss: 21323.453125\nEpoch 4300, Loss: 21095.3125\nEpoch 4400, Loss: 20874.583984375\nEpoch 4500, Loss: 20660.71875\nEpoch 4600, Loss: 20453.04296875\nEpoch 4700, Loss: 20251.0390625\nEpoch 4800, Loss: 20054.275390625\nEpoch 4900, Loss: 19862.3671875\nEpoch 5000, Loss: 19674.99609375\nEpoch 5100, Loss: 19491.8671875\nEpoch 5200, Loss: 19312.734375\nEpoch 5300, Loss: 19137.380859375\nEpoch 5400, Loss: 18965.6171875\nEpoch 5500, Loss: 18797.2734375\nEpoch 5600, Loss: 18632.19140625\nEpoch 5700, Loss: 18470.2421875\nEpoch 5800, Loss: 18311.302734375\nEpoch 5900, Loss: 18155.2578125\nEpoch 6000, Loss: 18002.029296875\nEpoch 6100, Loss: 17851.583984375\nEpoch 6200, Loss: 17703.751953125\nEpoch 6300, Loss: 17558.458984375\nEpoch 6400, Loss: 17415.6328125\nEpoch 6500, Loss: 17275.205078125\nEpoch 6600, Loss: 17137.115234375\nEpoch 6700, Loss: 17001.314453125\nEpoch 6800, Loss: 16867.7421875\nEpoch 6900, Loss: 16736.34765625\nEpoch 7000, Loss: 16607.083984375\nEpoch 7100, Loss: 16479.8984375\nEpoch 7200, Loss: 16354.818359375\nEpoch 7300, Loss: 16231.787109375\nEpoch 7400, Loss: 16110.828125\nEpoch 7500, Loss: 15991.7939453125\nEpoch 7600, Loss: 15874.615234375\nEpoch 7700, Loss: 15759.25\nEpoch 7800, Loss: 15645.6591796875\nEpoch 7900, Loss: 15533.8125\nEpoch 8000, Loss: 15423.6669921875\nEpoch 8100, Loss: 15315.201171875\nEpoch 8200, Loss: 15208.3662109375\nEpoch 8300, Loss: 15103.12890625\nEpoch 8400, Loss: 14999.458984375\nEpoch 8500, Loss: 14897.3369140625\nEpoch 8600, Loss: 14796.7236328125\nEpoch 8700, Loss: 14697.5947265625\nEpoch 8800, Loss: 14599.9296875\nEpoch 8900, Loss: 14503.7041015625\nEpoch 9000, Loss: 14408.900390625\nEpoch 9100, Loss: 14315.498046875\nEpoch 9200, Loss: 14223.478515625\nEpoch 9300, Loss: 14132.9072265625\nEpoch 9400, Loss: 14043.7353515625\nEpoch 9500, Loss: 13955.9033203125\nEpoch 9600, Loss: 13869.3984375\nEpoch 9700, Loss: 13784.2119140625\nEpoch 9800, Loss: 13700.337890625\nEpoch 9900, Loss: 13617.7705078125\nEpoch 10000, Loss: 13536.49609375\nEpoch 10100, Loss: 13456.5146484375\nEpoch 10200, Loss: 13377.8173828125\nEpoch 10300, Loss: 13300.3974609375\nEpoch 10400, Loss: 13224.2490234375\nEpoch 10500, Loss: 13149.3720703125\nEpoch 10600, Loss: 13075.751953125\nEpoch 10700, Loss: 13003.3876953125\nEpoch 10800, Loss: 12932.2734375\nEpoch 10900, Loss: 12862.3984375\nEpoch 11000, Loss: 12793.75390625\nEpoch 11100, Loss: 12726.33203125\nEpoch 11200, Loss: 12660.123046875\nEpoch 11300, Loss: 12595.1142578125\nEpoch 11400, Loss: 12531.287109375\nEpoch 11500, Loss: 12468.638671875\nEpoch 11600, Loss: 12407.162109375\nEpoch 11700, Loss: 12346.837890625\nEpoch 11800, Loss: 12287.634765625\nEpoch 11900, Loss: 12229.537109375\nEpoch 12000, Loss: 12172.525390625\nEpoch 12100, Loss: 12116.5791015625\nEpoch 12200, Loss: 12061.6826171875\nEpoch 12300, Loss: 12007.890625\nEpoch 12400, Loss: 11955.1513671875\nEpoch 12500, Loss: 11903.3974609375\nEpoch 12600, Loss: 11852.6162109375\nEpoch 12700, Loss: 11802.7900390625\nEpoch 12800, Loss: 11753.8984375\nEpoch 12900, Loss: 11705.9296875\nEpoch 13000, Loss: 11658.8662109375\nEpoch 13100, Loss: 11612.6923828125\nEpoch 13200, Loss: 11567.4013671875\nEpoch 13300, Loss: 11522.9736328125\nEpoch 13400, Loss: 11479.4013671875\nEpoch 13500, Loss: 11436.6708984375\nEpoch 13600, Loss: 11394.771484375\nEpoch 13700, Loss: 11353.6865234375\nEpoch 13800, Loss: 11313.4091796875\nEpoch 13900, Loss: 11273.9287109375\nEpoch 14000, Loss: 11235.2333984375\nEpoch 14100, Loss: 11197.3173828125\nEpoch 14200, Loss: 11160.1669921875\nEpoch 14300, Loss: 11123.8896484375\nEpoch 14400, Loss: 11088.4091796875\nEpoch 14500, Loss: 11053.6748046875\nEpoch 14600, Loss: 11019.669921875\nEpoch 14700, Loss: 10986.3896484375\nEpoch 14800, Loss: 10953.900390625\nEpoch 14900, Loss: 10922.12890625\nEpoch 15000, Loss: 10891.0673828125\nEpoch 15100, Loss: 10860.7421875\nEpoch 15200, Loss: 10831.1083984375\nEpoch 15300, Loss: 10802.158203125\nEpoch 15400, Loss: 10773.8974609375\nEpoch 15500, Loss: 10746.330078125\nEpoch 15600, Loss: 10719.447265625\nEpoch 15700, Loss: 10693.224609375\nEpoch 15800, Loss: 10667.646484375\nEpoch 15900, Loss: 10642.7109375\nEpoch 16000, Loss: 10618.4228515625\nEpoch 16100, Loss: 10594.80078125\nEpoch 16200, Loss: 10571.8017578125\nEpoch 16300, Loss: 10549.40234375\nEpoch 16400, Loss: 10527.5986328125\nEpoch 16500, Loss: 10506.3740234375\nEpoch 16600, Loss: 10485.7275390625\nEpoch 16700, Loss: 10465.6455078125\nEpoch 16800, Loss: 10446.1171875\nEpoch 16900, Loss: 10427.13671875\nEpoch 17000, Loss: 10408.6875\nEpoch 17100, Loss: 10390.763671875\nEpoch 17200, Loss: 10373.357421875\nEpoch 17300, Loss: 10356.45703125\nEpoch 17400, Loss: 10340.0537109375\nEpoch 17500, Loss: 10324.14453125\nEpoch 17600, Loss: 10308.767578125\nEpoch 17700, Loss: 10293.8818359375\nEpoch 17800, Loss: 10279.46875\nEpoch 17900, Loss: 10265.521484375\nEpoch 18000, Loss: 10252.037109375\nEpoch 18100, Loss: 10239.009765625\nEpoch 18200, Loss: 10226.43359375\nEpoch 18300, Loss: 10214.3017578125\nEpoch 18400, Loss: 10202.615234375\nEpoch 18500, Loss: 10191.392578125\nEpoch 18600, Loss: 10180.62109375\nEpoch 18700, Loss: 10170.2822265625\nEpoch 18800, Loss: 10160.3583984375\nEpoch 18900, Loss: 10150.845703125\nEpoch 19000, Loss: 10141.7373046875\nEpoch 19100, Loss: 10133.0458984375\nEpoch 19200, Loss: 10124.7607421875\nEpoch 19300, Loss: 10116.86328125\nEpoch 19400, Loss: 10109.3408203125\nEpoch 19500, Loss: 10102.1904296875\nEpoch 19600, Loss: 10095.41015625\nEpoch 19700, Loss: 10088.9873046875\nEpoch 19800, Loss: 10082.912109375\nEpoch 19900, Loss: 10077.1796875\nEpoch 20000, Loss: 10071.7900390625\nEpoch 20100, Loss: 10066.728515625\nEpoch 20200, Loss: 10061.984375\nEpoch 20300, Loss: 10057.54296875\nEpoch 20400, Loss: 10053.392578125\nEpoch 20500, Loss: 10049.515625\nEpoch 20600, Loss: 10045.900390625\nEpoch 20700, Loss: 10042.537109375\nEpoch 20800, Loss: 10039.4140625\nEpoch 20900, Loss: 10036.51953125\nEpoch 21000, Loss: 10033.841796875\nEpoch 21100, Loss: 10031.3681640625\nEpoch 21200, Loss: 10029.0927734375\nEpoch 21300, Loss: 10026.9990234375\nEpoch 21400, Loss: 10025.0751953125\nEpoch 21500, Loss: 10023.3134765625\nEpoch 21600, Loss: 10021.7099609375\nEpoch 21700, Loss: 10020.26171875\nEpoch 21800, Loss: 10018.9453125\nEpoch 21900, Loss: 10017.7451171875\nEpoch 22000, Loss: 10016.6533203125\nEpoch 22100, Loss: 10015.6630859375\nEpoch 22200, Loss: 10014.76171875\nEpoch 22300, Loss: 10013.9462890625\nEpoch 22400, Loss: 10013.205078125\nEpoch 22500, Loss: 10012.5341796875\nEpoch 22600, Loss: 10011.9267578125\nEpoch 22700, Loss: 10011.376953125\nEpoch 22800, Loss: 10010.8779296875\nEpoch 22900, Loss: 10010.4267578125\nEpoch 23000, Loss: 10010.017578125\nEpoch 23100, Loss: 10009.6474609375\nEpoch 23200, Loss: 10009.30859375\nEpoch 23300, Loss: 10009.001953125\nEpoch 23400, Loss: 10008.720703125\nEpoch 23500, Loss: 10008.4619140625\nEpoch 23600, Loss: 10008.224609375\nEpoch 23700, Loss: 10008.0029296875\nEpoch 23800, Loss: 10007.794921875\nEpoch 23900, Loss: 10007.5986328125\nEpoch 24000, Loss: 10007.41015625\nEpoch 24100, Loss: 10007.2294921875\nEpoch 24200, Loss: 10007.0537109375\nEpoch 24300, Loss: 10006.8818359375\nEpoch 24400, Loss: 10006.7099609375\nEpoch 24500, Loss: 10006.5380859375\nEpoch 24600, Loss: 10006.3681640625\nEpoch 24700, Loss: 10006.1953125\nEpoch 24800, Loss: 10006.0205078125\nEpoch 24900, Loss: 10005.841796875\nEpoch 25000, Loss: 10005.6572265625\nEpoch 25100, Loss: 10005.4697265625\nEpoch 25200, Loss: 10005.2744140625\nEpoch 25300, Loss: 10005.0732421875\nEpoch 25400, Loss: 10004.8662109375\nEpoch 25500, Loss: 10004.6533203125\nEpoch 25600, Loss: 10004.431640625\nEpoch 25700, Loss: 10004.203125\nEpoch 25800, Loss: 10003.966796875\nEpoch 25900, Loss: 10003.72265625\nEpoch 26000, Loss: 10003.470703125\nEpoch 26100, Loss: 10003.2109375\nEpoch 26200, Loss: 10002.943359375\nEpoch 26300, Loss: 10002.6689453125\nEpoch 26400, Loss: 10002.3837890625\nEpoch 26500, Loss: 10002.0908203125\nEpoch 26600, Loss: 10001.7900390625\nEpoch 26700, Loss: 10001.5078125\nEpoch 26800, Loss: 10001.2451171875\nEpoch 26900, Loss: 10000.9794921875\nEpoch 27000, Loss: 10000.708984375\nEpoch 27100, Loss: 10000.4365234375\nEpoch 27200, Loss: 10000.185546875\nEpoch 27300, Loss: 9999.9609375\nEpoch 27400, Loss: 9999.75\nEpoch 27500, Loss: 9999.533203125\nEpoch 27600, Loss: 9999.3095703125\nEpoch 27700, Loss: 9999.080078125\nEpoch 27800, Loss: 9998.84375\nEpoch 27900, Loss: 9998.599609375\nEpoch 28000, Loss: 9998.349609375\nEpoch 28100, Loss: 9998.0947265625\nEpoch 28200, Loss: 9997.8330078125\nEpoch 28300, Loss: 9997.564453125\nEpoch 28400, Loss: 9997.291015625\nEpoch 28500, Loss: 9997.01171875\nEpoch 28600, Loss: 9996.7275390625\nEpoch 28700, Loss: 9996.4736328125\nEpoch 28800, Loss: 9996.2578125\nEpoch 28900, Loss: 9996.064453125\nEpoch 29000, Loss: 9995.8759765625\nEpoch 29100, Loss: 9995.7314453125\nEpoch 29200, Loss: 9995.6123046875\nEpoch 29300, Loss: 9995.4892578125\nEpoch 29400, Loss: 9995.36328125\nEpoch 29500, Loss: 9995.234375\nEpoch 29600, Loss: 9995.099609375\nEpoch 29700, Loss: 9994.9619140625\nEpoch 29800, Loss: 9994.822265625\nEpoch 29900, Loss: 9994.6767578125\nEpoch 30000, Loss: 9994.529296875\nEpoch 30100, Loss: 9994.376953125\nEpoch 30200, Loss: 9994.220703125\nEpoch 30300, Loss: 9994.0625\nEpoch 30400, Loss: 9993.900390625\nEpoch 30500, Loss: 9993.736328125\nEpoch 30600, Loss: 9993.5703125\nEpoch 30700, Loss: 9993.4345703125\nEpoch 30800, Loss: 9993.3486328125\nEpoch 30900, Loss: 9993.26953125\nEpoch 31000, Loss: 9993.1884765625\nEpoch 31100, Loss: 9993.10546875\nEpoch 31200, Loss: 9993.048828125\nEpoch 31300, Loss: 9993.0146484375\nEpoch 31400, Loss: 9992.990234375\nEpoch 31500, Loss: 9992.966796875\nEpoch 31600, Loss: 9992.951171875\nEpoch 31700, Loss: 9992.943359375\nEpoch 31800, Loss: 9992.9365234375\nEpoch 31900, Loss: 9992.931640625\nEpoch 32000, Loss: 9992.9306640625\nEpoch 32100, Loss: 9992.927734375\nEpoch 32200, Loss: 9992.927734375\nEpoch 32300, Loss: 9992.92578125\nEpoch 32400, Loss: 9992.923828125\nEpoch 32500, Loss: 9992.9248046875\nEpoch 32600, Loss: 9992.9248046875\nEpoch 32700, Loss: 9992.923828125\nEpoch 32800, Loss: 9992.9228515625\nEpoch 32900, Loss: 9992.9228515625\nEpoch 33000, Loss: 9992.9228515625\nEpoch 33100, Loss: 9992.9228515625\nEpoch 33200, Loss: 9992.9228515625\nEpoch 33300, Loss: 9992.9228515625\nEpoch 33400, Loss: 9992.921875\nEpoch 33500, Loss: 9992.921875\nEpoch 33600, Loss: 9992.9208984375\nEpoch 33700, Loss: 9992.921875\nEpoch 33800, Loss: 9992.921875\nEpoch 33900, Loss: 9992.921875\nEpoch 34000, Loss: 9992.9208984375\nEpoch 34100, Loss: 9992.921875\nEpoch 34200, Loss: 9992.921875\nEpoch 34300, Loss: 9992.921875\nEpoch 34400, Loss: 9992.9208984375\nEpoch 34500, Loss: 9992.9208984375\nEpoch 34600, Loss: 9992.921875\nEpoch 34700, Loss: 9992.921875\nEpoch 34800, Loss: 9992.919921875\nEpoch 34900, Loss: 9992.9208984375\nEpoch 35000, Loss: 9992.9208984375\nEpoch 35100, Loss: 9992.9208984375\nEpoch 35200, Loss: 9992.921875\nEpoch 35300, Loss: 9992.921875\nEpoch 35400, Loss: 9992.921875\nEpoch 35500, Loss: 9992.9208984375\nEpoch 35600, Loss: 9992.9208984375\nEpoch 35700, Loss: 9992.9208984375\nEpoch 35800, Loss: 9992.921875\nEpoch 35900, Loss: 9992.9208984375\nEpoch 36000, Loss: 9992.921875\nEpoch 36100, Loss: 9992.9208984375\nEpoch 36200, Loss: 9992.9208984375\nEpoch 36300, Loss: 9992.9208984375\nEpoch 36400, Loss: 9992.9228515625\nEpoch 36500, Loss: 9992.9208984375\nEpoch 36600, Loss: 9992.9208984375\nEpoch 36700, Loss: 9992.9208984375\nEpoch 36800, Loss: 9992.921875\nEpoch 36900, Loss: 9992.9208984375\nEpoch 37000, Loss: 9992.921875\nEpoch 37100, Loss: 9992.921875\nEpoch 37200, Loss: 9992.9208984375\nEpoch 37300, Loss: 9992.9208984375\nEpoch 37400, Loss: 9992.9208984375\nEpoch 37500, Loss: 9992.9208984375\nEpoch 37600, Loss: 9992.921875\nEpoch 37700, Loss: 9992.921875\nEpoch 37800, Loss: 9992.9208984375\nEpoch 37900, Loss: 9992.921875\nEpoch 38000, Loss: 9992.919921875\nEpoch 38100, Loss: 9992.9208984375\nEpoch 38200, Loss: 9992.9208984375\nEpoch 38300, Loss: 9992.9208984375\nEpoch 38400, Loss: 9992.9208984375\nEpoch 38500, Loss: 9992.9208984375\nEpoch 38600, Loss: 9992.921875\nEpoch 38700, Loss: 9992.921875\nEpoch 38800, Loss: 9992.921875\nEpoch 38900, Loss: 9992.9228515625\nEpoch 39000, Loss: 9992.919921875\nEpoch 39100, Loss: 9992.921875\nEpoch 39200, Loss: 9992.9208984375\nEpoch 39300, Loss: 9992.921875\nEpoch 39400, Loss: 9992.9228515625\nEpoch 39500, Loss: 9992.919921875\nEpoch 39600, Loss: 9992.9208984375\nEpoch 39700, Loss: 9992.921875\nEpoch 39800, Loss: 9992.9208984375\nEpoch 39900, Loss: 9992.921875\nEpoch 40000, Loss: 9992.921875\nEpoch 40100, Loss: 9992.921875\nEpoch 40200, Loss: 9992.921875\nEpoch 40300, Loss: 9992.921875\nEpoch 40400, Loss: 9992.921875\nEpoch 40500, Loss: 9992.9208984375\nEpoch 40600, Loss: 9992.9208984375\nEpoch 40700, Loss: 9992.921875\nEpoch 40800, Loss: 9992.921875\nEpoch 40900, Loss: 9992.921875\nEpoch 41000, Loss: 9992.921875\nEpoch 41100, Loss: 9992.921875\nEpoch 41200, Loss: 9992.921875\nEpoch 41300, Loss: 9992.921875\nEpoch 41400, Loss: 9992.9208984375\nEpoch 41500, Loss: 9992.919921875\nEpoch 41600, Loss: 9992.921875\nEpoch 41700, Loss: 9992.9208984375\nEpoch 41800, Loss: 9992.921875\nEpoch 41900, Loss: 9992.9208984375\nEpoch 42000, Loss: 9992.9208984375\nEpoch 42100, Loss: 9992.921875\nEpoch 42200, Loss: 9992.9208984375\nEpoch 42300, Loss: 9992.921875\nEpoch 42400, Loss: 9992.921875\nEpoch 42500, Loss: 9992.921875\nEpoch 42600, Loss: 9992.921875\nEpoch 42700, Loss: 9992.921875\nEpoch 42800, Loss: 9992.9208984375\nEpoch 42900, Loss: 9992.921875\nEpoch 43000, Loss: 9992.921875\nEpoch 43100, Loss: 9992.921875\nEpoch 43200, Loss: 9992.921875\nEpoch 43300, Loss: 9992.921875\nEpoch 43400, Loss: 9992.921875\nEpoch 43500, Loss: 9992.9228515625\nEpoch 43600, Loss: 9992.921875\nEpoch 43700, Loss: 9992.921875\nEpoch 43800, Loss: 9992.9208984375\nEpoch 43900, Loss: 9992.921875\nEpoch 44000, Loss: 9992.9208984375\nEpoch 44100, Loss: 9992.9208984375\nEpoch 44200, Loss: 9992.921875\nEpoch 44300, Loss: 9992.921875\nEpoch 44400, Loss: 9992.9208984375\nEpoch 44500, Loss: 9992.921875\nEpoch 44600, Loss: 9992.9208984375\nEpoch 44700, Loss: 9992.9208984375\nEpoch 44800, Loss: 9992.921875\nEpoch 44900, Loss: 9992.919921875\nEpoch 45000, Loss: 9992.921875\nEpoch 45100, Loss: 9992.921875\nEpoch 45200, Loss: 9992.919921875\nEpoch 45300, Loss: 9992.9208984375\nEpoch 45400, Loss: 9992.921875\nEpoch 45500, Loss: 9992.9208984375\nEpoch 45600, Loss: 9992.921875\nEpoch 45700, Loss: 9992.9208984375\nEpoch 45800, Loss: 9992.9228515625\nEpoch 45900, Loss: 9992.921875\nEpoch 46000, Loss: 9992.921875\nEpoch 46100, Loss: 9992.9208984375\nEpoch 46200, Loss: 9992.921875\nEpoch 46300, Loss: 9992.921875\nEpoch 46400, Loss: 9992.9208984375\nEpoch 46500, Loss: 9992.921875\nEpoch 46600, Loss: 9992.9208984375\nEpoch 46700, Loss: 9992.921875\nEpoch 46800, Loss: 9992.9208984375\nEpoch 46900, Loss: 9992.921875\nEpoch 47000, Loss: 9992.9208984375\nEpoch 47100, Loss: 9992.9208984375\nEpoch 47200, Loss: 9992.9208984375\nEpoch 47300, Loss: 9992.919921875\nEpoch 47400, Loss: 9992.921875\nEpoch 47500, Loss: 9992.9208984375\nEpoch 47600, Loss: 9992.9208984375\nEpoch 47700, Loss: 9992.921875\nEpoch 47800, Loss: 9992.921875\nEpoch 47900, Loss: 9992.9228515625\nEpoch 48000, Loss: 9992.9228515625\nEpoch 48100, Loss: 9992.9208984375\nEpoch 48200, Loss: 9992.921875\nEpoch 48300, Loss: 9992.921875\nEpoch 48400, Loss: 9992.921875\nEpoch 48500, Loss: 9992.9208984375\nEpoch 48600, Loss: 9992.921875\nEpoch 48700, Loss: 9992.921875\nEpoch 48800, Loss: 9992.921875\nEpoch 48900, Loss: 9992.921875\nEpoch 49000, Loss: 9992.921875\nEpoch 49100, Loss: 9992.921875\nEpoch 49200, Loss: 9992.9208984375\nEpoch 49300, Loss: 9992.9208984375\nEpoch 49400, Loss: 9992.9208984375\nEpoch 49500, Loss: 9992.9208984375\nEpoch 49600, Loss: 9992.9208984375\nEpoch 49700, Loss: 9992.921875\nEpoch 49800, Loss: 9992.921875\nEpoch 49900, Loss: 9992.9208984375\nEpoch 0, Loss: 63828.96875\nEpoch 100, Loss: 60923.71875\nEpoch 200, Loss: 58207.2421875\nEpoch 300, Loss: 55665.81640625\nEpoch 400, Loss: 53286.33984375\nEpoch 500, Loss: 51057.8671875\nEpoch 600, Loss: 48971.37109375\nEpoch 700, Loss: 47018.453125\nEpoch 800, Loss: 45191.3984375\nEpoch 900, Loss: 43483.06640625\nEpoch 1000, Loss: 41886.62109375\nEpoch 1100, Loss: 40395.3046875\nEpoch 1200, Loss: 39002.734375\nEpoch 1300, Loss: 37702.75390625\nEpoch 1400, Loss: 36489.46484375\nEpoch 1500, Loss: 35357.3515625\nEpoch 1600, Loss: 34300.73046875\nEpoch 1700, Loss: 33314.32421875\nEpoch 1800, Loss: 32393.15625\nEpoch 1900, Loss: 31532.53515625\nEpoch 2000, Loss: 30728.08984375\nEpoch 2100, Loss: 29975.755859375\nEpoch 2200, Loss: 29271.939453125\nEpoch 2300, Loss: 28612.962890625\nEpoch 2400, Loss: 27995.796875\nEpoch 2500, Loss: 27417.236328125\nEpoch 2600, Loss: 26874.31640625\nEpoch 2700, Loss: 26364.396484375\nEpoch 2800, Loss: 25885.03125\nEpoch 2900, Loss: 25433.94140625\nEpoch 3000, Loss: 25009.0\nEpoch 3100, Loss: 24608.20703125\nEpoch 3200, Loss: 24229.6953125\nEpoch 3300, Loss: 23871.77734375\nEpoch 3400, Loss: 23532.80859375\nEpoch 3500, Loss: 23211.30859375\nEpoch 3600, Loss: 22905.76953125\nEpoch 3700, Loss: 22614.701171875\nEpoch 3800, Loss: 22336.794921875\nEpoch 3900, Loss: 22070.865234375\nEpoch 4000, Loss: 21815.8203125\nEpoch 4100, Loss: 21570.755859375\nEpoch 4200, Loss: 21334.7265625\nEpoch 4300, Loss: 21106.83203125\nEpoch 4400, Loss: 20886.33984375\nEpoch 4500, Loss: 20672.734375\nEpoch 4600, Loss: 20465.3125\nEpoch 4700, Loss: 20263.556640625\nEpoch 4800, Loss: 20067.03125\nEpoch 4900, Loss: 19875.359375\nEpoch 5000, Loss: 19688.212890625\nEpoch 5100, Loss: 19505.3046875\nEpoch 5200, Loss: 19326.388671875\nEpoch 5300, Loss: 19151.25\nEpoch 5400, Loss: 18979.69140625\nEpoch 5500, Loss: 18811.546875\nEpoch 5600, Loss: 18646.666015625\nEpoch 5700, Loss: 18484.91015625\nEpoch 5800, Loss: 18326.158203125\nEpoch 5900, Loss: 18170.296875\nEpoch 6000, Loss: 18017.26953125\nEpoch 6100, Loss: 17867.025390625\nEpoch 6200, Loss: 17719.390625\nEpoch 6300, Loss: 17574.287109375\nEpoch 6400, Loss: 17431.6484375\nEpoch 6500, Loss: 17291.400390625\nEpoch 6600, Loss: 17153.490234375\nEpoch 6700, Loss: 17017.861328125\nEpoch 6800, Loss: 16884.453125\nEpoch 6900, Loss: 16753.220703125\nEpoch 7000, Loss: 16624.1171875\nEpoch 7100, Loss: 16497.087890625\nEpoch 7200, Loss: 16372.1748046875\nEpoch 7300, Loss: 16249.3251953125\nEpoch 7400, Loss: 16128.5625\nEpoch 7500, Loss: 16009.73828125\nEpoch 7600, Loss: 15892.7470703125\nEpoch 7700, Loss: 15777.5654296875\nEpoch 7800, Loss: 15664.1513671875\nEpoch 7900, Loss: 15552.4765625\nEpoch 8000, Loss: 15442.5078125\nEpoch 8100, Loss: 15334.19921875\nEpoch 8200, Loss: 15227.5166015625\nEpoch 8300, Loss: 15122.4267578125\nEpoch 8400, Loss: 15018.90625\nEpoch 8500, Loss: 14916.9208984375\nEpoch 8600, Loss: 14816.439453125\nEpoch 8700, Loss: 14717.443359375\nEpoch 8800, Loss: 14619.9052734375\nEpoch 8900, Loss: 14523.8037109375\nEpoch 9000, Loss: 14429.119140625\nEpoch 9100, Loss: 14335.833984375\nEpoch 9200, Loss: 14243.9296875\nEpoch 9300, Loss: 14153.5166015625\nEpoch 9400, Loss: 14064.478515625\nEpoch 9500, Loss: 13976.779296875\nEpoch 9600, Loss: 13890.4052734375\nEpoch 9700, Loss: 13805.34765625\nEpoch 9800, Loss: 13721.599609375\nEpoch 9900, Loss: 13639.15234375\nEpoch 10000, Loss: 13558.0029296875\nEpoch 10100, Loss: 13478.138671875\nEpoch 10200, Loss: 13399.556640625\nEpoch 10300, Loss: 13322.2490234375\nEpoch 10400, Loss: 13246.2138671875\nEpoch 10500, Loss: 13171.44140625\nEpoch 10600, Loss: 13097.9287109375\nEpoch 10700, Loss: 13025.669921875\nEpoch 10800, Loss: 12954.65625\nEpoch 10900, Loss: 12884.8818359375\nEpoch 11000, Loss: 12816.337890625\nEpoch 11100, Loss: 12749.013671875\nEpoch 11200, Loss: 12682.89453125\nEpoch 11300, Loss: 12617.9775390625\nEpoch 11400, Loss: 12554.2431640625\nEpoch 11500, Loss: 12491.6806640625\nEpoch 11600, Loss: 12430.3017578125\nEpoch 11700, Loss: 12370.0654296875\nEpoch 11800, Loss: 12310.94921875\nEpoch 11900, Loss: 12252.9345703125\nEpoch 12000, Loss: 12196.001953125\nEpoch 12100, Loss: 12140.1337890625\nEpoch 12200, Loss: 12085.310546875\nEpoch 12300, Loss: 12031.630859375\nEpoch 12400, Loss: 11978.986328125\nEpoch 12500, Loss: 11927.3291015625\nEpoch 12600, Loss: 11876.642578125\nEpoch 12700, Loss: 11826.90234375\nEpoch 12800, Loss: 11778.095703125\nEpoch 12900, Loss: 11730.2060546875\nEpoch 13000, Loss: 11683.216796875\nEpoch 13100, Loss: 11637.1181640625\nEpoch 13200, Loss: 11591.8974609375\nEpoch 13300, Loss: 11547.5439453125\nEpoch 13400, Loss: 11504.0390625\nEpoch 13500, Loss: 11461.3720703125\nEpoch 13600, Loss: 11419.5322265625\nEpoch 13700, Loss: 11378.5029296875\nEpoch 13800, Loss: 11338.2802734375\nEpoch 13900, Loss: 11298.8544921875\nEpoch 14000, Loss: 11260.2109375\nEpoch 14100, Loss: 11222.3388671875\nEpoch 14200, Loss: 11185.2431640625\nEpoch 14300, Loss: 11149.0595703125\nEpoch 14400, Loss: 11113.658203125\nEpoch 14500, Loss: 11079.0009765625\nEpoch 14600, Loss: 11045.080078125\nEpoch 14700, Loss: 11011.953125\nEpoch 14800, Loss: 10979.6181640625\nEpoch 14900, Loss: 10948.0009765625\nEpoch 15000, Loss: 10917.0849609375\nEpoch 15100, Loss: 10886.8603515625\nEpoch 15200, Loss: 10857.3232421875\nEpoch 15300, Loss: 10828.466796875\nEpoch 15400, Loss: 10800.30078125\nEpoch 15500, Loss: 10772.8212890625\nEpoch 15600, Loss: 10746.0263671875\nEpoch 15700, Loss: 10719.8916015625\nEpoch 15800, Loss: 10694.4033203125\nEpoch 15900, Loss: 10669.548828125\nEpoch 16000, Loss: 10645.345703125\nEpoch 16100, Loss: 10621.8056640625\nEpoch 16200, Loss: 10598.8935546875\nEpoch 16300, Loss: 10576.580078125\nEpoch 16400, Loss: 10554.8515625\nEpoch 16500, Loss: 10533.70703125\nEpoch 16600, Loss: 10513.1318359375\nEpoch 16700, Loss: 10493.1171875\nEpoch 16800, Loss: 10473.6533203125\nEpoch 16900, Loss: 10454.73046875\nEpoch 17000, Loss: 10436.3603515625\nEpoch 17100, Loss: 10418.568359375\nEpoch 17200, Loss: 10401.2880859375\nEpoch 17300, Loss: 10384.5185546875\nEpoch 17400, Loss: 10368.2451171875\nEpoch 17500, Loss: 10352.4619140625\nEpoch 17600, Loss: 10337.1630859375\nEpoch 17700, Loss: 10322.341796875\nEpoch 17800, Loss: 10307.990234375\nEpoch 17900, Loss: 10294.1044921875\nEpoch 18000, Loss: 10280.677734375\nEpoch 18100, Loss: 10267.70703125\nEpoch 18200, Loss: 10255.1826171875\nEpoch 18300, Loss: 10243.1044921875\nEpoch 18400, Loss: 10231.4677734375\nEpoch 18500, Loss: 10220.291015625\nEpoch 18600, Loss: 10209.5791015625\nEpoch 18700, Loss: 10199.3369140625\nEpoch 18800, Loss: 10189.5166015625\nEpoch 18900, Loss: 10180.1064453125\nEpoch 19000, Loss: 10171.0986328125\nEpoch 19100, Loss: 10162.490234375\nEpoch 19200, Loss: 10154.283203125\nEpoch 19300, Loss: 10146.4609375\nEpoch 19400, Loss: 10139.015625\nEpoch 19500, Loss: 10131.9384765625\nEpoch 19600, Loss: 10125.2197265625\nEpoch 19700, Loss: 10118.8515625\nEpoch 19800, Loss: 10112.83203125\nEpoch 19900, Loss: 10107.162109375\nEpoch 20000, Loss: 10101.828125\nEpoch 20100, Loss: 10096.8193359375\nEpoch 20200, Loss: 10092.123046875\nEpoch 20300, Loss: 10087.7314453125\nEpoch 20400, Loss: 10083.626953125\nEpoch 20500, Loss: 10079.7958984375\nEpoch 20600, Loss: 10076.2255859375\nEpoch 20700, Loss: 10072.9052734375\nEpoch 20800, Loss: 10069.8193359375\nEpoch 20900, Loss: 10066.9619140625\nEpoch 21000, Loss: 10064.3154296875\nEpoch 21100, Loss: 10061.884765625\nEpoch 21200, Loss: 10059.671875\nEpoch 21300, Loss: 10057.6435546875\nEpoch 21400, Loss: 10055.78125\nEpoch 21500, Loss: 10054.0771484375\nEpoch 21600, Loss: 10052.517578125\nEpoch 21700, Loss: 10051.0947265625\nEpoch 21800, Loss: 10049.794921875\nEpoch 21900, Loss: 10048.6123046875\nEpoch 22000, Loss: 10047.53515625\nEpoch 22100, Loss: 10046.552734375\nEpoch 22200, Loss: 10045.6611328125\nEpoch 22300, Loss: 10044.8505859375\nEpoch 22400, Loss: 10044.111328125\nEpoch 22500, Loss: 10043.4384765625\nEpoch 22600, Loss: 10042.8251953125\nEpoch 22700, Loss: 10042.267578125\nEpoch 22800, Loss: 10041.7568359375\nEpoch 22900, Loss: 10041.291015625\nEpoch 23000, Loss: 10040.8623046875\nEpoch 23100, Loss: 10040.46875\nEpoch 23200, Loss: 10040.1044921875\nEpoch 23300, Loss: 10039.7666015625\nEpoch 23400, Loss: 10039.451171875\nEpoch 23500, Loss: 10039.1552734375\nEpoch 23600, Loss: 10038.8759765625\nEpoch 23700, Loss: 10038.6083984375\nEpoch 23800, Loss: 10038.3525390625\nEpoch 23900, Loss: 10038.10546875\nEpoch 24000, Loss: 10037.8642578125\nEpoch 24100, Loss: 10037.626953125\nEpoch 24200, Loss: 10037.390625\nEpoch 24300, Loss: 10037.15625\nEpoch 24400, Loss: 10036.9189453125\nEpoch 24500, Loss: 10036.6787109375\nEpoch 24600, Loss: 10036.43359375\nEpoch 24700, Loss: 10036.185546875\nEpoch 24800, Loss: 10035.9296875\nEpoch 24900, Loss: 10035.6669921875\nEpoch 25000, Loss: 10035.3974609375\nEpoch 25100, Loss: 10035.119140625\nEpoch 25200, Loss: 10034.8330078125\nEpoch 25300, Loss: 10034.53515625\nEpoch 25400, Loss: 10034.2294921875\nEpoch 25500, Loss: 10033.9150390625\nEpoch 25600, Loss: 10033.5888671875\nEpoch 25700, Loss: 10033.2548828125\nEpoch 25800, Loss: 10032.91015625\nEpoch 25900, Loss: 10032.5546875\nEpoch 26000, Loss: 10032.1884765625\nEpoch 26100, Loss: 10031.8173828125\nEpoch 26200, Loss: 10031.4892578125\nEpoch 26300, Loss: 10031.1611328125\nEpoch 26400, Loss: 10030.830078125\nEpoch 26500, Loss: 10030.4931640625\nEpoch 26600, Loss: 10030.1630859375\nEpoch 26700, Loss: 10029.857421875\nEpoch 26800, Loss: 10029.5751953125\nEpoch 26900, Loss: 10029.3095703125\nEpoch 27000, Loss: 10029.0400390625\nEpoch 27100, Loss: 10028.7626953125\nEpoch 27200, Loss: 10028.478515625\nEpoch 27300, Loss: 10028.1845703125\nEpoch 27400, Loss: 10027.8818359375\nEpoch 27500, Loss: 10027.5712890625\nEpoch 27600, Loss: 10027.2529296875\nEpoch 27700, Loss: 10026.92578125\nEpoch 27800, Loss: 10026.5947265625\nEpoch 27900, Loss: 10026.251953125\nEpoch 28000, Loss: 10025.9033203125\nEpoch 28100, Loss: 10025.55078125\nEpoch 28200, Loss: 10025.2294921875\nEpoch 28300, Loss: 10024.9521484375\nEpoch 28400, Loss: 10024.703125\nEpoch 28500, Loss: 10024.4697265625\nEpoch 28600, Loss: 10024.2783203125\nEpoch 28700, Loss: 10024.1220703125\nEpoch 28800, Loss: 10023.970703125\nEpoch 28900, Loss: 10023.814453125\nEpoch 29000, Loss: 10023.6533203125\nEpoch 29100, Loss: 10023.48828125\nEpoch 29200, Loss: 10023.3193359375\nEpoch 29300, Loss: 10023.1435546875\nEpoch 29400, Loss: 10022.962890625\nEpoch 29500, Loss: 10022.779296875\nEpoch 29600, Loss: 10022.5888671875\nEpoch 29700, Loss: 10022.396484375\nEpoch 29800, Loss: 10022.1982421875\nEpoch 29900, Loss: 10021.99609375\nEpoch 30000, Loss: 10021.7890625\nEpoch 30100, Loss: 10021.580078125\nEpoch 30200, Loss: 10021.3955078125\nEpoch 30300, Loss: 10021.267578125\nEpoch 30400, Loss: 10021.166015625\nEpoch 30500, Loss: 10021.0634765625\nEpoch 30600, Loss: 10020.9599609375\nEpoch 30700, Loss: 10020.8779296875\nEpoch 30800, Loss: 10020.8212890625\nEpoch 30900, Loss: 10020.783203125\nEpoch 31000, Loss: 10020.7529296875\nEpoch 31100, Loss: 10020.7294921875\nEpoch 31200, Loss: 10020.712890625\nEpoch 31300, Loss: 10020.7001953125\nEpoch 31400, Loss: 10020.693359375\nEpoch 31500, Loss: 10020.6884765625\nEpoch 31600, Loss: 10020.6845703125\nEpoch 31700, Loss: 10020.6796875\nEpoch 31800, Loss: 10020.6787109375\nEpoch 31900, Loss: 10020.67578125\nEpoch 32000, Loss: 10020.67578125\nEpoch 32100, Loss: 10020.673828125\nEpoch 32200, Loss: 10020.6728515625\nEpoch 32300, Loss: 10020.671875\nEpoch 32400, Loss: 10020.671875\nEpoch 32500, Loss: 10020.671875\nEpoch 32600, Loss: 10020.671875\nEpoch 32700, Loss: 10020.6708984375\nEpoch 32800, Loss: 10020.669921875\nEpoch 32900, Loss: 10020.669921875\nEpoch 33000, Loss: 10020.669921875\nEpoch 33100, Loss: 10020.669921875\nEpoch 33200, Loss: 10020.669921875\nEpoch 33300, Loss: 10020.6689453125\nEpoch 33400, Loss: 10020.669921875\nEpoch 33500, Loss: 10020.6689453125\nEpoch 33600, Loss: 10020.6689453125\nEpoch 33700, Loss: 10020.669921875\nEpoch 33800, Loss: 10020.6689453125\nEpoch 33900, Loss: 10020.66796875\nEpoch 34000, Loss: 10020.669921875\nEpoch 34100, Loss: 10020.6689453125\nEpoch 34200, Loss: 10020.669921875\nEpoch 34300, Loss: 10020.669921875\nEpoch 34400, Loss: 10020.669921875\nEpoch 34500, Loss: 10020.6689453125\nEpoch 34600, Loss: 10020.66796875\nEpoch 34700, Loss: 10020.6689453125\nEpoch 34800, Loss: 10020.6689453125\nEpoch 34900, Loss: 10020.669921875\nEpoch 35000, Loss: 10020.669921875\nEpoch 35100, Loss: 10020.669921875\nEpoch 35200, Loss: 10020.66796875\nEpoch 35300, Loss: 10020.6689453125\nEpoch 35400, Loss: 10020.6689453125\nEpoch 35500, Loss: 10020.6689453125\nEpoch 35600, Loss: 10020.6689453125\nEpoch 35700, Loss: 10020.66796875\nEpoch 35800, Loss: 10020.6689453125\nEpoch 35900, Loss: 10020.66796875\nEpoch 36000, Loss: 10020.66796875\nEpoch 36100, Loss: 10020.6689453125\nEpoch 36200, Loss: 10020.6689453125\nEpoch 36300, Loss: 10020.66796875\nEpoch 36400, Loss: 10020.6689453125\nEpoch 36500, Loss: 10020.6689453125\nEpoch 36600, Loss: 10020.6689453125\nEpoch 36700, Loss: 10020.6689453125\nEpoch 36800, Loss: 10020.66796875\nEpoch 36900, Loss: 10020.66796875\nEpoch 37000, Loss: 10020.6689453125\nEpoch 37100, Loss: 10020.66796875\nEpoch 37200, Loss: 10020.6689453125\nEpoch 37300, Loss: 10020.6689453125\nEpoch 37400, Loss: 10020.6689453125\nEpoch 37500, Loss: 10020.66796875\nEpoch 37600, Loss: 10020.669921875\nEpoch 37700, Loss: 10020.6689453125\nEpoch 37800, Loss: 10020.66796875\nEpoch 37900, Loss: 10020.6689453125\nEpoch 38000, Loss: 10020.669921875\nEpoch 38100, Loss: 10020.66796875\nEpoch 38200, Loss: 10020.66796875\nEpoch 38300, Loss: 10020.66796875\nEpoch 38400, Loss: 10020.6689453125\nEpoch 38500, Loss: 10020.669921875\nEpoch 38600, Loss: 10020.66796875\nEpoch 38700, Loss: 10020.6689453125\nEpoch 38800, Loss: 10020.66796875\nEpoch 38900, Loss: 10020.66796875\nEpoch 39000, Loss: 10020.66796875\nEpoch 39100, Loss: 10020.6689453125\nEpoch 39200, Loss: 10020.6689453125\nEpoch 39300, Loss: 10020.6689453125\nEpoch 39400, Loss: 10020.66796875\nEpoch 39500, Loss: 10020.66796875\nEpoch 39600, Loss: 10020.66796875\nEpoch 39700, Loss: 10020.669921875\nEpoch 39800, Loss: 10020.6689453125\nEpoch 39900, Loss: 10020.66796875\nEpoch 40000, Loss: 10020.66796875\nEpoch 40100, Loss: 10020.66796875\nEpoch 40200, Loss: 10020.6689453125\nEpoch 40300, Loss: 10020.669921875\nEpoch 40400, Loss: 10020.6689453125\nEpoch 40500, Loss: 10020.6689453125\nEpoch 40600, Loss: 10020.6689453125\nEpoch 40700, Loss: 10020.6689453125\nEpoch 40800, Loss: 10020.6689453125\nEpoch 40900, Loss: 10020.66796875\nEpoch 41000, Loss: 10020.6689453125\nEpoch 41100, Loss: 10020.66796875\nEpoch 41200, Loss: 10020.66796875\nEpoch 41300, Loss: 10020.66796875\nEpoch 41400, Loss: 10020.6689453125\nEpoch 41500, Loss: 10020.669921875\nEpoch 41600, Loss: 10020.66796875\nEpoch 41700, Loss: 10020.6689453125\nEpoch 41800, Loss: 10020.6689453125\nEpoch 41900, Loss: 10020.6689453125\nEpoch 42000, Loss: 10020.669921875\nEpoch 42100, Loss: 10020.66796875\nEpoch 42200, Loss: 10020.66796875\nEpoch 42300, Loss: 10020.6689453125\nEpoch 42400, Loss: 10020.66796875\nEpoch 42500, Loss: 10020.6689453125\nEpoch 42600, Loss: 10020.6689453125\nEpoch 42700, Loss: 10020.66796875\nEpoch 42800, Loss: 10020.6689453125\nEpoch 42900, Loss: 10020.6689453125\nEpoch 43000, Loss: 10020.6689453125\nEpoch 43100, Loss: 10020.6689453125\nEpoch 43200, Loss: 10020.66796875\nEpoch 43300, Loss: 10020.6689453125\nEpoch 43400, Loss: 10020.6689453125\nEpoch 43500, Loss: 10020.66796875\nEpoch 43600, Loss: 10020.66796875\nEpoch 43700, Loss: 10020.66796875\nEpoch 43800, Loss: 10020.6689453125\nEpoch 43900, Loss: 10020.6689453125\nEpoch 44000, Loss: 10020.6689453125\nEpoch 44100, Loss: 10020.6689453125\nEpoch 44200, Loss: 10020.669921875\nEpoch 44300, Loss: 10020.66796875\nEpoch 44400, Loss: 10020.66796875\nEpoch 44500, Loss: 10020.66796875\nEpoch 44600, Loss: 10020.66796875\nEpoch 44700, Loss: 10020.6689453125\nEpoch 44800, Loss: 10020.669921875\nEpoch 44900, Loss: 10020.6689453125\nEpoch 45000, Loss: 10020.66796875\nEpoch 45100, Loss: 10020.66796875\nEpoch 45200, Loss: 10020.669921875\nEpoch 45300, Loss: 10020.6689453125\nEpoch 45400, Loss: 10020.669921875\nEpoch 45500, Loss: 10020.66796875\nEpoch 45600, Loss: 10020.66796875\nEpoch 45700, Loss: 10020.6689453125\nEpoch 45800, Loss: 10020.66796875\nEpoch 45900, Loss: 10020.66796875\nEpoch 46000, Loss: 10020.6689453125\nEpoch 46100, Loss: 10020.669921875\nEpoch 46200, Loss: 10020.66796875\nEpoch 46300, Loss: 10020.66796875\nEpoch 46400, Loss: 10020.6689453125\nEpoch 46500, Loss: 10020.669921875\nEpoch 46600, Loss: 10020.6689453125\nEpoch 46700, Loss: 10020.66796875\nEpoch 46800, Loss: 10020.66796875\nEpoch 46900, Loss: 10020.6689453125\nEpoch 47000, Loss: 10020.6689453125\nEpoch 47100, Loss: 10020.6689453125\nEpoch 47200, Loss: 10020.6689453125\nEpoch 47300, Loss: 10020.66796875\nEpoch 47400, Loss: 10020.66796875\nEpoch 47500, Loss: 10020.6689453125\nEpoch 47600, Loss: 10020.6689453125\nEpoch 47700, Loss: 10020.669921875\nEpoch 47800, Loss: 10020.66796875\nEpoch 47900, Loss: 10020.6689453125\nEpoch 48000, Loss: 10020.6689453125\nEpoch 48100, Loss: 10020.6689453125\nEpoch 48200, Loss: 10020.66796875\nEpoch 48300, Loss: 10020.6689453125\nEpoch 48400, Loss: 10020.6689453125\nEpoch 48500, Loss: 10020.66796875\nEpoch 48600, Loss: 10020.6689453125\nEpoch 48700, Loss: 10020.669921875\nEpoch 48800, Loss: 10020.6689453125\nEpoch 48900, Loss: 10020.6689453125\nEpoch 49000, Loss: 10020.66796875\nEpoch 49100, Loss: 10020.6689453125\nEpoch 49200, Loss: 10020.6689453125\nEpoch 49300, Loss: 10020.6689453125\nEpoch 49400, Loss: 10020.6689453125\nEpoch 49500, Loss: 10020.66796875\nEpoch 49600, Loss: 10020.6689453125\nEpoch 49700, Loss: 10020.66796875\nEpoch 49800, Loss: 10020.6689453125\nEpoch 49900, Loss: 10020.6689453125\nEpoch 0, Loss: 63828.96875\nEpoch 100, Loss: 60924.35546875\nEpoch 200, Loss: 58208.44140625\nEpoch 300, Loss: 55667.51171875\nEpoch 400, Loss: 53288.54296875\nEpoch 500, Loss: 51060.5546875\nEpoch 600, Loss: 48974.5625\nEpoch 700, Loss: 47022.11328125\nEpoch 800, Loss: 45195.4921875\nEpoch 900, Loss: 43487.58984375\nEpoch 1000, Loss: 41891.56640625\nEpoch 1100, Loss: 40400.65234375\nEpoch 1200, Loss: 39008.453125\nEpoch 1300, Loss: 37708.8125\nEpoch 1400, Loss: 36495.8359375\nEpoch 1500, Loss: 35364.0703125\nEpoch 1600, Loss: 34307.7578125\nEpoch 1700, Loss: 33321.640625\nEpoch 1800, Loss: 32400.73046875\nEpoch 1900, Loss: 31540.34765625\nEpoch 2000, Loss: 30736.123046875\nEpoch 2100, Loss: 29983.994140625\nEpoch 2200, Loss: 29280.431640625\nEpoch 2300, Loss: 28621.6953125\nEpoch 2400, Loss: 28004.841796875\nEpoch 2500, Loss: 27426.603515625\nEpoch 2600, Loss: 26883.986328125\nEpoch 2700, Loss: 26374.35546875\nEpoch 2800, Loss: 25895.265625\nEpoch 2900, Loss: 25444.4375\nEpoch 3000, Loss: 25019.748046875\nEpoch 3100, Loss: 24619.197265625\nEpoch 3200, Loss: 24240.921875\nEpoch 3300, Loss: 23883.2421875\nEpoch 3400, Loss: 23544.53515625\nEpoch 3500, Loss: 23223.3359375\nEpoch 3600, Loss: 22918.123046875\nEpoch 3700, Loss: 22627.373046875\nEpoch 3800, Loss: 22349.771484375\nEpoch 3900, Loss: 22084.138671875\nEpoch 4000, Loss: 21829.373046875\nEpoch 4100, Loss: 21584.630859375\nEpoch 4200, Loss: 21348.9140625\nEpoch 4300, Loss: 21121.326171875\nEpoch 4400, Loss: 20901.130859375\nEpoch 4500, Loss: 20687.859375\nEpoch 4600, Loss: 20480.751953125\nEpoch 4700, Loss: 20279.306640625\nEpoch 4800, Loss: 20083.080078125\nEpoch 4900, Loss: 19891.703125\nEpoch 5000, Loss: 19704.841796875\nEpoch 5100, Loss: 19522.21484375\nEpoch 5200, Loss: 19343.5703125\nEpoch 5300, Loss: 19168.693359375\nEpoch 5400, Loss: 18997.396484375\nEpoch 5500, Loss: 18829.5078125\nEpoch 5600, Loss: 18664.873046875\nEpoch 5700, Loss: 18503.359375\nEpoch 5800, Loss: 18344.845703125\nEpoch 5900, Loss: 18189.220703125\nEpoch 6000, Loss: 18036.4453125\nEpoch 6100, Loss: 17886.455078125\nEpoch 6200, Loss: 17739.064453125\nEpoch 6300, Loss: 17594.201171875\nEpoch 6400, Loss: 17451.79296875\nEpoch 6500, Loss: 17311.775390625\nEpoch 6600, Loss: 17174.0859375\nEpoch 6700, Loss: 17038.669921875\nEpoch 6800, Loss: 16905.47265625\nEpoch 6900, Loss: 16774.447265625\nEpoch 7000, Loss: 16645.541015625\nEpoch 7100, Loss: 16518.7109375\nEpoch 7200, Loss: 16394.0078125\nEpoch 7300, Loss: 16271.39453125\nEpoch 7400, Loss: 16150.8681640625\nEpoch 7500, Loss: 16032.2919921875\nEpoch 7600, Loss: 15915.5498046875\nEpoch 7700, Loss: 15800.611328125\nEpoch 7800, Loss: 15687.4345703125\nEpoch 7900, Loss: 15575.9853515625\nEpoch 8000, Loss: 15466.220703125\nEpoch 8100, Loss: 15358.109375\nEpoch 8200, Loss: 15251.6201171875\nEpoch 8300, Loss: 15146.7138671875\nEpoch 8400, Loss: 15043.3720703125\nEpoch 8500, Loss: 14941.5595703125\nEpoch 8600, Loss: 14841.2470703125\nEpoch 8700, Loss: 14742.416015625\nEpoch 8800, Loss: 14645.03515625\nEpoch 8900, Loss: 14549.0869140625\nEpoch 9000, Loss: 14454.55078125\nEpoch 9100, Loss: 14361.4130859375\nEpoch 9200, Loss: 14269.6796875\nEpoch 9300, Loss: 14179.4521484375\nEpoch 9400, Loss: 14090.5791015625\nEpoch 9500, Loss: 14003.044921875\nEpoch 9600, Loss: 13916.833984375\nEpoch 9700, Loss: 13831.9345703125\nEpoch 9800, Loss: 13748.3408203125\nEpoch 9900, Loss: 13666.046875\nEpoch 10000, Loss: 13585.0458984375\nEpoch 10100, Loss: 13505.326171875\nEpoch 10200, Loss: 13426.8857421875\nEpoch 10300, Loss: 13349.7177734375\nEpoch 10400, Loss: 13273.8193359375\nEpoch 10500, Loss: 13199.181640625\nEpoch 10600, Loss: 13125.80078125\nEpoch 10700, Loss: 13053.6689453125\nEpoch 10800, Loss: 12982.78125\nEpoch 10900, Loss: 12913.1298828125\nEpoch 11000, Loss: 12844.7041015625\nEpoch 11100, Loss: 12777.4990234375\nEpoch 11200, Loss: 12711.4951171875\nEpoch 11300, Loss: 12646.6904296875\nEpoch 11400, Loss: 12583.0634765625\nEpoch 11500, Loss: 12520.6171875\nEpoch 11600, Loss: 12459.349609375\nEpoch 11700, Loss: 12399.2197265625\nEpoch 11800, Loss: 12340.208984375\nEpoch 11900, Loss: 12282.296875\nEpoch 12000, Loss: 12225.462890625\nEpoch 12100, Loss: 12169.6884765625\nEpoch 12200, Loss: 12114.9541015625\nEpoch 12300, Loss: 12061.4169921875\nEpoch 12400, Loss: 12008.890625\nEpoch 12500, Loss: 11957.3466796875\nEpoch 12600, Loss: 11906.7666015625\nEpoch 12700, Loss: 11857.126953125\nEpoch 12800, Loss: 11808.4189453125\nEpoch 12900, Loss: 11760.62890625\nEpoch 13000, Loss: 11713.7333984375\nEpoch 13100, Loss: 11667.7294921875\nEpoch 13200, Loss: 11622.5947265625\nEpoch 13300, Loss: 11578.314453125\nEpoch 13400, Loss: 11534.8837890625\nEpoch 13500, Loss: 11492.287109375\nEpoch 13600, Loss: 11450.51171875\nEpoch 13700, Loss: 11409.5458984375\nEpoch 13800, Loss: 11369.38671875\nEpoch 13900, Loss: 11330.0146484375\nEpoch 14000, Loss: 11291.423828125\nEpoch 14100, Loss: 11253.6025390625\nEpoch 14200, Loss: 11216.6103515625\nEpoch 14300, Loss: 11180.6005859375\nEpoch 14400, Loss: 11145.3828125\nEpoch 14500, Loss: 11110.9052734375\nEpoch 14600, Loss: 11077.1533203125\nEpoch 14700, Loss: 11044.158203125\nEpoch 14800, Loss: 11011.9501953125\nEpoch 14900, Loss: 10980.4580078125\nEpoch 15000, Loss: 10949.6611328125\nEpoch 15100, Loss: 10919.5517578125\nEpoch 15200, Loss: 10890.1220703125\nEpoch 15300, Loss: 10861.3779296875\nEpoch 15400, Loss: 10833.322265625\nEpoch 15500, Loss: 10805.94921875\nEpoch 15600, Loss: 10779.248046875\nEpoch 15700, Loss: 10753.21484375\nEpoch 15800, Loss: 10727.826171875\nEpoch 15900, Loss: 10703.0673828125\nEpoch 16000, Loss: 10678.9638671875\nEpoch 16100, Loss: 10655.5166015625\nEpoch 16200, Loss: 10632.703125\nEpoch 16300, Loss: 10610.486328125\nEpoch 16400, Loss: 10588.8544921875\nEpoch 16500, Loss: 10567.8603515625\nEpoch 16600, Loss: 10547.466796875\nEpoch 16700, Loss: 10527.6337890625\nEpoch 16800, Loss: 10508.345703125\nEpoch 16900, Loss: 10489.5986328125\nEpoch 17000, Loss: 10471.3798828125\nEpoch 17100, Loss: 10453.681640625\nEpoch 17200, Loss: 10436.4951171875\nEpoch 17300, Loss: 10419.8115234375\nEpoch 17400, Loss: 10403.6220703125\nEpoch 17500, Loss: 10387.9208984375\nEpoch 17600, Loss: 10372.69921875\nEpoch 17700, Loss: 10357.9521484375\nEpoch 17800, Loss: 10343.673828125\nEpoch 17900, Loss: 10329.857421875\nEpoch 18000, Loss: 10316.49609375\nEpoch 18100, Loss: 10303.603515625\nEpoch 18200, Loss: 10291.1953125\nEpoch 18300, Loss: 10279.2333984375\nEpoch 18400, Loss: 10267.7138671875\nEpoch 18500, Loss: 10256.6513671875\nEpoch 18600, Loss: 10246.037109375\nEpoch 18700, Loss: 10235.8759765625\nEpoch 18800, Loss: 10226.154296875\nEpoch 18900, Loss: 10216.84765625\nEpoch 19000, Loss: 10207.9443359375\nEpoch 19100, Loss: 10199.4326171875\nEpoch 19200, Loss: 10191.3095703125\nEpoch 19300, Loss: 10183.564453125\nEpoch 19400, Loss: 10176.1904296875\nEpoch 19500, Loss: 10169.1806640625\nEpoch 19600, Loss: 10162.52734375\nEpoch 19700, Loss: 10156.2265625\nEpoch 19800, Loss: 10150.2841796875\nEpoch 19900, Loss: 10144.6845703125\nEpoch 20000, Loss: 10139.4140625\nEpoch 20100, Loss: 10134.462890625\nEpoch 20200, Loss: 10129.8251953125\nEpoch 20300, Loss: 10125.4833984375\nEpoch 20400, Loss: 10121.4306640625\nEpoch 20500, Loss: 10117.6513671875\nEpoch 20600, Loss: 10114.1376953125\nEpoch 20700, Loss: 10110.9111328125\nEpoch 20800, Loss: 10107.927734375\nEpoch 20900, Loss: 10105.166015625\nEpoch 21000, Loss: 10102.6142578125\nEpoch 21100, Loss: 10100.259765625\nEpoch 21200, Loss: 10098.091796875\nEpoch 21300, Loss: 10096.09765625\nEpoch 21400, Loss: 10094.267578125\nEpoch 21500, Loss: 10092.5908203125\nEpoch 21600, Loss: 10091.0556640625\nEpoch 21700, Loss: 10089.654296875\nEpoch 21800, Loss: 10088.3720703125\nEpoch 21900, Loss: 10087.201171875\nEpoch 22000, Loss: 10086.1337890625\nEpoch 22100, Loss: 10085.158203125\nEpoch 22200, Loss: 10084.2666015625\nEpoch 22300, Loss: 10083.453125\nEpoch 22400, Loss: 10082.70703125\nEpoch 22500, Loss: 10082.0234375\nEpoch 22600, Loss: 10081.3935546875\nEpoch 22700, Loss: 10080.8134765625\nEpoch 22800, Loss: 10080.2763671875\nEpoch 22900, Loss: 10079.779296875\nEpoch 23000, Loss: 10079.3154296875\nEpoch 23100, Loss: 10078.8818359375\nEpoch 23200, Loss: 10078.47265625\nEpoch 23300, Loss: 10078.0869140625\nEpoch 23400, Loss: 10077.7158203125\nEpoch 23500, Loss: 10077.3603515625\nEpoch 23600, Loss: 10077.0166015625\nEpoch 23700, Loss: 10076.6806640625\nEpoch 23800, Loss: 10076.349609375\nEpoch 23900, Loss: 10076.021484375\nEpoch 24000, Loss: 10075.6962890625\nEpoch 24100, Loss: 10075.369140625\nEpoch 24200, Loss: 10075.0390625\nEpoch 24300, Loss: 10074.7041015625\nEpoch 24400, Loss: 10074.36328125\nEpoch 24500, Loss: 10074.015625\nEpoch 24600, Loss: 10073.6591796875\nEpoch 24700, Loss: 10073.29296875\nEpoch 24800, Loss: 10072.91796875\nEpoch 24900, Loss: 10072.5302734375\nEpoch 25000, Loss: 10072.1318359375\nEpoch 25100, Loss: 10071.7216796875\nEpoch 25200, Loss: 10071.2998046875\nEpoch 25300, Loss: 10070.8642578125\nEpoch 25400, Loss: 10070.4189453125\nEpoch 25500, Loss: 10069.9580078125\nEpoch 25600, Loss: 10069.54296875\nEpoch 25700, Loss: 10069.1376953125\nEpoch 25800, Loss: 10068.728515625\nEpoch 25900, Loss: 10068.314453125\nEpoch 26000, Loss: 10067.8994140625\nEpoch 26100, Loss: 10067.50390625\nEpoch 26200, Loss: 10067.1298828125\nEpoch 26300, Loss: 10066.77734375\nEpoch 26400, Loss: 10066.4423828125\nEpoch 26500, Loss: 10066.10546875\nEpoch 26600, Loss: 10065.76171875\nEpoch 26700, Loss: 10065.404296875\nEpoch 26800, Loss: 10065.0390625\nEpoch 26900, Loss: 10064.6611328125\nEpoch 27000, Loss: 10064.2744140625\nEpoch 27100, Loss: 10063.8759765625\nEpoch 27200, Loss: 10063.4697265625\nEpoch 27300, Loss: 10063.052734375\nEpoch 27400, Loss: 10062.626953125\nEpoch 27500, Loss: 10062.19140625\nEpoch 27600, Loss: 10061.7578125\nEpoch 27700, Loss: 10061.3603515625\nEpoch 27800, Loss: 10061.00390625\nEpoch 27900, Loss: 10060.6826171875\nEpoch 28000, Loss: 10060.39453125\nEpoch 28100, Loss: 10060.150390625\nEpoch 28200, Loss: 10059.9443359375\nEpoch 28300, Loss: 10059.7548828125\nEpoch 28400, Loss: 10059.560546875\nEpoch 28500, Loss: 10059.3623046875\nEpoch 28600, Loss: 10059.1572265625\nEpoch 28700, Loss: 10058.9453125\nEpoch 28800, Loss: 10058.728515625\nEpoch 28900, Loss: 10058.5048828125\nEpoch 29000, Loss: 10058.2744140625\nEpoch 29100, Loss: 10058.0390625\nEpoch 29200, Loss: 10057.7978515625\nEpoch 29300, Loss: 10057.5517578125\nEpoch 29400, Loss: 10057.298828125\nEpoch 29500, Loss: 10057.0419921875\nEpoch 29600, Loss: 10056.7783203125\nEpoch 29700, Loss: 10056.5390625\nEpoch 29800, Loss: 10056.357421875\nEpoch 29900, Loss: 10056.21875\nEpoch 30000, Loss: 10056.091796875\nEpoch 30100, Loss: 10055.9609375\nEpoch 30200, Loss: 10055.849609375\nEpoch 30300, Loss: 10055.765625\nEpoch 30400, Loss: 10055.703125\nEpoch 30500, Loss: 10055.6591796875\nEpoch 30600, Loss: 10055.6259765625\nEpoch 30700, Loss: 10055.599609375\nEpoch 30800, Loss: 10055.5810546875\nEpoch 30900, Loss: 10055.56640625\nEpoch 31000, Loss: 10055.5556640625\nEpoch 31100, Loss: 10055.546875\nEpoch 31200, Loss: 10055.5419921875\nEpoch 31300, Loss: 10055.5361328125\nEpoch 31400, Loss: 10055.533203125\nEpoch 31500, Loss: 10055.529296875\nEpoch 31600, Loss: 10055.52734375\nEpoch 31700, Loss: 10055.525390625\nEpoch 31800, Loss: 10055.525390625\nEpoch 31900, Loss: 10055.5224609375\nEpoch 32000, Loss: 10055.521484375\nEpoch 32100, Loss: 10055.51953125\nEpoch 32200, Loss: 10055.5205078125\nEpoch 32300, Loss: 10055.517578125\nEpoch 32400, Loss: 10055.5185546875\nEpoch 32500, Loss: 10055.5185546875\nEpoch 32600, Loss: 10055.5166015625\nEpoch 32700, Loss: 10055.517578125\nEpoch 32800, Loss: 10055.5166015625\nEpoch 32900, Loss: 10055.5166015625\nEpoch 33000, Loss: 10055.515625\nEpoch 33100, Loss: 10055.515625\nEpoch 33200, Loss: 10055.5146484375\nEpoch 33300, Loss: 10055.5146484375\nEpoch 33400, Loss: 10055.515625\nEpoch 33500, Loss: 10055.515625\nEpoch 33600, Loss: 10055.515625\nEpoch 33700, Loss: 10055.515625\nEpoch 33800, Loss: 10055.5146484375\nEpoch 33900, Loss: 10055.515625\nEpoch 34000, Loss: 10055.5146484375\nEpoch 34100, Loss: 10055.515625\nEpoch 34200, Loss: 10055.515625\nEpoch 34300, Loss: 10055.5146484375\nEpoch 34400, Loss: 10055.515625\nEpoch 34500, Loss: 10055.5146484375\nEpoch 34600, Loss: 10055.515625\nEpoch 34700, Loss: 10055.5146484375\nEpoch 34800, Loss: 10055.515625\nEpoch 34900, Loss: 10055.5146484375\nEpoch 35000, Loss: 10055.515625\nEpoch 35100, Loss: 10055.515625\nEpoch 35200, Loss: 10055.5146484375\nEpoch 35300, Loss: 10055.515625\nEpoch 35400, Loss: 10055.515625\nEpoch 35500, Loss: 10055.5146484375\nEpoch 35600, Loss: 10055.515625\nEpoch 35700, Loss: 10055.515625\nEpoch 35800, Loss: 10055.515625\nEpoch 35900, Loss: 10055.515625\nEpoch 36000, Loss: 10055.5146484375\nEpoch 36100, Loss: 10055.5146484375\nEpoch 36200, Loss: 10055.5146484375\nEpoch 36300, Loss: 10055.5146484375\nEpoch 36400, Loss: 10055.515625\nEpoch 36500, Loss: 10055.515625\nEpoch 36600, Loss: 10055.515625\nEpoch 36700, Loss: 10055.515625\nEpoch 36800, Loss: 10055.515625\nEpoch 36900, Loss: 10055.515625\nEpoch 37000, Loss: 10055.515625\nEpoch 37100, Loss: 10055.515625\nEpoch 37200, Loss: 10055.515625\nEpoch 37300, Loss: 10055.515625\nEpoch 37400, Loss: 10055.515625\nEpoch 37500, Loss: 10055.515625\nEpoch 37600, Loss: 10055.515625\nEpoch 37700, Loss: 10055.5146484375\nEpoch 37800, Loss: 10055.515625\nEpoch 37900, Loss: 10055.515625\nEpoch 38000, Loss: 10055.515625\nEpoch 38100, Loss: 10055.5146484375\nEpoch 38200, Loss: 10055.515625\nEpoch 38300, Loss: 10055.5146484375\nEpoch 38400, Loss: 10055.515625\nEpoch 38500, Loss: 10055.5146484375\nEpoch 38600, Loss: 10055.515625\nEpoch 38700, Loss: 10055.515625\nEpoch 38800, Loss: 10055.515625\nEpoch 38900, Loss: 10055.515625\nEpoch 39000, Loss: 10055.515625\nEpoch 39100, Loss: 10055.515625\nEpoch 39200, Loss: 10055.515625\nEpoch 39300, Loss: 10055.515625\nEpoch 39400, Loss: 10055.515625\nEpoch 39500, Loss: 10055.5146484375\nEpoch 39600, Loss: 10055.515625\nEpoch 39700, Loss: 10055.515625\nEpoch 39800, Loss: 10055.515625\nEpoch 39900, Loss: 10055.5146484375\nEpoch 40000, Loss: 10055.515625\nEpoch 40100, Loss: 10055.5146484375\nEpoch 40200, Loss: 10055.515625\nEpoch 40300, Loss: 10055.515625\nEpoch 40400, Loss: 10055.5166015625\nEpoch 40500, Loss: 10055.515625\nEpoch 40600, Loss: 10055.515625\nEpoch 40700, Loss: 10055.515625\nEpoch 40800, Loss: 10055.515625\nEpoch 40900, Loss: 10055.515625\nEpoch 41000, Loss: 10055.515625\nEpoch 41100, Loss: 10055.515625\nEpoch 41200, Loss: 10055.515625\nEpoch 41300, Loss: 10055.5146484375\nEpoch 41400, Loss: 10055.515625\nEpoch 41500, Loss: 10055.515625\nEpoch 41600, Loss: 10055.515625\nEpoch 41700, Loss: 10055.515625\nEpoch 41800, Loss: 10055.515625\nEpoch 41900, Loss: 10055.5146484375\nEpoch 42000, Loss: 10055.5146484375\nEpoch 42100, Loss: 10055.515625\nEpoch 42200, Loss: 10055.515625\nEpoch 42300, Loss: 10055.515625\nEpoch 42400, Loss: 10055.515625\nEpoch 42500, Loss: 10055.515625\nEpoch 42600, Loss: 10055.5146484375\nEpoch 42700, Loss: 10055.515625\nEpoch 42800, Loss: 10055.515625\nEpoch 42900, Loss: 10055.515625\nEpoch 43000, Loss: 10055.515625\nEpoch 43100, Loss: 10055.515625\nEpoch 43200, Loss: 10055.515625\nEpoch 43300, Loss: 10055.515625\nEpoch 43400, Loss: 10055.515625\nEpoch 43500, Loss: 10055.515625\nEpoch 43600, Loss: 10055.515625\nEpoch 43700, Loss: 10055.515625\nEpoch 43800, Loss: 10055.515625\nEpoch 43900, Loss: 10055.5146484375\nEpoch 44000, Loss: 10055.5146484375\nEpoch 44100, Loss: 10055.515625\nEpoch 44200, Loss: 10055.515625\nEpoch 44300, Loss: 10055.515625\nEpoch 44400, Loss: 10055.515625\nEpoch 44500, Loss: 10055.515625\nEpoch 44600, Loss: 10055.515625\nEpoch 44700, Loss: 10055.515625\nEpoch 44800, Loss: 10055.5146484375\nEpoch 44900, Loss: 10055.515625\nEpoch 45000, Loss: 10055.515625\nEpoch 45100, Loss: 10055.5146484375\nEpoch 45200, Loss: 10055.515625\nEpoch 45300, Loss: 10055.515625\nEpoch 45400, Loss: 10055.5146484375\nEpoch 45500, Loss: 10055.515625\nEpoch 45600, Loss: 10055.515625\nEpoch 45700, Loss: 10055.5146484375\nEpoch 45800, Loss: 10055.515625\nEpoch 45900, Loss: 10055.515625\nEpoch 46000, Loss: 10055.515625\nEpoch 46100, Loss: 10055.515625\nEpoch 46200, Loss: 10055.515625\nEpoch 46300, Loss: 10055.515625\nEpoch 46400, Loss: 10055.515625\nEpoch 46500, Loss: 10055.515625\nEpoch 46600, Loss: 10055.515625\nEpoch 46700, Loss: 10055.515625\nEpoch 46800, Loss: 10055.515625\nEpoch 46900, Loss: 10055.515625\nEpoch 47000, Loss: 10055.515625\nEpoch 47100, Loss: 10055.515625\nEpoch 47200, Loss: 10055.515625\nEpoch 47300, Loss: 10055.5146484375\nEpoch 47400, Loss: 10055.515625\nEpoch 47500, Loss: 10055.515625\nEpoch 47600, Loss: 10055.515625\nEpoch 47700, Loss: 10055.515625\nEpoch 47800, Loss: 10055.515625\nEpoch 47900, Loss: 10055.5146484375\nEpoch 48000, Loss: 10055.515625\nEpoch 48100, Loss: 10055.515625\nEpoch 48200, Loss: 10055.515625\nEpoch 48300, Loss: 10055.515625\nEpoch 48400, Loss: 10055.515625\nEpoch 48500, Loss: 10055.515625\nEpoch 48600, Loss: 10055.515625\nEpoch 48700, Loss: 10055.515625\nEpoch 48800, Loss: 10055.515625\nEpoch 48900, Loss: 10055.515625\nEpoch 49000, Loss: 10055.515625\nEpoch 49100, Loss: 10055.515625\nEpoch 49200, Loss: 10055.515625\nEpoch 49300, Loss: 10055.515625\nEpoch 49400, Loss: 10055.515625\nEpoch 49500, Loss: 10055.515625\nEpoch 49600, Loss: 10055.515625\nEpoch 49700, Loss: 10055.515625\nEpoch 49800, Loss: 10055.515625\nEpoch 49900, Loss: 10055.515625\nEpoch 0, Loss: 63828.96875\nEpoch 100, Loss: 60925.1484375\nEpoch 200, Loss: 58209.96875\nEpoch 300, Loss: 55669.66015625\nEpoch 400, Loss: 53291.32421875\nEpoch 500, Loss: 51063.953125\nEpoch 600, Loss: 48978.60546875\nEpoch 700, Loss: 47026.7421875\nEpoch 800, Loss: 45200.66015625\nEpoch 900, Loss: 43493.3046875\nEpoch 1000, Loss: 41897.82421875\nEpoch 1100, Loss: 40407.41015625\nEpoch 1200, Loss: 39015.66796875\nEpoch 1300, Loss: 37716.44921875\nEpoch 1400, Loss: 36503.88671875\nEpoch 1500, Loss: 35372.546875\nEpoch 1600, Loss: 34316.62890625\nEpoch 1700, Loss: 33330.86328125\nEpoch 1800, Loss: 32410.28515625\nEpoch 1900, Loss: 31550.205078125\nEpoch 2000, Loss: 30746.2578125\nEpoch 2100, Loss: 29994.384765625\nEpoch 2200, Loss: 29291.146484375\nEpoch 2300, Loss: 28632.7109375\nEpoch 2400, Loss: 28016.265625\nEpoch 2500, Loss: 27438.423828125\nEpoch 2600, Loss: 26896.18359375\nEpoch 2700, Loss: 26386.91796875\nEpoch 2800, Loss: 25908.177734375\nEpoch 2900, Loss: 25457.6796875\nEpoch 3000, Loss: 25033.30078125\nEpoch 3100, Loss: 24633.056640625\nEpoch 3200, Loss: 24255.072265625\nEpoch 3300, Loss: 23897.6953125\nEpoch 3400, Loss: 23559.314453125\nEpoch 3500, Loss: 23238.4921875\nEpoch 3600, Loss: 22933.69921875\nEpoch 3700, Loss: 22643.34375\nEpoch 3800, Loss: 22366.125\nEpoch 3900, Loss: 22100.86328125\nEpoch 4000, Loss: 21846.458984375\nEpoch 4100, Loss: 21602.1171875\nEpoch 4200, Loss: 21366.796875\nEpoch 4300, Loss: 21139.591796875\nEpoch 4400, Loss: 20919.77734375\nEpoch 4500, Loss: 20706.91796875\nEpoch 4600, Loss: 20500.2109375\nEpoch 4700, Loss: 20299.1484375\nEpoch 4800, Loss: 20103.30078125\nEpoch 4900, Loss: 19912.291015625\nEpoch 5000, Loss: 19725.787109375\nEpoch 5100, Loss: 19543.50390625\nEpoch 5200, Loss: 19365.203125\nEpoch 5300, Loss: 19190.66015625\nEpoch 5400, Loss: 19019.689453125\nEpoch 5500, Loss: 18852.11328125\nEpoch 5600, Loss: 18687.794921875\nEpoch 5700, Loss: 18526.583984375\nEpoch 5800, Loss: 18368.369140625\nEpoch 5900, Loss: 18213.02734375\nEpoch 6000, Loss: 18060.5859375\nEpoch 6100, Loss: 17910.91015625\nEpoch 6200, Loss: 17763.828125\nEpoch 6300, Loss: 17619.26171875\nEpoch 6400, Loss: 17477.1484375\nEpoch 6500, Loss: 17337.412109375\nEpoch 6600, Loss: 17200.001953125\nEpoch 6700, Loss: 17064.853515625\nEpoch 6800, Loss: 16931.921875\nEpoch 6900, Loss: 16801.150390625\nEpoch 7000, Loss: 16672.494140625\nEpoch 7100, Loss: 16545.912109375\nEpoch 7200, Loss: 16421.478515625\nEpoch 7300, Loss: 16299.169921875\nEpoch 7400, Loss: 16178.9326171875\nEpoch 7500, Loss: 16060.666015625\nEpoch 7600, Loss: 15944.220703125\nEpoch 7700, Loss: 15829.568359375\nEpoch 7800, Loss: 15716.671875\nEpoch 7900, Loss: 15605.482421875\nEpoch 8000, Loss: 15495.9794921875\nEpoch 8100, Loss: 15388.115234375\nEpoch 8200, Loss: 15281.8623046875\nEpoch 8300, Loss: 15177.1943359375\nEpoch 8400, Loss: 15074.0791015625\nEpoch 8500, Loss: 14972.4814453125\nEpoch 8600, Loss: 14872.3798828125\nEpoch 8700, Loss: 14773.7490234375\nEpoch 8800, Loss: 14676.5654296875\nEpoch 8900, Loss: 14580.8095703125\nEpoch 9000, Loss: 14486.4599609375\nEpoch 9100, Loss: 14393.501953125\nEpoch 9200, Loss: 14302.0205078125\nEpoch 9300, Loss: 14212.005859375\nEpoch 9400, Loss: 14123.3408203125\nEpoch 9500, Loss: 14036.0078125\nEpoch 9600, Loss: 13949.9970703125\nEpoch 9700, Loss: 13865.2939453125\nEpoch 9800, Loss: 13781.8955078125\nEpoch 9900, Loss: 13699.787109375\nEpoch 10000, Loss: 13618.96875\nEpoch 10100, Loss: 13539.4287109375\nEpoch 10200, Loss: 13461.166015625\nEpoch 10300, Loss: 13384.16796875\nEpoch 10400, Loss: 13308.4375\nEpoch 10500, Loss: 13233.962890625\nEpoch 10600, Loss: 13160.7451171875\nEpoch 10700, Loss: 13088.771484375\nEpoch 10800, Loss: 13018.0380859375\nEpoch 10900, Loss: 12948.5361328125\nEpoch 11000, Loss: 12880.2578125\nEpoch 11100, Loss: 12813.1923828125\nEpoch 11200, Loss: 12747.3330078125\nEpoch 11300, Loss: 12682.662109375\nEpoch 11400, Loss: 12619.1669921875\nEpoch 11500, Loss: 12556.8662109375\nEpoch 11600, Loss: 12495.73046875\nEpoch 11700, Loss: 12435.7353515625\nEpoch 11800, Loss: 12376.84765625\nEpoch 11900, Loss: 12319.052734375\nEpoch 12000, Loss: 12262.3330078125\nEpoch 12100, Loss: 12206.66796875\nEpoch 12200, Loss: 12152.08203125\nEpoch 12300, Loss: 12098.6962890625\nEpoch 12400, Loss: 12046.30078125\nEpoch 12500, Loss: 11994.8876953125\nEpoch 12600, Loss: 11944.4267578125\nEpoch 12700, Loss: 11894.90625\nEpoch 12800, Loss: 11846.3173828125\nEpoch 12900, Loss: 11798.6318359375\nEpoch 13000, Loss: 11751.83984375\nEpoch 13100, Loss: 11705.9306640625\nEpoch 13200, Loss: 11660.8837890625\nEpoch 13300, Loss: 11616.69140625\nEpoch 13400, Loss: 11573.3408203125\nEpoch 13500, Loss: 11530.8212890625\nEpoch 13600, Loss: 11489.1142578125\nEpoch 13700, Loss: 11448.2255859375\nEpoch 13800, Loss: 11408.1298828125\nEpoch 13900, Loss: 11368.90234375\nEpoch 14000, Loss: 11330.490234375\nEpoch 14100, Loss: 11292.84375\nEpoch 14200, Loss: 11256.1181640625\nEpoch 14300, Loss: 11220.2578125\nEpoch 14400, Loss: 11185.1767578125\nEpoch 14500, Loss: 11150.83203125\nEpoch 14600, Loss: 11117.2109375\nEpoch 14700, Loss: 11084.37890625\nEpoch 14800, Loss: 11052.3193359375\nEpoch 14900, Loss: 11020.9736328125\nEpoch 15000, Loss: 10990.3193359375\nEpoch 15100, Loss: 10960.345703125\nEpoch 15200, Loss: 10931.0478515625\nEpoch 15300, Loss: 10902.4345703125\nEpoch 15400, Loss: 10874.5068359375\nEpoch 15500, Loss: 10847.2529296875\nEpoch 15600, Loss: 10820.666015625\nEpoch 15700, Loss: 10794.73828125\nEpoch 15800, Loss: 10769.4619140625\nEpoch 15900, Loss: 10744.8193359375\nEpoch 16000, Loss: 10720.921875\nEpoch 16100, Loss: 10697.7099609375\nEpoch 16200, Loss: 10675.1396484375\nEpoch 16300, Loss: 10653.173828125\nEpoch 16400, Loss: 10631.7900390625\nEpoch 16500, Loss: 10610.9765625\nEpoch 16600, Loss: 10590.7255859375\nEpoch 16700, Loss: 10571.0244140625\nEpoch 16800, Loss: 10551.865234375\nEpoch 16900, Loss: 10533.2392578125\nEpoch 17000, Loss: 10515.138671875\nEpoch 17100, Loss: 10497.552734375\nEpoch 17200, Loss: 10480.4755859375\nEpoch 17300, Loss: 10463.8916015625\nEpoch 17400, Loss: 10447.802734375\nEpoch 17500, Loss: 10432.1953125\nEpoch 17600, Loss: 10417.08203125\nEpoch 17700, Loss: 10402.486328125\nEpoch 17800, Loss: 10388.36328125\nEpoch 17900, Loss: 10374.7001953125\nEpoch 18000, Loss: 10361.494140625\nEpoch 18100, Loss: 10348.7392578125\nEpoch 18200, Loss: 10336.4306640625\nEpoch 18300, Loss: 10324.5830078125\nEpoch 18400, Loss: 10313.1787109375\nEpoch 18500, Loss: 10302.2333984375\nEpoch 18600, Loss: 10291.73046875\nEpoch 18700, Loss: 10281.6669921875\nEpoch 18800, Loss: 10272.029296875\nEpoch 18900, Loss: 10262.8154296875\nEpoch 19000, Loss: 10254.009765625\nEpoch 19100, Loss: 10245.5986328125\nEpoch 19200, Loss: 10237.5693359375\nEpoch 19300, Loss: 10229.9150390625\nEpoch 19400, Loss: 10222.62890625\nEpoch 19500, Loss: 10215.701171875\nEpoch 19600, Loss: 10209.134765625\nEpoch 19700, Loss: 10202.9296875\nEpoch 19800, Loss: 10197.0703125\nEpoch 19900, Loss: 10191.5498046875\nEpoch 20000, Loss: 10186.353515625\nEpoch 20100, Loss: 10181.474609375\nEpoch 20200, Loss: 10176.95703125\nEpoch 20300, Loss: 10172.75390625\nEpoch 20400, Loss: 10168.8369140625\nEpoch 20500, Loss: 10165.1904296875\nEpoch 20600, Loss: 10161.80078125\nEpoch 20700, Loss: 10158.6533203125\nEpoch 20800, Loss: 10155.7333984375\nEpoch 20900, Loss: 10153.029296875\nEpoch 21000, Loss: 10150.529296875\nEpoch 21100, Loss: 10148.2216796875\nEpoch 21200, Loss: 10146.0966796875\nEpoch 21300, Loss: 10144.1396484375\nEpoch 21400, Loss: 10142.341796875\nEpoch 21500, Loss: 10140.6923828125\nEpoch 21600, Loss: 10139.1796875\nEpoch 21700, Loss: 10137.7939453125\nEpoch 21800, Loss: 10136.5234375\nEpoch 21900, Loss: 10135.3603515625\nEpoch 22000, Loss: 10134.291015625\nEpoch 22100, Loss: 10133.3125\nEpoch 22200, Loss: 10132.412109375\nEpoch 22300, Loss: 10131.58203125\nEpoch 22400, Loss: 10130.8134765625\nEpoch 22500, Loss: 10130.1005859375\nEpoch 22600, Loss: 10129.4365234375\nEpoch 22700, Loss: 10128.81640625\nEpoch 22800, Loss: 10128.2314453125\nEpoch 22900, Loss: 10127.6787109375\nEpoch 23000, Loss: 10127.150390625\nEpoch 23100, Loss: 10126.646484375\nEpoch 23200, Loss: 10126.16015625\nEpoch 23300, Loss: 10125.6884765625\nEpoch 23400, Loss: 10125.2275390625\nEpoch 23500, Loss: 10124.771484375\nEpoch 23600, Loss: 10124.3212890625\nEpoch 23700, Loss: 10123.873046875\nEpoch 23800, Loss: 10123.423828125\nEpoch 23900, Loss: 10122.970703125\nEpoch 24000, Loss: 10122.5126953125\nEpoch 24100, Loss: 10122.0458984375\nEpoch 24200, Loss: 10121.572265625\nEpoch 24300, Loss: 10121.087890625\nEpoch 24400, Loss: 10120.5908203125\nEpoch 24500, Loss: 10120.0830078125\nEpoch 24600, Loss: 10119.560546875\nEpoch 24700, Loss: 10119.0244140625\nEpoch 24800, Loss: 10118.4736328125\nEpoch 24900, Loss: 10117.9111328125\nEpoch 25000, Loss: 10117.4013671875\nEpoch 25100, Loss: 10116.89453125\nEpoch 25200, Loss: 10116.38671875\nEpoch 25300, Loss: 10115.87109375\nEpoch 25400, Loss: 10115.3583984375\nEpoch 25500, Loss: 10114.8642578125\nEpoch 25600, Loss: 10114.3876953125\nEpoch 25700, Loss: 10113.9306640625\nEpoch 25800, Loss: 10113.490234375\nEpoch 25900, Loss: 10113.0673828125\nEpoch 26000, Loss: 10112.642578125\nEpoch 26100, Loss: 10112.208984375\nEpoch 26200, Loss: 10111.76171875\nEpoch 26300, Loss: 10111.3017578125\nEpoch 26400, Loss: 10110.8291015625\nEpoch 26500, Loss: 10110.3427734375\nEpoch 26600, Loss: 10109.8447265625\nEpoch 26700, Loss: 10109.333984375\nEpoch 26800, Loss: 10108.8115234375\nEpoch 26900, Loss: 10108.2763671875\nEpoch 27000, Loss: 10107.732421875\nEpoch 27100, Loss: 10107.205078125\nEpoch 27200, Loss: 10106.7177734375\nEpoch 27300, Loss: 10106.2685546875\nEpoch 27400, Loss: 10105.85546875\nEpoch 27500, Loss: 10105.50390625\nEpoch 27600, Loss: 10105.19921875\nEpoch 27700, Loss: 10104.9306640625\nEpoch 27800, Loss: 10104.6884765625\nEpoch 27900, Loss: 10104.447265625\nEpoch 28000, Loss: 10104.19921875\nEpoch 28100, Loss: 10103.9443359375\nEpoch 28200, Loss: 10103.681640625\nEpoch 28300, Loss: 10103.412109375\nEpoch 28400, Loss: 10103.134765625\nEpoch 28500, Loss: 10102.8486328125\nEpoch 28600, Loss: 10102.5556640625\nEpoch 28700, Loss: 10102.2548828125\nEpoch 28800, Loss: 10101.947265625\nEpoch 28900, Loss: 10101.6328125\nEpoch 29000, Loss: 10101.310546875\nEpoch 29100, Loss: 10100.982421875\nEpoch 29200, Loss: 10100.6796875\nEpoch 29300, Loss: 10100.4365234375\nEpoch 29400, Loss: 10100.2392578125\nEpoch 29500, Loss: 10100.07421875\nEpoch 29600, Loss: 10099.9130859375\nEpoch 29700, Loss: 10099.76953125\nEpoch 29800, Loss: 10099.6552734375\nEpoch 29900, Loss: 10099.5625\nEpoch 30000, Loss: 10099.490234375\nEpoch 30100, Loss: 10099.439453125\nEpoch 30200, Loss: 10099.40234375\nEpoch 30300, Loss: 10099.373046875\nEpoch 30400, Loss: 10099.349609375\nEpoch 30500, Loss: 10099.3291015625\nEpoch 30600, Loss: 10099.3154296875\nEpoch 30700, Loss: 10099.3037109375\nEpoch 30800, Loss: 10099.2939453125\nEpoch 30900, Loss: 10099.287109375\nEpoch 31000, Loss: 10099.2802734375\nEpoch 31100, Loss: 10099.2763671875\nEpoch 31200, Loss: 10099.271484375\nEpoch 31300, Loss: 10099.26953125\nEpoch 31400, Loss: 10099.265625\nEpoch 31500, Loss: 10099.263671875\nEpoch 31600, Loss: 10099.26171875\nEpoch 31700, Loss: 10099.2607421875\nEpoch 31800, Loss: 10099.2587890625\nEpoch 31900, Loss: 10099.2568359375\nEpoch 32000, Loss: 10099.2568359375\nEpoch 32100, Loss: 10099.2548828125\nEpoch 32200, Loss: 10099.2548828125\nEpoch 32300, Loss: 10099.25390625\nEpoch 32400, Loss: 10099.2529296875\nEpoch 32500, Loss: 10099.2529296875\nEpoch 32600, Loss: 10099.2529296875\nEpoch 32700, Loss: 10099.251953125\nEpoch 32800, Loss: 10099.251953125\nEpoch 32900, Loss: 10099.251953125\nEpoch 33000, Loss: 10099.2529296875\nEpoch 33100, Loss: 10099.2529296875\nEpoch 33200, Loss: 10099.251953125\nEpoch 33300, Loss: 10099.251953125\nEpoch 33400, Loss: 10099.251953125\nEpoch 33500, Loss: 10099.251953125\nEpoch 33600, Loss: 10099.2529296875\nEpoch 33700, Loss: 10099.2529296875\nEpoch 33800, Loss: 10099.251953125\nEpoch 33900, Loss: 10099.251953125\nEpoch 34000, Loss: 10099.251953125\nEpoch 34100, Loss: 10099.2509765625\nEpoch 34200, Loss: 10099.251953125\nEpoch 34300, Loss: 10099.2509765625\nEpoch 34400, Loss: 10099.251953125\nEpoch 34500, Loss: 10099.251953125\nEpoch 34600, Loss: 10099.251953125\nEpoch 34700, Loss: 10099.2509765625\nEpoch 34800, Loss: 10099.2509765625\nEpoch 34900, Loss: 10099.251953125\nEpoch 35000, Loss: 10099.2509765625\nEpoch 35100, Loss: 10099.2509765625\nEpoch 35200, Loss: 10099.251953125\nEpoch 35300, Loss: 10099.2509765625\nEpoch 35400, Loss: 10099.2509765625\nEpoch 35500, Loss: 10099.251953125\nEpoch 35600, Loss: 10099.2509765625\nEpoch 35700, Loss: 10099.2509765625\nEpoch 35800, Loss: 10099.251953125\nEpoch 35900, Loss: 10099.2509765625\nEpoch 36000, Loss: 10099.2509765625\nEpoch 36100, Loss: 10099.2509765625\nEpoch 36200, Loss: 10099.251953125\nEpoch 36300, Loss: 10099.2509765625\nEpoch 36400, Loss: 10099.2509765625\nEpoch 36500, Loss: 10099.2529296875\nEpoch 36600, Loss: 10099.2509765625\nEpoch 36700, Loss: 10099.2509765625\nEpoch 36800, Loss: 10099.251953125\nEpoch 36900, Loss: 10099.251953125\nEpoch 37000, Loss: 10099.2529296875\nEpoch 37100, Loss: 10099.2509765625\nEpoch 37200, Loss: 10099.2509765625\nEpoch 37300, Loss: 10099.251953125\nEpoch 37400, Loss: 10099.251953125\nEpoch 37500, Loss: 10099.2509765625\nEpoch 37600, Loss: 10099.2509765625\nEpoch 37700, Loss: 10099.2509765625\nEpoch 37800, Loss: 10099.2509765625\nEpoch 37900, Loss: 10099.251953125\nEpoch 38000, Loss: 10099.2509765625\nEpoch 38100, Loss: 10099.2509765625\nEpoch 38200, Loss: 10099.251953125\nEpoch 38300, Loss: 10099.2509765625\nEpoch 38400, Loss: 10099.251953125\nEpoch 38500, Loss: 10099.251953125\nEpoch 38600, Loss: 10099.251953125\nEpoch 38700, Loss: 10099.251953125\nEpoch 38800, Loss: 10099.251953125\nEpoch 38900, Loss: 10099.2509765625\nEpoch 39000, Loss: 10099.2509765625\nEpoch 39100, Loss: 10099.2509765625\nEpoch 39200, Loss: 10099.2509765625\nEpoch 39300, Loss: 10099.251953125\nEpoch 39400, Loss: 10099.2509765625\nEpoch 39500, Loss: 10099.2509765625\nEpoch 39600, Loss: 10099.2509765625\nEpoch 39700, Loss: 10099.251953125\nEpoch 39800, Loss: 10099.2509765625\nEpoch 39900, Loss: 10099.251953125\nEpoch 40000, Loss: 10099.251953125\nEpoch 40100, Loss: 10099.2509765625\nEpoch 40200, Loss: 10099.2509765625\nEpoch 40300, Loss: 10099.2509765625\nEpoch 40400, Loss: 10099.251953125\nEpoch 40500, Loss: 10099.251953125\nEpoch 40600, Loss: 10099.2509765625\nEpoch 40700, Loss: 10099.2509765625\nEpoch 40800, Loss: 10099.2509765625\nEpoch 40900, Loss: 10099.2509765625\nEpoch 41000, Loss: 10099.251953125\nEpoch 41100, Loss: 10099.2509765625\nEpoch 41200, Loss: 10099.251953125\nEpoch 41300, Loss: 10099.2509765625\nEpoch 41400, Loss: 10099.251953125\nEpoch 41500, Loss: 10099.2509765625\nEpoch 41600, Loss: 10099.2509765625\nEpoch 41700, Loss: 10099.2509765625\nEpoch 41800, Loss: 10099.2509765625\nEpoch 41900, Loss: 10099.2509765625\nEpoch 42000, Loss: 10099.251953125\nEpoch 42100, Loss: 10099.251953125\nEpoch 42200, Loss: 10099.2509765625\nEpoch 42300, Loss: 10099.2509765625\nEpoch 42400, Loss: 10099.2509765625\nEpoch 42500, Loss: 10099.2509765625\nEpoch 42600, Loss: 10099.251953125\nEpoch 42700, Loss: 10099.2509765625\nEpoch 42800, Loss: 10099.2509765625\nEpoch 42900, Loss: 10099.2509765625\nEpoch 43000, Loss: 10099.251953125\nEpoch 43100, Loss: 10099.2509765625\nEpoch 43200, Loss: 10099.2509765625\nEpoch 43300, Loss: 10099.2509765625\nEpoch 43400, Loss: 10099.251953125\nEpoch 43500, Loss: 10099.251953125\nEpoch 43600, Loss: 10099.251953125\nEpoch 43700, Loss: 10099.251953125\nEpoch 43800, Loss: 10099.2509765625\nEpoch 43900, Loss: 10099.2509765625\nEpoch 44000, Loss: 10099.2509765625\nEpoch 44100, Loss: 10099.251953125\nEpoch 44200, Loss: 10099.2509765625\nEpoch 44300, Loss: 10099.251953125\nEpoch 44400, Loss: 10099.2529296875\nEpoch 44500, Loss: 10099.251953125\nEpoch 44600, Loss: 10099.251953125\nEpoch 44700, Loss: 10099.251953125\nEpoch 44800, Loss: 10099.2509765625\nEpoch 44900, Loss: 10099.2509765625\nEpoch 45000, Loss: 10099.251953125\nEpoch 45100, Loss: 10099.251953125\nEpoch 45200, Loss: 10099.251953125\nEpoch 45300, Loss: 10099.251953125\nEpoch 45400, Loss: 10099.2509765625\nEpoch 45500, Loss: 10099.251953125\nEpoch 45600, Loss: 10099.251953125\nEpoch 45700, Loss: 10099.2509765625\nEpoch 45800, Loss: 10099.251953125\nEpoch 45900, Loss: 10099.2509765625\nEpoch 46000, Loss: 10099.2509765625\nEpoch 46100, Loss: 10099.251953125\nEpoch 46200, Loss: 10099.251953125\nEpoch 46300, Loss: 10099.2509765625\nEpoch 46400, Loss: 10099.251953125\nEpoch 46500, Loss: 10099.251953125\nEpoch 46600, Loss: 10099.2509765625\nEpoch 46700, Loss: 10099.2509765625\nEpoch 46800, Loss: 10099.251953125\nEpoch 46900, Loss: 10099.2509765625\nEpoch 47000, Loss: 10099.2529296875\nEpoch 47100, Loss: 10099.2509765625\nEpoch 47200, Loss: 10099.251953125\nEpoch 47300, Loss: 10099.251953125\nEpoch 47400, Loss: 10099.2529296875\nEpoch 47500, Loss: 10099.251953125\nEpoch 47600, Loss: 10099.251953125\nEpoch 47700, Loss: 10099.2509765625\nEpoch 47800, Loss: 10099.2509765625\nEpoch 47900, Loss: 10099.2509765625\nEpoch 48000, Loss: 10099.251953125\nEpoch 48100, Loss: 10099.251953125\nEpoch 48200, Loss: 10099.251953125\nEpoch 48300, Loss: 10099.2509765625\nEpoch 48400, Loss: 10099.251953125\nEpoch 48500, Loss: 10099.251953125\nEpoch 48600, Loss: 10099.251953125\nEpoch 48700, Loss: 10099.251953125\nEpoch 48800, Loss: 10099.251953125\nEpoch 48900, Loss: 10099.251953125\nEpoch 49000, Loss: 10099.2509765625\nEpoch 49100, Loss: 10099.251953125\nEpoch 49200, Loss: 10099.251953125\nEpoch 49300, Loss: 10099.2509765625\nEpoch 49400, Loss: 10099.251953125\nEpoch 49500, Loss: 10099.2509765625\nEpoch 49600, Loss: 10099.2509765625\nEpoch 49700, Loss: 10099.2509765625\nEpoch 49800, Loss: 10099.2529296875\nEpoch 49900, Loss: 10099.251953125\nEpoch 0, Loss: 63828.96875\nEpoch 100, Loss: 60926.1484375\nEpoch 200, Loss: 58211.88671875\nEpoch 300, Loss: 55672.3671875\nEpoch 400, Loss: 53294.84765625\nEpoch 500, Loss: 51068.2578125\nEpoch 600, Loss: 48983.70703125\nEpoch 700, Loss: 47032.58203125\nEpoch 800, Loss: 45207.171875\nEpoch 900, Loss: 43500.515625\nEpoch 1000, Loss: 41905.71484375\nEpoch 1100, Loss: 40415.9296875\nEpoch 1200, Loss: 39024.76953125\nEpoch 1300, Loss: 37726.08203125\nEpoch 1400, Loss: 36514.0390625\nEpoch 1500, Loss: 35383.23828125\nEpoch 1600, Loss: 34327.8203125\nEpoch 1700, Loss: 33342.5078125\nEpoch 1800, Loss: 32422.3359375\nEpoch 1900, Loss: 31562.625\nEpoch 2000, Loss: 30759.02734375\nEpoch 2100, Loss: 30007.494140625\nEpoch 2200, Loss: 29304.654296875\nEpoch 2300, Loss: 28646.591796875\nEpoch 2400, Loss: 28030.66796875\nEpoch 2500, Loss: 27453.330078125\nEpoch 2600, Loss: 26911.5703125\nEpoch 2700, Loss: 26402.76171875\nEpoch 2800, Loss: 25924.451171875\nEpoch 2900, Loss: 25474.365234375\nEpoch 3000, Loss: 25050.384765625\nEpoch 3100, Loss: 24650.51953125\nEpoch 3200, Loss: 24272.900390625\nEpoch 3300, Loss: 23915.908203125\nEpoch 3400, Loss: 23577.94140625\nEpoch 3500, Loss: 23257.599609375\nEpoch 3600, Loss: 22953.33203125\nEpoch 3700, Loss: 22663.470703125\nEpoch 3800, Loss: 22386.732421875\nEpoch 3900, Loss: 22121.9296875\nEpoch 4000, Loss: 21867.9765625\nEpoch 4100, Loss: 21624.158203125\nEpoch 4200, Loss: 21389.330078125\nEpoch 4300, Loss: 21162.603515625\nEpoch 4400, Loss: 20943.2734375\nEpoch 4500, Loss: 20730.9296875\nEpoch 4600, Loss: 20524.716796875\nEpoch 4700, Loss: 20324.142578125\nEpoch 4800, Loss: 20128.765625\nEpoch 4900, Loss: 19938.2109375\nEpoch 5000, Loss: 19752.15625\nEpoch 5100, Loss: 19570.314453125\nEpoch 5200, Loss: 19392.4375\nEpoch 5300, Loss: 19218.3125\nEpoch 5400, Loss: 19047.748046875\nEpoch 5500, Loss: 18880.57421875\nEpoch 5600, Loss: 18716.63671875\nEpoch 5700, Loss: 18555.80859375\nEpoch 5800, Loss: 18397.962890625\nEpoch 5900, Loss: 18242.986328125\nEpoch 6000, Loss: 18090.978515625\nEpoch 6100, Loss: 17941.693359375\nEpoch 6200, Loss: 17794.9921875\nEpoch 6300, Loss: 17650.80078125\nEpoch 6400, Loss: 17509.048828125\nEpoch 6500, Loss: 17369.66796875\nEpoch 6600, Loss: 17232.6015625\nEpoch 6700, Loss: 17097.791015625\nEpoch 6800, Loss: 16965.185546875\nEpoch 6900, Loss: 16834.73046875\nEpoch 7000, Loss: 16706.384765625\nEpoch 7100, Loss: 16580.1171875\nEpoch 7200, Loss: 16456.041015625\nEpoch 7300, Loss: 16334.1064453125\nEpoch 7400, Loss: 16214.23046875\nEpoch 7500, Loss: 16096.341796875\nEpoch 7600, Loss: 15980.26953125\nEpoch 7700, Loss: 15865.978515625\nEpoch 7800, Loss: 15753.419921875\nEpoch 7900, Loss: 15642.56640625\nEpoch 8000, Loss: 15533.376953125\nEpoch 8100, Loss: 15425.8232421875\nEpoch 8200, Loss: 15319.869140625\nEpoch 8300, Loss: 15215.4912109375\nEpoch 8400, Loss: 15112.65234375\nEpoch 8500, Loss: 15011.322265625\nEpoch 8600, Loss: 14911.4853515625\nEpoch 8700, Loss: 14813.1044921875\nEpoch 8800, Loss: 14716.166015625\nEpoch 8900, Loss: 14620.6474609375\nEpoch 9000, Loss: 14526.5322265625\nEpoch 9100, Loss: 14433.796875\nEpoch 9200, Loss: 14342.662109375\nEpoch 9300, Loss: 14252.9091796875\nEpoch 9400, Loss: 14164.4990234375\nEpoch 9500, Loss: 14077.4150390625\nEpoch 9600, Loss: 13991.6474609375\nEpoch 9700, Loss: 13907.185546875\nEpoch 9800, Loss: 13824.0234375\nEpoch 9900, Loss: 13742.1474609375\nEpoch 10000, Loss: 13661.552734375\nEpoch 10100, Loss: 13582.232421875\nEpoch 10200, Loss: 13504.1845703125\nEpoch 10300, Loss: 13427.3984375\nEpoch 10400, Loss: 13351.8720703125\nEpoch 10500, Loss: 13277.59765625\nEpoch 10600, Loss: 13204.57421875\nEpoch 10700, Loss: 13132.7919921875\nEpoch 10800, Loss: 13062.2451171875\nEpoch 10900, Loss: 12992.9248046875\nEpoch 11000, Loss: 12924.8251953125\nEpoch 11100, Loss: 12857.93359375\nEpoch 11200, Loss: 12792.240234375\nEpoch 11300, Loss: 12727.73046875\nEpoch 11400, Loss: 12664.40234375\nEpoch 11500, Loss: 12602.271484375\nEpoch 11600, Loss: 12541.294921875\nEpoch 11700, Loss: 12481.447265625\nEpoch 11800, Loss: 12422.705078125\nEpoch 11900, Loss: 12365.048828125\nEpoch 12000, Loss: 12308.4580078125\nEpoch 12100, Loss: 12252.916015625\nEpoch 12200, Loss: 12198.5341796875\nEpoch 12300, Loss: 12145.3046875\nEpoch 12400, Loss: 12093.0703125\nEpoch 12500, Loss: 12041.8056640625\nEpoch 12600, Loss: 11991.48828125\nEpoch 12700, Loss: 11942.1083984375\nEpoch 12800, Loss: 11893.640625\nEpoch 12900, Loss: 11846.0732421875\nEpoch 13000, Loss: 11799.396484375\nEpoch 13100, Loss: 11753.587890625\nEpoch 13200, Loss: 11708.640625\nEpoch 13300, Loss: 11664.5419921875\nEpoch 13400, Loss: 11621.2783203125\nEpoch 13500, Loss: 11578.9033203125\nEpoch 13600, Loss: 11537.4306640625\nEpoch 13700, Loss: 11496.7724609375\nEpoch 13800, Loss: 11456.9072265625\nEpoch 13900, Loss: 11417.8271484375\nEpoch 14000, Loss: 11379.517578125\nEpoch 14100, Loss: 11342.0322265625\nEpoch 14200, Loss: 11305.494140625\nEpoch 14300, Loss: 11269.791015625\nEpoch 14400, Loss: 11234.8681640625\nEpoch 14500, Loss: 11200.6826171875\nEpoch 14600, Loss: 11167.22265625\nEpoch 14700, Loss: 11134.5751953125\nEpoch 14800, Loss: 11102.6865234375\nEpoch 14900, Loss: 11071.509765625\nEpoch 15000, Loss: 11041.0205078125\nEpoch 15100, Loss: 11011.203125\nEpoch 15200, Loss: 10982.0654296875\nEpoch 15300, Loss: 10953.6103515625\nEpoch 15400, Loss: 10925.8310546875\nEpoch 15500, Loss: 10898.8203125\nEpoch 15600, Loss: 10872.52734375\nEpoch 15700, Loss: 10846.892578125\nEpoch 15800, Loss: 10821.9072265625\nEpoch 15900, Loss: 10797.5712890625\nEpoch 16000, Loss: 10773.8955078125\nEpoch 16100, Loss: 10750.8642578125\nEpoch 16200, Loss: 10728.4658203125\nEpoch 16300, Loss: 10706.685546875\nEpoch 16400, Loss: 10685.4833984375\nEpoch 16500, Loss: 10664.8447265625\nEpoch 16600, Loss: 10644.7568359375\nEpoch 16700, Loss: 10625.2158203125\nEpoch 16800, Loss: 10606.20703125\nEpoch 16900, Loss: 10587.728515625\nEpoch 17000, Loss: 10569.7646484375\nEpoch 17100, Loss: 10552.3251953125\nEpoch 17200, Loss: 10535.4462890625\nEpoch 17300, Loss: 10519.0751953125\nEpoch 17400, Loss: 10503.1953125\nEpoch 17500, Loss: 10487.79296875\nEpoch 17600, Loss: 10472.869140625\nEpoch 17700, Loss: 10458.41015625\nEpoch 17800, Loss: 10444.4130859375\nEpoch 17900, Loss: 10430.8994140625\nEpoch 18000, Loss: 10417.8486328125\nEpoch 18100, Loss: 10405.2470703125\nEpoch 18200, Loss: 10393.0908203125\nEpoch 18300, Loss: 10381.3720703125\nEpoch 18400, Loss: 10370.0869140625\nEpoch 18500, Loss: 10359.2509765625\nEpoch 18600, Loss: 10348.853515625\nEpoch 18700, Loss: 10338.890625\nEpoch 18800, Loss: 10329.3505859375\nEpoch 18900, Loss: 10320.2275390625\nEpoch 19000, Loss: 10311.513671875\nEpoch 19100, Loss: 10303.201171875\nEpoch 19200, Loss: 10295.27734375\nEpoch 19300, Loss: 10287.73046875\nEpoch 19400, Loss: 10280.5458984375\nEpoch 19500, Loss: 10273.73828125\nEpoch 19600, Loss: 10267.291015625\nEpoch 19700, Loss: 10261.26171875\nEpoch 19800, Loss: 10255.6044921875\nEpoch 19900, Loss: 10250.28125\nEpoch 20000, Loss: 10245.2802734375\nEpoch 20100, Loss: 10240.5849609375\nEpoch 20200, Loss: 10236.1884765625\nEpoch 20300, Loss: 10232.076171875\nEpoch 20400, Loss: 10228.2392578125\nEpoch 20500, Loss: 10224.666015625\nEpoch 20600, Loss: 10221.34375\nEpoch 20700, Loss: 10218.2626953125\nEpoch 20800, Loss: 10215.4072265625\nEpoch 20900, Loss: 10212.7646484375\nEpoch 21000, Loss: 10210.3193359375\nEpoch 21100, Loss: 10208.0615234375\nEpoch 21200, Loss: 10205.974609375\nEpoch 21300, Loss: 10204.0556640625\nEpoch 21400, Loss: 10202.2890625\nEpoch 21500, Loss: 10200.6650390625\nEpoch 21600, Loss: 10199.169921875\nEpoch 21700, Loss: 10197.7919921875\nEpoch 21800, Loss: 10196.521484375\nEpoch 21900, Loss: 10195.349609375\nEpoch 22000, Loss: 10194.2646484375\nEpoch 22100, Loss: 10193.259765625\nEpoch 22200, Loss: 10192.322265625\nEpoch 22300, Loss: 10191.447265625\nEpoch 22400, Loss: 10190.625\nEpoch 22500, Loss: 10189.849609375\nEpoch 22600, Loss: 10189.11328125\nEpoch 22700, Loss: 10188.408203125\nEpoch 22800, Loss: 10187.732421875\nEpoch 22900, Loss: 10187.078125\nEpoch 23000, Loss: 10186.4404296875\nEpoch 23100, Loss: 10185.81640625\nEpoch 23200, Loss: 10185.19921875\nEpoch 23300, Loss: 10184.5888671875\nEpoch 23400, Loss: 10183.9775390625\nEpoch 23500, Loss: 10183.3642578125\nEpoch 23600, Loss: 10182.7490234375\nEpoch 23700, Loss: 10182.125\nEpoch 23800, Loss: 10181.490234375\nEpoch 23900, Loss: 10180.8466796875\nEpoch 24000, Loss: 10180.1884765625\nEpoch 24100, Loss: 10179.515625\nEpoch 24200, Loss: 10178.8291015625\nEpoch 24300, Loss: 10178.1806640625\nEpoch 24400, Loss: 10177.5517578125\nEpoch 24500, Loss: 10176.919921875\nEpoch 24600, Loss: 10176.28125\nEpoch 24700, Loss: 10175.638671875\nEpoch 24800, Loss: 10175.0087890625\nEpoch 24900, Loss: 10174.3984375\nEpoch 25000, Loss: 10173.802734375\nEpoch 25100, Loss: 10173.2197265625\nEpoch 25200, Loss: 10172.6572265625\nEpoch 25300, Loss: 10172.1064453125\nEpoch 25400, Loss: 10171.568359375\nEpoch 25500, Loss: 10171.029296875\nEpoch 25600, Loss: 10170.4775390625\nEpoch 25700, Loss: 10169.91015625\nEpoch 25800, Loss: 10169.328125\nEpoch 25900, Loss: 10168.7314453125\nEpoch 26000, Loss: 10168.1181640625\nEpoch 26100, Loss: 10167.48828125\nEpoch 26200, Loss: 10166.84375\nEpoch 26300, Loss: 10166.18359375\nEpoch 26400, Loss: 10165.5107421875\nEpoch 26500, Loss: 10164.841796875\nEpoch 26600, Loss: 10164.2119140625\nEpoch 26700, Loss: 10163.6123046875\nEpoch 26800, Loss: 10163.05078125\nEpoch 26900, Loss: 10162.5439453125\nEpoch 27000, Loss: 10162.107421875\nEpoch 27100, Loss: 10161.728515625\nEpoch 27200, Loss: 10161.384765625\nEpoch 27300, Loss: 10161.0703125\nEpoch 27400, Loss: 10160.7666015625\nEpoch 27500, Loss: 10160.45703125\nEpoch 27600, Loss: 10160.1376953125\nEpoch 27700, Loss: 10159.810546875\nEpoch 27800, Loss: 10159.474609375\nEpoch 27900, Loss: 10159.1279296875\nEpoch 28000, Loss: 10158.7724609375\nEpoch 28100, Loss: 10158.4072265625\nEpoch 28200, Loss: 10158.0322265625\nEpoch 28300, Loss: 10157.646484375\nEpoch 28400, Loss: 10157.2548828125\nEpoch 28500, Loss: 10156.8515625\nEpoch 28600, Loss: 10156.443359375\nEpoch 28700, Loss: 10156.0693359375\nEpoch 28800, Loss: 10155.7529296875\nEpoch 28900, Loss: 10155.48828125\nEpoch 29000, Loss: 10155.26171875\nEpoch 29100, Loss: 10155.060546875\nEpoch 29200, Loss: 10154.8818359375\nEpoch 29300, Loss: 10154.732421875\nEpoch 29400, Loss: 10154.603515625\nEpoch 29500, Loss: 10154.4970703125\nEpoch 29600, Loss: 10154.416015625\nEpoch 29700, Loss: 10154.3564453125\nEpoch 29800, Loss: 10154.3115234375\nEpoch 29900, Loss: 10154.2763671875\nEpoch 30000, Loss: 10154.2470703125\nEpoch 30100, Loss: 10154.220703125\nEpoch 30200, Loss: 10154.201171875\nEpoch 30300, Loss: 10154.18359375\nEpoch 30400, Loss: 10154.169921875\nEpoch 30500, Loss: 10154.158203125\nEpoch 30600, Loss: 10154.1474609375\nEpoch 30700, Loss: 10154.1396484375\nEpoch 30800, Loss: 10154.1337890625\nEpoch 30900, Loss: 10154.1279296875\nEpoch 31000, Loss: 10154.125\nEpoch 31100, Loss: 10154.119140625\nEpoch 31200, Loss: 10154.1171875\nEpoch 31300, Loss: 10154.115234375\nEpoch 31400, Loss: 10154.1123046875\nEpoch 31500, Loss: 10154.1103515625\nEpoch 31600, Loss: 10154.1083984375\nEpoch 31700, Loss: 10154.107421875\nEpoch 31800, Loss: 10154.10546875\nEpoch 31900, Loss: 10154.10546875\nEpoch 32000, Loss: 10154.1044921875\nEpoch 32100, Loss: 10154.103515625\nEpoch 32200, Loss: 10154.103515625\nEpoch 32300, Loss: 10154.1025390625\nEpoch 32400, Loss: 10154.1025390625\nEpoch 32500, Loss: 10154.1015625\nEpoch 32600, Loss: 10154.1025390625\nEpoch 32700, Loss: 10154.1015625\nEpoch 32800, Loss: 10154.1005859375\nEpoch 32900, Loss: 10154.1005859375\nEpoch 33000, Loss: 10154.1005859375\nEpoch 33100, Loss: 10154.1005859375\nEpoch 33200, Loss: 10154.099609375\nEpoch 33300, Loss: 10154.1005859375\nEpoch 33400, Loss: 10154.0986328125\nEpoch 33500, Loss: 10154.1005859375\nEpoch 33600, Loss: 10154.1005859375\nEpoch 33700, Loss: 10154.1005859375\nEpoch 33800, Loss: 10154.1005859375\nEpoch 33900, Loss: 10154.1005859375\nEpoch 34000, Loss: 10154.099609375\nEpoch 34100, Loss: 10154.099609375\nEpoch 34200, Loss: 10154.1005859375\nEpoch 34300, Loss: 10154.099609375\nEpoch 34400, Loss: 10154.1005859375\nEpoch 34500, Loss: 10154.1005859375\nEpoch 34600, Loss: 10154.1005859375\nEpoch 34700, Loss: 10154.1005859375\nEpoch 34800, Loss: 10154.1005859375\nEpoch 34900, Loss: 10154.1005859375\nEpoch 35000, Loss: 10154.1015625\nEpoch 35100, Loss: 10154.1005859375\nEpoch 35200, Loss: 10154.099609375\nEpoch 35300, Loss: 10154.1005859375\nEpoch 35400, Loss: 10154.1005859375\nEpoch 35500, Loss: 10154.1005859375\nEpoch 35600, Loss: 10154.1005859375\nEpoch 35700, Loss: 10154.1005859375\nEpoch 35800, Loss: 10154.099609375\nEpoch 35900, Loss: 10154.0986328125\nEpoch 36000, Loss: 10154.099609375\nEpoch 36100, Loss: 10154.099609375\nEpoch 36200, Loss: 10154.1005859375\nEpoch 36300, Loss: 10154.1005859375\nEpoch 36400, Loss: 10154.099609375\nEpoch 36500, Loss: 10154.1005859375\nEpoch 36600, Loss: 10154.099609375\nEpoch 36700, Loss: 10154.1005859375\nEpoch 36800, Loss: 10154.099609375\nEpoch 36900, Loss: 10154.099609375\nEpoch 37000, Loss: 10154.1005859375\nEpoch 37100, Loss: 10154.0986328125\nEpoch 37200, Loss: 10154.099609375\nEpoch 37300, Loss: 10154.099609375\nEpoch 37400, Loss: 10154.1005859375\nEpoch 37500, Loss: 10154.0986328125\nEpoch 37600, Loss: 10154.099609375\nEpoch 37700, Loss: 10154.099609375\nEpoch 37800, Loss: 10154.099609375\nEpoch 37900, Loss: 10154.1005859375\nEpoch 38000, Loss: 10154.099609375\nEpoch 38100, Loss: 10154.1005859375\nEpoch 38200, Loss: 10154.1005859375\nEpoch 38300, Loss: 10154.1005859375\nEpoch 38400, Loss: 10154.1005859375\nEpoch 38500, Loss: 10154.1005859375\nEpoch 38600, Loss: 10154.099609375\nEpoch 38700, Loss: 10154.1005859375\nEpoch 38800, Loss: 10154.1005859375\nEpoch 38900, Loss: 10154.099609375\nEpoch 39000, Loss: 10154.1005859375\nEpoch 39100, Loss: 10154.1005859375\nEpoch 39200, Loss: 10154.1005859375\nEpoch 39300, Loss: 10154.1005859375\nEpoch 39400, Loss: 10154.099609375\nEpoch 39500, Loss: 10154.1005859375\nEpoch 39600, Loss: 10154.099609375\nEpoch 39700, Loss: 10154.099609375\nEpoch 39800, Loss: 10154.099609375\nEpoch 39900, Loss: 10154.1005859375\nEpoch 40000, Loss: 10154.099609375\nEpoch 40100, Loss: 10154.099609375\nEpoch 40200, Loss: 10154.1005859375\nEpoch 40300, Loss: 10154.099609375\nEpoch 40400, Loss: 10154.099609375\nEpoch 40500, Loss: 10154.1005859375\nEpoch 40600, Loss: 10154.0986328125\nEpoch 40700, Loss: 10154.1005859375\nEpoch 40800, Loss: 10154.1005859375\nEpoch 40900, Loss: 10154.099609375\nEpoch 41000, Loss: 10154.099609375\nEpoch 41100, Loss: 10154.099609375\nEpoch 41200, Loss: 10154.1005859375\nEpoch 41300, Loss: 10154.1005859375\nEpoch 41400, Loss: 10154.1015625\nEpoch 41500, Loss: 10154.1005859375\nEpoch 41600, Loss: 10154.0986328125\nEpoch 41700, Loss: 10154.099609375\nEpoch 41800, Loss: 10154.1005859375\nEpoch 41900, Loss: 10154.099609375\nEpoch 42000, Loss: 10154.0986328125\nEpoch 42100, Loss: 10154.099609375\nEpoch 42200, Loss: 10154.099609375\nEpoch 42300, Loss: 10154.1005859375\nEpoch 42400, Loss: 10154.1005859375\nEpoch 42500, Loss: 10154.099609375\nEpoch 42600, Loss: 10154.1005859375\nEpoch 42700, Loss: 10154.099609375\nEpoch 42800, Loss: 10154.099609375\nEpoch 42900, Loss: 10154.1005859375\nEpoch 43000, Loss: 10154.1005859375\nEpoch 43100, Loss: 10154.099609375\nEpoch 43200, Loss: 10154.099609375\nEpoch 43300, Loss: 10154.099609375\nEpoch 43400, Loss: 10154.1005859375\nEpoch 43500, Loss: 10154.1005859375\nEpoch 43600, Loss: 10154.099609375\nEpoch 43700, Loss: 10154.099609375\nEpoch 43800, Loss: 10154.1005859375\nEpoch 43900, Loss: 10154.1005859375\nEpoch 44000, Loss: 10154.099609375\nEpoch 44100, Loss: 10154.1005859375\nEpoch 44200, Loss: 10154.099609375\nEpoch 44300, Loss: 10154.1005859375\nEpoch 44400, Loss: 10154.1005859375\nEpoch 44500, Loss: 10154.099609375\nEpoch 44600, Loss: 10154.1005859375\nEpoch 44700, Loss: 10154.099609375\nEpoch 44800, Loss: 10154.1005859375\nEpoch 44900, Loss: 10154.1005859375\nEpoch 45000, Loss: 10154.099609375\nEpoch 45100, Loss: 10154.1005859375\nEpoch 45200, Loss: 10154.1005859375\nEpoch 45300, Loss: 10154.1005859375\nEpoch 45400, Loss: 10154.1005859375\nEpoch 45500, Loss: 10154.0986328125\nEpoch 45600, Loss: 10154.099609375\nEpoch 45700, Loss: 10154.1005859375\nEpoch 45800, Loss: 10154.1005859375\nEpoch 45900, Loss: 10154.099609375\nEpoch 46000, Loss: 10154.099609375\nEpoch 46100, Loss: 10154.1005859375\nEpoch 46200, Loss: 10154.1005859375\nEpoch 46300, Loss: 10154.099609375\nEpoch 46400, Loss: 10154.099609375\nEpoch 46500, Loss: 10154.099609375\nEpoch 46600, Loss: 10154.099609375\nEpoch 46700, Loss: 10154.1005859375\nEpoch 46800, Loss: 10154.1015625\nEpoch 46900, Loss: 10154.1005859375\nEpoch 47000, Loss: 10154.1005859375\nEpoch 47100, Loss: 10154.1005859375\nEpoch 47200, Loss: 10154.1005859375\nEpoch 47300, Loss: 10154.099609375\nEpoch 47400, Loss: 10154.1005859375\nEpoch 47500, Loss: 10154.099609375\nEpoch 47600, Loss: 10154.099609375\nEpoch 47700, Loss: 10154.099609375\nEpoch 47800, Loss: 10154.099609375\nEpoch 47900, Loss: 10154.099609375\nEpoch 48000, Loss: 10154.1005859375\nEpoch 48100, Loss: 10154.099609375\nEpoch 48200, Loss: 10154.1005859375\nEpoch 48300, Loss: 10154.099609375\nEpoch 48400, Loss: 10154.099609375\nEpoch 48500, Loss: 10154.099609375\nEpoch 48600, Loss: 10154.099609375\nEpoch 48700, Loss: 10154.1005859375\nEpoch 48800, Loss: 10154.1005859375\nEpoch 48900, Loss: 10154.099609375\nEpoch 49000, Loss: 10154.1005859375\nEpoch 49100, Loss: 10154.0986328125\nEpoch 49200, Loss: 10154.1005859375\nEpoch 49300, Loss: 10154.099609375\nEpoch 49400, Loss: 10154.099609375\nEpoch 49500, Loss: 10154.099609375\nEpoch 49600, Loss: 10154.099609375\nEpoch 49700, Loss: 10154.1005859375\nEpoch 49800, Loss: 10154.0986328125\nEpoch 49900, Loss: 10154.1005859375\nEpoch 0, Loss: 63828.96875\nEpoch 100, Loss: 60927.3984375\nEpoch 200, Loss: 58214.3125\nEpoch 300, Loss: 55675.796875\nEpoch 400, Loss: 53299.29296875\nEpoch 500, Loss: 51073.7109375\nEpoch 600, Loss: 48990.171875\nEpoch 700, Loss: 47039.97265625\nEpoch 800, Loss: 45215.41015625\nEpoch 900, Loss: 43509.640625\nEpoch 1000, Loss: 41915.6953125\nEpoch 1100, Loss: 40426.69921875\nEpoch 1200, Loss: 39036.26953125\nEpoch 1300, Loss: 37738.25\nEpoch 1400, Loss: 36526.8828125\nEpoch 1500, Loss: 35396.7421875\nEpoch 1600, Loss: 34341.9375\nEpoch 1700, Loss: 33357.19140625\nEpoch 1800, Loss: 32437.53125\nEpoch 1900, Loss: 31578.298828125\nEpoch 2000, Loss: 30775.13671875\nEpoch 2100, Loss: 30024.052734375\nEpoch 2200, Loss: 29321.71484375\nEpoch 2300, Loss: 28664.1328125\nEpoch 2400, Loss: 28048.8828125\nEpoch 2500, Loss: 27472.16796875\nEpoch 2600, Loss: 26931.01171875\nEpoch 2700, Loss: 26422.76953125\nEpoch 2800, Loss: 25945.00390625\nEpoch 2900, Loss: 25495.44140625\nEpoch 3000, Loss: 25071.953125\nEpoch 3100, Loss: 24672.564453125\nEpoch 3200, Loss: 24295.404296875\nEpoch 3300, Loss: 23938.88671875\nEpoch 3400, Loss: 23601.447265625\nEpoch 3500, Loss: 23281.728515625\nEpoch 3600, Loss: 22978.103515625\nEpoch 3700, Loss: 22688.861328125\nEpoch 3800, Loss: 22412.7265625\nEpoch 3900, Loss: 22148.5078125\nEpoch 4000, Loss: 21895.115234375\nEpoch 4100, Loss: 21651.95703125\nEpoch 4200, Loss: 21417.748046875\nEpoch 4300, Loss: 21191.619140625\nEpoch 4400, Loss: 20972.900390625\nEpoch 4500, Loss: 20761.197265625\nEpoch 4600, Loss: 20555.611328125\nEpoch 4700, Loss: 20355.640625\nEpoch 4800, Loss: 20160.8515625\nEpoch 4900, Loss: 19970.875\nEpoch 5000, Loss: 19785.376953125\nEpoch 5100, Loss: 19604.08203125\nEpoch 5200, Loss: 19426.73828125\nEpoch 5300, Loss: 19253.1328125\nEpoch 5400, Loss: 19083.078125\nEpoch 5500, Loss: 18916.400390625\nEpoch 5600, Loss: 18752.951171875\nEpoch 5700, Loss: 18592.595703125\nEpoch 5800, Loss: 18435.21484375\nEpoch 5900, Loss: 18280.703125\nEpoch 6000, Loss: 18129.2421875\nEpoch 6100, Loss: 17980.447265625\nEpoch 6200, Loss: 17834.224609375\nEpoch 6300, Loss: 17690.494140625\nEpoch 6400, Loss: 17549.1953125\nEpoch 6500, Loss: 17410.25390625\nEpoch 6600, Loss: 17273.615234375\nEpoch 6700, Loss: 17139.22265625\nEpoch 6800, Loss: 17007.021484375\nEpoch 6900, Loss: 16876.966796875\nEpoch 7000, Loss: 16749.00390625\nEpoch 7100, Loss: 16623.142578125\nEpoch 7200, Loss: 16499.55078125\nEpoch 7300, Loss: 16378.0791015625\nEpoch 7400, Loss: 16258.65234375\nEpoch 7500, Loss: 16141.244140625\nEpoch 7600, Loss: 16025.62890625\nEpoch 7700, Loss: 15911.7734375\nEpoch 7800, Loss: 15799.6416015625\nEpoch 7900, Loss: 15689.197265625\nEpoch 8000, Loss: 15580.400390625\nEpoch 8100, Loss: 15473.234375\nEpoch 8200, Loss: 15367.6572265625\nEpoch 8300, Loss: 15263.6328125\nEpoch 8400, Loss: 15161.1337890625\nEpoch 8500, Loss: 15060.1416015625\nEpoch 8600, Loss: 14960.6220703125\nEpoch 8700, Loss: 14862.5556640625\nEpoch 8800, Loss: 14765.9189453125\nEpoch 8900, Loss: 14670.693359375\nEpoch 9000, Loss: 14576.85546875\nEpoch 9100, Loss: 14484.53515625\nEpoch 9200, Loss: 14393.7451171875\nEpoch 9300, Loss: 14304.3046875\nEpoch 9400, Loss: 14216.2080078125\nEpoch 9500, Loss: 14129.4306640625\nEpoch 9600, Loss: 14043.96484375\nEpoch 9700, Loss: 13959.796875\nEpoch 9800, Loss: 13876.91796875\nEpoch 9900, Loss: 13795.3232421875\nEpoch 10000, Loss: 13715.001953125\nEpoch 10100, Loss: 13635.9482421875\nEpoch 10200, Loss: 13558.158203125\nEpoch 10300, Loss: 13481.6279296875\nEpoch 10400, Loss: 13406.349609375\nEpoch 10500, Loss: 13332.318359375\nEpoch 10600, Loss: 13259.53125\nEpoch 10700, Loss: 13187.9833984375\nEpoch 10800, Loss: 13117.6650390625\nEpoch 10900, Loss: 13048.5693359375\nEpoch 11000, Loss: 12980.6806640625\nEpoch 11100, Loss: 12913.9990234375\nEpoch 11200, Loss: 12848.5048828125\nEpoch 11300, Loss: 12784.1904296875\nEpoch 11400, Loss: 12721.0791015625\nEpoch 11500, Loss: 12659.138671875\nEpoch 11600, Loss: 12598.3388671875\nEpoch 11700, Loss: 12538.66015625\nEpoch 11800, Loss: 12480.0810546875\nEpoch 11900, Loss: 12422.580078125\nEpoch 12000, Loss: 12366.134765625\nEpoch 12100, Loss: 12310.7294921875\nEpoch 12200, Loss: 12256.62109375\nEpoch 12300, Loss: 12203.5712890625\nEpoch 12400, Loss: 12151.5126953125\nEpoch 12500, Loss: 12100.41015625\nEpoch 12600, Loss: 12050.255859375\nEpoch 12700, Loss: 12001.0185546875\nEpoch 12800, Loss: 11952.689453125\nEpoch 12900, Loss: 11905.25390625\nEpoch 13000, Loss: 11858.69140625\nEpoch 13100, Loss: 11813.0263671875\nEpoch 13200, Loss: 11768.35546875\nEpoch 13300, Loss: 11724.56640625\nEpoch 13400, Loss: 11681.609375\nEpoch 13500, Loss: 11639.4736328125\nEpoch 13600, Loss: 11598.1533203125\nEpoch 13700, Loss: 11557.6279296875\nEpoch 13800, Loss: 11517.8876953125\nEpoch 13900, Loss: 11478.9287109375\nEpoch 14000, Loss: 11440.736328125\nEpoch 14100, Loss: 11403.5126953125\nEpoch 14200, Loss: 11367.1533203125\nEpoch 14300, Loss: 11331.6240234375\nEpoch 14400, Loss: 11296.8798828125\nEpoch 14500, Loss: 11262.8740234375\nEpoch 14600, Loss: 11229.6328125\nEpoch 14700, Loss: 11197.1806640625\nEpoch 14800, Loss: 11165.4765625\nEpoch 14900, Loss: 11134.49609375\nEpoch 15000, Loss: 11104.3017578125\nEpoch 15100, Loss: 11074.8740234375\nEpoch 15200, Loss: 11046.1376953125\nEpoch 15300, Loss: 11018.0791015625\nEpoch 15400, Loss: 10990.6865234375\nEpoch 15500, Loss: 10963.9599609375\nEpoch 15600, Loss: 10937.8876953125\nEpoch 15700, Loss: 10912.4638671875\nEpoch 15800, Loss: 10887.681640625\nEpoch 15900, Loss: 10863.5537109375\nEpoch 16000, Loss: 10840.0908203125\nEpoch 16100, Loss: 10817.26953125\nEpoch 16200, Loss: 10795.0751953125\nEpoch 16300, Loss: 10773.4892578125\nEpoch 16400, Loss: 10752.4970703125\nEpoch 16500, Loss: 10732.064453125\nEpoch 16600, Loss: 10712.18359375\nEpoch 16700, Loss: 10692.908203125\nEpoch 16800, Loss: 10674.1865234375\nEpoch 16900, Loss: 10655.9912109375\nEpoch 17000, Loss: 10638.310546875\nEpoch 17100, Loss: 10621.1328125\nEpoch 17200, Loss: 10604.4541015625\nEpoch 17300, Loss: 10588.259765625\nEpoch 17400, Loss: 10572.5498046875\nEpoch 17500, Loss: 10557.3232421875\nEpoch 17600, Loss: 10542.595703125\nEpoch 17700, Loss: 10528.3388671875\nEpoch 17800, Loss: 10514.544921875\nEpoch 17900, Loss: 10501.205078125\nEpoch 18000, Loss: 10488.314453125\nEpoch 18100, Loss: 10475.86328125\nEpoch 18200, Loss: 10463.8525390625\nEpoch 18300, Loss: 10452.2763671875\nEpoch 18400, Loss: 10441.125\nEpoch 18500, Loss: 10430.4140625\nEpoch 18600, Loss: 10420.138671875\nEpoch 18700, Loss: 10410.2900390625\nEpoch 18800, Loss: 10400.8623046875\nEpoch 18900, Loss: 10391.84765625\nEpoch 19000, Loss: 10383.234375\nEpoch 19100, Loss: 10375.01953125\nEpoch 19200, Loss: 10367.2802734375\nEpoch 19300, Loss: 10359.98046875\nEpoch 19400, Loss: 10353.08203125\nEpoch 19500, Loss: 10346.5556640625\nEpoch 19600, Loss: 10340.3837890625\nEpoch 19700, Loss: 10334.5498046875\nEpoch 19800, Loss: 10329.0419921875\nEpoch 19900, Loss: 10323.8544921875\nEpoch 20000, Loss: 10318.9736328125\nEpoch 20100, Loss: 10314.3916015625\nEpoch 20200, Loss: 10310.0966796875\nEpoch 20300, Loss: 10306.0791015625\nEpoch 20400, Loss: 10302.328125\nEpoch 20500, Loss: 10298.83203125\nEpoch 20600, Loss: 10295.5791015625\nEpoch 20700, Loss: 10292.5654296875\nEpoch 20800, Loss: 10289.775390625\nEpoch 20900, Loss: 10287.1923828125\nEpoch 21000, Loss: 10284.8017578125\nEpoch 21100, Loss: 10282.5908203125\nEpoch 21200, Loss: 10280.5439453125\nEpoch 21300, Loss: 10278.650390625\nEpoch 21400, Loss: 10276.8974609375\nEpoch 21500, Loss: 10275.2734375\nEpoch 21600, Loss: 10273.7666015625\nEpoch 21700, Loss: 10272.3671875\nEpoch 21800, Loss: 10271.0634765625\nEpoch 21900, Loss: 10269.845703125\nEpoch 22000, Loss: 10268.703125\nEpoch 22100, Loss: 10267.6259765625\nEpoch 22200, Loss: 10266.6064453125\nEpoch 22300, Loss: 10265.6376953125\nEpoch 22400, Loss: 10264.70703125\nEpoch 22500, Loss: 10263.8095703125\nEpoch 22600, Loss: 10262.94140625\nEpoch 22700, Loss: 10262.0908203125\nEpoch 22800, Loss: 10261.2548828125\nEpoch 22900, Loss: 10260.4287109375\nEpoch 23000, Loss: 10259.6044921875\nEpoch 23100, Loss: 10258.783203125\nEpoch 23200, Loss: 10257.9560546875\nEpoch 23300, Loss: 10257.1240234375\nEpoch 23400, Loss: 10256.279296875\nEpoch 23500, Loss: 10255.4365234375\nEpoch 23600, Loss: 10254.6494140625\nEpoch 23700, Loss: 10253.8642578125\nEpoch 23800, Loss: 10253.072265625\nEpoch 23900, Loss: 10252.2744140625\nEpoch 24000, Loss: 10251.478515625\nEpoch 24100, Loss: 10250.6962890625\nEpoch 24200, Loss: 10249.9267578125\nEpoch 24300, Loss: 10249.171875\nEpoch 24400, Loss: 10248.427734375\nEpoch 24500, Loss: 10247.697265625\nEpoch 24600, Loss: 10246.9794921875\nEpoch 24700, Loss: 10246.2734375\nEpoch 24800, Loss: 10245.5791015625\nEpoch 24900, Loss: 10244.892578125\nEpoch 25000, Loss: 10244.203125\nEpoch 25100, Loss: 10243.494140625\nEpoch 25200, Loss: 10242.7705078125\nEpoch 25300, Loss: 10242.0263671875\nEpoch 25400, Loss: 10241.265625\nEpoch 25500, Loss: 10240.484375\nEpoch 25600, Loss: 10239.685546875\nEpoch 25700, Loss: 10238.8671875\nEpoch 25800, Loss: 10238.0322265625\nEpoch 25900, Loss: 10237.201171875\nEpoch 26000, Loss: 10236.404296875\nEpoch 26100, Loss: 10235.6376953125\nEpoch 26200, Loss: 10234.90234375\nEpoch 26300, Loss: 10234.220703125\nEpoch 26400, Loss: 10233.6171875\nEpoch 26500, Loss: 10233.0810546875\nEpoch 26600, Loss: 10232.6005859375\nEpoch 26700, Loss: 10232.1640625\nEpoch 26800, Loss: 10231.7607421875\nEpoch 26900, Loss: 10231.37109375\nEpoch 27000, Loss: 10230.9765625\nEpoch 27100, Loss: 10230.5751953125\nEpoch 27200, Loss: 10230.1650390625\nEpoch 27300, Loss: 10229.7412109375\nEpoch 27400, Loss: 10229.306640625\nEpoch 27500, Loss: 10228.8623046875\nEpoch 27600, Loss: 10228.404296875\nEpoch 27700, Loss: 10227.9345703125\nEpoch 27800, Loss: 10227.4521484375\nEpoch 27900, Loss: 10226.958984375\nEpoch 28000, Loss: 10226.4560546875\nEpoch 28100, Loss: 10225.9521484375\nEpoch 28200, Loss: 10225.501953125\nEpoch 28300, Loss: 10225.1044921875\nEpoch 28400, Loss: 10224.755859375\nEpoch 28500, Loss: 10224.4501953125\nEpoch 28600, Loss: 10224.193359375\nEpoch 28700, Loss: 10223.974609375\nEpoch 28800, Loss: 10223.783203125\nEpoch 28900, Loss: 10223.6123046875\nEpoch 29000, Loss: 10223.4638671875\nEpoch 29100, Loss: 10223.3466796875\nEpoch 29200, Loss: 10223.25\nEpoch 29300, Loss: 10223.17578125\nEpoch 29400, Loss: 10223.1162109375\nEpoch 29500, Loss: 10223.0703125\nEpoch 29600, Loss: 10223.0322265625\nEpoch 29700, Loss: 10223.0\nEpoch 29800, Loss: 10222.97265625\nEpoch 29900, Loss: 10222.9482421875\nEpoch 30000, Loss: 10222.92578125\nEpoch 30100, Loss: 10222.9091796875\nEpoch 30200, Loss: 10222.8935546875\nEpoch 30300, Loss: 10222.880859375\nEpoch 30400, Loss: 10222.87109375\nEpoch 30500, Loss: 10222.861328125\nEpoch 30600, Loss: 10222.853515625\nEpoch 30700, Loss: 10222.8486328125\nEpoch 30800, Loss: 10222.8427734375\nEpoch 30900, Loss: 10222.83984375\nEpoch 31000, Loss: 10222.8359375\nEpoch 31100, Loss: 10222.83203125\nEpoch 31200, Loss: 10222.8291015625\nEpoch 31300, Loss: 10222.8271484375\nEpoch 31400, Loss: 10222.8251953125\nEpoch 31500, Loss: 10222.8232421875\nEpoch 31600, Loss: 10222.822265625\nEpoch 31700, Loss: 10222.8212890625\nEpoch 31800, Loss: 10222.8193359375\nEpoch 31900, Loss: 10222.8193359375\nEpoch 32000, Loss: 10222.8173828125\nEpoch 32100, Loss: 10222.8173828125\nEpoch 32200, Loss: 10222.81640625\nEpoch 32300, Loss: 10222.81640625\nEpoch 32400, Loss: 10222.81640625\nEpoch 32500, Loss: 10222.81640625\nEpoch 32600, Loss: 10222.81640625\nEpoch 32700, Loss: 10222.8154296875\nEpoch 32800, Loss: 10222.8154296875\nEpoch 32900, Loss: 10222.8154296875\nEpoch 33000, Loss: 10222.8154296875\nEpoch 33100, Loss: 10222.8154296875\nEpoch 33200, Loss: 10222.8154296875\nEpoch 33300, Loss: 10222.8154296875\nEpoch 33400, Loss: 10222.814453125\nEpoch 33500, Loss: 10222.8154296875\nEpoch 33600, Loss: 10222.814453125\nEpoch 33700, Loss: 10222.814453125\nEpoch 33800, Loss: 10222.814453125\nEpoch 33900, Loss: 10222.8154296875\nEpoch 34000, Loss: 10222.8154296875\nEpoch 34100, Loss: 10222.814453125\nEpoch 34200, Loss: 10222.8154296875\nEpoch 34300, Loss: 10222.814453125\nEpoch 34400, Loss: 10222.814453125\nEpoch 34500, Loss: 10222.8154296875\nEpoch 34600, Loss: 10222.814453125\nEpoch 34700, Loss: 10222.81640625\nEpoch 34800, Loss: 10222.8154296875\nEpoch 34900, Loss: 10222.8154296875\nEpoch 35000, Loss: 10222.8154296875\nEpoch 35100, Loss: 10222.814453125\nEpoch 35200, Loss: 10222.8154296875\nEpoch 35300, Loss: 10222.8154296875\nEpoch 35400, Loss: 10222.8154296875\nEpoch 35500, Loss: 10222.8154296875\nEpoch 35600, Loss: 10222.814453125\nEpoch 35700, Loss: 10222.814453125\nEpoch 35800, Loss: 10222.8154296875\nEpoch 35900, Loss: 10222.8154296875\nEpoch 36000, Loss: 10222.8154296875\nEpoch 36100, Loss: 10222.8154296875\nEpoch 36200, Loss: 10222.8154296875\nEpoch 36300, Loss: 10222.814453125\nEpoch 36400, Loss: 10222.8154296875\nEpoch 36500, Loss: 10222.8154296875\nEpoch 36600, Loss: 10222.814453125\nEpoch 36700, Loss: 10222.8154296875\nEpoch 36800, Loss: 10222.8154296875\nEpoch 36900, Loss: 10222.8154296875\nEpoch 37000, Loss: 10222.8154296875\nEpoch 37100, Loss: 10222.8154296875\nEpoch 37200, Loss: 10222.8154296875\nEpoch 37300, Loss: 10222.8154296875\nEpoch 37400, Loss: 10222.8154296875\nEpoch 37500, Loss: 10222.814453125\nEpoch 37600, Loss: 10222.8154296875\nEpoch 37700, Loss: 10222.8154296875\nEpoch 37800, Loss: 10222.8154296875\nEpoch 37900, Loss: 10222.8154296875\nEpoch 38000, Loss: 10222.8154296875\nEpoch 38100, Loss: 10222.8154296875\nEpoch 38200, Loss: 10222.814453125\nEpoch 38300, Loss: 10222.814453125\nEpoch 38400, Loss: 10222.8154296875\nEpoch 38500, Loss: 10222.8154296875\nEpoch 38600, Loss: 10222.8154296875\nEpoch 38700, Loss: 10222.8154296875\nEpoch 38800, Loss: 10222.8154296875\nEpoch 38900, Loss: 10222.8154296875\nEpoch 39000, Loss: 10222.814453125\nEpoch 39100, Loss: 10222.8154296875\nEpoch 39200, Loss: 10222.8154296875\nEpoch 39300, Loss: 10222.8154296875\nEpoch 39400, Loss: 10222.8154296875\nEpoch 39500, Loss: 10222.8154296875\nEpoch 39600, Loss: 10222.81640625\nEpoch 39700, Loss: 10222.8154296875\nEpoch 39800, Loss: 10222.8154296875\nEpoch 39900, Loss: 10222.814453125\nEpoch 40000, Loss: 10222.8154296875\nEpoch 40100, Loss: 10222.8154296875\nEpoch 40200, Loss: 10222.8154296875\nEpoch 40300, Loss: 10222.8154296875\nEpoch 40400, Loss: 10222.8154296875\nEpoch 40500, Loss: 10222.8154296875\nEpoch 40600, Loss: 10222.8154296875\nEpoch 40700, Loss: 10222.8154296875\nEpoch 40800, Loss: 10222.8154296875\nEpoch 40900, Loss: 10222.814453125\nEpoch 41000, Loss: 10222.814453125\nEpoch 41100, Loss: 10222.8154296875\nEpoch 41200, Loss: 10222.8154296875\nEpoch 41300, Loss: 10222.8154296875\nEpoch 41400, Loss: 10222.8154296875\nEpoch 41500, Loss: 10222.8154296875\nEpoch 41600, Loss: 10222.8154296875\nEpoch 41700, Loss: 10222.8154296875\nEpoch 41800, Loss: 10222.814453125\nEpoch 41900, Loss: 10222.8154296875\nEpoch 42000, Loss: 10222.8154296875\nEpoch 42100, Loss: 10222.8154296875\nEpoch 42200, Loss: 10222.8154296875\nEpoch 42300, Loss: 10222.8154296875\nEpoch 42400, Loss: 10222.8154296875\nEpoch 42500, Loss: 10222.8154296875\nEpoch 42600, Loss: 10222.8154296875\nEpoch 42700, Loss: 10222.814453125\nEpoch 42800, Loss: 10222.8154296875\nEpoch 42900, Loss: 10222.8154296875\nEpoch 43000, Loss: 10222.8154296875\nEpoch 43100, Loss: 10222.8154296875\nEpoch 43200, Loss: 10222.8154296875\nEpoch 43300, Loss: 10222.8154296875\nEpoch 43400, Loss: 10222.8154296875\nEpoch 43500, Loss: 10222.8154296875\nEpoch 43600, Loss: 10222.8154296875\nEpoch 43700, Loss: 10222.814453125\nEpoch 43800, Loss: 10222.8154296875\nEpoch 43900, Loss: 10222.8154296875\nEpoch 44000, Loss: 10222.8154296875\nEpoch 44100, Loss: 10222.8154296875\nEpoch 44200, Loss: 10222.8154296875\nEpoch 44300, Loss: 10222.8154296875\nEpoch 44400, Loss: 10222.8154296875\nEpoch 44500, Loss: 10222.814453125\nEpoch 44600, Loss: 10222.8154296875\nEpoch 44700, Loss: 10222.8154296875\nEpoch 44800, Loss: 10222.8154296875\nEpoch 44900, Loss: 10222.8154296875\nEpoch 45000, Loss: 10222.81640625\nEpoch 45100, Loss: 10222.8154296875\nEpoch 45200, Loss: 10222.814453125\nEpoch 45300, Loss: 10222.8154296875\nEpoch 45400, Loss: 10222.8154296875\nEpoch 45500, Loss: 10222.8154296875\nEpoch 45600, Loss: 10222.8154296875\nEpoch 45700, Loss: 10222.8154296875\nEpoch 45800, Loss: 10222.8154296875\nEpoch 45900, Loss: 10222.8154296875\nEpoch 46000, Loss: 10222.8154296875\nEpoch 46100, Loss: 10222.814453125\nEpoch 46200, Loss: 10222.8154296875\nEpoch 46300, Loss: 10222.8154296875\nEpoch 46400, Loss: 10222.8154296875\nEpoch 46500, Loss: 10222.8154296875\nEpoch 46600, Loss: 10222.814453125\nEpoch 46700, Loss: 10222.8154296875\nEpoch 46800, Loss: 10222.814453125\nEpoch 46900, Loss: 10222.814453125\nEpoch 47000, Loss: 10222.814453125\nEpoch 47100, Loss: 10222.814453125\nEpoch 47200, Loss: 10222.8154296875\nEpoch 47300, Loss: 10222.814453125\nEpoch 47400, Loss: 10222.814453125\nEpoch 47500, Loss: 10222.814453125\nEpoch 47600, Loss: 10222.8154296875\nEpoch 47700, Loss: 10222.814453125\nEpoch 47800, Loss: 10222.814453125\nEpoch 47900, Loss: 10222.814453125\nEpoch 48000, Loss: 10222.8154296875\nEpoch 48100, Loss: 10222.814453125\nEpoch 48200, Loss: 10222.814453125\nEpoch 48300, Loss: 10222.814453125\nEpoch 48400, Loss: 10222.814453125\nEpoch 48500, Loss: 10222.814453125\nEpoch 48600, Loss: 10222.814453125\nEpoch 48700, Loss: 10222.814453125\nEpoch 48800, Loss: 10222.8154296875\nEpoch 48900, Loss: 10222.8154296875\nEpoch 49000, Loss: 10222.814453125\nEpoch 49100, Loss: 10222.814453125\nEpoch 49200, Loss: 10222.8154296875\nEpoch 49300, Loss: 10222.8154296875\nEpoch 49400, Loss: 10222.814453125\nEpoch 49500, Loss: 10222.814453125\nEpoch 49600, Loss: 10222.814453125\nEpoch 49700, Loss: 10222.814453125\nEpoch 49800, Loss: 10222.814453125\nEpoch 49900, Loss: 10222.8154296875\nEpoch 0, Loss: 63828.96875\nEpoch 100, Loss: 60928.9765625\nEpoch 200, Loss: 58217.36328125\nEpoch 300, Loss: 55680.1171875\nEpoch 400, Loss: 53304.94921875\nEpoch 500, Loss: 51080.6484375\nEpoch 600, Loss: 48998.37890625\nEpoch 700, Loss: 47049.35546875\nEpoch 800, Loss: 45225.8671875\nEpoch 900, Loss: 43521.2421875\nEpoch 1000, Loss: 41928.359375\nEpoch 1100, Loss: 40440.36328125\nEpoch 1200, Loss: 39050.8515625\nEpoch 1300, Loss: 37753.66796875\nEpoch 1400, Loss: 36543.1640625\nEpoch 1500, Loss: 35413.87890625\nEpoch 1600, Loss: 34359.8515625\nEpoch 1700, Loss: 33375.8125\nEpoch 1800, Loss: 32456.810546875\nEpoch 1900, Loss: 31598.1640625\nEpoch 2000, Loss: 30795.546875\nEpoch 2100, Loss: 30045.0625\nEpoch 2200, Loss: 29343.34375\nEpoch 2300, Loss: 28686.416015625\nEpoch 2400, Loss: 28071.998046875\nEpoch 2500, Loss: 27496.07421875\nEpoch 2600, Loss: 26955.6640625\nEpoch 2700, Loss: 26448.142578125\nEpoch 2800, Loss: 25971.056640625\nEpoch 2900, Loss: 25522.140625\nEpoch 3000, Loss: 25099.27734375\nEpoch 3100, Loss: 24700.482421875\nEpoch 3200, Loss: 24323.896484375\nEpoch 3300, Loss: 23967.974609375\nEpoch 3400, Loss: 23631.208984375\nEpoch 3500, Loss: 23312.28515625\nEpoch 3600, Loss: 23009.4609375\nEpoch 3700, Loss: 22721.0\nEpoch 3800, Loss: 22445.611328125\nEpoch 3900, Loss: 22182.1171875\nEpoch 4000, Loss: 21929.45703125\nEpoch 4100, Loss: 21687.126953125\nEpoch 4200, Loss: 21453.68359375\nEpoch 4300, Loss: 21228.302734375\nEpoch 4400, Loss: 21010.3671875\nEpoch 4500, Loss: 20799.458984375\nEpoch 4600, Loss: 20594.6484375\nEpoch 4700, Loss: 20395.431640625\nEpoch 4800, Loss: 20201.37890625\nEpoch 4900, Loss: 20012.11328125\nEpoch 5000, Loss: 19827.314453125\nEpoch 5100, Loss: 19646.69921875\nEpoch 5200, Loss: 19470.017578125\nEpoch 5300, Loss: 19297.060546875\nEpoch 5400, Loss: 19127.634765625\nEpoch 5500, Loss: 18961.57421875\nEpoch 5600, Loss: 18798.732421875\nEpoch 5700, Loss: 18638.966796875\nEpoch 5800, Loss: 18482.15625\nEpoch 5900, Loss: 18328.32421875\nEpoch 6000, Loss: 18177.490234375\nEpoch 6100, Loss: 18029.296875\nEpoch 6200, Loss: 17883.662109375\nEpoch 6300, Loss: 17740.5078125\nEpoch 6400, Loss: 17599.763671875\nEpoch 6500, Loss: 17461.3671875\nEpoch 6600, Loss: 17325.259765625\nEpoch 6700, Loss: 17191.38671875\nEpoch 6800, Loss: 17059.6875\nEpoch 6900, Loss: 16930.119140625\nEpoch 7000, Loss: 16802.630859375\nEpoch 7100, Loss: 16677.296875\nEpoch 7200, Loss: 16554.333984375\nEpoch 7300, Loss: 16433.39453125\nEpoch 7400, Loss: 16314.49609375\nEpoch 7500, Loss: 16197.6845703125\nEpoch 7600, Loss: 16082.642578125\nEpoch 7700, Loss: 15969.3525390625\nEpoch 7800, Loss: 15857.755859375\nEpoch 7900, Loss: 15747.8271484375\nEpoch 8000, Loss: 15639.533203125\nEpoch 8100, Loss: 15532.833984375\nEpoch 8200, Loss: 15427.7060546875\nEpoch 8300, Loss: 15324.115234375\nEpoch 8400, Loss: 15222.0517578125\nEpoch 8500, Loss: 15121.4638671875\nEpoch 8600, Loss: 15022.33984375\nEpoch 8700, Loss: 14924.6572265625\nEpoch 8800, Loss: 14828.38671875\nEpoch 8900, Loss: 14733.521484375\nEpoch 9000, Loss: 14640.0791015625\nEpoch 9100, Loss: 14548.3056640625\nEpoch 9200, Loss: 14457.9111328125\nEpoch 9300, Loss: 14368.859375\nEpoch 9400, Loss: 14281.138671875\nEpoch 9500, Loss: 14194.7333984375\nEpoch 9600, Loss: 14109.6328125\nEpoch 9700, Loss: 14025.8203125\nEpoch 9800, Loss: 13943.2890625\nEpoch 9900, Loss: 13862.0283203125\nEpoch 10000, Loss: 13782.0400390625\nEpoch 10100, Loss: 13703.3095703125\nEpoch 10200, Loss: 13625.8388671875\nEpoch 10300, Loss: 13549.6123046875\nEpoch 10400, Loss: 13474.6357421875\nEpoch 10500, Loss: 13400.8974609375\nEpoch 10600, Loss: 13328.39453125\nEpoch 10700, Loss: 13257.1181640625\nEpoch 10800, Loss: 13187.0625\nEpoch 10900, Loss: 13118.2177734375\nEpoch 11000, Loss: 13050.5791015625\nEpoch 11100, Loss: 12984.130859375\nEpoch 11200, Loss: 12918.8671875\nEpoch 11300, Loss: 12854.7958984375\nEpoch 11400, Loss: 12791.9140625\nEpoch 11500, Loss: 12730.18359375\nEpoch 11600, Loss: 12669.58203125\nEpoch 11700, Loss: 12610.0927734375\nEpoch 11800, Loss: 12551.6943359375\nEpoch 11900, Loss: 12494.3623046875\nEpoch 12000, Loss: 12438.080078125\nEpoch 12100, Loss: 12382.927734375\nEpoch 12200, Loss: 12329.076171875\nEpoch 12300, Loss: 12276.23046875\nEpoch 12400, Loss: 12224.357421875\nEpoch 12500, Loss: 12173.4423828125\nEpoch 12600, Loss: 12123.44921875\nEpoch 12700, Loss: 12074.373046875\nEpoch 12800, Loss: 12026.3271484375\nEpoch 12900, Loss: 11979.287109375\nEpoch 13000, Loss: 11933.13671875\nEpoch 13100, Loss: 11887.84765625\nEpoch 13200, Loss: 11843.4033203125\nEpoch 13300, Loss: 11799.796875\nEpoch 13400, Loss: 11757.013671875\nEpoch 13500, Loss: 11715.041015625\nEpoch 13600, Loss: 11673.8681640625\nEpoch 13700, Loss: 11633.4873046875\nEpoch 13800, Loss: 11593.890625\nEpoch 13900, Loss: 11555.0625\nEpoch 14000, Loss: 11517.1484375\nEpoch 14100, Loss: 11480.142578125\nEpoch 14200, Loss: 11443.9775390625\nEpoch 14300, Loss: 11408.6376953125\nEpoch 14400, Loss: 11374.0888671875\nEpoch 14500, Loss: 11340.3935546875\nEpoch 14600, Loss: 11307.623046875\nEpoch 14700, Loss: 11275.6494140625\nEpoch 14800, Loss: 11244.4208984375\nEpoch 14900, Loss: 11213.9267578125\nEpoch 15000, Loss: 11184.1474609375\nEpoch 15100, Loss: 11155.048828125\nEpoch 15200, Loss: 11126.62109375\nEpoch 15300, Loss: 11098.8583984375\nEpoch 15400, Loss: 11071.751953125\nEpoch 15500, Loss: 11045.2958984375\nEpoch 15600, Loss: 11019.484375\nEpoch 15700, Loss: 10994.3076171875\nEpoch 15800, Loss: 10969.767578125\nEpoch 15900, Loss: 10945.888671875\nEpoch 16000, Loss: 10922.654296875\nEpoch 16100, Loss: 10900.05859375\nEpoch 16200, Loss: 10878.1416015625\nEpoch 16300, Loss: 10856.90234375\nEpoch 16400, Loss: 10836.275390625\nEpoch 16500, Loss: 10816.2294921875\nEpoch 16600, Loss: 10796.728515625\nEpoch 16700, Loss: 10777.75390625\nEpoch 16800, Loss: 10759.296875\nEpoch 16900, Loss: 10741.3515625\nEpoch 17000, Loss: 10723.908203125\nEpoch 17100, Loss: 10706.9560546875\nEpoch 17200, Loss: 10690.5107421875\nEpoch 17300, Loss: 10674.5810546875\nEpoch 17400, Loss: 10659.134765625\nEpoch 17500, Loss: 10644.1650390625\nEpoch 17600, Loss: 10629.65625\nEpoch 17700, Loss: 10615.609375\nEpoch 17800, Loss: 10602.01171875\nEpoch 17900, Loss: 10588.86328125\nEpoch 18000, Loss: 10576.154296875\nEpoch 18100, Loss: 10563.8837890625\nEpoch 18200, Loss: 10552.044921875\nEpoch 18300, Loss: 10540.6328125\nEpoch 18400, Loss: 10529.6416015625\nEpoch 18500, Loss: 10519.078125\nEpoch 18600, Loss: 10508.94921875\nEpoch 18700, Loss: 10499.357421875\nEpoch 18800, Loss: 10490.2275390625\nEpoch 18900, Loss: 10481.5205078125\nEpoch 19000, Loss: 10473.2177734375\nEpoch 19100, Loss: 10465.32421875\nEpoch 19200, Loss: 10457.8330078125\nEpoch 19300, Loss: 10450.71875\nEpoch 19400, Loss: 10443.978515625\nEpoch 19500, Loss: 10437.5966796875\nEpoch 19600, Loss: 10431.5634765625\nEpoch 19700, Loss: 10425.8701171875\nEpoch 19800, Loss: 10420.5029296875\nEpoch 19900, Loss: 10415.451171875\nEpoch 20000, Loss: 10410.70703125\nEpoch 20100, Loss: 10406.26171875\nEpoch 20200, Loss: 10402.0986328125\nEpoch 20300, Loss: 10398.197265625\nEpoch 20400, Loss: 10394.552734375\nEpoch 20500, Loss: 10391.1484375\nEpoch 20600, Loss: 10387.9736328125\nEpoch 20700, Loss: 10385.017578125\nEpoch 20800, Loss: 10382.271484375\nEpoch 20900, Loss: 10379.7158203125\nEpoch 21000, Loss: 10377.3447265625\nEpoch 21100, Loss: 10375.138671875\nEpoch 21200, Loss: 10373.0908203125\nEpoch 21300, Loss: 10371.1845703125\nEpoch 21400, Loss: 10369.404296875\nEpoch 21500, Loss: 10367.73828125\nEpoch 21600, Loss: 10366.1767578125\nEpoch 21700, Loss: 10364.7041015625\nEpoch 21800, Loss: 10363.30859375\nEpoch 21900, Loss: 10361.982421875\nEpoch 22000, Loss: 10360.7138671875\nEpoch 22100, Loss: 10359.494140625\nEpoch 22200, Loss: 10358.3134765625\nEpoch 22300, Loss: 10357.1630859375\nEpoch 22400, Loss: 10356.0361328125\nEpoch 22500, Loss: 10354.9267578125\nEpoch 22600, Loss: 10353.8251953125\nEpoch 22700, Loss: 10352.7275390625\nEpoch 22800, Loss: 10351.7001953125\nEpoch 22900, Loss: 10350.705078125\nEpoch 23000, Loss: 10349.7109375\nEpoch 23100, Loss: 10348.72265625\nEpoch 23200, Loss: 10347.7421875\nEpoch 23300, Loss: 10346.7705078125\nEpoch 23400, Loss: 10345.80859375\nEpoch 23500, Loss: 10344.8515625\nEpoch 23600, Loss: 10343.9072265625\nEpoch 23700, Loss: 10342.9677734375\nEpoch 23800, Loss: 10342.0390625\nEpoch 23900, Loss: 10341.119140625\nEpoch 24000, Loss: 10340.2080078125\nEpoch 24100, Loss: 10339.306640625\nEpoch 24200, Loss: 10338.416015625\nEpoch 24300, Loss: 10337.53125\nEpoch 24400, Loss: 10336.6494140625\nEpoch 24500, Loss: 10335.7548828125\nEpoch 24600, Loss: 10334.841796875\nEpoch 24700, Loss: 10333.9072265625\nEpoch 24800, Loss: 10332.9521484375\nEpoch 24900, Loss: 10331.974609375\nEpoch 25000, Loss: 10330.9736328125\nEpoch 25100, Loss: 10329.9501953125\nEpoch 25200, Loss: 10328.9140625\nEpoch 25300, Loss: 10327.8955078125\nEpoch 25400, Loss: 10326.9052734375\nEpoch 25500, Loss: 10325.9423828125\nEpoch 25600, Loss: 10325.009765625\nEpoch 25700, Loss: 10324.142578125\nEpoch 25800, Loss: 10323.349609375\nEpoch 25900, Loss: 10322.6240234375\nEpoch 26000, Loss: 10321.9599609375\nEpoch 26100, Loss: 10321.3505859375\nEpoch 26200, Loss: 10320.7919921875\nEpoch 26300, Loss: 10320.26953125\nEpoch 26400, Loss: 10319.765625\nEpoch 26500, Loss: 10319.2646484375\nEpoch 26600, Loss: 10318.7529296875\nEpoch 26700, Loss: 10318.2294921875\nEpoch 26800, Loss: 10317.693359375\nEpoch 26900, Loss: 10317.1435546875\nEpoch 27000, Loss: 10316.5810546875\nEpoch 27100, Loss: 10316.001953125\nEpoch 27200, Loss: 10315.412109375\nEpoch 27300, Loss: 10314.8056640625\nEpoch 27400, Loss: 10314.1845703125\nEpoch 27500, Loss: 10313.5546875\nEpoch 27600, Loss: 10312.958984375\nEpoch 27700, Loss: 10312.41796875\nEpoch 27800, Loss: 10311.9287109375\nEpoch 27900, Loss: 10311.482421875\nEpoch 28000, Loss: 10311.091796875\nEpoch 28100, Loss: 10310.76171875\nEpoch 28200, Loss: 10310.4833984375\nEpoch 28300, Loss: 10310.2412109375\nEpoch 28400, Loss: 10310.0205078125\nEpoch 28500, Loss: 10309.826171875\nEpoch 28600, Loss: 10309.662109375\nEpoch 28700, Loss: 10309.5224609375\nEpoch 28800, Loss: 10309.40625\nEpoch 28900, Loss: 10309.310546875\nEpoch 29000, Loss: 10309.23046875\nEpoch 29100, Loss: 10309.166015625\nEpoch 29200, Loss: 10309.1123046875\nEpoch 29300, Loss: 10309.0693359375\nEpoch 29400, Loss: 10309.03125\nEpoch 29500, Loss: 10308.9970703125\nEpoch 29600, Loss: 10308.9677734375\nEpoch 29700, Loss: 10308.9423828125\nEpoch 29800, Loss: 10308.9189453125\nEpoch 29900, Loss: 10308.900390625\nEpoch 30000, Loss: 10308.8818359375\nEpoch 30100, Loss: 10308.869140625\nEpoch 30200, Loss: 10308.85546875\nEpoch 30300, Loss: 10308.845703125\nEpoch 30400, Loss: 10308.8369140625\nEpoch 30500, Loss: 10308.828125\nEpoch 30600, Loss: 10308.822265625\nEpoch 30700, Loss: 10308.81640625\nEpoch 30800, Loss: 10308.8125\nEpoch 30900, Loss: 10308.8076171875\nEpoch 31000, Loss: 10308.8056640625\nEpoch 31100, Loss: 10308.8046875\nEpoch 31200, Loss: 10308.7998046875\nEpoch 31300, Loss: 10308.798828125\nEpoch 31400, Loss: 10308.796875\nEpoch 31500, Loss: 10308.7958984375\nEpoch 31600, Loss: 10308.7958984375\nEpoch 31700, Loss: 10308.7939453125\nEpoch 31800, Loss: 10308.7939453125\nEpoch 31900, Loss: 10308.7919921875\nEpoch 32000, Loss: 10308.7919921875\nEpoch 32100, Loss: 10308.7919921875\nEpoch 32200, Loss: 10308.791015625\nEpoch 32300, Loss: 10308.7919921875\nEpoch 32400, Loss: 10308.791015625\nEpoch 32500, Loss: 10308.791015625\nEpoch 32600, Loss: 10308.791015625\nEpoch 32700, Loss: 10308.7900390625\nEpoch 32800, Loss: 10308.7890625\nEpoch 32900, Loss: 10308.7900390625\nEpoch 33000, Loss: 10308.7900390625\nEpoch 33100, Loss: 10308.7890625\nEpoch 33200, Loss: 10308.7890625\nEpoch 33300, Loss: 10308.7890625\nEpoch 33400, Loss: 10308.7900390625\nEpoch 33500, Loss: 10308.7900390625\nEpoch 33600, Loss: 10308.7900390625\nEpoch 33700, Loss: 10308.7900390625\nEpoch 33800, Loss: 10308.7900390625\nEpoch 33900, Loss: 10308.7890625\nEpoch 34000, Loss: 10308.7890625\nEpoch 34100, Loss: 10308.7900390625\nEpoch 34200, Loss: 10308.7900390625\nEpoch 34300, Loss: 10308.7900390625\nEpoch 34400, Loss: 10308.7900390625\nEpoch 34500, Loss: 10308.7890625\nEpoch 34600, Loss: 10308.7900390625\nEpoch 34700, Loss: 10308.7900390625\nEpoch 34800, Loss: 10308.7890625\nEpoch 34900, Loss: 10308.7900390625\nEpoch 35000, Loss: 10308.7900390625\nEpoch 35100, Loss: 10308.7900390625\nEpoch 35200, Loss: 10308.7900390625\nEpoch 35300, Loss: 10308.7900390625\nEpoch 35400, Loss: 10308.7900390625\nEpoch 35500, Loss: 10308.7900390625\nEpoch 35600, Loss: 10308.7900390625\nEpoch 35700, Loss: 10308.7890625\nEpoch 35800, Loss: 10308.7900390625\nEpoch 35900, Loss: 10308.7890625\nEpoch 36000, Loss: 10308.7890625\nEpoch 36100, Loss: 10308.7900390625\nEpoch 36200, Loss: 10308.7900390625\nEpoch 36300, Loss: 10308.7890625\nEpoch 36400, Loss: 10308.7900390625\nEpoch 36500, Loss: 10308.7890625\nEpoch 36600, Loss: 10308.7900390625\nEpoch 36700, Loss: 10308.7890625\nEpoch 36800, Loss: 10308.791015625\nEpoch 36900, Loss: 10308.7900390625\nEpoch 37000, Loss: 10308.7890625\nEpoch 37100, Loss: 10308.7900390625\nEpoch 37200, Loss: 10308.7900390625\nEpoch 37300, Loss: 10308.7900390625\nEpoch 37400, Loss: 10308.7890625\nEpoch 37500, Loss: 10308.7890625\nEpoch 37600, Loss: 10308.7900390625\nEpoch 37700, Loss: 10308.7900390625\nEpoch 37800, Loss: 10308.7900390625\nEpoch 37900, Loss: 10308.7900390625\nEpoch 38000, Loss: 10308.7900390625\nEpoch 38100, Loss: 10308.7900390625\nEpoch 38200, Loss: 10308.7890625\nEpoch 38300, Loss: 10308.791015625\nEpoch 38400, Loss: 10308.7900390625\nEpoch 38500, Loss: 10308.7900390625\nEpoch 38600, Loss: 10308.7890625\nEpoch 38700, Loss: 10308.7900390625\nEpoch 38800, Loss: 10308.7890625\nEpoch 38900, Loss: 10308.7890625\nEpoch 39000, Loss: 10308.7900390625\nEpoch 39100, Loss: 10308.7890625\nEpoch 39200, Loss: 10308.7900390625\nEpoch 39300, Loss: 10308.7890625\nEpoch 39400, Loss: 10308.7900390625\nEpoch 39500, Loss: 10308.7890625\nEpoch 39600, Loss: 10308.7900390625\nEpoch 39700, Loss: 10308.7890625\nEpoch 39800, Loss: 10308.791015625\nEpoch 39900, Loss: 10308.7890625\nEpoch 40000, Loss: 10308.7890625\nEpoch 40100, Loss: 10308.7890625\nEpoch 40200, Loss: 10308.7890625\nEpoch 40300, Loss: 10308.7890625\nEpoch 40400, Loss: 10308.7890625\nEpoch 40500, Loss: 10308.7890625\nEpoch 40600, Loss: 10308.7900390625\nEpoch 40700, Loss: 10308.7890625\nEpoch 40800, Loss: 10308.7890625\nEpoch 40900, Loss: 10308.7890625\nEpoch 41000, Loss: 10308.7900390625\nEpoch 41100, Loss: 10308.7890625\nEpoch 41200, Loss: 10308.7890625\nEpoch 41300, Loss: 10308.7890625\nEpoch 41400, Loss: 10308.7890625\nEpoch 41500, Loss: 10308.7900390625\nEpoch 41600, Loss: 10308.7900390625\nEpoch 41700, Loss: 10308.791015625\nEpoch 41800, Loss: 10308.7900390625\nEpoch 41900, Loss: 10308.7900390625\nEpoch 42000, Loss: 10308.7900390625\nEpoch 42100, Loss: 10308.7890625\nEpoch 42200, Loss: 10308.7900390625\nEpoch 42300, Loss: 10308.7890625\nEpoch 42400, Loss: 10308.7900390625\nEpoch 42500, Loss: 10308.791015625\nEpoch 42600, Loss: 10308.7890625\nEpoch 42700, Loss: 10308.7890625\nEpoch 42800, Loss: 10308.7900390625\nEpoch 42900, Loss: 10308.7900390625\nEpoch 43000, Loss: 10308.7880859375\nEpoch 43100, Loss: 10308.7900390625\nEpoch 43200, Loss: 10308.7890625\nEpoch 43300, Loss: 10308.7890625\nEpoch 43400, Loss: 10308.791015625\nEpoch 43500, Loss: 10308.7900390625\nEpoch 43600, Loss: 10308.7900390625\nEpoch 43700, Loss: 10308.7880859375\nEpoch 43800, Loss: 10308.7900390625\nEpoch 43900, Loss: 10308.7900390625\nEpoch 44000, Loss: 10308.791015625\nEpoch 44100, Loss: 10308.7900390625\nEpoch 44200, Loss: 10308.7900390625\nEpoch 44300, Loss: 10308.7900390625\nEpoch 44400, Loss: 10308.7890625\nEpoch 44500, Loss: 10308.7900390625\nEpoch 44600, Loss: 10308.7890625\nEpoch 44700, Loss: 10308.7900390625\nEpoch 44800, Loss: 10308.7900390625\nEpoch 44900, Loss: 10308.7900390625\nEpoch 45000, Loss: 10308.7900390625\nEpoch 45100, Loss: 10308.7890625\nEpoch 45200, Loss: 10308.7890625\nEpoch 45300, Loss: 10308.7890625\nEpoch 45400, Loss: 10308.7900390625\nEpoch 45500, Loss: 10308.7900390625\nEpoch 45600, Loss: 10308.7900390625\nEpoch 45700, Loss: 10308.7890625\nEpoch 45800, Loss: 10308.7890625\nEpoch 45900, Loss: 10308.7900390625\nEpoch 46000, Loss: 10308.7890625\nEpoch 46100, Loss: 10308.7900390625\nEpoch 46200, Loss: 10308.7900390625\nEpoch 46300, Loss: 10308.7900390625\nEpoch 46400, Loss: 10308.7890625\nEpoch 46500, Loss: 10308.7890625\nEpoch 46600, Loss: 10308.7890625\nEpoch 46700, Loss: 10308.7900390625\nEpoch 46800, Loss: 10308.7900390625\nEpoch 46900, Loss: 10308.7890625\nEpoch 47000, Loss: 10308.7900390625\nEpoch 47100, Loss: 10308.7890625\nEpoch 47200, Loss: 10308.7890625\nEpoch 47300, Loss: 10308.7900390625\nEpoch 47400, Loss: 10308.7890625\nEpoch 47500, Loss: 10308.7900390625\nEpoch 47600, Loss: 10308.7890625\nEpoch 47700, Loss: 10308.7900390625\nEpoch 47800, Loss: 10308.7890625\nEpoch 47900, Loss: 10308.7890625\nEpoch 48000, Loss: 10308.7900390625\nEpoch 48100, Loss: 10308.7890625\nEpoch 48200, Loss: 10308.7900390625\nEpoch 48300, Loss: 10308.7900390625\nEpoch 48400, Loss: 10308.7900390625\nEpoch 48500, Loss: 10308.7890625\nEpoch 48600, Loss: 10308.7890625\nEpoch 48700, Loss: 10308.7900390625\nEpoch 48800, Loss: 10308.7890625\nEpoch 48900, Loss: 10308.791015625\nEpoch 49000, Loss: 10308.7900390625\nEpoch 49100, Loss: 10308.7900390625\nEpoch 49200, Loss: 10308.7890625\nEpoch 49300, Loss: 10308.7890625\nEpoch 49400, Loss: 10308.7900390625\nEpoch 49500, Loss: 10308.7890625\nEpoch 49600, Loss: 10308.7900390625\nEpoch 49700, Loss: 10308.7890625\nEpoch 49800, Loss: 10308.7900390625\nEpoch 49900, Loss: 10308.7890625\nEpoch 0, Loss: 63828.96875\nEpoch 100, Loss: 60930.94140625\nEpoch 200, Loss: 58221.23828125\nEpoch 300, Loss: 55685.62109375\nEpoch 400, Loss: 53312.18359375\nEpoch 500, Loss: 51089.53515625\nEpoch 600, Loss: 49008.88671875\nEpoch 700, Loss: 47061.34765625\nEpoch 800, Loss: 45239.21875\nEpoch 900, Loss: 43536.0625\nEpoch 1000, Loss: 41944.55078125\nEpoch 1100, Loss: 40457.8203125\nEpoch 1200, Loss: 39069.47265625\nEpoch 1300, Loss: 37773.35546875\nEpoch 1400, Loss: 36563.984375\nEpoch 1500, Loss: 35435.76953125\nEpoch 1600, Loss: 34382.7265625\nEpoch 1700, Loss: 33399.59375\nEpoch 1800, Loss: 32481.404296875\nEpoch 1900, Loss: 31623.513671875\nEpoch 2000, Loss: 30821.595703125\nEpoch 2100, Loss: 30071.91015625\nEpoch 2200, Loss: 29370.978515625\nEpoch 2300, Loss: 28714.9140625\nEpoch 2400, Loss: 28101.564453125\nEpoch 2500, Loss: 27526.625\nEpoch 2600, Loss: 26987.16015625\nEpoch 2700, Loss: 26480.53515625\nEpoch 2800, Loss: 26004.3046875\nEpoch 2900, Loss: 25556.205078125\nEpoch 3000, Loss: 25134.1171875\nEpoch 3100, Loss: 24736.06640625\nEpoch 3200, Loss: 24360.20703125\nEpoch 3300, Loss: 24005.02734375\nEpoch 3400, Loss: 23669.123046875\nEpoch 3500, Loss: 23351.21875\nEpoch 3600, Loss: 23049.39453125\nEpoch 3700, Loss: 22761.89453125\nEpoch 3800, Loss: 22487.443359375\nEpoch 3900, Loss: 22224.8515625\nEpoch 4000, Loss: 21973.181640625\nEpoch 4100, Loss: 21731.83984375\nEpoch 4200, Loss: 21499.353515625\nEpoch 4300, Loss: 21274.904296875\nEpoch 4400, Loss: 21057.955078125\nEpoch 4500, Loss: 20848.041015625\nEpoch 4600, Loss: 20644.193359375\nEpoch 4700, Loss: 20445.91015625\nEpoch 4800, Loss: 20252.763671875\nEpoch 4900, Loss: 20064.390625\nEpoch 5000, Loss: 19880.453125\nEpoch 5100, Loss: 19700.68359375\nEpoch 5200, Loss: 19524.822265625\nEpoch 5300, Loss: 19352.66796875\nEpoch 5400, Loss: 19184.03125\nEpoch 5500, Loss: 19018.734375\nEpoch 5600, Loss: 18856.63671875\nEpoch 5700, Loss: 18697.599609375\nEpoch 5800, Loss: 18541.505859375\nEpoch 5900, Loss: 18388.578125\nEpoch 6000, Loss: 18238.505859375\nEpoch 6100, Loss: 18091.0546875\nEpoch 6200, Loss: 17946.140625\nEpoch 6300, Loss: 17803.689453125\nEpoch 6400, Loss: 17663.6328125\nEpoch 6500, Loss: 17525.91015625\nEpoch 6600, Loss: 17390.451171875\nEpoch 6700, Loss: 17257.2109375\nEpoch 6800, Loss: 17126.12890625\nEpoch 6900, Loss: 16997.162109375\nEpoch 7000, Loss: 16870.259765625\nEpoch 7100, Loss: 16745.6484375\nEpoch 7200, Loss: 16623.486328125\nEpoch 7300, Loss: 16503.20703125\nEpoch 7400, Loss: 16384.931640625\nEpoch 7500, Loss: 16268.8466796875\nEpoch 7600, Loss: 16154.50390625\nEpoch 7700, Loss: 16041.869140625\nEpoch 7800, Loss: 15930.9140625\nEpoch 7900, Loss: 15821.603515625\nEpoch 8000, Loss: 15713.8994140625\nEpoch 8100, Loss: 15607.775390625\nEpoch 8200, Loss: 15503.2099609375\nEpoch 8300, Loss: 15400.1630859375\nEpoch 8400, Loss: 15298.6064453125\nEpoch 8500, Loss: 15198.517578125\nEpoch 8600, Loss: 15099.876953125\nEpoch 8700, Loss: 15002.6552734375\nEpoch 8800, Loss: 14906.83984375\nEpoch 8900, Loss: 14812.4033203125\nEpoch 9000, Loss: 14719.72265625\nEpoch 9100, Loss: 14628.4384765625\nEpoch 9200, Loss: 14538.51171875\nEpoch 9300, Loss: 14449.9287109375\nEpoch 9400, Loss: 14362.6630859375\nEpoch 9500, Loss: 14276.70703125\nEpoch 9600, Loss: 14192.0390625\nEpoch 9700, Loss: 14108.6484375\nEpoch 9800, Loss: 14026.5341796875\nEpoch 9900, Loss: 13945.681640625\nEpoch 10000, Loss: 13866.08203125\nEpoch 10100, Loss: 13787.734375\nEpoch 10200, Loss: 13710.6298828125\nEpoch 10300, Loss: 13634.763671875\nEpoch 10400, Loss: 13560.1318359375\nEpoch 10500, Loss: 13486.7294921875\nEpoch 10600, Loss: 13414.5478515625\nEpoch 10700, Loss: 13343.5869140625\nEpoch 10800, Loss: 13273.8330078125\nEpoch 10900, Loss: 13205.2822265625\nEpoch 11000, Loss: 13137.921875\nEpoch 11100, Loss: 13071.7431640625\nEpoch 11200, Loss: 13006.75\nEpoch 11300, Loss: 12942.9619140625\nEpoch 11400, Loss: 12880.3271484375\nEpoch 11500, Loss: 12818.8291015625\nEpoch 11600, Loss: 12758.447265625\nEpoch 11700, Loss: 12699.158203125\nEpoch 11800, Loss: 12640.9453125\nEpoch 11900, Loss: 12583.783203125\nEpoch 12000, Loss: 12527.6552734375\nEpoch 12100, Loss: 12472.9189453125\nEpoch 12200, Loss: 12419.28515625\nEpoch 12300, Loss: 12366.6474609375\nEpoch 12400, Loss: 12315.017578125\nEpoch 12500, Loss: 12264.5390625\nEpoch 12600, Loss: 12215.083984375\nEpoch 12700, Loss: 12166.5556640625\nEpoch 12800, Loss: 12118.9091796875\nEpoch 12900, Loss: 12072.1318359375\nEpoch 13000, Loss: 12026.21875\nEpoch 13100, Loss: 11981.1484375\nEpoch 13200, Loss: 11936.9091796875\nEpoch 13300, Loss: 11893.4931640625\nEpoch 13400, Loss: 11850.8955078125\nEpoch 13500, Loss: 11809.0966796875\nEpoch 13600, Loss: 11768.0947265625\nEpoch 13700, Loss: 11727.873046875\nEpoch 13800, Loss: 11688.4208984375\nEpoch 13900, Loss: 11649.884765625\nEpoch 14000, Loss: 11612.3505859375\nEpoch 14100, Loss: 11575.7958984375\nEpoch 14200, Loss: 11540.142578125\nEpoch 14300, Loss: 11505.32421875\nEpoch 14400, Loss: 11471.3095703125\nEpoch 14500, Loss: 11438.1572265625\nEpoch 14600, Loss: 11405.7919921875\nEpoch 14700, Loss: 11374.1826171875\nEpoch 14800, Loss: 11343.318359375\nEpoch 14900, Loss: 11313.1865234375\nEpoch 15000, Loss: 11283.775390625\nEpoch 15100, Loss: 11255.046875\nEpoch 15200, Loss: 11226.9853515625\nEpoch 15300, Loss: 11199.5732421875\nEpoch 15400, Loss: 11172.798828125\nEpoch 15500, Loss: 11146.6630859375\nEpoch 15600, Loss: 11121.154296875\nEpoch 15700, Loss: 11096.2900390625\nEpoch 15800, Loss: 11072.1611328125\nEpoch 15900, Loss: 11048.7275390625\nEpoch 16000, Loss: 11025.9462890625\nEpoch 16100, Loss: 11003.796875\nEpoch 16200, Loss: 10982.26171875\nEpoch 16300, Loss: 10961.33203125\nEpoch 16400, Loss: 10940.994140625\nEpoch 16500, Loss: 10921.232421875\nEpoch 16600, Loss: 10902.03125\nEpoch 16700, Loss: 10883.37109375\nEpoch 16800, Loss: 10865.2177734375\nEpoch 16900, Loss: 10847.5791015625\nEpoch 17000, Loss: 10830.482421875\nEpoch 17100, Loss: 10813.884765625\nEpoch 17200, Loss: 10797.7734375\nEpoch 17300, Loss: 10782.1357421875\nEpoch 17400, Loss: 10766.9638671875\nEpoch 17500, Loss: 10752.2548828125\nEpoch 17600, Loss: 10737.9990234375\nEpoch 17700, Loss: 10724.1923828125\nEpoch 17800, Loss: 10710.828125\nEpoch 17900, Loss: 10697.90234375\nEpoch 18000, Loss: 10685.4091796875\nEpoch 18100, Loss: 10673.3837890625\nEpoch 18200, Loss: 10661.9169921875\nEpoch 18300, Loss: 10650.9130859375\nEpoch 18400, Loss: 10640.3427734375\nEpoch 18500, Loss: 10630.19140625\nEpoch 18600, Loss: 10620.46484375\nEpoch 18700, Loss: 10611.15234375\nEpoch 18800, Loss: 10602.25\nEpoch 18900, Loss: 10593.7744140625\nEpoch 19000, Loss: 10585.6982421875\nEpoch 19100, Loss: 10578.013671875\nEpoch 19200, Loss: 10570.7080078125\nEpoch 19300, Loss: 10563.7685546875\nEpoch 19400, Loss: 10557.197265625\nEpoch 19500, Loss: 10550.9873046875\nEpoch 19600, Loss: 10545.115234375\nEpoch 19700, Loss: 10539.5751953125\nEpoch 19800, Loss: 10534.349609375\nEpoch 19900, Loss: 10529.427734375\nEpoch 20000, Loss: 10524.802734375\nEpoch 20100, Loss: 10520.4580078125\nEpoch 20200, Loss: 10516.3828125\nEpoch 20300, Loss: 10512.564453125\nEpoch 20400, Loss: 10508.9873046875\nEpoch 20500, Loss: 10505.642578125\nEpoch 20600, Loss: 10502.5146484375\nEpoch 20700, Loss: 10499.5908203125\nEpoch 20800, Loss: 10496.857421875\nEpoch 20900, Loss: 10494.30078125\nEpoch 21000, Loss: 10491.904296875\nEpoch 21100, Loss: 10489.6591796875\nEpoch 21200, Loss: 10487.55078125\nEpoch 21300, Loss: 10485.5654296875\nEpoch 21400, Loss: 10483.689453125\nEpoch 21500, Loss: 10481.9072265625\nEpoch 21600, Loss: 10480.20703125\nEpoch 21700, Loss: 10478.5771484375\nEpoch 21800, Loss: 10477.005859375\nEpoch 21900, Loss: 10475.48046875\nEpoch 22000, Loss: 10473.994140625\nEpoch 22100, Loss: 10472.5888671875\nEpoch 22200, Loss: 10471.29296875\nEpoch 22300, Loss: 10470.029296875\nEpoch 22400, Loss: 10468.7900390625\nEpoch 22500, Loss: 10467.5654296875\nEpoch 22600, Loss: 10466.357421875\nEpoch 22700, Loss: 10465.1591796875\nEpoch 22800, Loss: 10463.970703125\nEpoch 22900, Loss: 10462.7939453125\nEpoch 23000, Loss: 10461.6220703125\nEpoch 23100, Loss: 10460.4560546875\nEpoch 23200, Loss: 10459.294921875\nEpoch 23300, Loss: 10458.138671875\nEpoch 23400, Loss: 10456.986328125\nEpoch 23500, Loss: 10455.83984375\nEpoch 23600, Loss: 10454.6962890625\nEpoch 23700, Loss: 10453.55859375\nEpoch 23800, Loss: 10452.4228515625\nEpoch 23900, Loss: 10451.27734375\nEpoch 24000, Loss: 10450.11328125\nEpoch 24100, Loss: 10448.9306640625\nEpoch 24200, Loss: 10447.72265625\nEpoch 24300, Loss: 10446.486328125\nEpoch 24400, Loss: 10445.2255859375\nEpoch 24500, Loss: 10443.947265625\nEpoch 24600, Loss: 10442.6796875\nEpoch 24700, Loss: 10441.435546875\nEpoch 24800, Loss: 10440.208984375\nEpoch 24900, Loss: 10439.005859375\nEpoch 25000, Loss: 10437.853515625\nEpoch 25100, Loss: 10436.7734375\nEpoch 25200, Loss: 10435.763671875\nEpoch 25300, Loss: 10434.8173828125\nEpoch 25400, Loss: 10433.931640625\nEpoch 25500, Loss: 10433.1025390625\nEpoch 25600, Loss: 10432.326171875\nEpoch 25700, Loss: 10431.599609375\nEpoch 25800, Loss: 10430.9169921875\nEpoch 25900, Loss: 10430.2646484375\nEpoch 26000, Loss: 10429.61328125\nEpoch 26100, Loss: 10428.9560546875\nEpoch 26200, Loss: 10428.2861328125\nEpoch 26300, Loss: 10427.6025390625\nEpoch 26400, Loss: 10426.9052734375\nEpoch 26500, Loss: 10426.189453125\nEpoch 26600, Loss: 10425.45703125\nEpoch 26700, Loss: 10424.7060546875\nEpoch 26800, Loss: 10423.939453125\nEpoch 26900, Loss: 10423.1572265625\nEpoch 27000, Loss: 10422.41015625\nEpoch 27100, Loss: 10421.712890625\nEpoch 27200, Loss: 10421.0615234375\nEpoch 27300, Loss: 10420.45703125\nEpoch 27400, Loss: 10419.9130859375\nEpoch 27500, Loss: 10419.4365234375\nEpoch 27600, Loss: 10419.01953125\nEpoch 27700, Loss: 10418.65234375\nEpoch 27800, Loss: 10418.330078125\nEpoch 27900, Loss: 10418.048828125\nEpoch 28000, Loss: 10417.80078125\nEpoch 28100, Loss: 10417.5830078125\nEpoch 28200, Loss: 10417.390625\nEpoch 28300, Loss: 10417.2236328125\nEpoch 28400, Loss: 10417.078125\nEpoch 28500, Loss: 10416.951171875\nEpoch 28600, Loss: 10416.8447265625\nEpoch 28700, Loss: 10416.7529296875\nEpoch 28800, Loss: 10416.6748046875\nEpoch 28900, Loss: 10416.6083984375\nEpoch 29000, Loss: 10416.5537109375\nEpoch 29100, Loss: 10416.5068359375\nEpoch 29200, Loss: 10416.4658203125\nEpoch 29300, Loss: 10416.4287109375\nEpoch 29400, Loss: 10416.396484375\nEpoch 29500, Loss: 10416.3662109375\nEpoch 29600, Loss: 10416.3408203125\nEpoch 29700, Loss: 10416.3173828125\nEpoch 29800, Loss: 10416.296875\nEpoch 29900, Loss: 10416.279296875\nEpoch 30000, Loss: 10416.2626953125\nEpoch 30100, Loss: 10416.2509765625\nEpoch 30200, Loss: 10416.23828125\nEpoch 30300, Loss: 10416.2294921875\nEpoch 30400, Loss: 10416.220703125\nEpoch 30500, Loss: 10416.2138671875\nEpoch 30600, Loss: 10416.208984375\nEpoch 30700, Loss: 10416.2041015625\nEpoch 30800, Loss: 10416.19921875\nEpoch 30900, Loss: 10416.1962890625\nEpoch 31000, Loss: 10416.1943359375\nEpoch 31100, Loss: 10416.193359375\nEpoch 31200, Loss: 10416.19140625\nEpoch 31300, Loss: 10416.189453125\nEpoch 31400, Loss: 10416.189453125\nEpoch 31500, Loss: 10416.1865234375\nEpoch 31600, Loss: 10416.1865234375\nEpoch 31700, Loss: 10416.185546875\nEpoch 31800, Loss: 10416.1865234375\nEpoch 31900, Loss: 10416.18359375\nEpoch 32000, Loss: 10416.1845703125\nEpoch 32100, Loss: 10416.1845703125\nEpoch 32200, Loss: 10416.18359375\nEpoch 32300, Loss: 10416.18359375\nEpoch 32400, Loss: 10416.1826171875\nEpoch 32500, Loss: 10416.1826171875\nEpoch 32600, Loss: 10416.1845703125\nEpoch 32700, Loss: 10416.18359375\nEpoch 32800, Loss: 10416.18359375\nEpoch 32900, Loss: 10416.1826171875\nEpoch 33000, Loss: 10416.181640625\nEpoch 33100, Loss: 10416.1826171875\nEpoch 33200, Loss: 10416.1826171875\nEpoch 33300, Loss: 10416.18359375\nEpoch 33400, Loss: 10416.181640625\nEpoch 33500, Loss: 10416.181640625\nEpoch 33600, Loss: 10416.181640625\nEpoch 33700, Loss: 10416.181640625\nEpoch 33800, Loss: 10416.1826171875\nEpoch 33900, Loss: 10416.181640625\nEpoch 34000, Loss: 10416.1826171875\nEpoch 34100, Loss: 10416.18359375\nEpoch 34200, Loss: 10416.1826171875\nEpoch 34300, Loss: 10416.1826171875\nEpoch 34400, Loss: 10416.1826171875\nEpoch 34500, Loss: 10416.1826171875\nEpoch 34600, Loss: 10416.181640625\nEpoch 34700, Loss: 10416.1826171875\nEpoch 34800, Loss: 10416.1826171875\nEpoch 34900, Loss: 10416.181640625\nEpoch 35000, Loss: 10416.18359375\nEpoch 35100, Loss: 10416.18359375\nEpoch 35200, Loss: 10416.181640625\nEpoch 35300, Loss: 10416.1826171875\nEpoch 35400, Loss: 10416.1826171875\nEpoch 35500, Loss: 10416.181640625\nEpoch 35600, Loss: 10416.1826171875\nEpoch 35700, Loss: 10416.18359375\nEpoch 35800, Loss: 10416.1826171875\nEpoch 35900, Loss: 10416.1826171875\nEpoch 36000, Loss: 10416.1826171875\nEpoch 36100, Loss: 10416.1826171875\nEpoch 36200, Loss: 10416.1826171875\nEpoch 36300, Loss: 10416.1826171875\nEpoch 36400, Loss: 10416.1826171875\nEpoch 36500, Loss: 10416.181640625\nEpoch 36600, Loss: 10416.1826171875\nEpoch 36700, Loss: 10416.181640625\nEpoch 36800, Loss: 10416.1826171875\nEpoch 36900, Loss: 10416.1826171875\nEpoch 37000, Loss: 10416.181640625\nEpoch 37100, Loss: 10416.181640625\nEpoch 37200, Loss: 10416.181640625\nEpoch 37300, Loss: 10416.1826171875\nEpoch 37400, Loss: 10416.1826171875\nEpoch 37500, Loss: 10416.1826171875\nEpoch 37600, Loss: 10416.1826171875\nEpoch 37700, Loss: 10416.1826171875\nEpoch 37800, Loss: 10416.181640625\nEpoch 37900, Loss: 10416.181640625\nEpoch 38000, Loss: 10416.1826171875\nEpoch 38100, Loss: 10416.1826171875\nEpoch 38200, Loss: 10416.181640625\nEpoch 38300, Loss: 10416.181640625\nEpoch 38400, Loss: 10416.1826171875\nEpoch 38500, Loss: 10416.1826171875\nEpoch 38600, Loss: 10416.1826171875\nEpoch 38700, Loss: 10416.181640625\nEpoch 38800, Loss: 10416.181640625\nEpoch 38900, Loss: 10416.181640625\nEpoch 39000, Loss: 10416.181640625\nEpoch 39100, Loss: 10416.1826171875\nEpoch 39200, Loss: 10416.18359375\nEpoch 39300, Loss: 10416.1826171875\nEpoch 39400, Loss: 10416.1826171875\nEpoch 39500, Loss: 10416.1826171875\nEpoch 39600, Loss: 10416.1826171875\nEpoch 39700, Loss: 10416.1826171875\nEpoch 39800, Loss: 10416.181640625\nEpoch 39900, Loss: 10416.1826171875\nEpoch 40000, Loss: 10416.1826171875\nEpoch 40100, Loss: 10416.181640625\nEpoch 40200, Loss: 10416.1826171875\nEpoch 40300, Loss: 10416.181640625\nEpoch 40400, Loss: 10416.1826171875\nEpoch 40500, Loss: 10416.181640625\nEpoch 40600, Loss: 10416.181640625\nEpoch 40700, Loss: 10416.1826171875\nEpoch 40800, Loss: 10416.181640625\nEpoch 40900, Loss: 10416.181640625\nEpoch 41000, Loss: 10416.1826171875\nEpoch 41100, Loss: 10416.1826171875\nEpoch 41200, Loss: 10416.181640625\nEpoch 41300, Loss: 10416.181640625\nEpoch 41400, Loss: 10416.181640625\nEpoch 41500, Loss: 10416.181640625\nEpoch 41600, Loss: 10416.18359375\nEpoch 41700, Loss: 10416.18359375\nEpoch 41800, Loss: 10416.181640625\nEpoch 41900, Loss: 10416.181640625\nEpoch 42000, Loss: 10416.181640625\nEpoch 42100, Loss: 10416.1826171875\nEpoch 42200, Loss: 10416.1826171875\nEpoch 42300, Loss: 10416.1826171875\nEpoch 42400, Loss: 10416.1826171875\nEpoch 42500, Loss: 10416.1826171875\nEpoch 42600, Loss: 10416.1826171875\nEpoch 42700, Loss: 10416.181640625\nEpoch 42800, Loss: 10416.1826171875\nEpoch 42900, Loss: 10416.181640625\nEpoch 43000, Loss: 10416.181640625\nEpoch 43100, Loss: 10416.181640625\nEpoch 43200, Loss: 10416.1826171875\nEpoch 43300, Loss: 10416.181640625\nEpoch 43400, Loss: 10416.1826171875\nEpoch 43500, Loss: 10416.1826171875\nEpoch 43600, Loss: 10416.181640625\nEpoch 43700, Loss: 10416.181640625\nEpoch 43800, Loss: 10416.181640625\nEpoch 43900, Loss: 10416.1826171875\nEpoch 44000, Loss: 10416.1826171875\nEpoch 44100, Loss: 10416.1826171875\nEpoch 44200, Loss: 10416.1826171875\nEpoch 44300, Loss: 10416.1826171875\nEpoch 44400, Loss: 10416.181640625\nEpoch 44500, Loss: 10416.181640625\nEpoch 44600, Loss: 10416.1826171875\nEpoch 44700, Loss: 10416.1826171875\nEpoch 44800, Loss: 10416.181640625\nEpoch 44900, Loss: 10416.181640625\nEpoch 45000, Loss: 10416.18359375\nEpoch 45100, Loss: 10416.181640625\nEpoch 45200, Loss: 10416.1826171875\nEpoch 45300, Loss: 10416.181640625\nEpoch 45400, Loss: 10416.181640625\nEpoch 45500, Loss: 10416.1826171875\nEpoch 45600, Loss: 10416.1826171875\nEpoch 45700, Loss: 10416.1826171875\nEpoch 45800, Loss: 10416.1826171875\nEpoch 45900, Loss: 10416.1826171875\nEpoch 46000, Loss: 10416.1826171875\nEpoch 46100, Loss: 10416.1826171875\nEpoch 46200, Loss: 10416.181640625\nEpoch 46300, Loss: 10416.1826171875\nEpoch 46400, Loss: 10416.1826171875\nEpoch 46500, Loss: 10416.1826171875\nEpoch 46600, Loss: 10416.1826171875\nEpoch 46700, Loss: 10416.1826171875\nEpoch 46800, Loss: 10416.1826171875\nEpoch 46900, Loss: 10416.181640625\nEpoch 47000, Loss: 10416.1826171875\nEpoch 47100, Loss: 10416.1826171875\nEpoch 47200, Loss: 10416.1826171875\nEpoch 47300, Loss: 10416.1826171875\nEpoch 47400, Loss: 10416.181640625\nEpoch 47500, Loss: 10416.1826171875\nEpoch 47600, Loss: 10416.1826171875\nEpoch 47700, Loss: 10416.1826171875\nEpoch 47800, Loss: 10416.1826171875\nEpoch 47900, Loss: 10416.1826171875\nEpoch 48000, Loss: 10416.181640625\nEpoch 48100, Loss: 10416.181640625\nEpoch 48200, Loss: 10416.1826171875\nEpoch 48300, Loss: 10416.181640625\nEpoch 48400, Loss: 10416.181640625\nEpoch 48500, Loss: 10416.181640625\nEpoch 48600, Loss: 10416.18359375\nEpoch 48700, Loss: 10416.1826171875\nEpoch 48800, Loss: 10416.181640625\nEpoch 48900, Loss: 10416.1826171875\nEpoch 49000, Loss: 10416.181640625\nEpoch 49100, Loss: 10416.1826171875\nEpoch 49200, Loss: 10416.1826171875\nEpoch 49300, Loss: 10416.1826171875\nEpoch 49400, Loss: 10416.1826171875\nEpoch 49500, Loss: 10416.1826171875\nEpoch 49600, Loss: 10416.1826171875\nEpoch 49700, Loss: 10416.1826171875\nEpoch 49800, Loss: 10416.181640625\nEpoch 49900, Loss: 10416.181640625\nEpoch 0, Loss: 63828.96875\nEpoch 100, Loss: 60933.44921875\nEpoch 200, Loss: 58226.15234375\nEpoch 300, Loss: 55692.67578125\nEpoch 400, Loss: 53321.5390625\nEpoch 500, Loss: 51101.0859375\nEpoch 600, Loss: 49022.53125\nEpoch 700, Loss: 47076.921875\nEpoch 800, Loss: 45256.5546875\nEpoch 900, Loss: 43555.359375\nEpoch 1000, Loss: 41965.6171875\nEpoch 1100, Loss: 40480.5234375\nEpoch 1200, Loss: 39093.671875\nEpoch 1300, Loss: 37798.953125\nEpoch 1400, Loss: 36591.09375\nEpoch 1500, Loss: 35464.26171875\nEpoch 1600, Loss: 34412.48828125\nEpoch 1700, Loss: 33430.515625\nEpoch 1800, Loss: 32513.38671875\nEpoch 1900, Loss: 31656.47265625\nEpoch 2000, Loss: 30855.447265625\nEpoch 2100, Loss: 30106.85546875\nEpoch 2200, Loss: 29406.92578125\nEpoch 2300, Loss: 28752.064453125\nEpoch 2400, Loss: 28140.06640625\nEpoch 2500, Loss: 27566.37890625\nEpoch 2600, Loss: 27028.109375\nEpoch 2700, Loss: 26522.62109375\nEpoch 2800, Loss: 26047.46484375\nEpoch 2900, Loss: 25600.390625\nEpoch 3000, Loss: 25179.283203125\nEpoch 3100, Loss: 24782.177734375\nEpoch 3200, Loss: 24407.21875\nEpoch 3300, Loss: 24052.97265625\nEpoch 3400, Loss: 23718.17578125\nEpoch 3500, Loss: 23401.578125\nEpoch 3600, Loss: 23101.0\nEpoch 3700, Loss: 22814.703125\nEpoch 3800, Loss: 22541.41015625\nEpoch 3900, Loss: 22279.9375\nEpoch 4000, Loss: 22029.568359375\nEpoch 4100, Loss: 21789.44921875\nEpoch 4200, Loss: 21558.142578125\nEpoch 4300, Loss: 21334.837890625\nEpoch 4400, Loss: 21119.15234375\nEpoch 4500, Loss: 20910.458984375\nEpoch 4600, Loss: 20707.791015625\nEpoch 4700, Loss: 20510.6640625\nEpoch 4800, Loss: 20318.640625\nEpoch 4900, Loss: 20131.3515625\nEpoch 5000, Loss: 19948.48046875\nEpoch 5100, Loss: 19769.73828125\nEpoch 5200, Loss: 19594.892578125\nEpoch 5300, Loss: 19423.7265625\nEpoch 5400, Loss: 19256.046875\nEpoch 5500, Loss: 19091.69140625\nEpoch 5600, Loss: 18930.5078125\nEpoch 5700, Loss: 18772.357421875\nEpoch 5800, Loss: 18617.2421875\nEpoch 5900, Loss: 18465.40234375\nEpoch 6000, Loss: 18316.25390625\nEpoch 6100, Loss: 18169.701171875\nEpoch 6200, Loss: 18025.671875\nEpoch 6300, Loss: 17884.076171875\nEpoch 6400, Loss: 17744.85546875\nEpoch 6500, Loss: 17607.943359375\nEpoch 6600, Loss: 17473.275390625\nEpoch 6700, Loss: 17340.802734375\nEpoch 6800, Loss: 17210.47265625\nEpoch 6900, Loss: 17082.23046875\nEpoch 7000, Loss: 16956.029296875\nEpoch 7100, Loss: 16832.537109375\nEpoch 7200, Loss: 16711.18359375\nEpoch 7300, Loss: 16591.6875\nEpoch 7400, Loss: 16474.142578125\nEpoch 7500, Loss: 16358.927734375\nEpoch 7600, Loss: 16245.4375\nEpoch 7700, Loss: 16133.6240234375\nEpoch 7800, Loss: 16023.451171875\nEpoch 7900, Loss: 15914.8994140625\nEpoch 8000, Loss: 15807.923828125\nEpoch 8100, Loss: 15702.498046875\nEpoch 8200, Loss: 15598.5947265625\nEpoch 8300, Loss: 15496.185546875\nEpoch 8400, Loss: 15395.251953125\nEpoch 8500, Loss: 15295.76171875\nEpoch 8600, Loss: 15197.693359375\nEpoch 8700, Loss: 15101.029296875\nEpoch 8800, Loss: 15005.7578125\nEpoch 8900, Loss: 14912.265625\nEpoch 9000, Loss: 14820.1826171875\nEpoch 9100, Loss: 14729.4716796875\nEpoch 9200, Loss: 14640.1123046875\nEpoch 9300, Loss: 14552.080078125\nEpoch 9400, Loss: 14465.353515625\nEpoch 9500, Loss: 14379.919921875\nEpoch 9600, Loss: 14295.7626953125\nEpoch 9700, Loss: 14212.873046875\nEpoch 9800, Loss: 14131.23828125\nEpoch 9900, Loss: 14050.8515625\nEpoch 10000, Loss: 13971.70703125\nEpoch 10100, Loss: 13893.7958984375\nEpoch 10200, Loss: 13817.1171875\nEpoch 10300, Loss: 13741.6630859375\nEpoch 10400, Loss: 13667.427734375\nEpoch 10500, Loss: 13594.408203125\nEpoch 10600, Loss: 13522.595703125\nEpoch 10700, Loss: 13451.9853515625\nEpoch 10800, Loss: 13382.568359375\nEpoch 10900, Loss: 13314.3369140625\nEpoch 11000, Loss: 13247.2822265625\nEpoch 11100, Loss: 13181.416015625\nEpoch 11200, Loss: 13116.751953125\nEpoch 11300, Loss: 13053.2392578125\nEpoch 11400, Loss: 12990.861328125\nEpoch 11500, Loss: 12929.59765625\nEpoch 11600, Loss: 12869.4296875\nEpoch 11700, Loss: 12810.3310546875\nEpoch 11800, Loss: 12752.2939453125\nEpoch 11900, Loss: 12695.2861328125\nEpoch 12000, Loss: 12639.552734375\nEpoch 12100, Loss: 12585.2470703125\nEpoch 12200, Loss: 12532.208984375\nEpoch 12300, Loss: 12480.267578125\nEpoch 12400, Loss: 12429.3017578125\nEpoch 12500, Loss: 12379.267578125\nEpoch 12600, Loss: 12330.1357421875\nEpoch 12700, Loss: 12281.890625\nEpoch 12800, Loss: 12234.509765625\nEpoch 12900, Loss: 12187.98828125\nEpoch 13000, Loss: 12142.3134765625\nEpoch 13100, Loss: 12097.4716796875\nEpoch 13200, Loss: 12053.455078125\nEpoch 13300, Loss: 12010.2470703125\nEpoch 13400, Loss: 11967.8408203125\nEpoch 13500, Loss: 11926.2578125\nEpoch 13600, Loss: 11885.642578125\nEpoch 13700, Loss: 11845.9267578125\nEpoch 13800, Loss: 11807.236328125\nEpoch 13900, Loss: 11769.4462890625\nEpoch 14000, Loss: 11732.509765625\nEpoch 14100, Loss: 11696.4072265625\nEpoch 14200, Loss: 11661.1240234375\nEpoch 14300, Loss: 11626.642578125\nEpoch 14400, Loss: 11593.044921875\nEpoch 14500, Loss: 11560.287109375\nEpoch 14600, Loss: 11528.34375\nEpoch 14700, Loss: 11497.16796875\nEpoch 14800, Loss: 11466.7265625\nEpoch 14900, Loss: 11437.0029296875\nEpoch 15000, Loss: 11407.9833984375\nEpoch 15100, Loss: 11379.658203125\nEpoch 15200, Loss: 11352.009765625\nEpoch 15300, Loss: 11325.0986328125\nEpoch 15400, Loss: 11298.884765625\nEpoch 15500, Loss: 11273.3251953125\nEpoch 15600, Loss: 11248.3974609375\nEpoch 15700, Loss: 11224.109375\nEpoch 15800, Loss: 11200.4609375\nEpoch 15900, Loss: 11177.4443359375\nEpoch 16000, Loss: 11155.044921875\nEpoch 16100, Loss: 11133.2509765625\nEpoch 16200, Loss: 11112.05078125\nEpoch 16300, Loss: 11091.4375\nEpoch 16400, Loss: 11071.396484375\nEpoch 16500, Loss: 11051.9208984375\nEpoch 16600, Loss: 11032.9951171875\nEpoch 16700, Loss: 11014.6748046875\nEpoch 16800, Loss: 10996.927734375\nEpoch 16900, Loss: 10979.7119140625\nEpoch 17000, Loss: 10963.001953125\nEpoch 17100, Loss: 10946.76953125\nEpoch 17200, Loss: 10931.0087890625\nEpoch 17300, Loss: 10915.7080078125\nEpoch 17400, Loss: 10900.857421875\nEpoch 17500, Loss: 10886.4609375\nEpoch 17600, Loss: 10872.6513671875\nEpoch 17700, Loss: 10859.3759765625\nEpoch 17800, Loss: 10846.564453125\nEpoch 17900, Loss: 10834.1982421875\nEpoch 18000, Loss: 10822.259765625\nEpoch 18100, Loss: 10810.7451171875\nEpoch 18200, Loss: 10799.64453125\nEpoch 18300, Loss: 10788.9521484375\nEpoch 18400, Loss: 10778.6630859375\nEpoch 18500, Loss: 10768.779296875\nEpoch 18600, Loss: 10759.3349609375\nEpoch 18700, Loss: 10750.310546875\nEpoch 18800, Loss: 10741.701171875\nEpoch 18900, Loss: 10733.4892578125\nEpoch 19000, Loss: 10725.662109375\nEpoch 19100, Loss: 10718.208984375\nEpoch 19200, Loss: 10711.125\nEpoch 19300, Loss: 10704.3974609375\nEpoch 19400, Loss: 10698.017578125\nEpoch 19500, Loss: 10691.970703125\nEpoch 19600, Loss: 10686.25\nEpoch 19700, Loss: 10680.8427734375\nEpoch 19800, Loss: 10675.7353515625\nEpoch 19900, Loss: 10670.9228515625\nEpoch 20000, Loss: 10666.3837890625\nEpoch 20100, Loss: 10662.11328125\nEpoch 20200, Loss: 10658.091796875\nEpoch 20300, Loss: 10654.314453125\nEpoch 20400, Loss: 10650.7734375\nEpoch 20500, Loss: 10647.443359375\nEpoch 20600, Loss: 10644.3173828125\nEpoch 20700, Loss: 10641.3779296875\nEpoch 20800, Loss: 10638.609375\nEpoch 20900, Loss: 10635.99609375\nEpoch 21000, Loss: 10633.5244140625\nEpoch 21100, Loss: 10631.1787109375\nEpoch 21200, Loss: 10628.943359375\nEpoch 21300, Loss: 10626.8076171875\nEpoch 21400, Loss: 10624.7529296875\nEpoch 21500, Loss: 10622.8193359375\nEpoch 21600, Loss: 10621.0771484375\nEpoch 21700, Loss: 10619.404296875\nEpoch 21800, Loss: 10617.78125\nEpoch 21900, Loss: 10616.197265625\nEpoch 22000, Loss: 10614.64453125\nEpoch 22100, Loss: 10613.115234375\nEpoch 22200, Loss: 10611.6083984375\nEpoch 22300, Loss: 10610.115234375\nEpoch 22400, Loss: 10608.630859375\nEpoch 22500, Loss: 10607.15625\nEpoch 22600, Loss: 10605.6845703125\nEpoch 22700, Loss: 10604.216796875\nEpoch 22800, Loss: 10602.7509765625\nEpoch 22900, Loss: 10601.287109375\nEpoch 23000, Loss: 10599.826171875\nEpoch 23100, Loss: 10598.3662109375\nEpoch 23200, Loss: 10596.9033203125\nEpoch 23300, Loss: 10595.4306640625\nEpoch 23400, Loss: 10593.9384765625\nEpoch 23500, Loss: 10592.421875\nEpoch 23600, Loss: 10590.8779296875\nEpoch 23700, Loss: 10589.3095703125\nEpoch 23800, Loss: 10587.7373046875\nEpoch 23900, Loss: 10586.1796875\nEpoch 24000, Loss: 10584.630859375\nEpoch 24100, Loss: 10583.09765625\nEpoch 24200, Loss: 10581.5810546875\nEpoch 24300, Loss: 10580.1240234375\nEpoch 24400, Loss: 10578.734375\nEpoch 24500, Loss: 10577.4111328125\nEpoch 24600, Loss: 10576.146484375\nEpoch 24700, Loss: 10574.9423828125\nEpoch 24800, Loss: 10573.7978515625\nEpoch 24900, Loss: 10572.705078125\nEpoch 25000, Loss: 10571.666015625\nEpoch 25100, Loss: 10570.677734375\nEpoch 25200, Loss: 10569.7392578125\nEpoch 25300, Loss: 10568.8466796875\nEpoch 25400, Loss: 10567.986328125\nEpoch 25500, Loss: 10567.13671875\nEpoch 25600, Loss: 10566.2841796875\nEpoch 25700, Loss: 10565.4189453125\nEpoch 25800, Loss: 10564.5419921875\nEpoch 25900, Loss: 10563.64453125\nEpoch 26000, Loss: 10562.728515625\nEpoch 26100, Loss: 10561.7939453125\nEpoch 26200, Loss: 10560.8369140625\nEpoch 26300, Loss: 10559.8876953125\nEpoch 26400, Loss: 10558.98046875\nEpoch 26500, Loss: 10558.1171875\nEpoch 26600, Loss: 10557.2958984375\nEpoch 26700, Loss: 10556.525390625\nEpoch 26800, Loss: 10555.8251953125\nEpoch 26900, Loss: 10555.1904296875\nEpoch 27000, Loss: 10554.615234375\nEpoch 27100, Loss: 10554.0927734375\nEpoch 27200, Loss: 10553.623046875\nEpoch 27300, Loss: 10553.205078125\nEpoch 27400, Loss: 10552.837890625\nEpoch 27500, Loss: 10552.515625\nEpoch 27600, Loss: 10552.2314453125\nEpoch 27700, Loss: 10551.9765625\nEpoch 27800, Loss: 10551.7490234375\nEpoch 27900, Loss: 10551.5439453125\nEpoch 28000, Loss: 10551.3603515625\nEpoch 28100, Loss: 10551.1962890625\nEpoch 28200, Loss: 10551.052734375\nEpoch 28300, Loss: 10550.9248046875\nEpoch 28400, Loss: 10550.8115234375\nEpoch 28500, Loss: 10550.7138671875\nEpoch 28600, Loss: 10550.62890625\nEpoch 28700, Loss: 10550.55859375\nEpoch 28800, Loss: 10550.4951171875\nEpoch 28900, Loss: 10550.44140625\nEpoch 29000, Loss: 10550.3935546875\nEpoch 29100, Loss: 10550.3515625\nEpoch 29200, Loss: 10550.3134765625\nEpoch 29300, Loss: 10550.2783203125\nEpoch 29400, Loss: 10550.2470703125\nEpoch 29500, Loss: 10550.220703125\nEpoch 29600, Loss: 10550.193359375\nEpoch 29700, Loss: 10550.171875\nEpoch 29800, Loss: 10550.15234375\nEpoch 29900, Loss: 10550.134765625\nEpoch 30000, Loss: 10550.119140625\nEpoch 30100, Loss: 10550.109375\nEpoch 30200, Loss: 10550.0966796875\nEpoch 30300, Loss: 10550.0869140625\nEpoch 30400, Loss: 10550.0810546875\nEpoch 30500, Loss: 10550.07421875\nEpoch 30600, Loss: 10550.068359375\nEpoch 30700, Loss: 10550.064453125\nEpoch 30800, Loss: 10550.0615234375\nEpoch 30900, Loss: 10550.05859375\nEpoch 31000, Loss: 10550.0546875\nEpoch 31100, Loss: 10550.0546875\nEpoch 31200, Loss: 10550.052734375\nEpoch 31300, Loss: 10550.05078125\nEpoch 31400, Loss: 10550.0498046875\nEpoch 31500, Loss: 10550.0498046875\nEpoch 31600, Loss: 10550.0498046875\nEpoch 31700, Loss: 10550.0478515625\nEpoch 31800, Loss: 10550.0478515625\nEpoch 31900, Loss: 10550.0478515625\nEpoch 32000, Loss: 10550.0478515625\nEpoch 32100, Loss: 10550.0458984375\nEpoch 32200, Loss: 10550.046875\nEpoch 32300, Loss: 10550.0458984375\nEpoch 32400, Loss: 10550.046875\nEpoch 32500, Loss: 10550.046875\nEpoch 32600, Loss: 10550.0458984375\nEpoch 32700, Loss: 10550.046875\nEpoch 32800, Loss: 10550.0458984375\nEpoch 32900, Loss: 10550.0478515625\nEpoch 33000, Loss: 10550.0458984375\nEpoch 33100, Loss: 10550.046875\nEpoch 33200, Loss: 10550.046875\nEpoch 33300, Loss: 10550.0458984375\nEpoch 33400, Loss: 10550.046875\nEpoch 33500, Loss: 10550.0478515625\nEpoch 33600, Loss: 10550.046875\nEpoch 33700, Loss: 10550.046875\nEpoch 33800, Loss: 10550.046875\nEpoch 33900, Loss: 10550.0458984375\nEpoch 34000, Loss: 10550.046875\nEpoch 34100, Loss: 10550.046875\nEpoch 34200, Loss: 10550.0478515625\nEpoch 34300, Loss: 10550.0458984375\nEpoch 34400, Loss: 10550.0478515625\nEpoch 34500, Loss: 10550.046875\nEpoch 34600, Loss: 10550.046875\nEpoch 34700, Loss: 10550.046875\nEpoch 34800, Loss: 10550.046875\nEpoch 34900, Loss: 10550.046875\nEpoch 35000, Loss: 10550.046875\nEpoch 35100, Loss: 10550.046875\nEpoch 35200, Loss: 10550.046875\nEpoch 35300, Loss: 10550.0478515625\nEpoch 35400, Loss: 10550.046875\nEpoch 35500, Loss: 10550.0458984375\nEpoch 35600, Loss: 10550.046875\nEpoch 35700, Loss: 10550.0478515625\nEpoch 35800, Loss: 10550.046875\nEpoch 35900, Loss: 10550.0478515625\nEpoch 36000, Loss: 10550.0458984375\nEpoch 36100, Loss: 10550.0458984375\nEpoch 36200, Loss: 10550.046875\nEpoch 36300, Loss: 10550.046875\nEpoch 36400, Loss: 10550.0478515625\nEpoch 36500, Loss: 10550.046875\nEpoch 36600, Loss: 10550.046875\nEpoch 36700, Loss: 10550.046875\nEpoch 36800, Loss: 10550.0478515625\nEpoch 36900, Loss: 10550.0478515625\nEpoch 37000, Loss: 10550.0478515625\nEpoch 37100, Loss: 10550.046875\nEpoch 37200, Loss: 10550.046875\nEpoch 37300, Loss: 10550.0478515625\nEpoch 37400, Loss: 10550.046875\nEpoch 37500, Loss: 10550.046875\nEpoch 37600, Loss: 10550.0478515625\nEpoch 37700, Loss: 10550.0478515625\nEpoch 37800, Loss: 10550.0478515625\nEpoch 37900, Loss: 10550.046875\nEpoch 38000, Loss: 10550.0478515625\nEpoch 38100, Loss: 10550.046875\nEpoch 38200, Loss: 10550.0478515625\nEpoch 38300, Loss: 10550.0458984375\nEpoch 38400, Loss: 10550.046875\nEpoch 38500, Loss: 10550.0478515625\nEpoch 38600, Loss: 10550.0478515625\nEpoch 38700, Loss: 10550.046875\nEpoch 38800, Loss: 10550.046875\nEpoch 38900, Loss: 10550.046875\nEpoch 39000, Loss: 10550.046875\nEpoch 39100, Loss: 10550.046875\nEpoch 39200, Loss: 10550.046875\nEpoch 39300, Loss: 10550.046875\nEpoch 39400, Loss: 10550.046875\nEpoch 39500, Loss: 10550.0478515625\nEpoch 39600, Loss: 10550.046875\nEpoch 39700, Loss: 10550.046875\nEpoch 39800, Loss: 10550.046875\nEpoch 39900, Loss: 10550.0478515625\nEpoch 40000, Loss: 10550.046875\nEpoch 40100, Loss: 10550.0478515625\nEpoch 40200, Loss: 10550.046875\nEpoch 40300, Loss: 10550.046875\nEpoch 40400, Loss: 10550.046875\nEpoch 40500, Loss: 10550.046875\nEpoch 40600, Loss: 10550.046875\nEpoch 40700, Loss: 10550.0478515625\nEpoch 40800, Loss: 10550.046875\nEpoch 40900, Loss: 10550.046875\nEpoch 41000, Loss: 10550.046875\nEpoch 41100, Loss: 10550.0458984375\nEpoch 41200, Loss: 10550.0478515625\nEpoch 41300, Loss: 10550.046875\nEpoch 41400, Loss: 10550.046875\nEpoch 41500, Loss: 10550.0478515625\nEpoch 41600, Loss: 10550.046875\nEpoch 41700, Loss: 10550.046875\nEpoch 41800, Loss: 10550.046875\nEpoch 41900, Loss: 10550.0478515625\nEpoch 42000, Loss: 10550.046875\nEpoch 42100, Loss: 10550.0478515625\nEpoch 42200, Loss: 10550.046875\nEpoch 42300, Loss: 10550.046875\nEpoch 42400, Loss: 10550.0478515625\nEpoch 42500, Loss: 10550.046875\nEpoch 42600, Loss: 10550.0478515625\nEpoch 42700, Loss: 10550.046875\nEpoch 42800, Loss: 10550.0478515625\nEpoch 42900, Loss: 10550.0458984375\nEpoch 43000, Loss: 10550.0478515625\nEpoch 43100, Loss: 10550.0478515625\nEpoch 43200, Loss: 10550.0458984375\nEpoch 43300, Loss: 10550.046875\nEpoch 43400, Loss: 10550.046875\nEpoch 43500, Loss: 10550.0478515625\nEpoch 43600, Loss: 10550.046875\nEpoch 43700, Loss: 10550.0478515625\nEpoch 43800, Loss: 10550.0458984375\nEpoch 43900, Loss: 10550.046875\nEpoch 44000, Loss: 10550.046875\nEpoch 44100, Loss: 10550.046875\nEpoch 44200, Loss: 10550.0458984375\nEpoch 44300, Loss: 10550.046875\nEpoch 44400, Loss: 10550.046875\nEpoch 44500, Loss: 10550.046875\nEpoch 44600, Loss: 10550.0478515625\nEpoch 44700, Loss: 10550.046875\nEpoch 44800, Loss: 10550.046875\nEpoch 44900, Loss: 10550.046875\nEpoch 45000, Loss: 10550.046875\nEpoch 45100, Loss: 10550.0478515625\nEpoch 45200, Loss: 10550.0478515625\nEpoch 45300, Loss: 10550.046875\nEpoch 45400, Loss: 10550.0478515625\nEpoch 45500, Loss: 10550.0478515625\nEpoch 45600, Loss: 10550.046875\nEpoch 45700, Loss: 10550.0478515625\nEpoch 45800, Loss: 10550.0478515625\nEpoch 45900, Loss: 10550.046875\nEpoch 46000, Loss: 10550.046875\nEpoch 46100, Loss: 10550.046875\nEpoch 46200, Loss: 10550.046875\nEpoch 46300, Loss: 10550.046875\nEpoch 46400, Loss: 10550.046875\nEpoch 46500, Loss: 10550.046875\nEpoch 46600, Loss: 10550.046875\nEpoch 46700, Loss: 10550.0458984375\nEpoch 46800, Loss: 10550.046875\nEpoch 46900, Loss: 10550.0478515625\nEpoch 47000, Loss: 10550.046875\nEpoch 47100, Loss: 10550.046875\nEpoch 47200, Loss: 10550.046875\nEpoch 47300, Loss: 10550.046875\nEpoch 47400, Loss: 10550.0478515625\nEpoch 47500, Loss: 10550.0478515625\nEpoch 47600, Loss: 10550.046875\nEpoch 47700, Loss: 10550.046875\nEpoch 47800, Loss: 10550.046875\nEpoch 47900, Loss: 10550.0478515625\nEpoch 48000, Loss: 10550.0478515625\nEpoch 48100, Loss: 10550.046875\nEpoch 48200, Loss: 10550.046875\nEpoch 48300, Loss: 10550.0458984375\nEpoch 48400, Loss: 10550.0458984375\nEpoch 48500, Loss: 10550.046875\nEpoch 48600, Loss: 10550.0478515625\nEpoch 48700, Loss: 10550.046875\nEpoch 48800, Loss: 10550.046875\nEpoch 48900, Loss: 10550.048828125\nEpoch 49000, Loss: 10550.046875\nEpoch 49100, Loss: 10550.0478515625\nEpoch 49200, Loss: 10550.046875\nEpoch 49300, Loss: 10550.046875\nEpoch 49400, Loss: 10550.046875\nEpoch 49500, Loss: 10550.046875\nEpoch 49600, Loss: 10550.046875\nEpoch 49700, Loss: 10550.046875\nEpoch 49800, Loss: 10550.046875\nEpoch 49900, Loss: 10550.046875\nEpoch 0, Loss: 63828.96875\nEpoch 100, Loss: 60936.609375\nEpoch 200, Loss: 58232.41015625\nEpoch 300, Loss: 55702.01171875\nEpoch 400, Loss: 53333.890625\nEpoch 500, Loss: 51116.4453125\nEpoch 600, Loss: 49040.71875\nEpoch 700, Loss: 47097.6875\nEpoch 800, Loss: 45279.8203125\nEpoch 900, Loss: 43581.19921875\nEpoch 1000, Loss: 41993.8203125\nEpoch 1100, Loss: 40510.91015625\nEpoch 1200, Loss: 39126.0859375\nEpoch 1300, Loss: 37833.2109375\nEpoch 1400, Loss: 36627.45703125\nEpoch 1500, Loss: 35502.4609375\nEpoch 1600, Loss: 34452.38671875\nEpoch 1700, Loss: 33471.9609375\nEpoch 1800, Loss: 32556.248046875\nEpoch 1900, Loss: 31700.62890625\nEpoch 2000, Loss: 30900.810546875\nEpoch 2100, Loss: 30153.763671875\nEpoch 2200, Loss: 29455.162109375\nEpoch 2300, Loss: 28802.060546875\nEpoch 2400, Loss: 28191.755859375\nEpoch 2500, Loss: 27619.677734375\nEpoch 2600, Loss: 27082.9375\nEpoch 2700, Loss: 26578.8984375\nEpoch 2800, Loss: 26105.123046875\nEpoch 2900, Loss: 25659.35546875\nEpoch 3000, Loss: 25239.49609375\nEpoch 3100, Loss: 24843.578125\nEpoch 3200, Loss: 24469.76171875\nEpoch 3300, Loss: 24116.689453125\nEpoch 3400, Loss: 23783.32421875\nEpoch 3500, Loss: 23468.41796875\nEpoch 3600, Loss: 23169.380859375\nEpoch 3700, Loss: 22884.572265625\nEpoch 3800, Loss: 22612.7109375\nEpoch 3900, Loss: 22352.619140625\nEpoch 4000, Loss: 22103.9453125\nEpoch 4100, Loss: 21865.318359375\nEpoch 4200, Loss: 21635.458984375\nEpoch 4300, Loss: 21413.5546875\nEpoch 4400, Loss: 21199.48046875\nEpoch 4500, Loss: 20992.27734375\nEpoch 4600, Loss: 20791.052734375\nEpoch 4700, Loss: 20595.3203125\nEpoch 4800, Loss: 20404.65625\nEpoch 4900, Loss: 20218.689453125\nEpoch 5000, Loss: 20037.103515625\nEpoch 5100, Loss: 19859.61328125\nEpoch 5200, Loss: 19685.982421875\nEpoch 5300, Loss: 19516.0\nEpoch 5400, Loss: 19349.4765625\nEpoch 5500, Loss: 19186.244140625\nEpoch 5600, Loss: 19026.15625\nEpoch 5700, Loss: 18869.080078125\nEpoch 5800, Loss: 18715.4296875\nEpoch 5900, Loss: 18564.720703125\nEpoch 6000, Loss: 18416.673828125\nEpoch 6100, Loss: 18271.19921875\nEpoch 6200, Loss: 18128.2109375\nEpoch 6300, Loss: 17987.638671875\nEpoch 6400, Loss: 17849.408203125\nEpoch 6500, Loss: 17713.458984375\nEpoch 6600, Loss: 17579.73046875\nEpoch 6700, Loss: 17448.166015625\nEpoch 6800, Loss: 17318.71484375\nEpoch 6900, Loss: 17191.328125\nEpoch 7000, Loss: 17066.248046875\nEpoch 7100, Loss: 16943.986328125\nEpoch 7200, Loss: 16823.6015625\nEpoch 7300, Loss: 16705.041015625\nEpoch 7400, Loss: 16588.3125\nEpoch 7500, Loss: 16474.11328125\nEpoch 7600, Loss: 16361.607421875\nEpoch 7700, Loss: 16250.744140625\nEpoch 7800, Loss: 16141.484375\nEpoch 7900, Loss: 16033.8046875\nEpoch 8000, Loss: 15927.669921875\nEpoch 8100, Loss: 15823.06640625\nEpoch 8200, Loss: 15719.953125\nEpoch 8300, Loss: 15618.2978515625\nEpoch 8400, Loss: 15518.08203125\nEpoch 8500, Loss: 15419.287109375\nEpoch 8600, Loss: 15321.8857421875\nEpoch 8700, Loss: 15225.998046875\nEpoch 8800, Loss: 15131.7919921875\nEpoch 8900, Loss: 15039.0\nEpoch 9000, Loss: 14947.59375\nEpoch 9100, Loss: 14857.54296875\nEpoch 9200, Loss: 14768.83203125\nEpoch 9300, Loss: 14681.4296875\nEpoch 9400, Loss: 14595.3154296875\nEpoch 9500, Loss: 14510.4775390625\nEpoch 9600, Loss: 14426.8994140625\nEpoch 9700, Loss: 14344.568359375\nEpoch 9800, Loss: 14263.47265625\nEpoch 9900, Loss: 14183.6044921875\nEpoch 10000, Loss: 14104.9609375\nEpoch 10100, Loss: 14027.5341796875\nEpoch 10200, Loss: 13951.3193359375\nEpoch 10300, Loss: 13876.3076171875\nEpoch 10400, Loss: 13802.4990234375\nEpoch 10500, Loss: 13729.8837890625\nEpoch 10600, Loss: 13658.458984375\nEpoch 10700, Loss: 13588.2099609375\nEpoch 10800, Loss: 13519.1328125\nEpoch 10900, Loss: 13451.22265625\nEpoch 11000, Loss: 13384.5087890625\nEpoch 11100, Loss: 13318.984375\nEpoch 11200, Loss: 13254.607421875\nEpoch 11300, Loss: 13191.3603515625\nEpoch 11400, Loss: 13129.2177734375\nEpoch 11500, Loss: 13068.166015625\nEpoch 11600, Loss: 13008.18359375\nEpoch 11700, Loss: 12949.2646484375\nEpoch 11800, Loss: 12891.6494140625\nEpoch 11900, Loss: 12835.4521484375\nEpoch 12000, Loss: 12780.7958984375\nEpoch 12100, Loss: 12727.244140625\nEpoch 12200, Loss: 12674.71875\nEpoch 12300, Loss: 12623.166015625\nEpoch 12400, Loss: 12572.548828125\nEpoch 12500, Loss: 12522.8408203125\nEpoch 12600, Loss: 12474.01953125\nEpoch 12700, Loss: 12426.064453125\nEpoch 12800, Loss: 12378.9619140625\nEpoch 12900, Loss: 12332.69921875\nEpoch 13000, Loss: 12287.2626953125\nEpoch 13100, Loss: 12242.759765625\nEpoch 13200, Loss: 12199.2685546875\nEpoch 13300, Loss: 12156.7041015625\nEpoch 13400, Loss: 12115.0185546875\nEpoch 13500, Loss: 12074.1416015625\nEpoch 13600, Loss: 12034.09375\nEpoch 13700, Loss: 11995.0322265625\nEpoch 13800, Loss: 11956.8349609375\nEpoch 13900, Loss: 11919.486328125\nEpoch 14000, Loss: 11882.970703125\nEpoch 14100, Loss: 11847.2666015625\nEpoch 14200, Loss: 11812.37890625\nEpoch 14300, Loss: 11778.3935546875\nEpoch 14400, Loss: 11745.244140625\nEpoch 14500, Loss: 11712.9111328125\nEpoch 14600, Loss: 11681.376953125\nEpoch 14700, Loss: 11650.6220703125\nEpoch 14800, Loss: 11620.6796875\nEpoch 14900, Loss: 11591.5625\nEpoch 15000, Loss: 11563.193359375\nEpoch 15100, Loss: 11535.541015625\nEpoch 15200, Loss: 11508.578125\nEpoch 15300, Loss: 11482.2841796875\nEpoch 15400, Loss: 11456.6357421875\nEpoch 15500, Loss: 11431.6044921875\nEpoch 15600, Loss: 11407.201171875\nEpoch 15700, Loss: 11383.4169921875\nEpoch 15800, Loss: 11360.24609375\nEpoch 15900, Loss: 11337.673828125\nEpoch 16000, Loss: 11315.6904296875\nEpoch 16100, Loss: 11294.291015625\nEpoch 16200, Loss: 11273.46484375\nEpoch 16300, Loss: 11253.1982421875\nEpoch 16400, Loss: 11233.490234375\nEpoch 16500, Loss: 11214.4296875\nEpoch 16600, Loss: 11195.9404296875\nEpoch 16700, Loss: 11177.9951171875\nEpoch 16800, Loss: 11160.5791015625\nEpoch 16900, Loss: 11143.681640625\nEpoch 17000, Loss: 11127.3837890625\nEpoch 17100, Loss: 11111.7841796875\nEpoch 17200, Loss: 11096.7373046875\nEpoch 17300, Loss: 11082.1728515625\nEpoch 17400, Loss: 11068.0712890625\nEpoch 17500, Loss: 11054.4130859375\nEpoch 17600, Loss: 11041.1845703125\nEpoch 17700, Loss: 11028.3798828125\nEpoch 17800, Loss: 11015.9921875\nEpoch 17900, Loss: 11004.025390625\nEpoch 18000, Loss: 10992.470703125\nEpoch 18100, Loss: 10981.3203125\nEpoch 18200, Loss: 10970.615234375\nEpoch 18300, Loss: 10960.333984375\nEpoch 18400, Loss: 10950.451171875\nEpoch 18500, Loss: 10940.962890625\nEpoch 18600, Loss: 10931.859375\nEpoch 18700, Loss: 10923.138671875\nEpoch 18800, Loss: 10914.8046875\nEpoch 18900, Loss: 10906.845703125\nEpoch 19000, Loss: 10899.255859375\nEpoch 19100, Loss: 10892.0244140625\nEpoch 19200, Loss: 10885.1376953125\nEpoch 19300, Loss: 10878.5927734375\nEpoch 19400, Loss: 10872.3740234375\nEpoch 19500, Loss: 10866.478515625\nEpoch 19600, Loss: 10860.8935546875\nEpoch 19700, Loss: 10855.607421875\nEpoch 19800, Loss: 10850.611328125\nEpoch 19900, Loss: 10845.8896484375\nEpoch 20000, Loss: 10841.431640625\nEpoch 20100, Loss: 10837.21875\nEpoch 20200, Loss: 10833.23828125\nEpoch 20300, Loss: 10829.4736328125\nEpoch 20400, Loss: 10825.9140625\nEpoch 20500, Loss: 10822.537109375\nEpoch 20600, Loss: 10819.33203125\nEpoch 20700, Loss: 10816.283203125\nEpoch 20800, Loss: 10813.3740234375\nEpoch 20900, Loss: 10810.5869140625\nEpoch 21000, Loss: 10808.0712890625\nEpoch 21100, Loss: 10805.728515625\nEpoch 21200, Loss: 10803.478515625\nEpoch 21300, Loss: 10801.306640625\nEpoch 21400, Loss: 10799.201171875\nEpoch 21500, Loss: 10797.150390625\nEpoch 21600, Loss: 10795.142578125\nEpoch 21700, Loss: 10793.171875\nEpoch 21800, Loss: 10791.2255859375\nEpoch 21900, Loss: 10789.3046875\nEpoch 22000, Loss: 10787.3984375\nEpoch 22100, Loss: 10785.501953125\nEpoch 22200, Loss: 10783.615234375\nEpoch 22300, Loss: 10781.734375\nEpoch 22400, Loss: 10779.8525390625\nEpoch 22500, Loss: 10777.9755859375\nEpoch 22600, Loss: 10776.08984375\nEpoch 22700, Loss: 10774.185546875\nEpoch 22800, Loss: 10772.26171875\nEpoch 22900, Loss: 10770.3330078125\nEpoch 23000, Loss: 10768.4111328125\nEpoch 23100, Loss: 10766.48828125\nEpoch 23200, Loss: 10764.5693359375\nEpoch 23300, Loss: 10762.6572265625\nEpoch 23400, Loss: 10760.7685546875\nEpoch 23500, Loss: 10758.9345703125\nEpoch 23600, Loss: 10757.1572265625\nEpoch 23700, Loss: 10755.4375\nEpoch 23800, Loss: 10753.7734375\nEpoch 23900, Loss: 10752.1650390625\nEpoch 24000, Loss: 10750.6083984375\nEpoch 24100, Loss: 10749.107421875\nEpoch 24200, Loss: 10747.66015625\nEpoch 24300, Loss: 10746.26171875\nEpoch 24400, Loss: 10744.91796875\nEpoch 24500, Loss: 10743.623046875\nEpoch 24600, Loss: 10742.37890625\nEpoch 24700, Loss: 10741.1826171875\nEpoch 24800, Loss: 10740.02734375\nEpoch 24900, Loss: 10738.8974609375\nEpoch 25000, Loss: 10737.7783203125\nEpoch 25100, Loss: 10736.6591796875\nEpoch 25200, Loss: 10735.53125\nEpoch 25300, Loss: 10734.3876953125\nEpoch 25400, Loss: 10733.2255859375\nEpoch 25500, Loss: 10732.0537109375\nEpoch 25600, Loss: 10730.9072265625\nEpoch 25700, Loss: 10729.796875\nEpoch 25800, Loss: 10728.7197265625\nEpoch 25900, Loss: 10727.6806640625\nEpoch 26000, Loss: 10726.703125\nEpoch 26100, Loss: 10725.7919921875\nEpoch 26200, Loss: 10724.9404296875\nEpoch 26300, Loss: 10724.1494140625\nEpoch 26400, Loss: 10723.4130859375\nEpoch 26500, Loss: 10722.7314453125\nEpoch 26600, Loss: 10722.1064453125\nEpoch 26700, Loss: 10721.5439453125\nEpoch 26800, Loss: 10721.029296875\nEpoch 26900, Loss: 10720.5673828125\nEpoch 27000, Loss: 10720.1494140625\nEpoch 27100, Loss: 10719.7724609375\nEpoch 27200, Loss: 10719.4326171875\nEpoch 27300, Loss: 10719.1279296875\nEpoch 27400, Loss: 10718.8525390625\nEpoch 27500, Loss: 10718.6025390625\nEpoch 27600, Loss: 10718.3740234375\nEpoch 27700, Loss: 10718.1640625\nEpoch 27800, Loss: 10717.9765625\nEpoch 27900, Loss: 10717.8056640625\nEpoch 28000, Loss: 10717.6533203125\nEpoch 28100, Loss: 10717.517578125\nEpoch 28200, Loss: 10717.3935546875\nEpoch 28300, Loss: 10717.2890625\nEpoch 28400, Loss: 10717.193359375\nEpoch 28500, Loss: 10717.109375\nEpoch 28600, Loss: 10717.0400390625\nEpoch 28700, Loss: 10716.9775390625\nEpoch 28800, Loss: 10716.9208984375\nEpoch 28900, Loss: 10716.873046875\nEpoch 29000, Loss: 10716.8271484375\nEpoch 29100, Loss: 10716.78515625\nEpoch 29200, Loss: 10716.75\nEpoch 29300, Loss: 10716.71484375\nEpoch 29400, Loss: 10716.6845703125\nEpoch 29500, Loss: 10716.6572265625\nEpoch 29600, Loss: 10716.6337890625\nEpoch 29700, Loss: 10716.6123046875\nEpoch 29800, Loss: 10716.5947265625\nEpoch 29900, Loss: 10716.578125\nEpoch 30000, Loss: 10716.5654296875\nEpoch 30100, Loss: 10716.5537109375\nEpoch 30200, Loss: 10716.5419921875\nEpoch 30300, Loss: 10716.53515625\nEpoch 30400, Loss: 10716.5283203125\nEpoch 30500, Loss: 10716.5234375\nEpoch 30600, Loss: 10716.5185546875\nEpoch 30700, Loss: 10716.515625\nEpoch 30800, Loss: 10716.5126953125\nEpoch 30900, Loss: 10716.51171875\nEpoch 31000, Loss: 10716.5078125\nEpoch 31100, Loss: 10716.5078125\nEpoch 31200, Loss: 10716.5068359375\nEpoch 31300, Loss: 10716.5048828125\nEpoch 31400, Loss: 10716.5048828125\nEpoch 31500, Loss: 10716.5048828125\nEpoch 31600, Loss: 10716.5048828125\nEpoch 31700, Loss: 10716.505859375\nEpoch 31800, Loss: 10716.5048828125\nEpoch 31900, Loss: 10716.5068359375\nEpoch 32000, Loss: 10716.505859375\nEpoch 32100, Loss: 10716.505859375\nEpoch 32200, Loss: 10716.5048828125\nEpoch 32300, Loss: 10716.5048828125\nEpoch 32400, Loss: 10716.5048828125\nEpoch 32500, Loss: 10716.5048828125\nEpoch 32600, Loss: 10716.50390625\nEpoch 32700, Loss: 10716.505859375\nEpoch 32800, Loss: 10716.5048828125\nEpoch 32900, Loss: 10716.5048828125\nEpoch 33000, Loss: 10716.50390625\nEpoch 33100, Loss: 10716.5048828125\nEpoch 33200, Loss: 10716.505859375\nEpoch 33300, Loss: 10716.5048828125\nEpoch 33400, Loss: 10716.5048828125\nEpoch 33500, Loss: 10716.50390625\nEpoch 33600, Loss: 10716.5048828125\nEpoch 33700, Loss: 10716.5048828125\nEpoch 33800, Loss: 10716.50390625\nEpoch 33900, Loss: 10716.5048828125\nEpoch 34000, Loss: 10716.5048828125\nEpoch 34100, Loss: 10716.5048828125\nEpoch 34200, Loss: 10716.5048828125\nEpoch 34300, Loss: 10716.50390625\nEpoch 34400, Loss: 10716.505859375\nEpoch 34500, Loss: 10716.505859375\nEpoch 34600, Loss: 10716.5048828125\nEpoch 34700, Loss: 10716.5048828125\nEpoch 34800, Loss: 10716.50390625\nEpoch 34900, Loss: 10716.5048828125\nEpoch 35000, Loss: 10716.50390625\nEpoch 35100, Loss: 10716.505859375\nEpoch 35200, Loss: 10716.50390625\nEpoch 35300, Loss: 10716.5048828125\nEpoch 35400, Loss: 10716.50390625\nEpoch 35500, Loss: 10716.5048828125\nEpoch 35600, Loss: 10716.5048828125\nEpoch 35700, Loss: 10716.505859375\nEpoch 35800, Loss: 10716.5048828125\nEpoch 35900, Loss: 10716.5048828125\nEpoch 36000, Loss: 10716.5048828125\nEpoch 36100, Loss: 10716.50390625\nEpoch 36200, Loss: 10716.5048828125\nEpoch 36300, Loss: 10716.5048828125\nEpoch 36400, Loss: 10716.5048828125\nEpoch 36500, Loss: 10716.505859375\nEpoch 36600, Loss: 10716.50390625\nEpoch 36700, Loss: 10716.5048828125\nEpoch 36800, Loss: 10716.5048828125\nEpoch 36900, Loss: 10716.50390625\nEpoch 37000, Loss: 10716.5048828125\nEpoch 37100, Loss: 10716.5048828125\nEpoch 37200, Loss: 10716.5048828125\nEpoch 37300, Loss: 10716.5029296875\nEpoch 37400, Loss: 10716.5048828125\nEpoch 37500, Loss: 10716.5048828125\nEpoch 37600, Loss: 10716.5048828125\nEpoch 37700, Loss: 10716.5048828125\nEpoch 37800, Loss: 10716.50390625\nEpoch 37900, Loss: 10716.5048828125\nEpoch 38000, Loss: 10716.5048828125\nEpoch 38100, Loss: 10716.5048828125\nEpoch 38200, Loss: 10716.5048828125\nEpoch 38300, Loss: 10716.5048828125\nEpoch 38400, Loss: 10716.505859375\nEpoch 38500, Loss: 10716.5048828125\nEpoch 38600, Loss: 10716.505859375\nEpoch 38700, Loss: 10716.505859375\nEpoch 38800, Loss: 10716.505859375\nEpoch 38900, Loss: 10716.50390625\nEpoch 39000, Loss: 10716.50390625\nEpoch 39100, Loss: 10716.5048828125\nEpoch 39200, Loss: 10716.50390625\nEpoch 39300, Loss: 10716.5048828125\nEpoch 39400, Loss: 10716.5048828125\nEpoch 39500, Loss: 10716.5048828125\nEpoch 39600, Loss: 10716.50390625\nEpoch 39700, Loss: 10716.5048828125\nEpoch 39800, Loss: 10716.5048828125\nEpoch 39900, Loss: 10716.50390625\nEpoch 40000, Loss: 10716.5048828125\nEpoch 40100, Loss: 10716.5048828125\nEpoch 40200, Loss: 10716.50390625\nEpoch 40300, Loss: 10716.505859375\nEpoch 40400, Loss: 10716.50390625\nEpoch 40500, Loss: 10716.50390625\nEpoch 40600, Loss: 10716.5048828125\nEpoch 40700, Loss: 10716.5048828125\nEpoch 40800, Loss: 10716.50390625\nEpoch 40900, Loss: 10716.505859375\nEpoch 41000, Loss: 10716.50390625\nEpoch 41100, Loss: 10716.5048828125\nEpoch 41200, Loss: 10716.505859375\nEpoch 41300, Loss: 10716.505859375\nEpoch 41400, Loss: 10716.5048828125\nEpoch 41500, Loss: 10716.505859375\nEpoch 41600, Loss: 10716.50390625\nEpoch 41700, Loss: 10716.5048828125\nEpoch 41800, Loss: 10716.50390625\nEpoch 41900, Loss: 10716.505859375\nEpoch 42000, Loss: 10716.5048828125\nEpoch 42100, Loss: 10716.5048828125\nEpoch 42200, Loss: 10716.505859375\nEpoch 42300, Loss: 10716.5048828125\nEpoch 42400, Loss: 10716.5048828125\nEpoch 42500, Loss: 10716.5048828125\nEpoch 42600, Loss: 10716.5048828125\nEpoch 42700, Loss: 10716.5048828125\nEpoch 42800, Loss: 10716.50390625\nEpoch 42900, Loss: 10716.505859375\nEpoch 43000, Loss: 10716.5048828125\nEpoch 43100, Loss: 10716.5048828125\nEpoch 43200, Loss: 10716.5048828125\nEpoch 43300, Loss: 10716.50390625\nEpoch 43400, Loss: 10716.505859375\nEpoch 43500, Loss: 10716.50390625\nEpoch 43600, Loss: 10716.50390625\nEpoch 43700, Loss: 10716.5048828125\nEpoch 43800, Loss: 10716.5048828125\nEpoch 43900, Loss: 10716.50390625\nEpoch 44000, Loss: 10716.5048828125\nEpoch 44100, Loss: 10716.5048828125\nEpoch 44200, Loss: 10716.5048828125\nEpoch 44300, Loss: 10716.5048828125\nEpoch 44400, Loss: 10716.5048828125\nEpoch 44500, Loss: 10716.5048828125\nEpoch 44600, Loss: 10716.5048828125\nEpoch 44700, Loss: 10716.5048828125\nEpoch 44800, Loss: 10716.5048828125\nEpoch 44900, Loss: 10716.5048828125\nEpoch 45000, Loss: 10716.50390625\nEpoch 45100, Loss: 10716.505859375\nEpoch 45200, Loss: 10716.50390625\nEpoch 45300, Loss: 10716.50390625\nEpoch 45400, Loss: 10716.50390625\nEpoch 45500, Loss: 10716.5048828125\nEpoch 45600, Loss: 10716.5048828125\nEpoch 45700, Loss: 10716.5048828125\nEpoch 45800, Loss: 10716.5048828125\nEpoch 45900, Loss: 10716.50390625\nEpoch 46000, Loss: 10716.505859375\nEpoch 46100, Loss: 10716.505859375\nEpoch 46200, Loss: 10716.50390625\nEpoch 46300, Loss: 10716.5048828125\nEpoch 46400, Loss: 10716.5048828125\nEpoch 46500, Loss: 10716.505859375\nEpoch 46600, Loss: 10716.5048828125\nEpoch 46700, Loss: 10716.505859375\nEpoch 46800, Loss: 10716.50390625\nEpoch 46900, Loss: 10716.5048828125\nEpoch 47000, Loss: 10716.5048828125\nEpoch 47100, Loss: 10716.5048828125\nEpoch 47200, Loss: 10716.50390625\nEpoch 47300, Loss: 10716.5048828125\nEpoch 47400, Loss: 10716.5048828125\nEpoch 47500, Loss: 10716.5048828125\nEpoch 47600, Loss: 10716.5048828125\nEpoch 47700, Loss: 10716.50390625\nEpoch 47800, Loss: 10716.5048828125\nEpoch 47900, Loss: 10716.5048828125\nEpoch 48000, Loss: 10716.50390625\nEpoch 48100, Loss: 10716.5048828125\nEpoch 48200, Loss: 10716.505859375\nEpoch 48300, Loss: 10716.505859375\nEpoch 48400, Loss: 10716.505859375\nEpoch 48500, Loss: 10716.5048828125\nEpoch 48600, Loss: 10716.5048828125\nEpoch 48700, Loss: 10716.505859375\nEpoch 48800, Loss: 10716.5048828125\nEpoch 48900, Loss: 10716.50390625\nEpoch 49000, Loss: 10716.50390625\nEpoch 49100, Loss: 10716.5048828125\nEpoch 49200, Loss: 10716.5048828125\nEpoch 49300, Loss: 10716.5048828125\nEpoch 49400, Loss: 10716.5048828125\nEpoch 49500, Loss: 10716.5048828125\nEpoch 49600, Loss: 10716.505859375\nEpoch 49700, Loss: 10716.50390625\nEpoch 49800, Loss: 10716.50390625\nEpoch 49900, Loss: 10716.5048828125\nEpoch 0, Loss: 63828.96875\nEpoch 100, Loss: 60940.59765625\nEpoch 200, Loss: 58240.16796875\nEpoch 300, Loss: 55713.9921875\nEpoch 400, Loss: 53349.96484375\nEpoch 500, Loss: 51136.6640625\nEpoch 600, Loss: 49064.69921875\nEpoch 700, Loss: 47125.12109375\nEpoch 800, Loss: 45310.76171875\nEpoch 900, Loss: 43615.49609375\nEpoch 1000, Loss: 42031.2578125\nEpoch 1100, Loss: 40551.2734375\nEpoch 1200, Loss: 39169.13671875\nEpoch 1300, Loss: 37878.7734375\nEpoch 1400, Loss: 36675.87109375\nEpoch 1500, Loss: 35553.30859375\nEpoch 1600, Loss: 34505.46875\nEpoch 1700, Loss: 33527.08984375\nEpoch 1800, Loss: 32613.2421875\nEpoch 1900, Loss: 31759.341796875\nEpoch 2000, Loss: 30961.166015625\nEpoch 2100, Loss: 30216.2265625\nEpoch 2200, Loss: 29519.34375\nEpoch 2300, Loss: 28868.75\nEpoch 2400, Loss: 28260.6015625\nEpoch 2500, Loss: 27690.572265625\nEpoch 2600, Loss: 27155.779296875\nEpoch 2700, Loss: 26653.580078125\nEpoch 2800, Loss: 26181.55078125\nEpoch 2900, Loss: 25737.435546875\nEpoch 3000, Loss: 25319.146484375\nEpoch 3100, Loss: 24924.720703125\nEpoch 3200, Loss: 24552.33984375\nEpoch 3300, Loss: 24200.74609375\nEpoch 3400, Loss: 23869.240234375\nEpoch 3500, Loss: 23556.51953125\nEpoch 3600, Loss: 23259.3828125\nEpoch 3700, Loss: 22976.400390625\nEpoch 3800, Loss: 22706.302734375\nEpoch 3900, Loss: 22447.91015625\nEpoch 4000, Loss: 22201.443359375\nEpoch 4100, Loss: 21964.6328125\nEpoch 4200, Loss: 21736.533203125\nEpoch 4300, Loss: 21516.333984375\nEpoch 4400, Loss: 21304.322265625\nEpoch 4500, Loss: 21098.9140625\nEpoch 4600, Loss: 20899.435546875\nEpoch 4700, Loss: 20705.39453125\nEpoch 4800, Loss: 20516.3671875\nEpoch 4900, Loss: 20331.9921875\nEpoch 5000, Loss: 20151.951171875\nEpoch 5100, Loss: 19975.96484375\nEpoch 5200, Loss: 19803.796875\nEpoch 5300, Loss: 19635.236328125\nEpoch 5400, Loss: 19470.095703125\nEpoch 5500, Loss: 19308.20703125\nEpoch 5600, Loss: 19149.427734375\nEpoch 5700, Loss: 18993.998046875\nEpoch 5800, Loss: 18841.90234375\nEpoch 5900, Loss: 18692.53125\nEpoch 6000, Loss: 18545.7890625\nEpoch 6100, Loss: 18401.583984375\nEpoch 6200, Loss: 18259.833984375\nEpoch 6300, Loss: 18120.462890625\nEpoch 6400, Loss: 17983.3984375\nEpoch 6500, Loss: 17848.583984375\nEpoch 6600, Loss: 17715.955078125\nEpoch 6700, Loss: 17585.455078125\nEpoch 6800, Loss: 17457.041015625\nEpoch 6900, Loss: 17330.658203125\nEpoch 7000, Loss: 17207.42578125\nEpoch 7100, Loss: 17086.32421875\nEpoch 7200, Loss: 16967.05078125\nEpoch 7300, Loss: 16849.5546875\nEpoch 7400, Loss: 16733.80078125\nEpoch 7500, Loss: 16620.720703125\nEpoch 7600, Loss: 16509.3828125\nEpoch 7700, Loss: 16399.640625\nEpoch 7800, Loss: 16291.4580078125\nEpoch 7900, Loss: 16184.80859375\nEpoch 8000, Loss: 16079.6611328125\nEpoch 8100, Loss: 15975.9912109375\nEpoch 8200, Loss: 15873.7802734375\nEpoch 8300, Loss: 15772.994140625\nEpoch 8400, Loss: 15673.60546875\nEpoch 8500, Loss: 15575.60546875\nEpoch 8600, Loss: 15479.3779296875\nEpoch 8700, Loss: 15384.6005859375\nEpoch 8800, Loss: 15291.2099609375\nEpoch 8900, Loss: 15199.1904296875\nEpoch 9000, Loss: 15108.5224609375\nEpoch 9100, Loss: 15019.189453125\nEpoch 9200, Loss: 14931.17578125\nEpoch 9300, Loss: 14844.458984375\nEpoch 9400, Loss: 14759.0166015625\nEpoch 9500, Loss: 14674.830078125\nEpoch 9600, Loss: 14591.8857421875\nEpoch 9700, Loss: 14510.1669921875\nEpoch 9800, Loss: 14429.6630859375\nEpoch 9900, Loss: 14350.365234375\nEpoch 10000, Loss: 14272.2666015625\nEpoch 10100, Loss: 14195.3564453125\nEpoch 10200, Loss: 14119.6318359375\nEpoch 10300, Loss: 14045.0908203125\nEpoch 10400, Loss: 13971.71875\nEpoch 10500, Loss: 13899.5185546875\nEpoch 10600, Loss: 13828.4765625\nEpoch 10700, Loss: 13758.58984375\nEpoch 10800, Loss: 13689.8515625\nEpoch 10900, Loss: 13622.333984375\nEpoch 11000, Loss: 13555.966796875\nEpoch 11100, Loss: 13490.728515625\nEpoch 11200, Loss: 13426.6064453125\nEpoch 11300, Loss: 13363.578125\nEpoch 11400, Loss: 13301.74609375\nEpoch 11500, Loss: 13241.2998046875\nEpoch 11600, Loss: 13182.09765625\nEpoch 11700, Loss: 13124.0693359375\nEpoch 11800, Loss: 13067.34765625\nEpoch 11900, Loss: 13012.0380859375\nEpoch 12000, Loss: 12957.83203125\nEpoch 12100, Loss: 12904.6845703125\nEpoch 12200, Loss: 12852.5390625\nEpoch 12300, Loss: 12801.3525390625\nEpoch 12400, Loss: 12751.0810546875\nEpoch 12500, Loss: 12701.701171875\nEpoch 12600, Loss: 12653.185546875\nEpoch 12700, Loss: 12605.7138671875\nEpoch 12800, Loss: 12559.2822265625\nEpoch 12900, Loss: 12513.818359375\nEpoch 13000, Loss: 12469.2802734375\nEpoch 13100, Loss: 12425.603515625\nEpoch 13200, Loss: 12382.736328125\nEpoch 13300, Loss: 12340.6552734375\nEpoch 13400, Loss: 12299.3779296875\nEpoch 13500, Loss: 12259.0849609375\nEpoch 13600, Loss: 12219.6865234375\nEpoch 13700, Loss: 12181.1416015625\nEpoch 13800, Loss: 12143.44921875\nEpoch 13900, Loss: 12106.5927734375\nEpoch 14000, Loss: 12070.560546875\nEpoch 14100, Loss: 12035.359375\nEpoch 14200, Loss: 12001.0380859375\nEpoch 14300, Loss: 11967.5693359375\nEpoch 14400, Loss: 11935.044921875\nEpoch 14500, Loss: 11903.412109375\nEpoch 14600, Loss: 11872.5908203125\nEpoch 14700, Loss: 11842.5517578125\nEpoch 14800, Loss: 11813.2744140625\nEpoch 14900, Loss: 11784.740234375\nEpoch 15000, Loss: 11756.92578125\nEpoch 15100, Loss: 11729.8115234375\nEpoch 15200, Loss: 11703.3671875\nEpoch 15300, Loss: 11677.5732421875\nEpoch 15400, Loss: 11652.419921875\nEpoch 15500, Loss: 11627.91796875\nEpoch 15600, Loss: 11604.052734375\nEpoch 15700, Loss: 11580.7958984375\nEpoch 15800, Loss: 11558.138671875\nEpoch 15900, Loss: 11536.05859375\nEpoch 16000, Loss: 11514.5458984375\nEpoch 16100, Loss: 11493.5859375\nEpoch 16200, Loss: 11473.1826171875\nEpoch 16300, Loss: 11453.45703125\nEpoch 16400, Loss: 11434.3134765625\nEpoch 16500, Loss: 11416.00390625\nEpoch 16600, Loss: 11398.390625\nEpoch 16700, Loss: 11381.3583984375\nEpoch 16800, Loss: 11364.859375\nEpoch 16900, Loss: 11348.8564453125\nEpoch 17000, Loss: 11333.3408203125\nEpoch 17100, Loss: 11318.296875\nEpoch 17200, Loss: 11303.708984375\nEpoch 17300, Loss: 11289.5810546875\nEpoch 17400, Loss: 11275.8974609375\nEpoch 17500, Loss: 11262.650390625\nEpoch 17600, Loss: 11249.8359375\nEpoch 17700, Loss: 11237.443359375\nEpoch 17800, Loss: 11225.5166015625\nEpoch 17900, Loss: 11214.04296875\nEpoch 18000, Loss: 11202.9775390625\nEpoch 18100, Loss: 11192.31640625\nEpoch 18200, Loss: 11182.0478515625\nEpoch 18300, Loss: 11172.171875\nEpoch 18400, Loss: 11162.6884765625\nEpoch 18500, Loss: 11153.5869140625\nEpoch 18600, Loss: 11144.857421875\nEpoch 18700, Loss: 11136.4892578125\nEpoch 18800, Loss: 11128.474609375\nEpoch 18900, Loss: 11120.80859375\nEpoch 19000, Loss: 11113.486328125\nEpoch 19100, Loss: 11106.498046875\nEpoch 19200, Loss: 11099.8349609375\nEpoch 19300, Loss: 11093.4892578125\nEpoch 19400, Loss: 11087.4501953125\nEpoch 19500, Loss: 11081.7041015625\nEpoch 19600, Loss: 11076.244140625\nEpoch 19700, Loss: 11071.046875\nEpoch 19800, Loss: 11066.107421875\nEpoch 19900, Loss: 11061.404296875\nEpoch 20000, Loss: 11056.927734375\nEpoch 20100, Loss: 11052.6572265625\nEpoch 20200, Loss: 11048.5791015625\nEpoch 20300, Loss: 11044.67578125\nEpoch 20400, Loss: 11040.9365234375\nEpoch 20500, Loss: 11037.53515625\nEpoch 20600, Loss: 11034.3330078125\nEpoch 20700, Loss: 11031.2705078125\nEpoch 20800, Loss: 11028.3203125\nEpoch 20900, Loss: 11025.4765625\nEpoch 21000, Loss: 11022.720703125\nEpoch 21100, Loss: 11020.048828125\nEpoch 21200, Loss: 11017.453125\nEpoch 21300, Loss: 11014.9140625\nEpoch 21400, Loss: 11012.4208984375\nEpoch 21500, Loss: 11009.9560546875\nEpoch 21600, Loss: 11007.5185546875\nEpoch 21700, Loss: 11005.09765625\nEpoch 21800, Loss: 11002.6953125\nEpoch 21900, Loss: 11000.3017578125\nEpoch 22000, Loss: 10997.916015625\nEpoch 22100, Loss: 10995.53125\nEpoch 22200, Loss: 10993.146484375\nEpoch 22300, Loss: 10990.76171875\nEpoch 22400, Loss: 10988.388671875\nEpoch 22500, Loss: 10986.05078125\nEpoch 22600, Loss: 10983.75390625\nEpoch 22700, Loss: 10981.501953125\nEpoch 22800, Loss: 10979.29296875\nEpoch 22900, Loss: 10977.1279296875\nEpoch 23000, Loss: 10975.0107421875\nEpoch 23100, Loss: 10972.939453125\nEpoch 23200, Loss: 10970.9189453125\nEpoch 23300, Loss: 10968.94140625\nEpoch 23400, Loss: 10967.021484375\nEpoch 23500, Loss: 10965.14453125\nEpoch 23600, Loss: 10963.318359375\nEpoch 23700, Loss: 10961.5439453125\nEpoch 23800, Loss: 10959.8193359375\nEpoch 23900, Loss: 10958.1416015625\nEpoch 24000, Loss: 10956.5166015625\nEpoch 24100, Loss: 10954.939453125\nEpoch 24200, Loss: 10953.404296875\nEpoch 24300, Loss: 10951.896484375\nEpoch 24400, Loss: 10950.408203125\nEpoch 24500, Loss: 10948.9287109375\nEpoch 24600, Loss: 10947.4765625\nEpoch 24700, Loss: 10946.0556640625\nEpoch 24800, Loss: 10944.6640625\nEpoch 24900, Loss: 10943.30078125\nEpoch 25000, Loss: 10941.96875\nEpoch 25100, Loss: 10940.68359375\nEpoch 25200, Loss: 10939.455078125\nEpoch 25300, Loss: 10938.2861328125\nEpoch 25400, Loss: 10937.1669921875\nEpoch 25500, Loss: 10936.0986328125\nEpoch 25600, Loss: 10935.0908203125\nEpoch 25700, Loss: 10934.13671875\nEpoch 25800, Loss: 10933.248046875\nEpoch 25900, Loss: 10932.4169921875\nEpoch 26000, Loss: 10931.6484375\nEpoch 26100, Loss: 10930.9228515625\nEpoch 26200, Loss: 10930.255859375\nEpoch 26300, Loss: 10929.634765625\nEpoch 26400, Loss: 10929.0634765625\nEpoch 26500, Loss: 10928.53125\nEpoch 26600, Loss: 10928.041015625\nEpoch 26700, Loss: 10927.58984375\nEpoch 26800, Loss: 10927.17578125\nEpoch 26900, Loss: 10926.798828125\nEpoch 27000, Loss: 10926.4521484375\nEpoch 27100, Loss: 10926.13671875\nEpoch 27200, Loss: 10925.849609375\nEpoch 27300, Loss: 10925.5888671875\nEpoch 27400, Loss: 10925.3447265625\nEpoch 27500, Loss: 10925.125\nEpoch 27600, Loss: 10924.919921875\nEpoch 27700, Loss: 10924.7353515625\nEpoch 27800, Loss: 10924.568359375\nEpoch 27900, Loss: 10924.4169921875\nEpoch 28000, Loss: 10924.2783203125\nEpoch 28100, Loss: 10924.1552734375\nEpoch 28200, Loss: 10924.046875\nEpoch 28300, Loss: 10923.94921875\nEpoch 28400, Loss: 10923.8662109375\nEpoch 28500, Loss: 10923.791015625\nEpoch 28600, Loss: 10923.724609375\nEpoch 28700, Loss: 10923.6689453125\nEpoch 28800, Loss: 10923.6171875\nEpoch 28900, Loss: 10923.5693359375\nEpoch 29000, Loss: 10923.525390625\nEpoch 29100, Loss: 10923.486328125\nEpoch 29200, Loss: 10923.4482421875\nEpoch 29300, Loss: 10923.4169921875\nEpoch 29400, Loss: 10923.38671875\nEpoch 29500, Loss: 10923.3603515625\nEpoch 29600, Loss: 10923.337890625\nEpoch 29700, Loss: 10923.31640625\nEpoch 29800, Loss: 10923.298828125\nEpoch 29900, Loss: 10923.2841796875\nEpoch 30000, Loss: 10923.271484375\nEpoch 30100, Loss: 10923.2587890625\nEpoch 30200, Loss: 10923.248046875\nEpoch 30300, Loss: 10923.240234375\nEpoch 30400, Loss: 10923.2333984375\nEpoch 30500, Loss: 10923.228515625\nEpoch 30600, Loss: 10923.2255859375\nEpoch 30700, Loss: 10923.22265625\nEpoch 30800, Loss: 10923.21875\nEpoch 30900, Loss: 10923.216796875\nEpoch 31000, Loss: 10923.2158203125\nEpoch 31100, Loss: 10923.2138671875\nEpoch 31200, Loss: 10923.2119140625\nEpoch 31300, Loss: 10923.2109375\nEpoch 31400, Loss: 10923.2109375\nEpoch 31500, Loss: 10923.2109375\nEpoch 31600, Loss: 10923.2109375\nEpoch 31700, Loss: 10923.2119140625\nEpoch 31800, Loss: 10923.2119140625\nEpoch 31900, Loss: 10923.2099609375\nEpoch 32000, Loss: 10923.2119140625\nEpoch 32100, Loss: 10923.2109375\nEpoch 32200, Loss: 10923.2099609375\nEpoch 32300, Loss: 10923.2099609375\nEpoch 32400, Loss: 10923.2099609375\nEpoch 32500, Loss: 10923.208984375\nEpoch 32600, Loss: 10923.208984375\nEpoch 32700, Loss: 10923.2109375\nEpoch 32800, Loss: 10923.208984375\nEpoch 32900, Loss: 10923.2099609375\nEpoch 33000, Loss: 10923.2099609375\nEpoch 33100, Loss: 10923.2109375\nEpoch 33200, Loss: 10923.2109375\nEpoch 33300, Loss: 10923.2099609375\nEpoch 33400, Loss: 10923.2109375\nEpoch 33500, Loss: 10923.2109375\nEpoch 33600, Loss: 10923.2099609375\nEpoch 33700, Loss: 10923.2109375\nEpoch 33800, Loss: 10923.2099609375\nEpoch 33900, Loss: 10923.2109375\nEpoch 34000, Loss: 10923.2109375\nEpoch 34100, Loss: 10923.2099609375\nEpoch 34200, Loss: 10923.2099609375\nEpoch 34300, Loss: 10923.2099609375\nEpoch 34400, Loss: 10923.2099609375\nEpoch 34500, Loss: 10923.2119140625\nEpoch 34600, Loss: 10923.2109375\nEpoch 34700, Loss: 10923.2109375\nEpoch 34800, Loss: 10923.2099609375\nEpoch 34900, Loss: 10923.2099609375\nEpoch 35000, Loss: 10923.2099609375\nEpoch 35100, Loss: 10923.2109375\nEpoch 35200, Loss: 10923.208984375\nEpoch 35300, Loss: 10923.2099609375\nEpoch 35400, Loss: 10923.2099609375\nEpoch 35500, Loss: 10923.2099609375\nEpoch 35600, Loss: 10923.2109375\nEpoch 35700, Loss: 10923.2109375\nEpoch 35800, Loss: 10923.2109375\nEpoch 35900, Loss: 10923.2109375\nEpoch 36000, Loss: 10923.2109375\nEpoch 36100, Loss: 10923.2109375\nEpoch 36200, Loss: 10923.208984375\nEpoch 36300, Loss: 10923.2109375\nEpoch 36400, Loss: 10923.2109375\nEpoch 36500, Loss: 10923.2099609375\nEpoch 36600, Loss: 10923.2099609375\nEpoch 36700, Loss: 10923.2109375\nEpoch 36800, Loss: 10923.2099609375\nEpoch 36900, Loss: 10923.2109375\nEpoch 37000, Loss: 10923.2109375\nEpoch 37100, Loss: 10923.2109375\nEpoch 37200, Loss: 10923.208984375\nEpoch 37300, Loss: 10923.2109375\nEpoch 37400, Loss: 10923.2109375\nEpoch 37500, Loss: 10923.2109375\nEpoch 37600, Loss: 10923.2109375\nEpoch 37700, Loss: 10923.2109375\nEpoch 37800, Loss: 10923.208984375\nEpoch 37900, Loss: 10923.2119140625\nEpoch 38000, Loss: 10923.2109375\nEpoch 38100, Loss: 10923.2109375\nEpoch 38200, Loss: 10923.2109375\nEpoch 38300, Loss: 10923.208984375\nEpoch 38400, Loss: 10923.2099609375\nEpoch 38500, Loss: 10923.2109375\nEpoch 38600, Loss: 10923.2080078125\nEpoch 38700, Loss: 10923.2109375\nEpoch 38800, Loss: 10923.2099609375\nEpoch 38900, Loss: 10923.2109375\nEpoch 39000, Loss: 10923.2109375\nEpoch 39100, Loss: 10923.2109375\nEpoch 39200, Loss: 10923.2109375\nEpoch 39300, Loss: 10923.2119140625\nEpoch 39400, Loss: 10923.2109375\nEpoch 39500, Loss: 10923.2099609375\nEpoch 39600, Loss: 10923.2099609375\nEpoch 39700, Loss: 10923.2109375\nEpoch 39800, Loss: 10923.2109375\nEpoch 39900, Loss: 10923.2109375\nEpoch 40000, Loss: 10923.2099609375\nEpoch 40100, Loss: 10923.2109375\nEpoch 40200, Loss: 10923.2109375\nEpoch 40300, Loss: 10923.2109375\nEpoch 40400, Loss: 10923.2109375\nEpoch 40500, Loss: 10923.2109375\nEpoch 40600, Loss: 10923.208984375\nEpoch 40700, Loss: 10923.2109375\nEpoch 40800, Loss: 10923.208984375\nEpoch 40900, Loss: 10923.2119140625\nEpoch 41000, Loss: 10923.2119140625\nEpoch 41100, Loss: 10923.2119140625\nEpoch 41200, Loss: 10923.2099609375\nEpoch 41300, Loss: 10923.2099609375\nEpoch 41400, Loss: 10923.2109375\nEpoch 41500, Loss: 10923.2109375\nEpoch 41600, Loss: 10923.2099609375\nEpoch 41700, Loss: 10923.2099609375\nEpoch 41800, Loss: 10923.208984375\nEpoch 41900, Loss: 10923.2109375\nEpoch 42000, Loss: 10923.2119140625\nEpoch 42100, Loss: 10923.2099609375\nEpoch 42200, Loss: 10923.208984375\nEpoch 42300, Loss: 10923.2099609375\nEpoch 42400, Loss: 10923.2109375\nEpoch 42500, Loss: 10923.2099609375\nEpoch 42600, Loss: 10923.2119140625\nEpoch 42700, Loss: 10923.208984375\nEpoch 42800, Loss: 10923.2099609375\nEpoch 42900, Loss: 10923.2119140625\nEpoch 43000, Loss: 10923.2109375\nEpoch 43100, Loss: 10923.2099609375\nEpoch 43200, Loss: 10923.2109375\nEpoch 43300, Loss: 10923.2099609375\nEpoch 43400, Loss: 10923.2099609375\nEpoch 43500, Loss: 10923.2119140625\nEpoch 43600, Loss: 10923.2109375\nEpoch 43700, Loss: 10923.2119140625\nEpoch 43800, Loss: 10923.2109375\nEpoch 43900, Loss: 10923.2119140625\nEpoch 44000, Loss: 10923.2099609375\nEpoch 44100, Loss: 10923.2109375\nEpoch 44200, Loss: 10923.2099609375\nEpoch 44300, Loss: 10923.2099609375\nEpoch 44400, Loss: 10923.2109375\nEpoch 44500, Loss: 10923.2119140625\nEpoch 44600, Loss: 10923.2109375\nEpoch 44700, Loss: 10923.2099609375\nEpoch 44800, Loss: 10923.208984375\nEpoch 44900, Loss: 10923.2099609375\nEpoch 45000, Loss: 10923.2099609375\nEpoch 45100, Loss: 10923.2099609375\nEpoch 45200, Loss: 10923.2109375\nEpoch 45300, Loss: 10923.2109375\nEpoch 45400, Loss: 10923.2109375\nEpoch 45500, Loss: 10923.2109375\nEpoch 45600, Loss: 10923.2109375\nEpoch 45700, Loss: 10923.2080078125\nEpoch 45800, Loss: 10923.2109375\nEpoch 45900, Loss: 10923.2099609375\nEpoch 46000, Loss: 10923.2099609375\nEpoch 46100, Loss: 10923.2109375\nEpoch 46200, Loss: 10923.2119140625\nEpoch 46300, Loss: 10923.2109375\nEpoch 46400, Loss: 10923.2109375\nEpoch 46500, Loss: 10923.2109375\nEpoch 46600, Loss: 10923.208984375\nEpoch 46700, Loss: 10923.2099609375\nEpoch 46800, Loss: 10923.2099609375\nEpoch 46900, Loss: 10923.2099609375\nEpoch 47000, Loss: 10923.2109375\nEpoch 47100, Loss: 10923.2109375\nEpoch 47200, Loss: 10923.208984375\nEpoch 47300, Loss: 10923.2109375\nEpoch 47400, Loss: 10923.2109375\nEpoch 47500, Loss: 10923.2109375\nEpoch 47600, Loss: 10923.208984375\nEpoch 47700, Loss: 10923.2109375\nEpoch 47800, Loss: 10923.2109375\nEpoch 47900, Loss: 10923.2099609375\nEpoch 48000, Loss: 10923.208984375\nEpoch 48100, Loss: 10923.208984375\nEpoch 48200, Loss: 10923.2109375\nEpoch 48300, Loss: 10923.2109375\nEpoch 48400, Loss: 10923.2099609375\nEpoch 48500, Loss: 10923.2099609375\nEpoch 48600, Loss: 10923.2109375\nEpoch 48700, Loss: 10923.2109375\nEpoch 48800, Loss: 10923.2109375\nEpoch 48900, Loss: 10923.2119140625\nEpoch 49000, Loss: 10923.2099609375\nEpoch 49100, Loss: 10923.208984375\nEpoch 49200, Loss: 10923.2109375\nEpoch 49300, Loss: 10923.2109375\nEpoch 49400, Loss: 10923.2099609375\nEpoch 49500, Loss: 10923.2099609375\nEpoch 49600, Loss: 10923.2099609375\nEpoch 49700, Loss: 10923.2109375\nEpoch 49800, Loss: 10923.2109375\nEpoch 49900, Loss: 10923.2099609375\nEpoch 0, Loss: 63828.96875\nEpoch 100, Loss: 60945.62890625\nEpoch 200, Loss: 58249.76953125\nEpoch 300, Loss: 55729.1015625\nEpoch 400, Loss: 53370.4375\nEpoch 500, Loss: 51162.8984375\nEpoch 600, Loss: 49096.1015625\nEpoch 700, Loss: 47161.23046875\nEpoch 800, Loss: 45351.796875\nEpoch 900, Loss: 43661.06640625\nEpoch 1000, Loss: 42081.0703125\nEpoch 1100, Loss: 40605.01171875\nEpoch 1200, Loss: 39226.4921875\nEpoch 1300, Loss: 37939.9609375\nEpoch 1400, Loss: 36740.59375\nEpoch 1500, Loss: 35621.2734375\nEpoch 1600, Loss: 34576.40234375\nEpoch 1700, Loss: 33600.73828125\nEpoch 1800, Loss: 32689.373046875\nEpoch 1900, Loss: 31837.748046875\nEpoch 2000, Loss: 31042.3046875\nEpoch 2100, Loss: 30299.76953125\nEpoch 2200, Loss: 29605.49609375\nEpoch 2300, Loss: 28958.15234375\nEpoch 2400, Loss: 28352.7421875\nEpoch 2500, Loss: 27785.31640625\nEpoch 2600, Loss: 27252.98828125\nEpoch 2700, Loss: 26753.1171875\nEpoch 2800, Loss: 26283.28125\nEpoch 2900, Loss: 25841.236328125\nEpoch 3000, Loss: 25424.904296875\nEpoch 3100, Loss: 25032.34765625\nEpoch 3200, Loss: 24661.74609375\nEpoch 3300, Loss: 24312.0078125\nEpoch 3400, Loss: 23983.310546875\nEpoch 3500, Loss: 23673.048828125\nEpoch 3600, Loss: 23378.23828125\nEpoch 3700, Loss: 23097.48828125\nEpoch 3800, Loss: 22829.53125\nEpoch 3900, Loss: 22573.765625\nEpoch 4000, Loss: 22329.62890625\nEpoch 4100, Loss: 22095.021484375\nEpoch 4200, Loss: 21869.04296875\nEpoch 4300, Loss: 21651.19140625\nEpoch 4400, Loss: 21441.513671875\nEpoch 4500, Loss: 21238.259765625\nEpoch 4600, Loss: 21040.861328125\nEpoch 4700, Loss: 20848.83203125\nEpoch 4800, Loss: 20661.7578125\nEpoch 4900, Loss: 20479.275390625\nEpoch 5000, Loss: 20301.064453125\nEpoch 5100, Loss: 20126.857421875\nEpoch 5200, Loss: 19956.41796875\nEpoch 5300, Loss: 19789.533203125\nEpoch 5400, Loss: 19626.017578125\nEpoch 5500, Loss: 19465.708984375\nEpoch 5600, Loss: 19308.724609375\nEpoch 5700, Loss: 19155.435546875\nEpoch 5800, Loss: 19004.94140625\nEpoch 5900, Loss: 18857.1328125\nEpoch 6000, Loss: 18711.908203125\nEpoch 6100, Loss: 18569.177734375\nEpoch 6200, Loss: 18428.861328125\nEpoch 6300, Loss: 18290.87890625\nEpoch 6400, Loss: 18155.162109375\nEpoch 6500, Loss: 18021.654296875\nEpoch 6600, Loss: 17890.291015625\nEpoch 6700, Loss: 17761.013671875\nEpoch 6800, Loss: 17633.7734375\nEpoch 6900, Loss: 17509.455078125\nEpoch 7000, Loss: 17387.828125\nEpoch 7100, Loss: 17268.029296875\nEpoch 7200, Loss: 17150.017578125\nEpoch 7300, Loss: 17033.732421875\nEpoch 7400, Loss: 16919.13671875\nEpoch 7500, Loss: 16807.205078125\nEpoch 7600, Loss: 16697.201171875\nEpoch 7700, Loss: 16588.744140625\nEpoch 7800, Loss: 16481.787109375\nEpoch 7900, Loss: 16376.2919921875\nEpoch 8000, Loss: 16272.2412109375\nEpoch 8100, Loss: 16169.6171875\nEpoch 8200, Loss: 16068.4052734375\nEpoch 8300, Loss: 15968.5751953125\nEpoch 8400, Loss: 15870.5107421875\nEpoch 8500, Loss: 15773.9169921875\nEpoch 8600, Loss: 15678.7080078125\nEpoch 8700, Loss: 15584.861328125\nEpoch 8800, Loss: 15492.3544921875\nEpoch 8900, Loss: 15401.177734375\nEpoch 9000, Loss: 15311.3076171875\nEpoch 9100, Loss: 15222.73046875\nEpoch 9200, Loss: 15135.4375\nEpoch 9300, Loss: 15049.4111328125\nEpoch 9400, Loss: 14964.6416015625\nEpoch 9500, Loss: 14881.11328125\nEpoch 9600, Loss: 14798.8134765625\nEpoch 9700, Loss: 14717.720703125\nEpoch 9800, Loss: 14637.8193359375\nEpoch 9900, Loss: 14559.0966796875\nEpoch 10000, Loss: 14481.55078125\nEpoch 10100, Loss: 14405.1650390625\nEpoch 10200, Loss: 14329.93359375\nEpoch 10300, Loss: 14255.84765625\nEpoch 10400, Loss: 14182.90234375\nEpoch 10500, Loss: 14111.087890625\nEpoch 10600, Loss: 14040.3974609375\nEpoch 10700, Loss: 13970.888671875\nEpoch 10800, Loss: 13902.55078125\nEpoch 10900, Loss: 13835.3359375\nEpoch 11000, Loss: 13769.2236328125\nEpoch 11100, Loss: 13704.5439453125\nEpoch 11200, Loss: 13641.224609375\nEpoch 11300, Loss: 13579.1845703125\nEpoch 11400, Loss: 13518.357421875\nEpoch 11500, Loss: 13458.634765625\nEpoch 11600, Loss: 13399.955078125\nEpoch 11700, Loss: 13342.6943359375\nEpoch 11800, Loss: 13286.7021484375\nEpoch 11900, Loss: 13231.79296875\nEpoch 12000, Loss: 13177.9423828125\nEpoch 12100, Loss: 13125.123046875\nEpoch 12200, Loss: 13073.3076171875\nEpoch 12300, Loss: 13022.740234375\nEpoch 12400, Loss: 12973.2802734375\nEpoch 12500, Loss: 12924.865234375\nEpoch 12600, Loss: 12877.4326171875\nEpoch 12700, Loss: 12830.9375\nEpoch 12800, Loss: 12785.296875\nEpoch 12900, Loss: 12740.4775390625\nEpoch 13000, Loss: 12696.462890625\nEpoch 13100, Loss: 12653.2392578125\nEpoch 13200, Loss: 12610.8251953125\nEpoch 13300, Loss: 12569.4072265625\nEpoch 13400, Loss: 12528.8994140625\nEpoch 13500, Loss: 12489.2607421875\nEpoch 13600, Loss: 12450.486328125\nEpoch 13700, Loss: 12412.611328125\nEpoch 13800, Loss: 12375.615234375\nEpoch 13900, Loss: 12339.509765625\nEpoch 14000, Loss: 12304.373046875\nEpoch 14100, Loss: 12270.11328125\nEpoch 14200, Loss: 12236.685546875\nEpoch 14300, Loss: 12204.0830078125\nEpoch 14400, Loss: 12172.26171875\nEpoch 14500, Loss: 12141.2255859375\nEpoch 14600, Loss: 12110.9404296875\nEpoch 14700, Loss: 12081.4033203125\nEpoch 14800, Loss: 12052.5947265625\nEpoch 14900, Loss: 12024.5\nEpoch 15000, Loss: 11997.1142578125\nEpoch 15100, Loss: 11970.416015625\nEpoch 15200, Loss: 11944.416015625\nEpoch 15300, Loss: 11919.0947265625\nEpoch 15400, Loss: 11894.4423828125\nEpoch 15500, Loss: 11870.4326171875\nEpoch 15600, Loss: 11847.04296875\nEpoch 15700, Loss: 11824.2578125\nEpoch 15800, Loss: 11802.115234375\nEpoch 15900, Loss: 11780.697265625\nEpoch 16000, Loss: 11760.16796875\nEpoch 16100, Loss: 11740.537109375\nEpoch 16200, Loss: 11721.5615234375\nEpoch 16300, Loss: 11703.1611328125\nEpoch 16400, Loss: 11685.283203125\nEpoch 16500, Loss: 11667.9091796875\nEpoch 16600, Loss: 11651.0166015625\nEpoch 16700, Loss: 11634.599609375\nEpoch 16800, Loss: 11618.6484375\nEpoch 16900, Loss: 11603.1513671875\nEpoch 17000, Loss: 11588.1044921875\nEpoch 17100, Loss: 11573.4970703125\nEpoch 17200, Loss: 11559.3349609375\nEpoch 17300, Loss: 11545.6220703125\nEpoch 17400, Loss: 11532.4287109375\nEpoch 17500, Loss: 11519.7275390625\nEpoch 17600, Loss: 11507.45703125\nEpoch 17700, Loss: 11495.609375\nEpoch 17800, Loss: 11484.177734375\nEpoch 17900, Loss: 11473.15234375\nEpoch 18000, Loss: 11462.52734375\nEpoch 18100, Loss: 11452.294921875\nEpoch 18200, Loss: 11442.4482421875\nEpoch 18300, Loss: 11432.9765625\nEpoch 18400, Loss: 11423.8759765625\nEpoch 18500, Loss: 11415.1298828125\nEpoch 18600, Loss: 11406.736328125\nEpoch 18700, Loss: 11398.685546875\nEpoch 18800, Loss: 11390.96875\nEpoch 18900, Loss: 11383.5810546875\nEpoch 19000, Loss: 11376.5009765625\nEpoch 19100, Loss: 11369.7216796875\nEpoch 19200, Loss: 11363.2275390625\nEpoch 19300, Loss: 11357.0068359375\nEpoch 19400, Loss: 11351.0439453125\nEpoch 19500, Loss: 11345.3291015625\nEpoch 19600, Loss: 11339.841796875\nEpoch 19700, Loss: 11334.572265625\nEpoch 19800, Loss: 11329.5048828125\nEpoch 19900, Loss: 11324.625\nEpoch 20000, Loss: 11320.1171875\nEpoch 20100, Loss: 11315.8916015625\nEpoch 20200, Loss: 11311.884765625\nEpoch 20300, Loss: 11308.05859375\nEpoch 20400, Loss: 11304.3828125\nEpoch 20500, Loss: 11300.8466796875\nEpoch 20600, Loss: 11297.421875\nEpoch 20700, Loss: 11294.095703125\nEpoch 20800, Loss: 11290.8447265625\nEpoch 20900, Loss: 11287.66015625\nEpoch 21000, Loss: 11284.5263671875\nEpoch 21100, Loss: 11281.4453125\nEpoch 21200, Loss: 11278.41015625\nEpoch 21300, Loss: 11275.41796875\nEpoch 21400, Loss: 11272.4599609375\nEpoch 21500, Loss: 11269.537109375\nEpoch 21600, Loss: 11266.6494140625\nEpoch 21700, Loss: 11263.7939453125\nEpoch 21800, Loss: 11260.9775390625\nEpoch 21900, Loss: 11258.19140625\nEpoch 22000, Loss: 11255.439453125\nEpoch 22100, Loss: 11252.728515625\nEpoch 22200, Loss: 11250.056640625\nEpoch 22300, Loss: 11247.421875\nEpoch 22400, Loss: 11244.828125\nEpoch 22500, Loss: 11242.28125\nEpoch 22600, Loss: 11239.7763671875\nEpoch 22700, Loss: 11237.3203125\nEpoch 22800, Loss: 11234.9091796875\nEpoch 22900, Loss: 11232.5439453125\nEpoch 23000, Loss: 11230.2275390625\nEpoch 23100, Loss: 11227.96484375\nEpoch 23200, Loss: 11225.7490234375\nEpoch 23300, Loss: 11223.5849609375\nEpoch 23400, Loss: 11221.4833984375\nEpoch 23500, Loss: 11219.451171875\nEpoch 23600, Loss: 11217.4912109375\nEpoch 23700, Loss: 11215.5869140625\nEpoch 23800, Loss: 11213.7392578125\nEpoch 23900, Loss: 11211.943359375\nEpoch 24000, Loss: 11210.201171875\nEpoch 24100, Loss: 11208.5263671875\nEpoch 24200, Loss: 11206.9140625\nEpoch 24300, Loss: 11205.359375\nEpoch 24400, Loss: 11203.8564453125\nEpoch 24500, Loss: 11202.40234375\nEpoch 24600, Loss: 11201.00390625\nEpoch 24700, Loss: 11199.6611328125\nEpoch 24800, Loss: 11198.376953125\nEpoch 24900, Loss: 11197.1591796875\nEpoch 25000, Loss: 11195.9921875\nEpoch 25100, Loss: 11194.880859375\nEpoch 25200, Loss: 11193.8212890625\nEpoch 25300, Loss: 11192.8193359375\nEpoch 25400, Loss: 11191.8642578125\nEpoch 25500, Loss: 11190.962890625\nEpoch 25600, Loss: 11190.10546875\nEpoch 25700, Loss: 11189.2978515625\nEpoch 25800, Loss: 11188.537109375\nEpoch 25900, Loss: 11187.81640625\nEpoch 26000, Loss: 11187.14453125\nEpoch 26100, Loss: 11186.5126953125\nEpoch 26200, Loss: 11185.919921875\nEpoch 26300, Loss: 11185.3671875\nEpoch 26400, Loss: 11184.8515625\nEpoch 26500, Loss: 11184.3720703125\nEpoch 26600, Loss: 11183.9287109375\nEpoch 26700, Loss: 11183.51953125\nEpoch 26800, Loss: 11183.140625\nEpoch 26900, Loss: 11182.7939453125\nEpoch 27000, Loss: 11182.4755859375\nEpoch 27100, Loss: 11182.1806640625\nEpoch 27200, Loss: 11181.9111328125\nEpoch 27300, Loss: 11181.6640625\nEpoch 27400, Loss: 11181.43359375\nEpoch 27500, Loss: 11181.22265625\nEpoch 27600, Loss: 11181.03125\nEpoch 27700, Loss: 11180.849609375\nEpoch 27800, Loss: 11180.6904296875\nEpoch 27900, Loss: 11180.5419921875\nEpoch 28000, Loss: 11180.41015625\nEpoch 28100, Loss: 11180.29296875\nEpoch 28200, Loss: 11180.1865234375\nEpoch 28300, Loss: 11180.091796875\nEpoch 28400, Loss: 11180.009765625\nEpoch 28500, Loss: 11179.935546875\nEpoch 28600, Loss: 11179.8701171875\nEpoch 28700, Loss: 11179.810546875\nEpoch 28800, Loss: 11179.7578125\nEpoch 28900, Loss: 11179.7109375\nEpoch 29000, Loss: 11179.6630859375\nEpoch 29100, Loss: 11179.6220703125\nEpoch 29200, Loss: 11179.583984375\nEpoch 29300, Loss: 11179.548828125\nEpoch 29400, Loss: 11179.5166015625\nEpoch 29500, Loss: 11179.490234375\nEpoch 29600, Loss: 11179.462890625\nEpoch 29700, Loss: 11179.4423828125\nEpoch 29800, Loss: 11179.419921875\nEpoch 29900, Loss: 11179.4052734375\nEpoch 30000, Loss: 11179.390625\nEpoch 30100, Loss: 11179.376953125\nEpoch 30200, Loss: 11179.3662109375\nEpoch 30300, Loss: 11179.3583984375\nEpoch 30400, Loss: 11179.34765625\nEpoch 30500, Loss: 11179.3427734375\nEpoch 30600, Loss: 11179.337890625\nEpoch 30700, Loss: 11179.33203125\nEpoch 30800, Loss: 11179.330078125\nEpoch 30900, Loss: 11179.3271484375\nEpoch 31000, Loss: 11179.32421875\nEpoch 31100, Loss: 11179.32421875\nEpoch 31200, Loss: 11179.32421875\nEpoch 31300, Loss: 11179.322265625\nEpoch 31400, Loss: 11179.3212890625\nEpoch 31500, Loss: 11179.322265625\nEpoch 31600, Loss: 11179.3193359375\nEpoch 31700, Loss: 11179.3203125\nEpoch 31800, Loss: 11179.3203125\nEpoch 31900, Loss: 11179.3193359375\nEpoch 32000, Loss: 11179.3203125\nEpoch 32100, Loss: 11179.3203125\nEpoch 32200, Loss: 11179.3212890625\nEpoch 32300, Loss: 11179.3203125\nEpoch 32400, Loss: 11179.3193359375\nEpoch 32500, Loss: 11179.3212890625\nEpoch 32600, Loss: 11179.3193359375\nEpoch 32700, Loss: 11179.3203125\nEpoch 32800, Loss: 11179.3203125\nEpoch 32900, Loss: 11179.3212890625\nEpoch 33000, Loss: 11179.3203125\nEpoch 33100, Loss: 11179.3193359375\nEpoch 33200, Loss: 11179.3203125\nEpoch 33300, Loss: 11179.3212890625\nEpoch 33400, Loss: 11179.3203125\nEpoch 33500, Loss: 11179.3193359375\nEpoch 33600, Loss: 11179.3203125\nEpoch 33700, Loss: 11179.318359375\nEpoch 33800, Loss: 11179.3203125\nEpoch 33900, Loss: 11179.3203125\nEpoch 34000, Loss: 11179.3193359375\nEpoch 34100, Loss: 11179.3203125\nEpoch 34200, Loss: 11179.3193359375\nEpoch 34300, Loss: 11179.3212890625\nEpoch 34400, Loss: 11179.322265625\nEpoch 34500, Loss: 11179.3203125\nEpoch 34600, Loss: 11179.318359375\nEpoch 34700, Loss: 11179.3203125\nEpoch 34800, Loss: 11179.3203125\nEpoch 34900, Loss: 11179.3193359375\nEpoch 35000, Loss: 11179.3203125\nEpoch 35100, Loss: 11179.3193359375\nEpoch 35200, Loss: 11179.3203125\nEpoch 35300, Loss: 11179.3203125\nEpoch 35400, Loss: 11179.3203125\nEpoch 35500, Loss: 11179.3203125\nEpoch 35600, Loss: 11179.3203125\nEpoch 35700, Loss: 11179.3193359375\nEpoch 35800, Loss: 11179.3203125\nEpoch 35900, Loss: 11179.3203125\nEpoch 36000, Loss: 11179.3203125\nEpoch 36100, Loss: 11179.318359375\nEpoch 36200, Loss: 11179.3193359375\nEpoch 36300, Loss: 11179.3212890625\nEpoch 36400, Loss: 11179.322265625\nEpoch 36500, Loss: 11179.3203125\nEpoch 36600, Loss: 11179.3212890625\nEpoch 36700, Loss: 11179.3203125\nEpoch 36800, Loss: 11179.3203125\nEpoch 36900, Loss: 11179.3203125\nEpoch 37000, Loss: 11179.318359375\nEpoch 37100, Loss: 11179.3203125\nEpoch 37200, Loss: 11179.3203125\nEpoch 37300, Loss: 11179.318359375\nEpoch 37400, Loss: 11179.322265625\nEpoch 37500, Loss: 11179.3193359375\nEpoch 37600, Loss: 11179.3212890625\nEpoch 37700, Loss: 11179.3203125\nEpoch 37800, Loss: 11179.318359375\nEpoch 37900, Loss: 11179.3203125\nEpoch 38000, Loss: 11179.322265625\nEpoch 38100, Loss: 11179.3193359375\nEpoch 38200, Loss: 11179.3212890625\nEpoch 38300, Loss: 11179.3193359375\nEpoch 38400, Loss: 11179.322265625\nEpoch 38500, Loss: 11179.3193359375\nEpoch 38600, Loss: 11179.3203125\nEpoch 38700, Loss: 11179.3203125\nEpoch 38800, Loss: 11179.3203125\nEpoch 38900, Loss: 11179.318359375\nEpoch 39000, Loss: 11179.3193359375\nEpoch 39100, Loss: 11179.318359375\nEpoch 39200, Loss: 11179.3173828125\nEpoch 39300, Loss: 11179.318359375\nEpoch 39400, Loss: 11179.318359375\nEpoch 39500, Loss: 11179.3193359375\nEpoch 39600, Loss: 11179.3212890625\nEpoch 39700, Loss: 11179.3203125\nEpoch 39800, Loss: 11179.3193359375\nEpoch 39900, Loss: 11179.322265625\nEpoch 40000, Loss: 11179.3212890625\nEpoch 40100, Loss: 11179.318359375\nEpoch 40200, Loss: 11179.3212890625\nEpoch 40300, Loss: 11179.3193359375\nEpoch 40400, Loss: 11179.3203125\nEpoch 40500, Loss: 11179.3212890625\nEpoch 40600, Loss: 11179.3212890625\nEpoch 40700, Loss: 11179.3212890625\nEpoch 40800, Loss: 11179.3212890625\nEpoch 40900, Loss: 11179.3203125\nEpoch 41000, Loss: 11179.3203125\nEpoch 41100, Loss: 11179.322265625\nEpoch 41200, Loss: 11179.3193359375\nEpoch 41300, Loss: 11179.3203125\nEpoch 41400, Loss: 11179.3203125\nEpoch 41500, Loss: 11179.3203125\nEpoch 41600, Loss: 11179.3193359375\nEpoch 41700, Loss: 11179.318359375\nEpoch 41800, Loss: 11179.3193359375\nEpoch 41900, Loss: 11179.318359375\nEpoch 42000, Loss: 11179.3203125\nEpoch 42100, Loss: 11179.3203125\nEpoch 42200, Loss: 11179.318359375\nEpoch 42300, Loss: 11179.3212890625\nEpoch 42400, Loss: 11179.3193359375\nEpoch 42500, Loss: 11179.322265625\nEpoch 42600, Loss: 11179.3203125\nEpoch 42700, Loss: 11179.3203125\nEpoch 42800, Loss: 11179.3193359375\nEpoch 42900, Loss: 11179.3193359375\nEpoch 43000, Loss: 11179.3203125\nEpoch 43100, Loss: 11179.3193359375\nEpoch 43200, Loss: 11179.3203125\nEpoch 43300, Loss: 11179.3203125\nEpoch 43400, Loss: 11179.3203125\nEpoch 43500, Loss: 11179.3203125\nEpoch 43600, Loss: 11179.3193359375\nEpoch 43700, Loss: 11179.3193359375\nEpoch 43800, Loss: 11179.3203125\nEpoch 43900, Loss: 11179.3193359375\nEpoch 44000, Loss: 11179.318359375\nEpoch 44100, Loss: 11179.322265625\nEpoch 44200, Loss: 11179.3212890625\nEpoch 44300, Loss: 11179.3193359375\nEpoch 44400, Loss: 11179.3212890625\nEpoch 44500, Loss: 11179.318359375\nEpoch 44600, Loss: 11179.3193359375\nEpoch 44700, Loss: 11179.318359375\nEpoch 44800, Loss: 11179.318359375\nEpoch 44900, Loss: 11179.3203125\nEpoch 45000, Loss: 11179.3193359375\nEpoch 45100, Loss: 11179.3203125\nEpoch 45200, Loss: 11179.3212890625\nEpoch 45300, Loss: 11179.3203125\nEpoch 45400, Loss: 11179.318359375\nEpoch 45500, Loss: 11179.322265625\nEpoch 45600, Loss: 11179.3193359375\nEpoch 45700, Loss: 11179.3203125\nEpoch 45800, Loss: 11179.3193359375\nEpoch 45900, Loss: 11179.3193359375\nEpoch 46000, Loss: 11179.3203125\nEpoch 46100, Loss: 11179.3203125\nEpoch 46200, Loss: 11179.322265625\nEpoch 46300, Loss: 11179.318359375\nEpoch 46400, Loss: 11179.318359375\nEpoch 46500, Loss: 11179.3203125\nEpoch 46600, Loss: 11179.3203125\nEpoch 46700, Loss: 11179.318359375\nEpoch 46800, Loss: 11179.3212890625\nEpoch 46900, Loss: 11179.318359375\nEpoch 47000, Loss: 11179.3203125\nEpoch 47100, Loss: 11179.3212890625\nEpoch 47200, Loss: 11179.3203125\nEpoch 47300, Loss: 11179.3193359375\nEpoch 47400, Loss: 11179.3203125\nEpoch 47500, Loss: 11179.3203125\nEpoch 47600, Loss: 11179.3203125\nEpoch 47700, Loss: 11179.3193359375\nEpoch 47800, Loss: 11179.3203125\nEpoch 47900, Loss: 11179.3193359375\nEpoch 48000, Loss: 11179.318359375\nEpoch 48100, Loss: 11179.318359375\nEpoch 48200, Loss: 11179.3193359375\nEpoch 48300, Loss: 11179.3212890625\nEpoch 48400, Loss: 11179.3203125\nEpoch 48500, Loss: 11179.3203125\nEpoch 48600, Loss: 11179.3212890625\nEpoch 48700, Loss: 11179.3203125\nEpoch 48800, Loss: 11179.3212890625\nEpoch 48900, Loss: 11179.3203125\nEpoch 49000, Loss: 11179.3193359375\nEpoch 49100, Loss: 11179.3203125\nEpoch 49200, Loss: 11179.318359375\nEpoch 49300, Loss: 11179.3203125\nEpoch 49400, Loss: 11179.3193359375\nEpoch 49500, Loss: 11179.3193359375\nEpoch 49600, Loss: 11179.3193359375\nEpoch 49700, Loss: 11179.3193359375\nEpoch 49800, Loss: 11179.3203125\nEpoch 49900, Loss: 11179.3193359375\nEpoch 0, Loss: 63828.96875\nEpoch 100, Loss: 60951.91796875\nEpoch 200, Loss: 58261.71875\nEpoch 300, Loss: 55747.71484375\nEpoch 400, Loss: 53396.109375\nEpoch 500, Loss: 51196.2421875\nEpoch 600, Loss: 49136.546875\nEpoch 700, Loss: 47208.14453125\nEpoch 800, Loss: 45405.6875\nEpoch 900, Loss: 43721.14453125\nEpoch 1000, Loss: 42146.89453125\nEpoch 1100, Loss: 40676.140625\nEpoch 1200, Loss: 39302.50390625\nEpoch 1300, Loss: 38021.40625\nEpoch 1400, Loss: 36826.72265625\nEpoch 1500, Loss: 35711.71484375\nEpoch 1600, Loss: 34670.78515625\nEpoch 1700, Loss: 33698.70703125\nEpoch 1800, Loss: 32790.625\nEpoch 1900, Loss: 31942.009765625\nEpoch 2000, Loss: 31150.509765625\nEpoch 2100, Loss: 30411.07421875\nEpoch 2200, Loss: 29720.75390625\nEpoch 2300, Loss: 29077.54296875\nEpoch 2400, Loss: 28475.6171875\nEpoch 2500, Loss: 27911.49609375\nEpoch 2600, Loss: 27382.27734375\nEpoch 2700, Loss: 26885.330078125\nEpoch 2800, Loss: 26418.240234375\nEpoch 2900, Loss: 25978.783203125\nEpoch 3000, Loss: 25564.8984375\nEpoch 3100, Loss: 25174.65625\nEpoch 3200, Loss: 24806.251953125\nEpoch 3300, Loss: 24458.861328125\nEpoch 3400, Loss: 24134.13671875\nEpoch 3500, Loss: 23826.8125\nEpoch 3600, Loss: 23534.8203125\nEpoch 3700, Loss: 23256.767578125\nEpoch 3800, Loss: 22991.38671875\nEpoch 3900, Loss: 22739.16796875\nEpoch 4000, Loss: 22497.76171875\nEpoch 4100, Loss: 22265.779296875\nEpoch 4200, Loss: 22042.32421875\nEpoch 4300, Loss: 21827.630859375\nEpoch 4400, Loss: 21620.599609375\nEpoch 4500, Loss: 21419.888671875\nEpoch 4600, Loss: 21224.951171875\nEpoch 4700, Loss: 21035.294921875\nEpoch 4800, Loss: 20850.515625\nEpoch 4900, Loss: 20670.251953125\nEpoch 5000, Loss: 20494.189453125\nEpoch 5100, Loss: 20322.0625\nEpoch 5200, Loss: 20153.6328125\nEpoch 5300, Loss: 19988.69921875\nEpoch 5400, Loss: 19827.07421875\nEpoch 5500, Loss: 19668.86328125\nEpoch 5600, Loss: 19514.6015625\nEpoch 5700, Loss: 19363.208984375\nEpoch 5800, Loss: 19214.5546875\nEpoch 5900, Loss: 19068.53515625\nEpoch 6000, Loss: 18925.044921875\nEpoch 6100, Loss: 18783.994140625\nEpoch 6200, Loss: 18645.30859375\nEpoch 6300, Loss: 18508.90625\nEpoch 6400, Loss: 18374.71875\nEpoch 6500, Loss: 18242.681640625\nEpoch 6600, Loss: 18112.73828125\nEpoch 6700, Loss: 17984.830078125\nEpoch 6800, Loss: 17859.80859375\nEpoch 6900, Loss: 17737.71875\nEpoch 7000, Loss: 17617.603515625\nEpoch 7100, Loss: 17499.25390625\nEpoch 7200, Loss: 17382.630859375\nEpoch 7300, Loss: 17267.685546875\nEpoch 7400, Loss: 17154.400390625\nEpoch 7500, Loss: 17043.71484375\nEpoch 7600, Loss: 16935.220703125\nEpoch 7700, Loss: 16828.16015625\nEpoch 7800, Loss: 16722.51171875\nEpoch 7900, Loss: 16618.2578125\nEpoch 8000, Loss: 16515.3828125\nEpoch 8100, Loss: 16413.98828125\nEpoch 8200, Loss: 16314.349609375\nEpoch 8300, Loss: 16216.10546875\nEpoch 8400, Loss: 16119.2470703125\nEpoch 8500, Loss: 16023.7373046875\nEpoch 8600, Loss: 15929.5615234375\nEpoch 8700, Loss: 15836.6884765625\nEpoch 8800, Loss: 15745.1103515625\nEpoch 8900, Loss: 15654.7998046875\nEpoch 9000, Loss: 15565.751953125\nEpoch 9100, Loss: 15477.947265625\nEpoch 9200, Loss: 15391.3720703125\nEpoch 9300, Loss: 15306.017578125\nEpoch 9400, Loss: 15221.8720703125\nEpoch 9500, Loss: 15138.93359375\nEpoch 9600, Loss: 15057.1845703125\nEpoch 9700, Loss: 14976.619140625\nEpoch 9800, Loss: 14897.234375\nEpoch 9900, Loss: 14819.0126953125\nEpoch 10000, Loss: 14741.939453125\nEpoch 10100, Loss: 14666.005859375\nEpoch 10200, Loss: 14591.193359375\nEpoch 10300, Loss: 14517.4931640625\nEpoch 10400, Loss: 14444.8935546875\nEpoch 10500, Loss: 14373.4189453125\nEpoch 10600, Loss: 14303.146484375\nEpoch 10700, Loss: 14234.068359375\nEpoch 10800, Loss: 14166.572265625\nEpoch 10900, Loss: 14100.4033203125\nEpoch 11000, Loss: 14035.53125\nEpoch 11100, Loss: 13971.9111328125\nEpoch 11200, Loss: 13909.4345703125\nEpoch 11300, Loss: 13848.025390625\nEpoch 11400, Loss: 13787.638671875\nEpoch 11500, Loss: 13728.373046875\nEpoch 11600, Loss: 13670.5986328125\nEpoch 11700, Loss: 13613.9169921875\nEpoch 11800, Loss: 13558.3330078125\nEpoch 11900, Loss: 13504.130859375\nEpoch 12000, Loss: 13451.158203125\nEpoch 12100, Loss: 13399.3603515625\nEpoch 12200, Loss: 13348.6923828125\nEpoch 12300, Loss: 13299.1142578125\nEpoch 12400, Loss: 13250.5634765625\nEpoch 12500, Loss: 13202.94140625\nEpoch 12600, Loss: 13156.1904296875\nEpoch 12700, Loss: 13110.28125\nEpoch 12800, Loss: 13065.2490234375\nEpoch 12900, Loss: 13021.0712890625\nEpoch 13000, Loss: 12977.873046875\nEpoch 13100, Loss: 12935.673828125\nEpoch 13200, Loss: 12894.373046875\nEpoch 13300, Loss: 12853.96484375\nEpoch 13400, Loss: 12814.43359375\nEpoch 13500, Loss: 12775.759765625\nEpoch 13600, Loss: 12737.95703125\nEpoch 13700, Loss: 12701.005859375\nEpoch 13800, Loss: 12664.9716796875\nEpoch 13900, Loss: 12629.7802734375\nEpoch 14000, Loss: 12595.421875\nEpoch 14100, Loss: 12561.8671875\nEpoch 14200, Loss: 12529.1083984375\nEpoch 14300, Loss: 12497.111328125\nEpoch 14400, Loss: 12465.8857421875\nEpoch 14500, Loss: 12435.392578125\nEpoch 14600, Loss: 12405.6259765625\nEpoch 14700, Loss: 12376.578125\nEpoch 14800, Loss: 12348.232421875\nEpoch 14900, Loss: 12320.5869140625\nEpoch 15000, Loss: 12293.63671875\nEpoch 15100, Loss: 12267.515625\nEpoch 15200, Loss: 12242.1953125\nEpoch 15300, Loss: 12217.6123046875\nEpoch 15400, Loss: 12193.7109375\nEpoch 15500, Loss: 12170.4833984375\nEpoch 15600, Loss: 12147.8984375\nEpoch 15700, Loss: 12126.4892578125\nEpoch 15800, Loss: 12105.87890625\nEpoch 15900, Loss: 12085.9345703125\nEpoch 16000, Loss: 12066.5869140625\nEpoch 16100, Loss: 12047.7939453125\nEpoch 16200, Loss: 12029.5556640625\nEpoch 16300, Loss: 12011.8408203125\nEpoch 16400, Loss: 11994.6328125\nEpoch 16500, Loss: 11977.912109375\nEpoch 16600, Loss: 11961.66796875\nEpoch 16700, Loss: 11945.8837890625\nEpoch 16800, Loss: 11930.5634765625\nEpoch 16900, Loss: 11915.68359375\nEpoch 17000, Loss: 11901.361328125\nEpoch 17100, Loss: 11887.55078125\nEpoch 17200, Loss: 11874.1728515625\nEpoch 17300, Loss: 11861.205078125\nEpoch 17400, Loss: 11848.650390625\nEpoch 17500, Loss: 11836.501953125\nEpoch 17600, Loss: 11824.7470703125\nEpoch 17700, Loss: 11813.380859375\nEpoch 17800, Loss: 11802.408203125\nEpoch 17900, Loss: 11791.8076171875\nEpoch 18000, Loss: 11781.5810546875\nEpoch 18100, Loss: 11771.720703125\nEpoch 18200, Loss: 11762.212890625\nEpoch 18300, Loss: 11753.0498046875\nEpoch 18400, Loss: 11744.216796875\nEpoch 18500, Loss: 11735.71484375\nEpoch 18600, Loss: 11727.521484375\nEpoch 18700, Loss: 11719.63671875\nEpoch 18800, Loss: 11712.0439453125\nEpoch 18900, Loss: 11704.7412109375\nEpoch 19000, Loss: 11697.71875\nEpoch 19100, Loss: 11690.9638671875\nEpoch 19200, Loss: 11684.4658203125\nEpoch 19300, Loss: 11678.2109375\nEpoch 19400, Loss: 11672.177734375\nEpoch 19500, Loss: 11666.4541015625\nEpoch 19600, Loss: 11661.2333984375\nEpoch 19700, Loss: 11656.287109375\nEpoch 19800, Loss: 11651.5517578125\nEpoch 19900, Loss: 11647.005859375\nEpoch 20000, Loss: 11642.6240234375\nEpoch 20100, Loss: 11638.3837890625\nEpoch 20200, Loss: 11634.271484375\nEpoch 20300, Loss: 11630.271484375\nEpoch 20400, Loss: 11626.3701171875\nEpoch 20500, Loss: 11622.5546875\nEpoch 20600, Loss: 11618.8095703125\nEpoch 20700, Loss: 11615.134765625\nEpoch 20800, Loss: 11611.51171875\nEpoch 20900, Loss: 11607.9443359375\nEpoch 21000, Loss: 11604.4130859375\nEpoch 21100, Loss: 11600.927734375\nEpoch 21200, Loss: 11597.4755859375\nEpoch 21300, Loss: 11594.06640625\nEpoch 21400, Loss: 11590.6884765625\nEpoch 21500, Loss: 11587.3486328125\nEpoch 21600, Loss: 11584.0458984375\nEpoch 21700, Loss: 11580.7841796875\nEpoch 21800, Loss: 11577.5556640625\nEpoch 21900, Loss: 11574.375\nEpoch 22000, Loss: 11571.232421875\nEpoch 22100, Loss: 11568.1455078125\nEpoch 22200, Loss: 11565.1337890625\nEpoch 22300, Loss: 11562.197265625\nEpoch 22400, Loss: 11559.3310546875\nEpoch 22500, Loss: 11556.552734375\nEpoch 22600, Loss: 11553.857421875\nEpoch 22700, Loss: 11551.2578125\nEpoch 22800, Loss: 11548.744140625\nEpoch 22900, Loss: 11546.322265625\nEpoch 23000, Loss: 11543.982421875\nEpoch 23100, Loss: 11541.728515625\nEpoch 23200, Loss: 11539.546875\nEpoch 23300, Loss: 11537.439453125\nEpoch 23400, Loss: 11535.400390625\nEpoch 23500, Loss: 11533.44140625\nEpoch 23600, Loss: 11531.55078125\nEpoch 23700, Loss: 11529.7294921875\nEpoch 23800, Loss: 11527.9794921875\nEpoch 23900, Loss: 11526.294921875\nEpoch 24000, Loss: 11524.671875\nEpoch 24100, Loss: 11523.1162109375\nEpoch 24200, Loss: 11521.625\nEpoch 24300, Loss: 11520.1923828125\nEpoch 24400, Loss: 11518.8193359375\nEpoch 24500, Loss: 11517.4951171875\nEpoch 24600, Loss: 11516.22265625\nEpoch 24700, Loss: 11515.0009765625\nEpoch 24800, Loss: 11513.8369140625\nEpoch 24900, Loss: 11512.7080078125\nEpoch 25000, Loss: 11511.6376953125\nEpoch 25100, Loss: 11510.6044921875\nEpoch 25200, Loss: 11509.6240234375\nEpoch 25300, Loss: 11508.6845703125\nEpoch 25400, Loss: 11507.79296875\nEpoch 25500, Loss: 11506.94140625\nEpoch 25600, Loss: 11506.134765625\nEpoch 25700, Loss: 11505.365234375\nEpoch 25800, Loss: 11504.6435546875\nEpoch 25900, Loss: 11503.9599609375\nEpoch 26000, Loss: 11503.3125\nEpoch 26100, Loss: 11502.7080078125\nEpoch 26200, Loss: 11502.134765625\nEpoch 26300, Loss: 11501.603515625\nEpoch 26400, Loss: 11501.1015625\nEpoch 26500, Loss: 11500.63671875\nEpoch 26600, Loss: 11500.208984375\nEpoch 26700, Loss: 11499.8046875\nEpoch 26800, Loss: 11499.4365234375\nEpoch 26900, Loss: 11499.095703125\nEpoch 27000, Loss: 11498.78125\nEpoch 27100, Loss: 11498.4912109375\nEpoch 27200, Loss: 11498.21875\nEpoch 27300, Loss: 11497.9736328125\nEpoch 27400, Loss: 11497.740234375\nEpoch 27500, Loss: 11497.525390625\nEpoch 27600, Loss: 11497.3330078125\nEpoch 27700, Loss: 11497.154296875\nEpoch 27800, Loss: 11496.990234375\nEpoch 27900, Loss: 11496.837890625\nEpoch 28000, Loss: 11496.7001953125\nEpoch 28100, Loss: 11496.576171875\nEpoch 28200, Loss: 11496.4638671875\nEpoch 28300, Loss: 11496.3671875\nEpoch 28400, Loss: 11496.28125\nEpoch 28500, Loss: 11496.201171875\nEpoch 28600, Loss: 11496.1328125\nEpoch 28700, Loss: 11496.060546875\nEpoch 28800, Loss: 11495.9990234375\nEpoch 28900, Loss: 11495.9423828125\nEpoch 29000, Loss: 11495.8896484375\nEpoch 29100, Loss: 11495.841796875\nEpoch 29200, Loss: 11495.7978515625\nEpoch 29300, Loss: 11495.755859375\nEpoch 29400, Loss: 11495.7197265625\nEpoch 29500, Loss: 11495.68359375\nEpoch 29600, Loss: 11495.6494140625\nEpoch 29700, Loss: 11495.625\nEpoch 29800, Loss: 11495.599609375\nEpoch 29900, Loss: 11495.576171875\nEpoch 30000, Loss: 11495.5556640625\nEpoch 30100, Loss: 11495.5380859375\nEpoch 30200, Loss: 11495.5234375\nEpoch 30300, Loss: 11495.509765625\nEpoch 30400, Loss: 11495.4990234375\nEpoch 30500, Loss: 11495.48828125\nEpoch 30600, Loss: 11495.4814453125\nEpoch 30700, Loss: 11495.4765625\nEpoch 30800, Loss: 11495.466796875\nEpoch 30900, Loss: 11495.4638671875\nEpoch 31000, Loss: 11495.462890625\nEpoch 31100, Loss: 11495.45703125\nEpoch 31200, Loss: 11495.4560546875\nEpoch 31300, Loss: 11495.453125\nEpoch 31400, Loss: 11495.4501953125\nEpoch 31500, Loss: 11495.4482421875\nEpoch 31600, Loss: 11495.44921875\nEpoch 31700, Loss: 11495.4462890625\nEpoch 31800, Loss: 11495.4482421875\nEpoch 31900, Loss: 11495.44921875\nEpoch 32000, Loss: 11495.4453125\nEpoch 32100, Loss: 11495.4453125\nEpoch 32200, Loss: 11495.4462890625\nEpoch 32300, Loss: 11495.4462890625\nEpoch 32400, Loss: 11495.447265625\nEpoch 32500, Loss: 11495.4443359375\nEpoch 32600, Loss: 11495.4443359375\nEpoch 32700, Loss: 11495.4453125\nEpoch 32800, Loss: 11495.4443359375\nEpoch 32900, Loss: 11495.4423828125\nEpoch 33000, Loss: 11495.4482421875\nEpoch 33100, Loss: 11495.443359375\nEpoch 33200, Loss: 11495.4453125\nEpoch 33300, Loss: 11495.447265625\nEpoch 33400, Loss: 11495.4443359375\nEpoch 33500, Loss: 11495.447265625\nEpoch 33600, Loss: 11495.4462890625\nEpoch 33700, Loss: 11495.4462890625\nEpoch 33800, Loss: 11495.4453125\nEpoch 33900, Loss: 11495.4462890625\nEpoch 34000, Loss: 11495.4482421875\nEpoch 34100, Loss: 11495.447265625\nEpoch 34200, Loss: 11495.4462890625\nEpoch 34300, Loss: 11495.4462890625\nEpoch 34400, Loss: 11495.4443359375\nEpoch 34500, Loss: 11495.447265625\nEpoch 34600, Loss: 11495.4453125\nEpoch 34700, Loss: 11495.4453125\nEpoch 34800, Loss: 11495.4482421875\nEpoch 34900, Loss: 11495.4443359375\nEpoch 35000, Loss: 11495.4443359375\nEpoch 35100, Loss: 11495.4462890625\nEpoch 35200, Loss: 11495.4443359375\nEpoch 35300, Loss: 11495.4453125\nEpoch 35400, Loss: 11495.4482421875\nEpoch 35500, Loss: 11495.4443359375\nEpoch 35600, Loss: 11495.4462890625\nEpoch 35700, Loss: 11495.4453125\nEpoch 35800, Loss: 11495.4443359375\nEpoch 35900, Loss: 11495.4453125\nEpoch 36000, Loss: 11495.4453125\nEpoch 36100, Loss: 11495.4443359375\nEpoch 36200, Loss: 11495.4453125\nEpoch 36300, Loss: 11495.4453125\nEpoch 36400, Loss: 11495.4462890625\nEpoch 36500, Loss: 11495.4453125\nEpoch 36600, Loss: 11495.44921875\nEpoch 36700, Loss: 11495.447265625\nEpoch 36800, Loss: 11495.4453125\nEpoch 36900, Loss: 11495.443359375\nEpoch 37000, Loss: 11495.4443359375\nEpoch 37100, Loss: 11495.4462890625\nEpoch 37200, Loss: 11495.443359375\nEpoch 37300, Loss: 11495.447265625\nEpoch 37400, Loss: 11495.4453125\nEpoch 37500, Loss: 11495.4482421875\nEpoch 37600, Loss: 11495.4462890625\nEpoch 37700, Loss: 11495.4443359375\nEpoch 37800, Loss: 11495.4453125\nEpoch 37900, Loss: 11495.443359375\nEpoch 38000, Loss: 11495.447265625\nEpoch 38100, Loss: 11495.4462890625\nEpoch 38200, Loss: 11495.4443359375\nEpoch 38300, Loss: 11495.443359375\nEpoch 38400, Loss: 11495.4443359375\nEpoch 38500, Loss: 11495.4501953125\nEpoch 38600, Loss: 11495.447265625\nEpoch 38700, Loss: 11495.4453125\nEpoch 38800, Loss: 11495.447265625\nEpoch 38900, Loss: 11495.447265625\nEpoch 39000, Loss: 11495.443359375\nEpoch 39100, Loss: 11495.4443359375\nEpoch 39200, Loss: 11495.447265625\nEpoch 39300, Loss: 11495.4443359375\nEpoch 39400, Loss: 11495.4462890625\nEpoch 39500, Loss: 11495.447265625\nEpoch 39600, Loss: 11495.4453125\nEpoch 39700, Loss: 11495.4453125\nEpoch 39800, Loss: 11495.4443359375\nEpoch 39900, Loss: 11495.4453125\nEpoch 40000, Loss: 11495.4462890625\nEpoch 40100, Loss: 11495.4482421875\nEpoch 40200, Loss: 11495.447265625\nEpoch 40300, Loss: 11495.4453125\nEpoch 40400, Loss: 11495.4453125\nEpoch 40500, Loss: 11495.4453125\nEpoch 40600, Loss: 11495.447265625\nEpoch 40700, Loss: 11495.4443359375\nEpoch 40800, Loss: 11495.4453125\nEpoch 40900, Loss: 11495.4462890625\nEpoch 41000, Loss: 11495.4443359375\nEpoch 41100, Loss: 11495.4453125\nEpoch 41200, Loss: 11495.4443359375\nEpoch 41300, Loss: 11495.4453125\nEpoch 41400, Loss: 11495.4453125\nEpoch 41500, Loss: 11495.4443359375\nEpoch 41600, Loss: 11495.4453125\nEpoch 41700, Loss: 11495.4462890625\nEpoch 41800, Loss: 11495.4453125\nEpoch 41900, Loss: 11495.447265625\nEpoch 42000, Loss: 11495.447265625\nEpoch 42100, Loss: 11495.4443359375\nEpoch 42200, Loss: 11495.4462890625\nEpoch 42300, Loss: 11495.4443359375\nEpoch 42400, Loss: 11495.447265625\nEpoch 42500, Loss: 11495.4453125\nEpoch 42600, Loss: 11495.4443359375\nEpoch 42700, Loss: 11495.4453125\nEpoch 42800, Loss: 11495.4443359375\nEpoch 42900, Loss: 11495.4482421875\nEpoch 43000, Loss: 11495.447265625\nEpoch 43100, Loss: 11495.4453125\nEpoch 43200, Loss: 11495.447265625\nEpoch 43300, Loss: 11495.4453125\nEpoch 43400, Loss: 11495.4462890625\nEpoch 43500, Loss: 11495.4453125\nEpoch 43600, Loss: 11495.4462890625\nEpoch 43700, Loss: 11495.4443359375\nEpoch 43800, Loss: 11495.4453125\nEpoch 43900, Loss: 11495.4453125\nEpoch 44000, Loss: 11495.4443359375\nEpoch 44100, Loss: 11495.4443359375\nEpoch 44200, Loss: 11495.4453125\nEpoch 44300, Loss: 11495.4453125\nEpoch 44400, Loss: 11495.4453125\nEpoch 44500, Loss: 11495.4443359375\nEpoch 44600, Loss: 11495.447265625\nEpoch 44700, Loss: 11495.4443359375\nEpoch 44800, Loss: 11495.4453125\nEpoch 44900, Loss: 11495.4482421875\nEpoch 45000, Loss: 11495.447265625\nEpoch 45100, Loss: 11495.4453125\nEpoch 45200, Loss: 11495.4453125\nEpoch 45300, Loss: 11495.447265625\nEpoch 45400, Loss: 11495.447265625\nEpoch 45500, Loss: 11495.4453125\nEpoch 45600, Loss: 11495.443359375\nEpoch 45700, Loss: 11495.4453125\nEpoch 45800, Loss: 11495.447265625\nEpoch 45900, Loss: 11495.4462890625\nEpoch 46000, Loss: 11495.4462890625\nEpoch 46100, Loss: 11495.4453125\nEpoch 46200, Loss: 11495.447265625\nEpoch 46300, Loss: 11495.4462890625\nEpoch 46400, Loss: 11495.4443359375\nEpoch 46500, Loss: 11495.4453125\nEpoch 46600, Loss: 11495.447265625\nEpoch 46700, Loss: 11495.443359375\nEpoch 46800, Loss: 11495.4453125\nEpoch 46900, Loss: 11495.4443359375\nEpoch 47000, Loss: 11495.4443359375\nEpoch 47100, Loss: 11495.4453125\nEpoch 47200, Loss: 11495.4443359375\nEpoch 47300, Loss: 11495.447265625\nEpoch 47400, Loss: 11495.443359375\nEpoch 47500, Loss: 11495.4482421875\nEpoch 47600, Loss: 11495.447265625\nEpoch 47700, Loss: 11495.4423828125\nEpoch 47800, Loss: 11495.4453125\nEpoch 47900, Loss: 11495.443359375\nEpoch 48000, Loss: 11495.4453125\nEpoch 48100, Loss: 11495.4423828125\nEpoch 48200, Loss: 11495.4443359375\nEpoch 48300, Loss: 11495.4443359375\nEpoch 48400, Loss: 11495.4462890625\nEpoch 48500, Loss: 11495.4453125\nEpoch 48600, Loss: 11495.4453125\nEpoch 48700, Loss: 11495.4462890625\nEpoch 48800, Loss: 11495.4453125\nEpoch 48900, Loss: 11495.4462890625\nEpoch 49000, Loss: 11495.4453125\nEpoch 49100, Loss: 11495.443359375\nEpoch 49200, Loss: 11495.4462890625\nEpoch 49300, Loss: 11495.4453125\nEpoch 49400, Loss: 11495.4443359375\nEpoch 49500, Loss: 11495.4462890625\nEpoch 49600, Loss: 11495.4453125\nEpoch 49700, Loss: 11495.4462890625\nEpoch 49800, Loss: 11495.4443359375\nEpoch 49900, Loss: 11495.4453125\nEpoch 0, Loss: 63828.96875\nEpoch 100, Loss: 60959.69140625\nEpoch 200, Loss: 58276.8359375\nEpoch 300, Loss: 55770.62109375\nEpoch 400, Loss: 53427.93359375\nEpoch 500, Loss: 51237.875\nEpoch 600, Loss: 49187.65234375\nEpoch 700, Loss: 47268.4921875\nEpoch 800, Loss: 45475.15234375\nEpoch 900, Loss: 43798.89453125\nEpoch 1000, Loss: 42232.31640625\nEpoch 1100, Loss: 40768.59765625\nEpoch 1200, Loss: 39401.58203125\nEpoch 1300, Loss: 38127.8671875\nEpoch 1400, Loss: 36939.33203125\nEpoch 1500, Loss: 35829.96484375\nEpoch 1600, Loss: 34794.1875\nEpoch 1700, Loss: 33826.79296875\nEpoch 1800, Loss: 32922.984375\nEpoch 1900, Loss: 32079.12890625\nEpoch 2000, Loss: 31292.41796875\nEpoch 2100, Loss: 30557.271484375\nEpoch 2200, Loss: 29872.8828125\nEpoch 2300, Loss: 29234.62890625\nEpoch 2400, Loss: 28637.119140625\nEpoch 2500, Loss: 28077.15625\nEpoch 2600, Loss: 27551.8359375\nEpoch 2700, Loss: 27058.541015625\nEpoch 2800, Loss: 26594.876953125\nEpoch 2900, Loss: 26158.638671875\nEpoch 3000, Loss: 25747.77734375\nEpoch 3100, Loss: 25360.390625\nEpoch 3200, Loss: 24994.689453125\nEpoch 3300, Loss: 24651.552734375\nEpoch 3400, Loss: 24331.083984375\nEpoch 3500, Loss: 24027.318359375\nEpoch 3600, Loss: 23738.724609375\nEpoch 3700, Loss: 23463.91015625\nEpoch 3800, Loss: 23202.5625\nEpoch 3900, Loss: 22954.126953125\nEpoch 4000, Loss: 22715.97265625\nEpoch 4100, Loss: 22487.10546875\nEpoch 4200, Loss: 22266.83203125\nEpoch 4300, Loss: 22056.04296875\nEpoch 4400, Loss: 21852.123046875\nEpoch 4500, Loss: 21654.404296875\nEpoch 4600, Loss: 21462.345703125\nEpoch 4700, Loss: 21275.46484375\nEpoch 4800, Loss: 21093.357421875\nEpoch 4900, Loss: 20915.673828125\nEpoch 5000, Loss: 20742.09765625\nEpoch 5100, Loss: 20572.375\nEpoch 5200, Loss: 20406.26953125\nEpoch 5300, Loss: 20243.580078125\nEpoch 5400, Loss: 20084.630859375\nEpoch 5500, Loss: 19929.658203125\nEpoch 5600, Loss: 19777.62109375\nEpoch 5700, Loss: 19628.380859375\nEpoch 5800, Loss: 19481.8125\nEpoch 5900, Loss: 19337.814453125\nEpoch 6000, Loss: 19196.279296875\nEpoch 6100, Loss: 19057.12109375\nEpoch 6200, Loss: 18920.2578125\nEpoch 6300, Loss: 18785.6171875\nEpoch 6400, Loss: 18653.125\nEpoch 6500, Loss: 18522.71875\nEpoch 6600, Loss: 18394.419921875\nEpoch 6700, Loss: 18269.17578125\nEpoch 6800, Loss: 18146.705078125\nEpoch 6900, Loss: 18026.458984375\nEpoch 7000, Loss: 17907.974609375\nEpoch 7100, Loss: 17791.1875\nEpoch 7200, Loss: 17676.169921875\nEpoch 7300, Loss: 17562.771484375\nEpoch 7400, Loss: 17450.880859375\nEpoch 7500, Loss: 17341.337890625\nEpoch 7600, Loss: 17234.4296875\nEpoch 7700, Loss: 17128.87109375\nEpoch 7800, Loss: 17024.693359375\nEpoch 7900, Loss: 16922.310546875\nEpoch 8000, Loss: 16821.3125\nEpoch 8100, Loss: 16721.65234375\nEpoch 8200, Loss: 16623.32421875\nEpoch 8300, Loss: 16526.314453125\nEpoch 8400, Loss: 16430.595703125\nEpoch 8500, Loss: 16336.1650390625\nEpoch 8600, Loss: 16242.9970703125\nEpoch 8700, Loss: 16151.083984375\nEpoch 8800, Loss: 16060.400390625\nEpoch 8900, Loss: 15970.9345703125\nEpoch 9000, Loss: 15882.6630859375\nEpoch 9100, Loss: 15795.5751953125\nEpoch 9200, Loss: 15709.6640625\nEpoch 9300, Loss: 15624.9091796875\nEpoch 9400, Loss: 15541.3095703125\nEpoch 9500, Loss: 15458.8544921875\nEpoch 9600, Loss: 15377.53515625\nEpoch 9700, Loss: 15297.349609375\nEpoch 9800, Loss: 15218.291015625\nEpoch 9900, Loss: 15140.3505859375\nEpoch 10000, Loss: 15063.5234375\nEpoch 10100, Loss: 14987.7998046875\nEpoch 10200, Loss: 14913.1767578125\nEpoch 10300, Loss: 14839.650390625\nEpoch 10400, Loss: 14767.71875\nEpoch 10500, Loss: 14697.37890625\nEpoch 10600, Loss: 14628.3896484375\nEpoch 10700, Loss: 14560.720703125\nEpoch 10800, Loss: 14494.337890625\nEpoch 10900, Loss: 14429.1474609375\nEpoch 11000, Loss: 14365.060546875\nEpoch 11100, Loss: 14302.0146484375\nEpoch 11200, Loss: 14239.970703125\nEpoch 11300, Loss: 14178.9013671875\nEpoch 11400, Loss: 14119.515625\nEpoch 11500, Loss: 14061.595703125\nEpoch 11600, Loss: 14004.947265625\nEpoch 11700, Loss: 13949.517578125\nEpoch 11800, Loss: 13895.3017578125\nEpoch 11900, Loss: 13842.302734375\nEpoch 12000, Loss: 13790.4287109375\nEpoch 12100, Loss: 13739.658203125\nEpoch 12200, Loss: 13689.9306640625\nEpoch 12300, Loss: 13641.19921875\nEpoch 12400, Loss: 13593.4296875\nEpoch 12500, Loss: 13546.6123046875\nEpoch 12600, Loss: 13500.7509765625\nEpoch 12700, Loss: 13455.908203125\nEpoch 12800, Loss: 13412.095703125\nEpoch 12900, Loss: 13369.203125\nEpoch 13000, Loss: 13327.2001953125\nEpoch 13100, Loss: 13286.0703125\nEpoch 13200, Loss: 13245.7919921875\nEpoch 13300, Loss: 13206.357421875\nEpoch 13400, Loss: 13167.7666015625\nEpoch 13500, Loss: 13130.0439453125\nEpoch 13600, Loss: 13093.201171875\nEpoch 13700, Loss: 13057.1787109375\nEpoch 13800, Loss: 13021.9677734375\nEpoch 13900, Loss: 12987.5517578125\nEpoch 14000, Loss: 12953.90625\nEpoch 14100, Loss: 12921.03125\nEpoch 14200, Loss: 12888.8955078125\nEpoch 14300, Loss: 12857.5986328125\nEpoch 14400, Loss: 12827.2353515625\nEpoch 14500, Loss: 12797.6943359375\nEpoch 14600, Loss: 12768.9453125\nEpoch 14700, Loss: 12740.955078125\nEpoch 14800, Loss: 12713.6943359375\nEpoch 14900, Loss: 12687.14453125\nEpoch 15000, Loss: 12661.2880859375\nEpoch 15100, Loss: 12636.09765625\nEpoch 15200, Loss: 12611.5615234375\nEpoch 15300, Loss: 12587.76171875\nEpoch 15400, Loss: 12565.2568359375\nEpoch 15500, Loss: 12543.544921875\nEpoch 15600, Loss: 12522.5263671875\nEpoch 15700, Loss: 12502.1298828125\nEpoch 15800, Loss: 12482.3271484375\nEpoch 15900, Loss: 12463.0830078125\nEpoch 16000, Loss: 12444.37890625\nEpoch 16100, Loss: 12426.1953125\nEpoch 16200, Loss: 12408.5185546875\nEpoch 16300, Loss: 12391.3427734375\nEpoch 16400, Loss: 12374.65234375\nEpoch 16500, Loss: 12358.427734375\nEpoch 16600, Loss: 12342.7578125\nEpoch 16700, Loss: 12327.736328125\nEpoch 16800, Loss: 12313.1796875\nEpoch 16900, Loss: 12299.083984375\nEpoch 17000, Loss: 12285.4375\nEpoch 17100, Loss: 12272.21875\nEpoch 17200, Loss: 12259.4150390625\nEpoch 17300, Loss: 12247.013671875\nEpoch 17400, Loss: 12235.0126953125\nEpoch 17500, Loss: 12223.4013671875\nEpoch 17600, Loss: 12212.16015625\nEpoch 17700, Loss: 12201.2861328125\nEpoch 17800, Loss: 12190.775390625\nEpoch 17900, Loss: 12180.60546875\nEpoch 18000, Loss: 12170.7783203125\nEpoch 18100, Loss: 12161.2724609375\nEpoch 18200, Loss: 12152.083984375\nEpoch 18300, Loss: 12143.203125\nEpoch 18400, Loss: 12134.62109375\nEpoch 18500, Loss: 12126.31640625\nEpoch 18600, Loss: 12118.2919921875\nEpoch 18700, Loss: 12110.529296875\nEpoch 18800, Loss: 12103.0205078125\nEpoch 18900, Loss: 12095.7451171875\nEpoch 19000, Loss: 12088.6953125\nEpoch 19100, Loss: 12082.1044921875\nEpoch 19200, Loss: 12076.0068359375\nEpoch 19300, Loss: 12070.1962890625\nEpoch 19400, Loss: 12064.6064453125\nEpoch 19500, Loss: 12059.2177734375\nEpoch 19600, Loss: 12054.0048828125\nEpoch 19700, Loss: 12048.9560546875\nEpoch 19800, Loss: 12044.044921875\nEpoch 19900, Loss: 12039.2548828125\nEpoch 20000, Loss: 12034.5859375\nEpoch 20100, Loss: 12030.009765625\nEpoch 20200, Loss: 12025.521484375\nEpoch 20300, Loss: 12021.1083984375\nEpoch 20400, Loss: 12016.78515625\nEpoch 20500, Loss: 12012.5146484375\nEpoch 20600, Loss: 12008.3232421875\nEpoch 20700, Loss: 12004.18359375\nEpoch 20800, Loss: 12000.1259765625\nEpoch 20900, Loss: 11996.125\nEpoch 21000, Loss: 11992.2080078125\nEpoch 21100, Loss: 11988.361328125\nEpoch 21200, Loss: 11984.5888671875\nEpoch 21300, Loss: 11980.8916015625\nEpoch 21400, Loss: 11977.2685546875\nEpoch 21500, Loss: 11973.728515625\nEpoch 21600, Loss: 11970.2734375\nEpoch 21700, Loss: 11966.908203125\nEpoch 21800, Loss: 11963.638671875\nEpoch 21900, Loss: 11960.451171875\nEpoch 22000, Loss: 11957.35546875\nEpoch 22100, Loss: 11954.3603515625\nEpoch 22200, Loss: 11951.443359375\nEpoch 22300, Loss: 11948.6201171875\nEpoch 22400, Loss: 11945.890625\nEpoch 22500, Loss: 11943.2509765625\nEpoch 22600, Loss: 11940.69140625\nEpoch 22700, Loss: 11938.2177734375\nEpoch 22800, Loss: 11935.8349609375\nEpoch 22900, Loss: 11933.533203125\nEpoch 23000, Loss: 11931.3076171875\nEpoch 23100, Loss: 11929.1484375\nEpoch 23200, Loss: 11927.0576171875\nEpoch 23300, Loss: 11925.04296875\nEpoch 23400, Loss: 11923.091796875\nEpoch 23500, Loss: 11921.203125\nEpoch 23600, Loss: 11919.3779296875\nEpoch 23700, Loss: 11917.6171875\nEpoch 23800, Loss: 11915.919921875\nEpoch 23900, Loss: 11914.279296875\nEpoch 24000, Loss: 11912.7041015625\nEpoch 24100, Loss: 11911.1845703125\nEpoch 24200, Loss: 11909.728515625\nEpoch 24300, Loss: 11908.326171875\nEpoch 24400, Loss: 11906.9833984375\nEpoch 24500, Loss: 11905.6923828125\nEpoch 24600, Loss: 11904.46484375\nEpoch 24700, Loss: 11903.28125\nEpoch 24800, Loss: 11902.1455078125\nEpoch 24900, Loss: 11901.06640625\nEpoch 25000, Loss: 11900.0224609375\nEpoch 25100, Loss: 11899.025390625\nEpoch 25200, Loss: 11898.076171875\nEpoch 25300, Loss: 11897.1640625\nEpoch 25400, Loss: 11896.296875\nEpoch 25500, Loss: 11895.4755859375\nEpoch 25600, Loss: 11894.6826171875\nEpoch 25700, Loss: 11893.931640625\nEpoch 25800, Loss: 11893.22265625\nEpoch 25900, Loss: 11892.5517578125\nEpoch 26000, Loss: 11891.908203125\nEpoch 26100, Loss: 11891.30859375\nEpoch 26200, Loss: 11890.7470703125\nEpoch 26300, Loss: 11890.21484375\nEpoch 26400, Loss: 11889.71875\nEpoch 26500, Loss: 11889.2529296875\nEpoch 26600, Loss: 11888.8203125\nEpoch 26700, Loss: 11888.416015625\nEpoch 26800, Loss: 11888.041015625\nEpoch 26900, Loss: 11887.6953125\nEpoch 27000, Loss: 11887.369140625\nEpoch 27100, Loss: 11887.07421875\nEpoch 27200, Loss: 11886.7880859375\nEpoch 27300, Loss: 11886.5341796875\nEpoch 27400, Loss: 11886.2890625\nEpoch 27500, Loss: 11886.0673828125\nEpoch 27600, Loss: 11885.86328125\nEpoch 27700, Loss: 11885.6669921875\nEpoch 27800, Loss: 11885.4921875\nEpoch 27900, Loss: 11885.328125\nEpoch 28000, Loss: 11885.1796875\nEpoch 28100, Loss: 11885.0458984375\nEpoch 28200, Loss: 11884.927734375\nEpoch 28300, Loss: 11884.8154296875\nEpoch 28400, Loss: 11884.7119140625\nEpoch 28500, Loss: 11884.62109375\nEpoch 28600, Loss: 11884.53515625\nEpoch 28700, Loss: 11884.45703125\nEpoch 28800, Loss: 11884.37890625\nEpoch 28900, Loss: 11884.3115234375\nEpoch 29000, Loss: 11884.240234375\nEpoch 29100, Loss: 11884.1767578125\nEpoch 29200, Loss: 11884.12109375\nEpoch 29300, Loss: 11884.0673828125\nEpoch 29400, Loss: 11884.0146484375\nEpoch 29500, Loss: 11883.9697265625\nEpoch 29600, Loss: 11883.9248046875\nEpoch 29700, Loss: 11883.88671875\nEpoch 29800, Loss: 11883.8525390625\nEpoch 29900, Loss: 11883.8134765625\nEpoch 30000, Loss: 11883.78515625\nEpoch 30100, Loss: 11883.75390625\nEpoch 30200, Loss: 11883.728515625\nEpoch 30300, Loss: 11883.70703125\nEpoch 30400, Loss: 11883.685546875\nEpoch 30500, Loss: 11883.669921875\nEpoch 30600, Loss: 11883.6533203125\nEpoch 30700, Loss: 11883.6376953125\nEpoch 30800, Loss: 11883.625\nEpoch 30900, Loss: 11883.6162109375\nEpoch 31000, Loss: 11883.6044921875\nEpoch 31100, Loss: 11883.59375\nEpoch 31200, Loss: 11883.5830078125\nEpoch 31300, Loss: 11883.5791015625\nEpoch 31400, Loss: 11883.5712890625\nEpoch 31500, Loss: 11883.5654296875\nEpoch 31600, Loss: 11883.5595703125\nEpoch 31700, Loss: 11883.5537109375\nEpoch 31800, Loss: 11883.55078125\nEpoch 31900, Loss: 11883.544921875\nEpoch 32000, Loss: 11883.5400390625\nEpoch 32100, Loss: 11883.5361328125\nEpoch 32200, Loss: 11883.529296875\nEpoch 32300, Loss: 11883.5302734375\nEpoch 32400, Loss: 11883.53125\nEpoch 32500, Loss: 11883.5263671875\nEpoch 32600, Loss: 11883.5244140625\nEpoch 32700, Loss: 11883.5244140625\nEpoch 32800, Loss: 11883.521484375\nEpoch 32900, Loss: 11883.5166015625\nEpoch 33000, Loss: 11883.521484375\nEpoch 33100, Loss: 11883.51953125\nEpoch 33200, Loss: 11883.513671875\nEpoch 33300, Loss: 11883.5185546875\nEpoch 33400, Loss: 11883.515625\nEpoch 33500, Loss: 11883.513671875\nEpoch 33600, Loss: 11883.5126953125\nEpoch 33700, Loss: 11883.51171875\nEpoch 33800, Loss: 11883.5107421875\nEpoch 33900, Loss: 11883.515625\nEpoch 34000, Loss: 11883.51171875\nEpoch 34100, Loss: 11883.509765625\nEpoch 34200, Loss: 11883.5126953125\nEpoch 34300, Loss: 11883.5107421875\nEpoch 34400, Loss: 11883.5087890625\nEpoch 34500, Loss: 11883.51171875\nEpoch 34600, Loss: 11883.51171875\nEpoch 34700, Loss: 11883.51171875\nEpoch 34800, Loss: 11883.5087890625\nEpoch 34900, Loss: 11883.51171875\nEpoch 35000, Loss: 11883.509765625\nEpoch 35100, Loss: 11883.5146484375\nEpoch 35200, Loss: 11883.5087890625\nEpoch 35300, Loss: 11883.5087890625\nEpoch 35400, Loss: 11883.509765625\nEpoch 35500, Loss: 11883.51171875\nEpoch 35600, Loss: 11883.509765625\nEpoch 35700, Loss: 11883.5107421875\nEpoch 35800, Loss: 11883.51171875\nEpoch 35900, Loss: 11883.51171875\nEpoch 36000, Loss: 11883.5126953125\nEpoch 36100, Loss: 11883.5107421875\nEpoch 36200, Loss: 11883.51171875\nEpoch 36300, Loss: 11883.509765625\nEpoch 36400, Loss: 11883.5126953125\nEpoch 36500, Loss: 11883.5087890625\nEpoch 36600, Loss: 11883.513671875\nEpoch 36700, Loss: 11883.513671875\nEpoch 36800, Loss: 11883.5087890625\nEpoch 36900, Loss: 11883.515625\nEpoch 37000, Loss: 11883.5087890625\nEpoch 37100, Loss: 11883.5126953125\nEpoch 37200, Loss: 11883.509765625\nEpoch 37300, Loss: 11883.513671875\nEpoch 37400, Loss: 11883.515625\nEpoch 37500, Loss: 11883.509765625\nEpoch 37600, Loss: 11883.51171875\nEpoch 37700, Loss: 11883.513671875\nEpoch 37800, Loss: 11883.513671875\nEpoch 37900, Loss: 11883.51171875\nEpoch 38000, Loss: 11883.5087890625\nEpoch 38100, Loss: 11883.5087890625\nEpoch 38200, Loss: 11883.509765625\nEpoch 38300, Loss: 11883.5107421875\nEpoch 38400, Loss: 11883.5107421875\nEpoch 38500, Loss: 11883.509765625\nEpoch 38600, Loss: 11883.5087890625\nEpoch 38700, Loss: 11883.509765625\nEpoch 38800, Loss: 11883.513671875\nEpoch 38900, Loss: 11883.5107421875\nEpoch 39000, Loss: 11883.51171875\nEpoch 39100, Loss: 11883.5107421875\nEpoch 39200, Loss: 11883.51171875\nEpoch 39300, Loss: 11883.5107421875\nEpoch 39400, Loss: 11883.5087890625\nEpoch 39500, Loss: 11883.51171875\nEpoch 39600, Loss: 11883.5126953125\nEpoch 39700, Loss: 11883.513671875\nEpoch 39800, Loss: 11883.5126953125\nEpoch 39900, Loss: 11883.51171875\nEpoch 40000, Loss: 11883.51171875\nEpoch 40100, Loss: 11883.5126953125\nEpoch 40200, Loss: 11883.5126953125\nEpoch 40300, Loss: 11883.51171875\nEpoch 40400, Loss: 11883.51171875\nEpoch 40500, Loss: 11883.515625\nEpoch 40600, Loss: 11883.51171875\nEpoch 40700, Loss: 11883.509765625\nEpoch 40800, Loss: 11883.51171875\nEpoch 40900, Loss: 11883.51171875\nEpoch 41000, Loss: 11883.509765625\nEpoch 41100, Loss: 11883.513671875\nEpoch 41200, Loss: 11883.5107421875\nEpoch 41300, Loss: 11883.5087890625\nEpoch 41400, Loss: 11883.515625\nEpoch 41500, Loss: 11883.5126953125\nEpoch 41600, Loss: 11883.509765625\nEpoch 41700, Loss: 11883.509765625\nEpoch 41800, Loss: 11883.5126953125\nEpoch 41900, Loss: 11883.5126953125\nEpoch 42000, Loss: 11883.513671875\nEpoch 42100, Loss: 11883.513671875\nEpoch 42200, Loss: 11883.5087890625\nEpoch 42300, Loss: 11883.51171875\nEpoch 42400, Loss: 11883.5107421875\nEpoch 42500, Loss: 11883.513671875\nEpoch 42600, Loss: 11883.5146484375\nEpoch 42700, Loss: 11883.515625\nEpoch 42800, Loss: 11883.513671875\nEpoch 42900, Loss: 11883.5087890625\nEpoch 43000, Loss: 11883.5087890625\nEpoch 43100, Loss: 11883.5107421875\nEpoch 43200, Loss: 11883.5126953125\nEpoch 43300, Loss: 11883.51171875\nEpoch 43400, Loss: 11883.5107421875\nEpoch 43500, Loss: 11883.51171875\nEpoch 43600, Loss: 11883.5146484375\nEpoch 43700, Loss: 11883.5107421875\nEpoch 43800, Loss: 11883.5126953125\nEpoch 43900, Loss: 11883.5107421875\nEpoch 44000, Loss: 11883.5126953125\nEpoch 44100, Loss: 11883.51171875\nEpoch 44200, Loss: 11883.5078125\nEpoch 44300, Loss: 11883.509765625\nEpoch 44400, Loss: 11883.5107421875\nEpoch 44500, Loss: 11883.5087890625\nEpoch 44600, Loss: 11883.5126953125\nEpoch 44700, Loss: 11883.509765625\nEpoch 44800, Loss: 11883.509765625\nEpoch 44900, Loss: 11883.5107421875\nEpoch 45000, Loss: 11883.515625\nEpoch 45100, Loss: 11883.5107421875\nEpoch 45200, Loss: 11883.5087890625\nEpoch 45300, Loss: 11883.5107421875\nEpoch 45400, Loss: 11883.5126953125\nEpoch 45500, Loss: 11883.509765625\nEpoch 45600, Loss: 11883.509765625\nEpoch 45700, Loss: 11883.5107421875\nEpoch 45800, Loss: 11883.5107421875\nEpoch 45900, Loss: 11883.5126953125\nEpoch 46000, Loss: 11883.51171875\nEpoch 46100, Loss: 11883.513671875\nEpoch 46200, Loss: 11883.513671875\nEpoch 46300, Loss: 11883.5107421875\nEpoch 46400, Loss: 11883.513671875\nEpoch 46500, Loss: 11883.51171875\nEpoch 46600, Loss: 11883.5126953125\nEpoch 46700, Loss: 11883.509765625\nEpoch 46800, Loss: 11883.515625\nEpoch 46900, Loss: 11883.5126953125\nEpoch 47000, Loss: 11883.5107421875\nEpoch 47100, Loss: 11883.5107421875\nEpoch 47200, Loss: 11883.5107421875\nEpoch 47300, Loss: 11883.513671875\nEpoch 47400, Loss: 11883.509765625\nEpoch 47500, Loss: 11883.509765625\nEpoch 47600, Loss: 11883.51171875\nEpoch 47700, Loss: 11883.5126953125\nEpoch 47800, Loss: 11883.51171875\nEpoch 47900, Loss: 11883.5126953125\nEpoch 48000, Loss: 11883.5107421875\nEpoch 48100, Loss: 11883.51171875\nEpoch 48200, Loss: 11883.51171875\nEpoch 48300, Loss: 11883.51171875\nEpoch 48400, Loss: 11883.51171875\nEpoch 48500, Loss: 11883.5107421875\nEpoch 48600, Loss: 11883.51171875\nEpoch 48700, Loss: 11883.5107421875\nEpoch 48800, Loss: 11883.51171875\nEpoch 48900, Loss: 11883.513671875\nEpoch 49000, Loss: 11883.5126953125\nEpoch 49100, Loss: 11883.509765625\nEpoch 49200, Loss: 11883.51171875\nEpoch 49300, Loss: 11883.5107421875\nEpoch 49400, Loss: 11883.513671875\nEpoch 49500, Loss: 11883.513671875\nEpoch 49600, Loss: 11883.5087890625\nEpoch 49700, Loss: 11883.5107421875\nEpoch 49800, Loss: 11883.51171875\nEpoch 49900, Loss: 11883.513671875\nEpoch 0, Loss: 63828.96875\nEpoch 100, Loss: 60969.26171875\nEpoch 200, Loss: 58295.921875\nEpoch 300, Loss: 55798.828125\nEpoch 400, Loss: 53466.9921875\nEpoch 500, Loss: 51288.4296875\nEpoch 600, Loss: 49250.1171875\nEpoch 700, Loss: 47343.90234375\nEpoch 800, Loss: 45562.37890625\nEpoch 900, Loss: 43897.1171875\nEpoch 1000, Loss: 42340.640625\nEpoch 1100, Loss: 40886.15234375\nEpoch 1200, Loss: 39529.34765625\nEpoch 1300, Loss: 38264.31640625\nEpoch 1400, Loss: 37083.765625\nEpoch 1500, Loss: 35981.703125\nEpoch 1600, Loss: 34952.55859375\nEpoch 1700, Loss: 33991.19921875\nEpoch 1800, Loss: 33092.890625\nEpoch 1900, Loss: 32256.509765625\nEpoch 2000, Loss: 31475.287109375\nEpoch 2100, Loss: 30747.44921875\nEpoch 2200, Loss: 30070.466796875\nEpoch 2300, Loss: 29438.12109375\nEpoch 2400, Loss: 28846.17578125\nEpoch 2500, Loss: 28291.41796875\nEpoch 2600, Loss: 27770.966796875\nEpoch 2700, Loss: 27282.2109375\nEpoch 2800, Loss: 26822.7890625\nEpoch 2900, Loss: 26390.515625\nEpoch 3000, Loss: 25983.3828125\nEpoch 3100, Loss: 25599.5\nEpoch 3200, Loss: 25237.255859375\nEpoch 3300, Loss: 24901.013671875\nEpoch 3400, Loss: 24585.015625\nEpoch 3500, Loss: 24285.513671875\nEpoch 3600, Loss: 24000.98046875\nEpoch 3700, Loss: 23730.30078125\nEpoch 3800, Loss: 23474.6484375\nEpoch 3900, Loss: 23230.228515625\nEpoch 4000, Loss: 22995.912109375\nEpoch 4100, Loss: 22770.70703125\nEpoch 4200, Loss: 22555.5078125\nEpoch 4300, Loss: 22348.517578125\nEpoch 4400, Loss: 22148.2109375\nEpoch 4500, Loss: 21953.96484375\nEpoch 4600, Loss: 21765.236328125\nEpoch 4700, Loss: 21581.556640625\nEpoch 4800, Loss: 21402.53125\nEpoch 4900, Loss: 21227.8046875\nEpoch 5000, Loss: 21057.080078125\nEpoch 5100, Loss: 20890.1015625\nEpoch 5200, Loss: 20726.634765625\nEpoch 5300, Loss: 20567.63671875\nEpoch 5400, Loss: 20412.236328125\nEpoch 5500, Loss: 20259.837890625\nEpoch 5600, Loss: 20110.28515625\nEpoch 5700, Loss: 19963.443359375\nEpoch 5800, Loss: 19819.197265625\nEpoch 5900, Loss: 19677.4375\nEpoch 6000, Loss: 19538.05859375\nEpoch 6100, Loss: 19400.978515625\nEpoch 6200, Loss: 19266.123046875\nEpoch 6300, Loss: 19133.392578125\nEpoch 6400, Loss: 19002.724609375\nEpoch 6500, Loss: 18874.65625\nEpoch 6600, Loss: 18749.236328125\nEpoch 6700, Loss: 18626.732421875\nEpoch 6800, Loss: 18506.529296875\nEpoch 6900, Loss: 18388.07421875\nEpoch 7000, Loss: 18271.560546875\nEpoch 7100, Loss: 18156.626953125\nEpoch 7200, Loss: 18043.181640625\nEpoch 7300, Loss: 17931.1796875\nEpoch 7400, Loss: 17820.595703125\nEpoch 7500, Loss: 17712.333984375\nEpoch 7600, Loss: 17607.701171875\nEpoch 7700, Loss: 17504.40234375\nEpoch 7800, Loss: 17402.388671875\nEpoch 7900, Loss: 17301.6484375\nEpoch 8000, Loss: 17202.166015625\nEpoch 8100, Loss: 17103.91796875\nEpoch 8200, Loss: 17006.89453125\nEpoch 8300, Loss: 16911.09375\nEpoch 8400, Loss: 16816.48828125\nEpoch 8500, Loss: 16723.0703125\nEpoch 8600, Loss: 16630.83203125\nEpoch 8700, Loss: 16539.75390625\nEpoch 8800, Loss: 16449.830078125\nEpoch 8900, Loss: 16361.0517578125\nEpoch 9000, Loss: 16273.4052734375\nEpoch 9100, Loss: 16186.87890625\nEpoch 9200, Loss: 16101.4658203125\nEpoch 9300, Loss: 16017.150390625\nEpoch 9400, Loss: 15933.92578125\nEpoch 9500, Loss: 15851.779296875\nEpoch 9600, Loss: 15770.7041015625\nEpoch 9700, Loss: 15690.703125\nEpoch 9800, Loss: 15611.7587890625\nEpoch 9900, Loss: 15533.8720703125\nEpoch 10000, Loss: 15457.0380859375\nEpoch 10100, Loss: 15382.044921875\nEpoch 10200, Loss: 15308.6083984375\nEpoch 10300, Loss: 15236.580078125\nEpoch 10400, Loss: 15165.904296875\nEpoch 10500, Loss: 15096.5458984375\nEpoch 10600, Loss: 15028.4609375\nEpoch 10700, Loss: 14961.548828125\nEpoch 10800, Loss: 14895.728515625\nEpoch 10900, Loss: 14830.943359375\nEpoch 11000, Loss: 14767.48828125\nEpoch 11100, Loss: 14705.4580078125\nEpoch 11200, Loss: 14645.3876953125\nEpoch 11300, Loss: 14586.716796875\nEpoch 11400, Loss: 14529.3125\nEpoch 11500, Loss: 14473.1103515625\nEpoch 11600, Loss: 14418.0546875\nEpoch 11700, Loss: 14364.10546875\nEpoch 11800, Loss: 14311.21875\nEpoch 11900, Loss: 14259.3798828125\nEpoch 12000, Loss: 14208.546875\nEpoch 12100, Loss: 14158.7041015625\nEpoch 12200, Loss: 14109.8623046875\nEpoch 12300, Loss: 14062.0712890625\nEpoch 12400, Loss: 14015.38671875\nEpoch 12500, Loss: 13969.701171875\nEpoch 12600, Loss: 13924.98828125\nEpoch 12700, Loss: 13881.22265625\nEpoch 12800, Loss: 13838.390625\nEpoch 12900, Loss: 13796.4765625\nEpoch 13000, Loss: 13755.45703125\nEpoch 13100, Loss: 13715.306640625\nEpoch 13200, Loss: 13676.08984375\nEpoch 13300, Loss: 13637.732421875\nEpoch 13400, Loss: 13600.2236328125\nEpoch 13500, Loss: 13563.517578125\nEpoch 13600, Loss: 13527.9140625\nEpoch 13700, Loss: 13493.2568359375\nEpoch 13800, Loss: 13459.46875\nEpoch 13900, Loss: 13426.509765625\nEpoch 14000, Loss: 13394.345703125\nEpoch 14100, Loss: 13362.9755859375\nEpoch 14200, Loss: 13332.3896484375\nEpoch 14300, Loss: 13302.576171875\nEpoch 14400, Loss: 13273.5009765625\nEpoch 14500, Loss: 13245.138671875\nEpoch 14600, Loss: 13217.44921875\nEpoch 14700, Loss: 13190.4404296875\nEpoch 14800, Loss: 13164.0703125\nEpoch 14900, Loss: 13138.3466796875\nEpoch 15000, Loss: 13113.4482421875\nEpoch 15100, Loss: 13089.9267578125\nEpoch 15200, Loss: 13067.205078125\nEpoch 15300, Loss: 13045.1748046875\nEpoch 15400, Loss: 13023.7822265625\nEpoch 15500, Loss: 13003.013671875\nEpoch 15600, Loss: 12982.826171875\nEpoch 15700, Loss: 12963.193359375\nEpoch 15800, Loss: 12944.1064453125\nEpoch 15900, Loss: 12925.5263671875\nEpoch 16000, Loss: 12907.44921875\nEpoch 16100, Loss: 12889.85546875\nEpoch 16200, Loss: 12872.93359375\nEpoch 16300, Loss: 12856.6376953125\nEpoch 16400, Loss: 12840.8369140625\nEpoch 16500, Loss: 12825.494140625\nEpoch 16600, Loss: 12810.6005859375\nEpoch 16700, Loss: 12796.1572265625\nEpoch 16800, Loss: 12782.13671875\nEpoch 16900, Loss: 12768.5458984375\nEpoch 17000, Loss: 12755.3583984375\nEpoch 17100, Loss: 12742.576171875\nEpoch 17200, Loss: 12730.1845703125\nEpoch 17300, Loss: 12718.1796875\nEpoch 17400, Loss: 12706.55078125\nEpoch 17500, Loss: 12695.279296875\nEpoch 17600, Loss: 12684.35546875\nEpoch 17700, Loss: 12673.7783203125\nEpoch 17800, Loss: 12663.541015625\nEpoch 17900, Loss: 12653.6171875\nEpoch 18000, Loss: 12644.0009765625\nEpoch 18100, Loss: 12634.69140625\nEpoch 18200, Loss: 12625.671875\nEpoch 18300, Loss: 12616.923828125\nEpoch 18400, Loss: 12608.4462890625\nEpoch 18500, Loss: 12600.212890625\nEpoch 18600, Loss: 12592.23046875\nEpoch 18700, Loss: 12584.7255859375\nEpoch 18800, Loss: 12577.7861328125\nEpoch 18900, Loss: 12571.18359375\nEpoch 19000, Loss: 12564.8388671875\nEpoch 19100, Loss: 12558.724609375\nEpoch 19200, Loss: 12552.8046875\nEpoch 19300, Loss: 12547.0625\nEpoch 19400, Loss: 12541.486328125\nEpoch 19500, Loss: 12536.0556640625\nEpoch 19600, Loss: 12530.7666015625\nEpoch 19700, Loss: 12525.5927734375\nEpoch 19800, Loss: 12520.5439453125\nEpoch 19900, Loss: 12515.59375\nEpoch 20000, Loss: 12510.7509765625\nEpoch 20100, Loss: 12506.001953125\nEpoch 20200, Loss: 12501.33984375\nEpoch 20300, Loss: 12496.767578125\nEpoch 20400, Loss: 12492.265625\nEpoch 20500, Loss: 12487.8515625\nEpoch 20600, Loss: 12483.51953125\nEpoch 20700, Loss: 12479.2734375\nEpoch 20800, Loss: 12475.1171875\nEpoch 20900, Loss: 12471.037109375\nEpoch 21000, Loss: 12467.041015625\nEpoch 21100, Loss: 12463.1328125\nEpoch 21200, Loss: 12459.32421875\nEpoch 21300, Loss: 12455.58984375\nEpoch 21400, Loss: 12451.9482421875\nEpoch 21500, Loss: 12448.40625\nEpoch 21600, Loss: 12444.9501953125\nEpoch 21700, Loss: 12441.583984375\nEpoch 21800, Loss: 12438.3125\nEpoch 21900, Loss: 12435.1298828125\nEpoch 22000, Loss: 12432.03515625\nEpoch 22100, Loss: 12429.0390625\nEpoch 22200, Loss: 12426.1318359375\nEpoch 22300, Loss: 12423.30859375\nEpoch 22400, Loss: 12420.583984375\nEpoch 22500, Loss: 12417.9384765625\nEpoch 22600, Loss: 12415.384765625\nEpoch 22700, Loss: 12412.916015625\nEpoch 22800, Loss: 12410.525390625\nEpoch 22900, Loss: 12408.2099609375\nEpoch 23000, Loss: 12405.974609375\nEpoch 23100, Loss: 12403.806640625\nEpoch 23200, Loss: 12401.7099609375\nEpoch 23300, Loss: 12399.67578125\nEpoch 23400, Loss: 12397.708984375\nEpoch 23500, Loss: 12395.810546875\nEpoch 23600, Loss: 12393.9716796875\nEpoch 23700, Loss: 12392.193359375\nEpoch 23800, Loss: 12390.482421875\nEpoch 23900, Loss: 12388.822265625\nEpoch 24000, Loss: 12387.232421875\nEpoch 24100, Loss: 12385.6953125\nEpoch 24200, Loss: 12384.2138671875\nEpoch 24300, Loss: 12382.7939453125\nEpoch 24400, Loss: 12381.435546875\nEpoch 24500, Loss: 12380.1298828125\nEpoch 24600, Loss: 12378.87109375\nEpoch 24700, Loss: 12377.677734375\nEpoch 24800, Loss: 12376.52734375\nEpoch 24900, Loss: 12375.435546875\nEpoch 25000, Loss: 12374.3896484375\nEpoch 25100, Loss: 12373.390625\nEpoch 25200, Loss: 12372.4453125\nEpoch 25300, Loss: 12371.5380859375\nEpoch 25400, Loss: 12370.6767578125\nEpoch 25500, Loss: 12369.859375\nEpoch 25600, Loss: 12369.0712890625\nEpoch 25700, Loss: 12368.3330078125\nEpoch 25800, Loss: 12367.6279296875\nEpoch 25900, Loss: 12366.958984375\nEpoch 26000, Loss: 12366.328125\nEpoch 26100, Loss: 12365.7275390625\nEpoch 26200, Loss: 12365.1630859375\nEpoch 26300, Loss: 12364.6259765625\nEpoch 26400, Loss: 12364.125\nEpoch 26500, Loss: 12363.65234375\nEpoch 26600, Loss: 12363.212890625\nEpoch 26700, Loss: 12362.7998046875\nEpoch 26800, Loss: 12362.412109375\nEpoch 26900, Loss: 12362.05078125\nEpoch 27000, Loss: 12361.7119140625\nEpoch 27100, Loss: 12361.3994140625\nEpoch 27200, Loss: 12361.099609375\nEpoch 27300, Loss: 12360.8203125\nEpoch 27400, Loss: 12360.55859375\nEpoch 27500, Loss: 12360.3134765625\nEpoch 27600, Loss: 12360.083984375\nEpoch 27700, Loss: 12359.8740234375\nEpoch 27800, Loss: 12359.673828125\nEpoch 27900, Loss: 12359.498046875\nEpoch 28000, Loss: 12359.330078125\nEpoch 28100, Loss: 12359.169921875\nEpoch 28200, Loss: 12359.0244140625\nEpoch 28300, Loss: 12358.8955078125\nEpoch 28400, Loss: 12358.775390625\nEpoch 28500, Loss: 12358.6640625\nEpoch 28600, Loss: 12358.5556640625\nEpoch 28700, Loss: 12358.4541015625\nEpoch 28800, Loss: 12358.3583984375\nEpoch 28900, Loss: 12358.271484375\nEpoch 29000, Loss: 12358.18359375\nEpoch 29100, Loss: 12358.111328125\nEpoch 29200, Loss: 12358.044921875\nEpoch 29300, Loss: 12357.9775390625\nEpoch 29400, Loss: 12357.9189453125\nEpoch 29500, Loss: 12357.86328125\nEpoch 29600, Loss: 12357.8173828125\nEpoch 29700, Loss: 12357.7744140625\nEpoch 29800, Loss: 12357.732421875\nEpoch 29900, Loss: 12357.69140625\nEpoch 30000, Loss: 12357.6591796875\nEpoch 30100, Loss: 12357.6240234375\nEpoch 30200, Loss: 12357.59375\nEpoch 30300, Loss: 12357.5625\nEpoch 30400, Loss: 12357.53515625\nEpoch 30500, Loss: 12357.5078125\nEpoch 30600, Loss: 12357.482421875\nEpoch 30700, Loss: 12357.455078125\nEpoch 30800, Loss: 12357.435546875\nEpoch 30900, Loss: 12357.412109375\nEpoch 31000, Loss: 12357.3916015625\nEpoch 31100, Loss: 12357.3671875\nEpoch 31200, Loss: 12357.349609375\nEpoch 31300, Loss: 12357.330078125\nEpoch 31400, Loss: 12357.3046875\nEpoch 31500, Loss: 12357.29296875\nEpoch 31600, Loss: 12357.271484375\nEpoch 31700, Loss: 12357.2548828125\nEpoch 31800, Loss: 12357.2421875\nEpoch 31900, Loss: 12357.224609375\nEpoch 32000, Loss: 12357.205078125\nEpoch 32100, Loss: 12357.193359375\nEpoch 32200, Loss: 12357.177734375\nEpoch 32300, Loss: 12357.162109375\nEpoch 32400, Loss: 12357.1484375\nEpoch 32500, Loss: 12357.134765625\nEpoch 32600, Loss: 12357.1318359375\nEpoch 32700, Loss: 12357.111328125\nEpoch 32800, Loss: 12357.1025390625\nEpoch 32900, Loss: 12357.0888671875\nEpoch 33000, Loss: 12357.0810546875\nEpoch 33100, Loss: 12357.0732421875\nEpoch 33200, Loss: 12357.060546875\nEpoch 33300, Loss: 12357.052734375\nEpoch 33400, Loss: 12357.048828125\nEpoch 33500, Loss: 12357.044921875\nEpoch 33600, Loss: 12357.03125\nEpoch 33700, Loss: 12357.02734375\nEpoch 33800, Loss: 12357.025390625\nEpoch 33900, Loss: 12357.015625\nEpoch 34000, Loss: 12357.0087890625\nEpoch 34100, Loss: 12357.001953125\nEpoch 34200, Loss: 12357.0009765625\nEpoch 34300, Loss: 12356.99609375\nEpoch 34400, Loss: 12356.9912109375\nEpoch 34500, Loss: 12356.98828125\nEpoch 34600, Loss: 12356.990234375\nEpoch 34700, Loss: 12356.98046875\nEpoch 34800, Loss: 12356.9775390625\nEpoch 34900, Loss: 12356.978515625\nEpoch 35000, Loss: 12356.9765625\nEpoch 35100, Loss: 12356.9716796875\nEpoch 35200, Loss: 12356.974609375\nEpoch 35300, Loss: 12356.9697265625\nEpoch 35400, Loss: 12356.9677734375\nEpoch 35500, Loss: 12356.96875\nEpoch 35600, Loss: 12356.96875\nEpoch 35700, Loss: 12356.9599609375\nEpoch 35800, Loss: 12356.96484375\nEpoch 35900, Loss: 12356.9619140625\nEpoch 36000, Loss: 12356.962890625\nEpoch 36100, Loss: 12356.9609375\nEpoch 36200, Loss: 12356.9619140625\nEpoch 36300, Loss: 12356.9638671875\nEpoch 36400, Loss: 12356.9619140625\nEpoch 36500, Loss: 12356.96484375\nEpoch 36600, Loss: 12356.96484375\nEpoch 36700, Loss: 12356.9677734375\nEpoch 36800, Loss: 12356.9619140625\nEpoch 36900, Loss: 12356.962890625\nEpoch 37000, Loss: 12356.9619140625\nEpoch 37100, Loss: 12356.9599609375\nEpoch 37200, Loss: 12356.958984375\nEpoch 37300, Loss: 12356.962890625\nEpoch 37400, Loss: 12356.9580078125\nEpoch 37500, Loss: 12356.958984375\nEpoch 37600, Loss: 12356.9599609375\nEpoch 37700, Loss: 12356.962890625\nEpoch 37800, Loss: 12356.9609375\nEpoch 37900, Loss: 12356.9638671875\nEpoch 38000, Loss: 12356.962890625\nEpoch 38100, Loss: 12356.958984375\nEpoch 38200, Loss: 12356.96484375\nEpoch 38300, Loss: 12356.962890625\nEpoch 38400, Loss: 12356.9638671875\nEpoch 38500, Loss: 12356.966796875\nEpoch 38600, Loss: 12356.9638671875\nEpoch 38700, Loss: 12356.958984375\nEpoch 38800, Loss: 12356.9609375\nEpoch 38900, Loss: 12356.9609375\nEpoch 39000, Loss: 12356.958984375\nEpoch 39100, Loss: 12356.966796875\nEpoch 39200, Loss: 12356.962890625\nEpoch 39300, Loss: 12356.9638671875\nEpoch 39400, Loss: 12356.9599609375\nEpoch 39500, Loss: 12356.958984375\nEpoch 39600, Loss: 12356.9638671875\nEpoch 39700, Loss: 12356.9638671875\nEpoch 39800, Loss: 12356.96484375\nEpoch 39900, Loss: 12356.966796875\nEpoch 40000, Loss: 12356.96484375\nEpoch 40100, Loss: 12356.9609375\nEpoch 40200, Loss: 12356.9638671875\nEpoch 40300, Loss: 12356.962890625\nEpoch 40400, Loss: 12356.962890625\nEpoch 40500, Loss: 12356.9560546875\nEpoch 40600, Loss: 12356.9609375\nEpoch 40700, Loss: 12356.958984375\nEpoch 40800, Loss: 12356.958984375\nEpoch 40900, Loss: 12356.96484375\nEpoch 41000, Loss: 12356.962890625\nEpoch 41100, Loss: 12356.9560546875\nEpoch 41200, Loss: 12356.96875\nEpoch 41300, Loss: 12356.9619140625\nEpoch 41400, Loss: 12356.9609375\nEpoch 41500, Loss: 12356.9599609375\nEpoch 41600, Loss: 12356.96484375\nEpoch 41700, Loss: 12356.9609375\nEpoch 41800, Loss: 12356.962890625\nEpoch 41900, Loss: 12356.958984375\nEpoch 42000, Loss: 12356.962890625\nEpoch 42100, Loss: 12356.962890625\nEpoch 42200, Loss: 12356.9609375\nEpoch 42300, Loss: 12356.958984375\nEpoch 42400, Loss: 12356.9609375\nEpoch 42500, Loss: 12356.9609375\nEpoch 42600, Loss: 12356.962890625\nEpoch 42700, Loss: 12356.9619140625\nEpoch 42800, Loss: 12356.96484375\nEpoch 42900, Loss: 12356.96484375\nEpoch 43000, Loss: 12356.9658203125\nEpoch 43100, Loss: 12356.962890625\nEpoch 43200, Loss: 12356.958984375\nEpoch 43300, Loss: 12356.9609375\nEpoch 43400, Loss: 12356.966796875\nEpoch 43500, Loss: 12356.9609375\nEpoch 43600, Loss: 12356.96484375\nEpoch 43700, Loss: 12356.9609375\nEpoch 43800, Loss: 12356.9619140625\nEpoch 43900, Loss: 12356.96484375\nEpoch 44000, Loss: 12356.962890625\nEpoch 44100, Loss: 12356.9609375\nEpoch 44200, Loss: 12356.962890625\nEpoch 44300, Loss: 12356.9599609375\nEpoch 44400, Loss: 12356.962890625\nEpoch 44500, Loss: 12356.9619140625\nEpoch 44600, Loss: 12356.9619140625\nEpoch 44700, Loss: 12356.9609375\nEpoch 44800, Loss: 12356.962890625\nEpoch 44900, Loss: 12356.9580078125\nEpoch 45000, Loss: 12356.9609375\nEpoch 45100, Loss: 12356.9580078125\nEpoch 45200, Loss: 12356.9609375\nEpoch 45300, Loss: 12356.9599609375\nEpoch 45400, Loss: 12356.96484375\nEpoch 45500, Loss: 12356.95703125\nEpoch 45600, Loss: 12356.9609375\nEpoch 45700, Loss: 12356.9638671875\nEpoch 45800, Loss: 12356.96484375\nEpoch 45900, Loss: 12356.9609375\nEpoch 46000, Loss: 12356.9638671875\nEpoch 46100, Loss: 12356.9638671875\nEpoch 46200, Loss: 12356.9609375\nEpoch 46300, Loss: 12356.962890625\nEpoch 46400, Loss: 12356.9580078125\nEpoch 46500, Loss: 12356.9599609375\nEpoch 46600, Loss: 12356.9658203125\nEpoch 46700, Loss: 12356.962890625\nEpoch 46800, Loss: 12356.9580078125\nEpoch 46900, Loss: 12356.96484375\nEpoch 47000, Loss: 12356.9609375\nEpoch 47100, Loss: 12356.958984375\nEpoch 47200, Loss: 12356.9599609375\nEpoch 47300, Loss: 12356.9599609375\nEpoch 47400, Loss: 12356.9599609375\nEpoch 47500, Loss: 12356.9619140625\nEpoch 47600, Loss: 12356.9609375\nEpoch 47700, Loss: 12356.9609375\nEpoch 47800, Loss: 12356.962890625\nEpoch 47900, Loss: 12356.9619140625\nEpoch 48000, Loss: 12356.958984375\nEpoch 48100, Loss: 12356.962890625\nEpoch 48200, Loss: 12356.95703125\nEpoch 48300, Loss: 12356.958984375\nEpoch 48400, Loss: 12356.9609375\nEpoch 48500, Loss: 12356.9609375\nEpoch 48600, Loss: 12356.9609375\nEpoch 48700, Loss: 12356.9609375\nEpoch 48800, Loss: 12356.9609375\nEpoch 48900, Loss: 12356.958984375\nEpoch 49000, Loss: 12356.96484375\nEpoch 49100, Loss: 12356.958984375\nEpoch 49200, Loss: 12356.9638671875\nEpoch 49300, Loss: 12356.9609375\nEpoch 49400, Loss: 12356.9619140625\nEpoch 49500, Loss: 12356.962890625\nEpoch 49600, Loss: 12356.962890625\nEpoch 49700, Loss: 12356.958984375\nEpoch 49800, Loss: 12356.9619140625\nEpoch 49900, Loss: 12356.9609375\nEpoch 0, Loss: 63828.96875\nEpoch 100, Loss: 60981.52734375\nEpoch 200, Loss: 58319.796875\nEpoch 300, Loss: 55834.2265625\nEpoch 400, Loss: 53514.84375\nEpoch 500, Loss: 51349.33203125\nEpoch 600, Loss: 49326.03125\nEpoch 700, Loss: 47436.07421875\nEpoch 800, Loss: 45669.8359375\nEpoch 900, Loss: 44018.9765625\nEpoch 1000, Loss: 42475.8046875\nEpoch 1100, Loss: 41034.44921875\nEpoch 1200, Loss: 39690.98828125\nEpoch 1300, Loss: 38437.40625\nEpoch 1400, Loss: 37267.33984375\nEpoch 1500, Loss: 36174.80078125\nEpoch 1600, Loss: 35154.30078125\nEpoch 1700, Loss: 34200.77734375\nEpoch 1800, Loss: 33312.5859375\nEpoch 1900, Loss: 32483.96875\nEpoch 2000, Loss: 31711.810546875\nEpoch 2100, Loss: 30994.130859375\nEpoch 2200, Loss: 30325.810546875\nEpoch 2300, Loss: 29700.9921875\nEpoch 2400, Loss: 29116.078125\nEpoch 2500, Loss: 28567.869140625\nEpoch 2600, Loss: 28053.501953125\nEpoch 2700, Loss: 27570.404296875\nEpoch 2800, Loss: 27116.23828125\nEpoch 2900, Loss: 26688.86328125\nEpoch 3000, Loss: 26286.296875\nEpoch 3100, Loss: 25906.69140625\nEpoch 3200, Loss: 25551.578125\nEpoch 3300, Loss: 25222.48828125\nEpoch 3400, Loss: 24911.8203125\nEpoch 3500, Loss: 24617.384765625\nEpoch 3600, Loss: 24337.65234375\nEpoch 3700, Loss: 24074.560546875\nEpoch 3800, Loss: 23823.861328125\nEpoch 3900, Loss: 23584.126953125\nEpoch 4000, Loss: 23354.2578125\nEpoch 4100, Loss: 23134.6796875\nEpoch 4200, Loss: 22924.80078125\nEpoch 4300, Loss: 22722.123046875\nEpoch 4400, Loss: 22525.94140625\nEpoch 4500, Loss: 22335.62890625\nEpoch 4600, Loss: 22150.66015625\nEpoch 4700, Loss: 21970.578125\nEpoch 4800, Loss: 21794.9921875\nEpoch 4900, Loss: 21623.560546875\nEpoch 5000, Loss: 21456.0\nEpoch 5100, Loss: 21292.615234375\nEpoch 5200, Loss: 21133.8046875\nEpoch 5300, Loss: 20978.2421875\nEpoch 5400, Loss: 20825.720703125\nEpoch 5500, Loss: 20676.095703125\nEpoch 5600, Loss: 20529.353515625\nEpoch 5700, Loss: 20385.16796875\nEpoch 5800, Loss: 20243.423828125\nEpoch 5900, Loss: 20104.02734375\nEpoch 6000, Loss: 19966.88671875\nEpoch 6100, Loss: 19831.921875\nEpoch 6200, Loss: 19699.052734375\nEpoch 6300, Loss: 19568.318359375\nEpoch 6400, Loss: 19440.84765625\nEpoch 6500, Loss: 19315.3359375\nEpoch 6600, Loss: 19193.125\nEpoch 6700, Loss: 19073.052734375\nEpoch 6800, Loss: 18955.03125\nEpoch 6900, Loss: 18838.5625\nEpoch 7000, Loss: 18723.58203125\nEpoch 7100, Loss: 18610.28125\nEpoch 7200, Loss: 18498.845703125\nEpoch 7300, Loss: 18388.8515625\nEpoch 7400, Loss: 18280.26171875\nEpoch 7500, Loss: 18173.48828125\nEpoch 7600, Loss: 18070.7578125\nEpoch 7700, Loss: 17969.25390625\nEpoch 7800, Loss: 17868.923828125\nEpoch 7900, Loss: 17769.7421875\nEpoch 8000, Loss: 17671.69921875\nEpoch 8100, Loss: 17574.77734375\nEpoch 8200, Loss: 17478.974609375\nEpoch 8300, Loss: 17384.265625\nEpoch 8400, Loss: 17290.640625\nEpoch 8500, Loss: 17198.09765625\nEpoch 8600, Loss: 17106.61328125\nEpoch 8700, Loss: 17016.197265625\nEpoch 8800, Loss: 16926.826171875\nEpoch 8900, Loss: 16838.5\nEpoch 9000, Loss: 16751.212890625\nEpoch 9100, Loss: 16664.955078125\nEpoch 9200, Loss: 16579.7265625\nEpoch 9300, Loss: 16495.513671875\nEpoch 9400, Loss: 16412.326171875\nEpoch 9500, Loss: 16330.146484375\nEpoch 9600, Loss: 16248.9755859375\nEpoch 9700, Loss: 16169.216796875\nEpoch 9800, Loss: 16091.2265625\nEpoch 9900, Loss: 16014.55859375\nEpoch 10000, Loss: 15939.3427734375\nEpoch 10100, Loss: 15865.5625\nEpoch 10200, Loss: 15793.1044921875\nEpoch 10300, Loss: 15721.9375\nEpoch 10400, Loss: 15652.0537109375\nEpoch 10500, Loss: 15583.498046875\nEpoch 10600, Loss: 15516.685546875\nEpoch 10700, Loss: 15451.498046875\nEpoch 10800, Loss: 15387.595703125\nEpoch 10900, Loss: 15325.4111328125\nEpoch 11000, Loss: 15264.640625\nEpoch 11100, Loss: 15205.0966796875\nEpoch 11200, Loss: 15146.720703125\nEpoch 11300, Loss: 15089.494140625\nEpoch 11400, Loss: 15033.3740234375\nEpoch 11500, Loss: 14978.330078125\nEpoch 11600, Loss: 14924.330078125\nEpoch 11700, Loss: 14871.39453125\nEpoch 11800, Loss: 14819.5830078125\nEpoch 11900, Loss: 14768.92578125\nEpoch 12000, Loss: 14719.3662109375\nEpoch 12100, Loss: 14670.84375\nEpoch 12200, Loss: 14623.314453125\nEpoch 12300, Loss: 14576.7919921875\nEpoch 12400, Loss: 14531.240234375\nEpoch 12500, Loss: 14486.65234375\nEpoch 12600, Loss: 14443.0\nEpoch 12700, Loss: 14400.265625\nEpoch 12800, Loss: 14358.525390625\nEpoch 12900, Loss: 14317.7509765625\nEpoch 13000, Loss: 14278.2099609375\nEpoch 13100, Loss: 14239.6875\nEpoch 13200, Loss: 14202.1220703125\nEpoch 13300, Loss: 14165.4794921875\nEpoch 13400, Loss: 14129.744140625\nEpoch 13500, Loss: 14094.8984375\nEpoch 13600, Loss: 14060.9482421875\nEpoch 13700, Loss: 14027.8798828125\nEpoch 13800, Loss: 13995.625\nEpoch 13900, Loss: 13964.15234375\nEpoch 14000, Loss: 13933.42578125\nEpoch 14100, Loss: 13903.38671875\nEpoch 14200, Loss: 13874.0498046875\nEpoch 14300, Loss: 13845.3828125\nEpoch 14400, Loss: 13817.37890625\nEpoch 14500, Loss: 13790.025390625\nEpoch 14600, Loss: 13763.2998046875\nEpoch 14700, Loss: 13737.541015625\nEpoch 14800, Loss: 13713.2529296875\nEpoch 14900, Loss: 13689.7998046875\nEpoch 15000, Loss: 13667.0517578125\nEpoch 15100, Loss: 13644.962890625\nEpoch 15200, Loss: 13623.462890625\nEpoch 15300, Loss: 13602.537109375\nEpoch 15400, Loss: 13582.154296875\nEpoch 15500, Loss: 13562.302734375\nEpoch 15600, Loss: 13542.951171875\nEpoch 15700, Loss: 13524.2607421875\nEpoch 15800, Loss: 13506.3330078125\nEpoch 15900, Loss: 13488.9150390625\nEpoch 16000, Loss: 13471.97265625\nEpoch 16100, Loss: 13455.5\nEpoch 16200, Loss: 13439.4853515625\nEpoch 16300, Loss: 13423.9150390625\nEpoch 16400, Loss: 13408.767578125\nEpoch 16500, Loss: 13394.044921875\nEpoch 16600, Loss: 13379.7314453125\nEpoch 16700, Loss: 13365.818359375\nEpoch 16800, Loss: 13352.310546875\nEpoch 16900, Loss: 13339.16015625\nEpoch 17000, Loss: 13326.40234375\nEpoch 17100, Loss: 13314.0146484375\nEpoch 17200, Loss: 13301.9892578125\nEpoch 17300, Loss: 13290.30859375\nEpoch 17400, Loss: 13278.9609375\nEpoch 17500, Loss: 13267.9609375\nEpoch 17600, Loss: 13257.267578125\nEpoch 17700, Loss: 13246.9052734375\nEpoch 17800, Loss: 13236.826171875\nEpoch 17900, Loss: 13227.0546875\nEpoch 18000, Loss: 13217.5546875\nEpoch 18100, Loss: 13208.326171875\nEpoch 18200, Loss: 13199.341796875\nEpoch 18300, Loss: 13190.861328125\nEpoch 18400, Loss: 13183.0341796875\nEpoch 18500, Loss: 13175.6162109375\nEpoch 18600, Loss: 13168.498046875\nEpoch 18700, Loss: 13161.638671875\nEpoch 18800, Loss: 13155.0107421875\nEpoch 18900, Loss: 13148.5810546875\nEpoch 19000, Loss: 13142.349609375\nEpoch 19100, Loss: 13136.271484375\nEpoch 19200, Loss: 13130.3671875\nEpoch 19300, Loss: 13124.6103515625\nEpoch 19400, Loss: 13118.9921875\nEpoch 19500, Loss: 13113.48828125\nEpoch 19600, Loss: 13108.1142578125\nEpoch 19700, Loss: 13102.8515625\nEpoch 19800, Loss: 13097.689453125\nEpoch 19900, Loss: 13092.62109375\nEpoch 20000, Loss: 13087.654296875\nEpoch 20100, Loss: 13082.78125\nEpoch 20200, Loss: 13077.9921875\nEpoch 20300, Loss: 13073.3076171875\nEpoch 20400, Loss: 13068.697265625\nEpoch 20500, Loss: 13064.17578125\nEpoch 20600, Loss: 13059.748046875\nEpoch 20700, Loss: 13055.404296875\nEpoch 20800, Loss: 13051.15234375\nEpoch 20900, Loss: 13046.9970703125\nEpoch 21000, Loss: 13042.935546875\nEpoch 21100, Loss: 13038.955078125\nEpoch 21200, Loss: 13035.076171875\nEpoch 21300, Loss: 13031.3017578125\nEpoch 21400, Loss: 13027.6064453125\nEpoch 21500, Loss: 13024.01171875\nEpoch 21600, Loss: 13020.5146484375\nEpoch 21700, Loss: 13017.1103515625\nEpoch 21800, Loss: 13013.7998046875\nEpoch 21900, Loss: 13010.5888671875\nEpoch 22000, Loss: 13007.4697265625\nEpoch 22100, Loss: 13004.4365234375\nEpoch 22200, Loss: 13001.4990234375\nEpoch 22300, Loss: 12998.6513671875\nEpoch 22400, Loss: 12995.896484375\nEpoch 22500, Loss: 12993.2333984375\nEpoch 22600, Loss: 12990.6484375\nEpoch 22700, Loss: 12988.1552734375\nEpoch 22800, Loss: 12985.73828125\nEpoch 22900, Loss: 12983.3876953125\nEpoch 23000, Loss: 12981.1240234375\nEpoch 23100, Loss: 12978.919921875\nEpoch 23200, Loss: 12976.79296875\nEpoch 23300, Loss: 12974.7216796875\nEpoch 23400, Loss: 12972.734375\nEpoch 23500, Loss: 12970.7890625\nEpoch 23600, Loss: 12968.92578125\nEpoch 23700, Loss: 12967.1083984375\nEpoch 23800, Loss: 12965.35546875\nEpoch 23900, Loss: 12963.666015625\nEpoch 24000, Loss: 12962.0390625\nEpoch 24100, Loss: 12960.46484375\nEpoch 24200, Loss: 12958.966796875\nEpoch 24300, Loss: 12957.5048828125\nEpoch 24400, Loss: 12956.1044921875\nEpoch 24500, Loss: 12954.7724609375\nEpoch 24600, Loss: 12953.4765625\nEpoch 24700, Loss: 12952.240234375\nEpoch 24800, Loss: 12951.0654296875\nEpoch 24900, Loss: 12949.93359375\nEpoch 25000, Loss: 12948.8525390625\nEpoch 25100, Loss: 12947.830078125\nEpoch 25200, Loss: 12946.837890625\nEpoch 25300, Loss: 12945.904296875\nEpoch 25400, Loss: 12945.0234375\nEpoch 25500, Loss: 12944.1875\nEpoch 25600, Loss: 12943.3955078125\nEpoch 25700, Loss: 12942.650390625\nEpoch 25800, Loss: 12941.951171875\nEpoch 25900, Loss: 12941.2890625\nEpoch 26000, Loss: 12940.6787109375\nEpoch 26100, Loss: 12940.091796875\nEpoch 26200, Loss: 12939.54296875\nEpoch 26300, Loss: 12939.03515625\nEpoch 26400, Loss: 12938.5625\nEpoch 26500, Loss: 12938.1201171875\nEpoch 26600, Loss: 12937.69921875\nEpoch 26700, Loss: 12937.318359375\nEpoch 26800, Loss: 12936.9619140625\nEpoch 26900, Loss: 12936.6259765625\nEpoch 27000, Loss: 12936.30859375\nEpoch 27100, Loss: 12936.0126953125\nEpoch 27200, Loss: 12935.7451171875\nEpoch 27300, Loss: 12935.494140625\nEpoch 27400, Loss: 12935.255859375\nEpoch 27500, Loss: 12935.046875\nEpoch 27600, Loss: 12934.84375\nEpoch 27700, Loss: 12934.654296875\nEpoch 27800, Loss: 12934.4873046875\nEpoch 27900, Loss: 12934.3212890625\nEpoch 28000, Loss: 12934.1728515625\nEpoch 28100, Loss: 12934.03515625\nEpoch 28200, Loss: 12933.9130859375\nEpoch 28300, Loss: 12933.79296875\nEpoch 28400, Loss: 12933.6787109375\nEpoch 28500, Loss: 12933.58203125\nEpoch 28600, Loss: 12933.48046875\nEpoch 28700, Loss: 12933.3994140625\nEpoch 28800, Loss: 12933.3173828125\nEpoch 28900, Loss: 12933.240234375\nEpoch 29000, Loss: 12933.1689453125\nEpoch 29100, Loss: 12933.09765625\nEpoch 29200, Loss: 12933.033203125\nEpoch 29300, Loss: 12932.96875\nEpoch 29400, Loss: 12932.904296875\nEpoch 29500, Loss: 12932.8505859375\nEpoch 29600, Loss: 12932.7890625\nEpoch 29700, Loss: 12932.7392578125\nEpoch 29800, Loss: 12932.6767578125\nEpoch 29900, Loss: 12932.626953125\nEpoch 30000, Loss: 12932.56640625\nEpoch 30100, Loss: 12932.525390625\nEpoch 30200, Loss: 12932.470703125\nEpoch 30300, Loss: 12932.416015625\nEpoch 30400, Loss: 12932.375\nEpoch 30500, Loss: 12932.3212890625\nEpoch 30600, Loss: 12932.2734375\nEpoch 30700, Loss: 12932.23046875\nEpoch 30800, Loss: 12932.177734375\nEpoch 30900, Loss: 12932.13671875\nEpoch 31000, Loss: 12932.09375\nEpoch 31100, Loss: 12932.04296875\nEpoch 31200, Loss: 12932.00390625\nEpoch 31300, Loss: 12931.9541015625\nEpoch 31400, Loss: 12931.916015625\nEpoch 31500, Loss: 12931.8720703125\nEpoch 31600, Loss: 12931.833984375\nEpoch 31700, Loss: 12931.8017578125\nEpoch 31800, Loss: 12931.755859375\nEpoch 31900, Loss: 12931.716796875\nEpoch 32000, Loss: 12931.6884765625\nEpoch 32100, Loss: 12931.6484375\nEpoch 32200, Loss: 12931.60546875\nEpoch 32300, Loss: 12931.5830078125\nEpoch 32400, Loss: 12931.5390625\nEpoch 32500, Loss: 12931.5078125\nEpoch 32600, Loss: 12931.470703125\nEpoch 32700, Loss: 12931.4375\nEpoch 32800, Loss: 12931.41015625\nEpoch 32900, Loss: 12931.37890625\nEpoch 33000, Loss: 12931.34765625\nEpoch 33100, Loss: 12931.31640625\nEpoch 33200, Loss: 12931.287109375\nEpoch 33300, Loss: 12931.2568359375\nEpoch 33400, Loss: 12931.234375\nEpoch 33500, Loss: 12931.2119140625\nEpoch 33600, Loss: 12931.177734375\nEpoch 33700, Loss: 12931.1572265625\nEpoch 33800, Loss: 12931.1259765625\nEpoch 33900, Loss: 12931.10546875\nEpoch 34000, Loss: 12931.08984375\nEpoch 34100, Loss: 12931.056640625\nEpoch 34200, Loss: 12931.033203125\nEpoch 34300, Loss: 12931.01171875\nEpoch 34400, Loss: 12930.998046875\nEpoch 34500, Loss: 12930.9716796875\nEpoch 34600, Loss: 12930.9580078125\nEpoch 34700, Loss: 12930.9345703125\nEpoch 34800, Loss: 12930.91796875\nEpoch 34900, Loss: 12930.896484375\nEpoch 35000, Loss: 12930.8857421875\nEpoch 35100, Loss: 12930.865234375\nEpoch 35200, Loss: 12930.84765625\nEpoch 35300, Loss: 12930.833984375\nEpoch 35400, Loss: 12930.826171875\nEpoch 35500, Loss: 12930.80859375\nEpoch 35600, Loss: 12930.7900390625\nEpoch 35700, Loss: 12930.78125\nEpoch 35800, Loss: 12930.76953125\nEpoch 35900, Loss: 12930.75390625\nEpoch 36000, Loss: 12930.7470703125\nEpoch 36100, Loss: 12930.736328125\nEpoch 36200, Loss: 12930.7275390625\nEpoch 36300, Loss: 12930.716796875\nEpoch 36400, Loss: 12930.70703125\nEpoch 36500, Loss: 12930.6982421875\nEpoch 36600, Loss: 12930.69140625\nEpoch 36700, Loss: 12930.681640625\nEpoch 36800, Loss: 12930.67578125\nEpoch 36900, Loss: 12930.669921875\nEpoch 37000, Loss: 12930.6630859375\nEpoch 37100, Loss: 12930.6591796875\nEpoch 37200, Loss: 12930.6533203125\nEpoch 37300, Loss: 12930.64453125\nEpoch 37400, Loss: 12930.640625\nEpoch 37500, Loss: 12930.638671875\nEpoch 37600, Loss: 12930.6328125\nEpoch 37700, Loss: 12930.6298828125\nEpoch 37800, Loss: 12930.62109375\nEpoch 37900, Loss: 12930.6279296875\nEpoch 38000, Loss: 12930.62890625\nEpoch 38100, Loss: 12930.619140625\nEpoch 38200, Loss: 12930.6181640625\nEpoch 38300, Loss: 12930.615234375\nEpoch 38400, Loss: 12930.6123046875\nEpoch 38500, Loss: 12930.61328125\nEpoch 38600, Loss: 12930.6123046875\nEpoch 38700, Loss: 12930.611328125\nEpoch 38800, Loss: 12930.6044921875\nEpoch 38900, Loss: 12930.603515625\nEpoch 39000, Loss: 12930.60546875\nEpoch 39100, Loss: 12930.6025390625\nEpoch 39200, Loss: 12930.60546875\nEpoch 39300, Loss: 12930.609375\nEpoch 39400, Loss: 12930.611328125\nEpoch 39500, Loss: 12930.6025390625\nEpoch 39600, Loss: 12930.603515625\nEpoch 39700, Loss: 12930.6083984375\nEpoch 39800, Loss: 12930.6044921875\nEpoch 39900, Loss: 12930.60546875\nEpoch 40000, Loss: 12930.6064453125\nEpoch 40100, Loss: 12930.6015625\nEpoch 40200, Loss: 12930.6064453125\nEpoch 40300, Loss: 12930.60546875\nEpoch 40400, Loss: 12930.6015625\nEpoch 40500, Loss: 12930.6044921875\nEpoch 40600, Loss: 12930.6025390625\nEpoch 40700, Loss: 12930.603515625\nEpoch 40800, Loss: 12930.6025390625\nEpoch 40900, Loss: 12930.5986328125\nEpoch 41000, Loss: 12930.603515625\nEpoch 41100, Loss: 12930.6025390625\nEpoch 41200, Loss: 12930.599609375\nEpoch 41300, Loss: 12930.6025390625\nEpoch 41400, Loss: 12930.609375\nEpoch 41500, Loss: 12930.6015625\nEpoch 41600, Loss: 12930.603515625\nEpoch 41700, Loss: 12930.6044921875\nEpoch 41800, Loss: 12930.599609375\nEpoch 41900, Loss: 12930.599609375\nEpoch 42000, Loss: 12930.6044921875\nEpoch 42100, Loss: 12930.6064453125\nEpoch 42200, Loss: 12930.5986328125\nEpoch 42300, Loss: 12930.6015625\nEpoch 42400, Loss: 12930.60546875\nEpoch 42500, Loss: 12930.599609375\nEpoch 42600, Loss: 12930.60546875\nEpoch 42700, Loss: 12930.5966796875\nEpoch 42800, Loss: 12930.59765625\nEpoch 42900, Loss: 12930.599609375\nEpoch 43000, Loss: 12930.6044921875\nEpoch 43100, Loss: 12930.609375\nEpoch 43200, Loss: 12930.59765625\nEpoch 43300, Loss: 12930.603515625\nEpoch 43400, Loss: 12930.59765625\nEpoch 43500, Loss: 12930.6064453125\nEpoch 43600, Loss: 12930.6044921875\nEpoch 43700, Loss: 12930.6044921875\nEpoch 43800, Loss: 12930.5986328125\nEpoch 43900, Loss: 12930.6015625\nEpoch 44000, Loss: 12930.6015625\nEpoch 44100, Loss: 12930.6015625\nEpoch 44200, Loss: 12930.6015625\nEpoch 44300, Loss: 12930.60546875\nEpoch 44400, Loss: 12930.6015625\nEpoch 44500, Loss: 12930.6005859375\nEpoch 44600, Loss: 12930.60546875\nEpoch 44700, Loss: 12930.603515625\nEpoch 44800, Loss: 12930.6025390625\nEpoch 44900, Loss: 12930.6015625\nEpoch 45000, Loss: 12930.6005859375\nEpoch 45100, Loss: 12930.603515625\nEpoch 45200, Loss: 12930.599609375\nEpoch 45300, Loss: 12930.6064453125\nEpoch 45400, Loss: 12930.6015625\nEpoch 45500, Loss: 12930.60546875\nEpoch 45600, Loss: 12930.6025390625\nEpoch 45700, Loss: 12930.6015625\nEpoch 45800, Loss: 12930.595703125\nEpoch 45900, Loss: 12930.60546875\nEpoch 46000, Loss: 12930.6015625\nEpoch 46100, Loss: 12930.6025390625\nEpoch 46200, Loss: 12930.603515625\nEpoch 46300, Loss: 12930.6015625\nEpoch 46400, Loss: 12930.59765625\nEpoch 46500, Loss: 12930.60546875\nEpoch 46600, Loss: 12930.6015625\nEpoch 46700, Loss: 12930.6015625\nEpoch 46800, Loss: 12930.599609375\nEpoch 46900, Loss: 12930.6044921875\nEpoch 47000, Loss: 12930.603515625\nEpoch 47100, Loss: 12930.59765625\nEpoch 47200, Loss: 12930.609375\nEpoch 47300, Loss: 12930.599609375\nEpoch 47400, Loss: 12930.5966796875\nEpoch 47500, Loss: 12930.6025390625\nEpoch 47600, Loss: 12930.603515625\nEpoch 47700, Loss: 12930.6015625\nEpoch 47800, Loss: 12930.599609375\nEpoch 47900, Loss: 12930.60546875\nEpoch 48000, Loss: 12930.5986328125\nEpoch 48100, Loss: 12930.603515625\nEpoch 48200, Loss: 12930.6025390625\nEpoch 48300, Loss: 12930.6005859375\nEpoch 48400, Loss: 12930.6064453125\nEpoch 48500, Loss: 12930.6103515625\nEpoch 48600, Loss: 12930.603515625\nEpoch 48700, Loss: 12930.6015625\nEpoch 48800, Loss: 12930.6005859375\nEpoch 48900, Loss: 12930.6015625\nEpoch 49000, Loss: 12930.6015625\nEpoch 49100, Loss: 12930.60546875\nEpoch 49200, Loss: 12930.60546875\nEpoch 49300, Loss: 12930.60546875\nEpoch 49400, Loss: 12930.603515625\nEpoch 49500, Loss: 12930.5986328125\nEpoch 49600, Loss: 12930.6015625\nEpoch 49700, Loss: 12930.599609375\nEpoch 49800, Loss: 12930.5986328125\nEpoch 49900, Loss: 12930.6064453125\nEpoch 0, Loss: 63828.96875\nEpoch 100, Loss: 60996.9375\nEpoch 200, Loss: 58349.37890625\nEpoch 300, Loss: 55878.4921875\nEpoch 400, Loss: 53573.59765625\nEpoch 500, Loss: 51422.99609375\nEpoch 600, Loss: 49418.08203125\nEpoch 700, Loss: 47546.9453125\nEpoch 800, Loss: 45799.41015625\nEpoch 900, Loss: 44166.7109375\nEpoch 1000, Loss: 42641.23046875\nEpoch 1100, Loss: 41218.9296875\nEpoch 1200, Loss: 39891.79296875\nEpoch 1300, Loss: 38653.25390625\nEpoch 1400, Loss: 37496.91796875\nEpoch 1500, Loss: 36416.86328125\nEpoch 1600, Loss: 35407.66015625\nEpoch 1700, Loss: 34467.671875\nEpoch 1800, Loss: 33590.98046875\nEpoch 1900, Loss: 32774.44921875\nEpoch 2000, Loss: 32015.833984375\nEpoch 2100, Loss: 31311.072265625\nEpoch 2200, Loss: 30653.103515625\nEpoch 2300, Loss: 30037.916015625\nEpoch 2400, Loss: 29461.9296875\nEpoch 2500, Loss: 28921.97265625\nEpoch 2600, Loss: 28415.224609375\nEpoch 2700, Loss: 27939.15234375\nEpoch 2800, Loss: 27491.482421875\nEpoch 2900, Loss: 27070.12109375\nEpoch 3000, Loss: 26673.384765625\nEpoch 3100, Loss: 26301.947265625\nEpoch 3200, Loss: 25956.23828125\nEpoch 3300, Loss: 25634.263671875\nEpoch 3400, Loss: 25329.873046875\nEpoch 3500, Loss: 25041.3984375\nEpoch 3600, Loss: 24771.2265625\nEpoch 3700, Loss: 24514.32421875\nEpoch 3800, Loss: 24269.275390625\nEpoch 3900, Loss: 24034.865234375\nEpoch 4000, Loss: 23811.521484375\nEpoch 4100, Loss: 23598.94921875\nEpoch 4200, Loss: 23394.12109375\nEpoch 4300, Loss: 23196.232421875\nEpoch 4400, Loss: 23004.58203125\nEpoch 4500, Loss: 22818.5703125\nEpoch 4600, Loss: 22637.6875\nEpoch 4700, Loss: 22461.486328125\nEpoch 4800, Loss: 22289.5859375\nEpoch 4900, Loss: 22122.046875\nEpoch 5000, Loss: 21959.755859375\nEpoch 5100, Loss: 21800.990234375\nEpoch 5200, Loss: 21645.87109375\nEpoch 5300, Loss: 21493.779296875\nEpoch 5400, Loss: 21344.4765625\nEpoch 5500, Loss: 21197.82421875\nEpoch 5600, Loss: 21053.677734375\nEpoch 5700, Loss: 20911.931640625\nEpoch 5800, Loss: 20772.4765625\nEpoch 5900, Loss: 20635.224609375\nEpoch 6000, Loss: 20500.0859375\nEpoch 6100, Loss: 20366.984375\nEpoch 6200, Loss: 20237.326171875\nEpoch 6300, Loss: 20109.7265625\nEpoch 6400, Loss: 19984.490234375\nEpoch 6500, Loss: 19862.9375\nEpoch 6600, Loss: 19743.271484375\nEpoch 6700, Loss: 19625.90234375\nEpoch 6800, Loss: 19510.0703125\nEpoch 6900, Loss: 19395.705078125\nEpoch 7000, Loss: 19282.7734375\nEpoch 7100, Loss: 19171.21875\nEpoch 7200, Loss: 19061.017578125\nEpoch 7300, Loss: 18952.109375\nEpoch 7400, Loss: 18844.4921875\nEpoch 7500, Loss: 18738.40234375\nEpoch 7600, Loss: 18637.13671875\nEpoch 7700, Loss: 18537.041015625\nEpoch 7800, Loss: 18438.015625\nEpoch 7900, Loss: 18340.0390625\nEpoch 8000, Loss: 18243.09765625\nEpoch 8100, Loss: 18147.1796875\nEpoch 8200, Loss: 18052.265625\nEpoch 8300, Loss: 17958.330078125\nEpoch 8400, Loss: 17865.369140625\nEpoch 8500, Loss: 17773.375\nEpoch 8600, Loss: 17682.3359375\nEpoch 8700, Loss: 17592.240234375\nEpoch 8800, Loss: 17503.08984375\nEpoch 8900, Loss: 17414.875\nEpoch 9000, Loss: 17327.59375\nEpoch 9100, Loss: 17241.240234375\nEpoch 9200, Loss: 17155.859375\nEpoch 9300, Loss: 17071.43359375\nEpoch 9400, Loss: 16989.2265625\nEpoch 9500, Loss: 16908.373046875\nEpoch 9600, Loss: 16828.81640625\nEpoch 9700, Loss: 16750.5546875\nEpoch 9800, Loss: 16673.68359375\nEpoch 9900, Loss: 16598.2734375\nEpoch 10000, Loss: 16524.201171875\nEpoch 10100, Loss: 16452.529296875\nEpoch 10200, Loss: 16382.771484375\nEpoch 10300, Loss: 16314.39453125\nEpoch 10400, Loss: 16247.3046875\nEpoch 10500, Loss: 16181.4560546875\nEpoch 10600, Loss: 16117.42578125\nEpoch 10700, Loss: 16054.6640625\nEpoch 10800, Loss: 15993.1513671875\nEpoch 10900, Loss: 15932.798828125\nEpoch 11000, Loss: 15873.55859375\nEpoch 11100, Loss: 15815.41015625\nEpoch 11200, Loss: 15758.4521484375\nEpoch 11300, Loss: 15702.6748046875\nEpoch 11400, Loss: 15648.10546875\nEpoch 11500, Loss: 15594.685546875\nEpoch 11600, Loss: 15542.2958984375\nEpoch 11700, Loss: 15490.98046875\nEpoch 11800, Loss: 15440.6640625\nEpoch 11900, Loss: 15391.35546875\nEpoch 12000, Loss: 15343.04296875\nEpoch 12100, Loss: 15295.6923828125\nEpoch 12200, Loss: 15249.326171875\nEpoch 12300, Loss: 15203.9921875\nEpoch 12400, Loss: 15159.66796875\nEpoch 12500, Loss: 15116.7294921875\nEpoch 12600, Loss: 15074.884765625\nEpoch 12700, Loss: 15034.0654296875\nEpoch 12800, Loss: 14994.2724609375\nEpoch 12900, Loss: 14955.482421875\nEpoch 13000, Loss: 14917.7412109375\nEpoch 13100, Loss: 14880.9853515625\nEpoch 13200, Loss: 14845.1474609375\nEpoch 13300, Loss: 14810.16796875\nEpoch 13400, Loss: 14776.029296875\nEpoch 13500, Loss: 14742.69140625\nEpoch 13600, Loss: 14710.12109375\nEpoch 13700, Loss: 14678.2890625\nEpoch 13800, Loss: 14647.177734375\nEpoch 13900, Loss: 14616.767578125\nEpoch 14000, Loss: 14587.05078125\nEpoch 14100, Loss: 14557.9833984375\nEpoch 14200, Loss: 14529.568359375\nEpoch 14300, Loss: 14501.7783203125\nEpoch 14400, Loss: 14475.8837890625\nEpoch 14500, Loss: 14450.96484375\nEpoch 14600, Loss: 14426.822265625\nEpoch 14700, Loss: 14403.3525390625\nEpoch 14800, Loss: 14380.521484375\nEpoch 14900, Loss: 14358.30078125\nEpoch 15000, Loss: 14336.611328125\nEpoch 15100, Loss: 14315.4736328125\nEpoch 15200, Loss: 14295.265625\nEpoch 15300, Loss: 14275.705078125\nEpoch 15400, Loss: 14256.6591796875\nEpoch 15500, Loss: 14238.11328125\nEpoch 15600, Loss: 14220.064453125\nEpoch 15700, Loss: 14202.458984375\nEpoch 15800, Loss: 14185.318359375\nEpoch 15900, Loss: 14168.626953125\nEpoch 16000, Loss: 14152.384765625\nEpoch 16100, Loss: 14136.564453125\nEpoch 16200, Loss: 14121.1728515625\nEpoch 16300, Loss: 14106.1806640625\nEpoch 16400, Loss: 14091.6015625\nEpoch 16500, Loss: 14077.4140625\nEpoch 16600, Loss: 14063.611328125\nEpoch 16700, Loss: 14050.1845703125\nEpoch 16800, Loss: 14037.14453125\nEpoch 16900, Loss: 14024.453125\nEpoch 17000, Loss: 14012.1103515625\nEpoch 17100, Loss: 14000.087890625\nEpoch 17200, Loss: 13988.3828125\nEpoch 17300, Loss: 13977.0078125\nEpoch 17400, Loss: 13965.9345703125\nEpoch 17500, Loss: 13955.150390625\nEpoch 17600, Loss: 13944.642578125\nEpoch 17700, Loss: 13934.404296875\nEpoch 17800, Loss: 13924.4482421875\nEpoch 17900, Loss: 13915.234375\nEpoch 18000, Loss: 13906.609375\nEpoch 18100, Loss: 13898.4033203125\nEpoch 18200, Loss: 13890.4892578125\nEpoch 18300, Loss: 13882.8662109375\nEpoch 18400, Loss: 13875.48828125\nEpoch 18500, Loss: 13868.33203125\nEpoch 18600, Loss: 13861.3642578125\nEpoch 18700, Loss: 13854.583984375\nEpoch 18800, Loss: 13847.9755859375\nEpoch 18900, Loss: 13841.53125\nEpoch 19000, Loss: 13835.2275390625\nEpoch 19100, Loss: 13829.05859375\nEpoch 19200, Loss: 13823.017578125\nEpoch 19300, Loss: 13817.125\nEpoch 19400, Loss: 13811.345703125\nEpoch 19500, Loss: 13805.671875\nEpoch 19600, Loss: 13800.1162109375\nEpoch 19700, Loss: 13794.66015625\nEpoch 19800, Loss: 13789.3203125\nEpoch 19900, Loss: 13784.0712890625\nEpoch 20000, Loss: 13778.916015625\nEpoch 20100, Loss: 13773.876953125\nEpoch 20200, Loss: 13768.9140625\nEpoch 20300, Loss: 13764.052734375\nEpoch 20400, Loss: 13759.302734375\nEpoch 20500, Loss: 13754.6376953125\nEpoch 20600, Loss: 13750.0791015625\nEpoch 20700, Loss: 13745.623046875\nEpoch 20800, Loss: 13741.251953125\nEpoch 20900, Loss: 13737.0107421875\nEpoch 21000, Loss: 13732.853515625\nEpoch 21100, Loss: 13728.7978515625\nEpoch 21200, Loss: 13724.8515625\nEpoch 21300, Loss: 13721.0126953125\nEpoch 21400, Loss: 13717.265625\nEpoch 21500, Loss: 13713.625\nEpoch 21600, Loss: 13710.0849609375\nEpoch 21700, Loss: 13706.6640625\nEpoch 21800, Loss: 13703.3291015625\nEpoch 21900, Loss: 13700.09375\nEpoch 22000, Loss: 13696.970703125\nEpoch 22100, Loss: 13693.94140625\nEpoch 22200, Loss: 13691.0087890625\nEpoch 22300, Loss: 13688.16796875\nEpoch 22400, Loss: 13685.4150390625\nEpoch 22500, Loss: 13682.7548828125\nEpoch 22600, Loss: 13680.1787109375\nEpoch 22700, Loss: 13677.693359375\nEpoch 22800, Loss: 13675.2705078125\nEpoch 22900, Loss: 13672.9375\nEpoch 23000, Loss: 13670.67578125\nEpoch 23100, Loss: 13668.4892578125\nEpoch 23200, Loss: 13666.373046875\nEpoch 23300, Loss: 13664.32421875\nEpoch 23400, Loss: 13662.361328125\nEpoch 23500, Loss: 13660.4599609375\nEpoch 23600, Loss: 13658.6279296875\nEpoch 23700, Loss: 13656.84375\nEpoch 23800, Loss: 13655.146484375\nEpoch 23900, Loss: 13653.5\nEpoch 24000, Loss: 13651.9248046875\nEpoch 24100, Loss: 13650.404296875\nEpoch 24200, Loss: 13648.953125\nEpoch 24300, Loss: 13647.552734375\nEpoch 24400, Loss: 13646.2099609375\nEpoch 24500, Loss: 13644.9404296875\nEpoch 24600, Loss: 13643.71484375\nEpoch 24700, Loss: 13642.5390625\nEpoch 24800, Loss: 13641.4296875\nEpoch 24900, Loss: 13640.3544921875\nEpoch 25000, Loss: 13639.3427734375\nEpoch 25100, Loss: 13638.37109375\nEpoch 25200, Loss: 13637.44921875\nEpoch 25300, Loss: 13636.583984375\nEpoch 25400, Loss: 13635.759765625\nEpoch 25500, Loss: 13634.974609375\nEpoch 25600, Loss: 13634.2353515625\nEpoch 25700, Loss: 13633.525390625\nEpoch 25800, Loss: 13632.876953125\nEpoch 25900, Loss: 13632.244140625\nEpoch 26000, Loss: 13631.654296875\nEpoch 26100, Loss: 13631.107421875\nEpoch 26200, Loss: 13630.58984375\nEpoch 26300, Loss: 13630.0986328125\nEpoch 26400, Loss: 13629.638671875\nEpoch 26500, Loss: 13629.224609375\nEpoch 26600, Loss: 13628.8330078125\nEpoch 26700, Loss: 13628.4521484375\nEpoch 26800, Loss: 13628.111328125\nEpoch 26900, Loss: 13627.7939453125\nEpoch 27000, Loss: 13627.501953125\nEpoch 27100, Loss: 13627.2275390625\nEpoch 27200, Loss: 13626.9794921875\nEpoch 27300, Loss: 13626.736328125\nEpoch 27400, Loss: 13626.52734375\nEpoch 27500, Loss: 13626.32421875\nEpoch 27600, Loss: 13626.1376953125\nEpoch 27700, Loss: 13625.95703125\nEpoch 27800, Loss: 13625.7939453125\nEpoch 27900, Loss: 13625.640625\nEpoch 28000, Loss: 13625.4892578125\nEpoch 28100, Loss: 13625.3564453125\nEpoch 28200, Loss: 13625.23828125\nEpoch 28300, Loss: 13625.1171875\nEpoch 28400, Loss: 13625.005859375\nEpoch 28500, Loss: 13624.884765625\nEpoch 28600, Loss: 13624.771484375\nEpoch 28700, Loss: 13624.67578125\nEpoch 28800, Loss: 13624.564453125\nEpoch 28900, Loss: 13624.46875\nEpoch 29000, Loss: 13624.365234375\nEpoch 29100, Loss: 13624.2783203125\nEpoch 29200, Loss: 13624.1796875\nEpoch 29300, Loss: 13624.087890625\nEpoch 29400, Loss: 13624.005859375\nEpoch 29500, Loss: 13623.9072265625\nEpoch 29600, Loss: 13623.8271484375\nEpoch 29700, Loss: 13623.73046875\nEpoch 29800, Loss: 13623.65234375\nEpoch 29900, Loss: 13623.56640625\nEpoch 30000, Loss: 13623.484375\nEpoch 30100, Loss: 13623.3955078125\nEpoch 30200, Loss: 13623.330078125\nEpoch 30300, Loss: 13623.2451171875\nEpoch 30400, Loss: 13623.1533203125\nEpoch 30500, Loss: 13623.072265625\nEpoch 30600, Loss: 13622.990234375\nEpoch 30700, Loss: 13622.9150390625\nEpoch 30800, Loss: 13622.8359375\nEpoch 30900, Loss: 13622.76953125\nEpoch 31000, Loss: 13622.6806640625\nEpoch 31100, Loss: 13622.60546875\nEpoch 31200, Loss: 13622.537109375\nEpoch 31300, Loss: 13622.45703125\nEpoch 31400, Loss: 13622.390625\nEpoch 31500, Loss: 13622.3125\nEpoch 31600, Loss: 13622.2421875\nEpoch 31700, Loss: 13622.1748046875\nEpoch 31800, Loss: 13622.1015625\nEpoch 31900, Loss: 13622.0380859375\nEpoch 32000, Loss: 13621.95703125\nEpoch 32100, Loss: 13621.9013671875\nEpoch 32200, Loss: 13621.830078125\nEpoch 32300, Loss: 13621.7607421875\nEpoch 32400, Loss: 13621.6943359375\nEpoch 32500, Loss: 13621.630859375\nEpoch 32600, Loss: 13621.5703125\nEpoch 32700, Loss: 13621.509765625\nEpoch 32800, Loss: 13621.453125\nEpoch 32900, Loss: 13621.3876953125\nEpoch 33000, Loss: 13621.3232421875\nEpoch 33100, Loss: 13621.2685546875\nEpoch 33200, Loss: 13621.2041015625\nEpoch 33300, Loss: 13621.1552734375\nEpoch 33400, Loss: 13621.0986328125\nEpoch 33500, Loss: 13621.037109375\nEpoch 33600, Loss: 13620.98046875\nEpoch 33700, Loss: 13620.921875\nEpoch 33800, Loss: 13620.8720703125\nEpoch 33900, Loss: 13620.818359375\nEpoch 34000, Loss: 13620.77734375\nEpoch 34100, Loss: 13620.724609375\nEpoch 34200, Loss: 13620.6630859375\nEpoch 34300, Loss: 13620.61328125\nEpoch 34400, Loss: 13620.564453125\nEpoch 34500, Loss: 13620.51953125\nEpoch 34600, Loss: 13620.4677734375\nEpoch 34700, Loss: 13620.4228515625\nEpoch 34800, Loss: 13620.3740234375\nEpoch 34900, Loss: 13620.330078125\nEpoch 35000, Loss: 13620.29296875\nEpoch 35100, Loss: 13620.240234375\nEpoch 35200, Loss: 13620.20703125\nEpoch 35300, Loss: 13620.1611328125\nEpoch 35400, Loss: 13620.119140625\nEpoch 35500, Loss: 13620.0810546875\nEpoch 35600, Loss: 13620.041015625\nEpoch 35700, Loss: 13620.0009765625\nEpoch 35800, Loss: 13619.9658203125\nEpoch 35900, Loss: 13619.923828125\nEpoch 36000, Loss: 13619.884765625\nEpoch 36100, Loss: 13619.85546875\nEpoch 36200, Loss: 13619.8173828125\nEpoch 36300, Loss: 13619.77734375\nEpoch 36400, Loss: 13619.751953125\nEpoch 36500, Loss: 13619.7119140625\nEpoch 36600, Loss: 13619.68359375\nEpoch 36700, Loss: 13619.6396484375\nEpoch 36800, Loss: 13619.6171875\nEpoch 36900, Loss: 13619.59375\nEpoch 37000, Loss: 13619.5595703125\nEpoch 37100, Loss: 13619.537109375\nEpoch 37200, Loss: 13619.498046875\nEpoch 37300, Loss: 13619.466796875\nEpoch 37400, Loss: 13619.4501953125\nEpoch 37500, Loss: 13619.416015625\nEpoch 37600, Loss: 13619.38671875\nEpoch 37700, Loss: 13619.3701171875\nEpoch 37800, Loss: 13619.3515625\nEpoch 37900, Loss: 13619.337890625\nEpoch 38000, Loss: 13619.349609375\nEpoch 38100, Loss: 13619.34375\nEpoch 38200, Loss: 13619.341796875\nEpoch 38300, Loss: 13619.337890625\nEpoch 38400, Loss: 13619.341796875\nEpoch 38500, Loss: 13619.341796875\nEpoch 38600, Loss: 13619.3349609375\nEpoch 38700, Loss: 13619.3466796875\nEpoch 38800, Loss: 13619.33984375\nEpoch 38900, Loss: 13619.341796875\nEpoch 39000, Loss: 13619.337890625\nEpoch 39100, Loss: 13619.3388671875\nEpoch 39200, Loss: 13619.337890625\nEpoch 39300, Loss: 13619.345703125\nEpoch 39400, Loss: 13619.33984375\nEpoch 39500, Loss: 13619.337890625\nEpoch 39600, Loss: 13619.345703125\nEpoch 39700, Loss: 13619.337890625\nEpoch 39800, Loss: 13619.3388671875\nEpoch 39900, Loss: 13619.3408203125\nEpoch 40000, Loss: 13619.333984375\nEpoch 40100, Loss: 13619.3359375\nEpoch 40200, Loss: 13619.341796875\nEpoch 40300, Loss: 13619.3447265625\nEpoch 40400, Loss: 13619.341796875\nEpoch 40500, Loss: 13619.333984375\nEpoch 40600, Loss: 13619.337890625\nEpoch 40700, Loss: 13619.341796875\nEpoch 40800, Loss: 13619.33984375\nEpoch 40900, Loss: 13619.345703125\nEpoch 41000, Loss: 13619.337890625\nEpoch 41100, Loss: 13619.341796875\nEpoch 41200, Loss: 13619.345703125\nEpoch 41300, Loss: 13619.341796875\nEpoch 41400, Loss: 13619.34375\nEpoch 41500, Loss: 13619.3388671875\nEpoch 41600, Loss: 13619.3359375\nEpoch 41700, Loss: 13619.34765625\nEpoch 41800, Loss: 13619.333984375\nEpoch 41900, Loss: 13619.3408203125\nEpoch 42000, Loss: 13619.3388671875\nEpoch 42100, Loss: 13619.3408203125\nEpoch 42200, Loss: 13619.3388671875\nEpoch 42300, Loss: 13619.3330078125\nEpoch 42400, Loss: 13619.345703125\nEpoch 42500, Loss: 13619.337890625\nEpoch 42600, Loss: 13619.33203125\nEpoch 42700, Loss: 13619.3310546875\nEpoch 42800, Loss: 13619.34375\nEpoch 42900, Loss: 13619.3359375\nEpoch 43000, Loss: 13619.3427734375\nEpoch 43100, Loss: 13619.3388671875\nEpoch 43200, Loss: 13619.3349609375\nEpoch 43300, Loss: 13619.3388671875\nEpoch 43400, Loss: 13619.3408203125\nEpoch 43500, Loss: 13619.34375\nEpoch 43600, Loss: 13619.3369140625\nEpoch 43700, Loss: 13619.3349609375\nEpoch 43800, Loss: 13619.337890625\nEpoch 43900, Loss: 13619.3427734375\nEpoch 44000, Loss: 13619.34375\nEpoch 44100, Loss: 13619.33203125\nEpoch 44200, Loss: 13619.33984375\nEpoch 44300, Loss: 13619.3427734375\nEpoch 44400, Loss: 13619.3359375\nEpoch 44500, Loss: 13619.341796875\nEpoch 44600, Loss: 13619.3388671875\nEpoch 44700, Loss: 13619.34375\nEpoch 44800, Loss: 13619.337890625\nEpoch 44900, Loss: 13619.33984375\nEpoch 45000, Loss: 13619.3359375\nEpoch 45100, Loss: 13619.345703125\nEpoch 45200, Loss: 13619.345703125\nEpoch 45300, Loss: 13619.341796875\nEpoch 45400, Loss: 13619.34375\nEpoch 45500, Loss: 13619.3408203125\nEpoch 45600, Loss: 13619.3388671875\nEpoch 45700, Loss: 13619.333984375\nEpoch 45800, Loss: 13619.33984375\nEpoch 45900, Loss: 13619.33984375\nEpoch 46000, Loss: 13619.341796875\nEpoch 46100, Loss: 13619.3447265625\nEpoch 46200, Loss: 13619.33203125\nEpoch 46300, Loss: 13619.3330078125\nEpoch 46400, Loss: 13619.3408203125\nEpoch 46500, Loss: 13619.3505859375\nEpoch 46600, Loss: 13619.3427734375\nEpoch 46700, Loss: 13619.337890625\nEpoch 46800, Loss: 13619.345703125\nEpoch 46900, Loss: 13619.3427734375\nEpoch 47000, Loss: 13619.3408203125\nEpoch 47100, Loss: 13619.3369140625\nEpoch 47200, Loss: 13619.337890625\nEpoch 47300, Loss: 13619.345703125\nEpoch 47400, Loss: 13619.345703125\nEpoch 47500, Loss: 13619.341796875\nEpoch 47600, Loss: 13619.3408203125\nEpoch 47700, Loss: 13619.341796875\nEpoch 47800, Loss: 13619.33984375\nEpoch 47900, Loss: 13619.3447265625\nEpoch 48000, Loss: 13619.3505859375\nEpoch 48100, Loss: 13619.3408203125\nEpoch 48200, Loss: 13619.341796875\nEpoch 48300, Loss: 13619.337890625\nEpoch 48400, Loss: 13619.3369140625\nEpoch 48500, Loss: 13619.3310546875\nEpoch 48600, Loss: 13619.3427734375\nEpoch 48700, Loss: 13619.3369140625\nEpoch 48800, Loss: 13619.3349609375\nEpoch 48900, Loss: 13619.33984375\nEpoch 49000, Loss: 13619.3447265625\nEpoch 49100, Loss: 13619.3349609375\nEpoch 49200, Loss: 13619.337890625\nEpoch 49300, Loss: 13619.3369140625\nEpoch 49400, Loss: 13619.33984375\nEpoch 49500, Loss: 13619.34375\nEpoch 49600, Loss: 13619.3359375\nEpoch 49700, Loss: 13619.330078125\nEpoch 49800, Loss: 13619.33984375\nEpoch 49900, Loss: 13619.341796875\nEpoch 0, Loss: 63828.96875\nEpoch 100, Loss: 61016.08203125\nEpoch 200, Loss: 58386.9296875\nEpoch 300, Loss: 55933.87109375\nEpoch 400, Loss: 53645.8828125\nEpoch 500, Loss: 51514.65234375\nEpoch 600, Loss: 49529.55078125\nEpoch 700, Loss: 47679.4921875\nEpoch 800, Loss: 45953.59375\nEpoch 900, Loss: 44343.03515625\nEpoch 1000, Loss: 42842.19140625\nEpoch 1100, Loss: 41441.9609375\nEpoch 1200, Loss: 40135.21875\nEpoch 1300, Loss: 38915.5078125\nEpoch 1400, Loss: 37776.58203125\nEpoch 1500, Loss: 36712.94921875\nEpoch 1600, Loss: 35722.59765625\nEpoch 1700, Loss: 34798.4453125\nEpoch 1800, Loss: 33938.85546875\nEpoch 1900, Loss: 33140.40625\nEpoch 2000, Loss: 32399.69921875\nEpoch 2100, Loss: 31709.287109375\nEpoch 2200, Loss: 31064.7109375\nEpoch 2300, Loss: 30461.958984375\nEpoch 2400, Loss: 29897.462890625\nEpoch 2500, Loss: 29368.080078125\nEpoch 2600, Loss: 28871.044921875\nEpoch 2700, Loss: 28403.884765625\nEpoch 2800, Loss: 27964.388671875\nEpoch 2900, Loss: 27552.05078125\nEpoch 3000, Loss: 27165.337890625\nEpoch 3100, Loss: 26804.6015625\nEpoch 3200, Loss: 26469.478515625\nEpoch 3300, Loss: 26155.396484375\nEpoch 3400, Loss: 25859.599609375\nEpoch 3500, Loss: 25582.515625\nEpoch 3600, Loss: 25319.658203125\nEpoch 3700, Loss: 25069.52734375\nEpoch 3800, Loss: 24830.794921875\nEpoch 3900, Loss: 24604.845703125\nEpoch 4000, Loss: 24389.802734375\nEpoch 4100, Loss: 24183.046875\nEpoch 4200, Loss: 23983.666015625\nEpoch 4300, Loss: 23790.8828125\nEpoch 4400, Loss: 23604.015625\nEpoch 4500, Loss: 23422.501953125\nEpoch 4600, Loss: 23245.837890625\nEpoch 4700, Loss: 23074.392578125\nEpoch 4800, Loss: 22908.6953125\nEpoch 4900, Loss: 22747.318359375\nEpoch 5000, Loss: 22589.349609375\nEpoch 5100, Loss: 22434.53125\nEpoch 5200, Loss: 22282.65234375\nEpoch 5300, Loss: 22133.50390625\nEpoch 5400, Loss: 21986.93359375\nEpoch 5500, Loss: 21842.798828125\nEpoch 5600, Loss: 21700.974609375\nEpoch 5700, Loss: 21561.3359375\nEpoch 5800, Loss: 21423.806640625\nEpoch 5900, Loss: 21288.4296875\nEpoch 6000, Loss: 21156.572265625\nEpoch 6100, Loss: 21026.99609375\nEpoch 6200, Loss: 20900.203125\nEpoch 6300, Loss: 20777.70703125\nEpoch 6400, Loss: 20657.0390625\nEpoch 6500, Loss: 20537.9140625\nEpoch 6600, Loss: 20420.220703125\nEpoch 6700, Loss: 20303.93359375\nEpoch 6800, Loss: 20188.984375\nEpoch 6900, Loss: 20075.33203125\nEpoch 7000, Loss: 19962.947265625\nEpoch 7100, Loss: 19851.79296875\nEpoch 7200, Loss: 19741.84765625\nEpoch 7300, Loss: 19633.078125\nEpoch 7400, Loss: 19525.47265625\nEpoch 7500, Loss: 19419.279296875\nEpoch 7600, Loss: 19318.642578125\nEpoch 7700, Loss: 19219.12890625\nEpoch 7800, Loss: 19120.609375\nEpoch 7900, Loss: 19023.0546875\nEpoch 8000, Loss: 18926.5546875\nEpoch 8100, Loss: 18831.076171875\nEpoch 8200, Loss: 18736.509765625\nEpoch 8300, Loss: 18642.837890625\nEpoch 8400, Loss: 18550.072265625\nEpoch 8500, Loss: 18458.203125\nEpoch 8600, Loss: 18367.22265625\nEpoch 8700, Loss: 18277.123046875\nEpoch 8800, Loss: 18187.890625\nEpoch 8900, Loss: 18099.52734375\nEpoch 9000, Loss: 18013.0703125\nEpoch 9100, Loss: 17928.38671875\nEpoch 9200, Loss: 17844.939453125\nEpoch 9300, Loss: 17762.728515625\nEpoch 9400, Loss: 17681.728515625\nEpoch 9500, Loss: 17601.97265625\nEpoch 9600, Loss: 17524.89453125\nEpoch 9700, Loss: 17450.4140625\nEpoch 9800, Loss: 17377.4609375\nEpoch 9900, Loss: 17305.95703125\nEpoch 10000, Loss: 17235.890625\nEpoch 10100, Loss: 17167.15625\nEpoch 10200, Loss: 17100.2421875\nEpoch 10300, Loss: 17034.67578125\nEpoch 10400, Loss: 16970.28125\nEpoch 10500, Loss: 16907.033203125\nEpoch 10600, Loss: 16845.091796875\nEpoch 10700, Loss: 16784.462890625\nEpoch 10800, Loss: 16725.123046875\nEpoch 10900, Loss: 16666.91796875\nEpoch 11000, Loss: 16609.755859375\nEpoch 11100, Loss: 16553.662109375\nEpoch 11200, Loss: 16498.572265625\nEpoch 11300, Loss: 16444.515625\nEpoch 11400, Loss: 16391.453125\nEpoch 11500, Loss: 16339.396484375\nEpoch 11600, Loss: 16288.3369140625\nEpoch 11700, Loss: 16238.380859375\nEpoch 11800, Loss: 16189.4140625\nEpoch 11900, Loss: 16141.72265625\nEpoch 12000, Loss: 16095.49609375\nEpoch 12100, Loss: 16050.4052734375\nEpoch 12200, Loss: 16006.4619140625\nEpoch 12300, Loss: 15963.8046875\nEpoch 12400, Loss: 15922.2099609375\nEpoch 12500, Loss: 15881.671875\nEpoch 12600, Loss: 15842.10546875\nEpoch 12700, Loss: 15803.5\nEpoch 12800, Loss: 15765.7958984375\nEpoch 12900, Loss: 15728.96484375\nEpoch 13000, Loss: 15692.9765625\nEpoch 13100, Loss: 15657.7978515625\nEpoch 13200, Loss: 15623.4140625\nEpoch 13300, Loss: 15589.78125\nEpoch 13400, Loss: 15556.8798828125\nEpoch 13500, Loss: 15524.6923828125\nEpoch 13600, Loss: 15493.2041015625\nEpoch 13700, Loss: 15462.37109375\nEpoch 13800, Loss: 15432.1796875\nEpoch 13900, Loss: 15402.640625\nEpoch 14000, Loss: 15374.35546875\nEpoch 14100, Loss: 15347.84765625\nEpoch 14200, Loss: 15322.205078125\nEpoch 14300, Loss: 15297.298828125\nEpoch 14400, Loss: 15273.060546875\nEpoch 14500, Loss: 15249.4521484375\nEpoch 14600, Loss: 15226.9541015625\nEpoch 14700, Loss: 15205.173828125\nEpoch 14800, Loss: 15183.953125\nEpoch 14900, Loss: 15163.265625\nEpoch 15000, Loss: 15143.078125\nEpoch 15100, Loss: 15123.3916015625\nEpoch 15200, Loss: 15104.1787109375\nEpoch 15300, Loss: 15085.431640625\nEpoch 15400, Loss: 15067.1474609375\nEpoch 15500, Loss: 15049.296875\nEpoch 15600, Loss: 15031.892578125\nEpoch 15700, Loss: 15014.9140625\nEpoch 15800, Loss: 14998.3564453125\nEpoch 15900, Loss: 14982.2314453125\nEpoch 16000, Loss: 14966.486328125\nEpoch 16100, Loss: 14951.162109375\nEpoch 16200, Loss: 14936.2197265625\nEpoch 16300, Loss: 14921.67578125\nEpoch 16400, Loss: 14907.498046875\nEpoch 16500, Loss: 14893.7099609375\nEpoch 16600, Loss: 14880.255859375\nEpoch 16700, Loss: 14867.1572265625\nEpoch 16800, Loss: 14854.40625\nEpoch 16900, Loss: 14841.986328125\nEpoch 17000, Loss: 14829.87890625\nEpoch 17100, Loss: 14818.0703125\nEpoch 17200, Loss: 14806.5478515625\nEpoch 17300, Loss: 14795.291015625\nEpoch 17400, Loss: 14784.759765625\nEpoch 17500, Loss: 14775.0078125\nEpoch 17600, Loss: 14765.708984375\nEpoch 17700, Loss: 14756.83984375\nEpoch 17800, Loss: 14748.2666015625\nEpoch 17900, Loss: 14739.9931640625\nEpoch 18000, Loss: 14731.953125\nEpoch 18100, Loss: 14724.1396484375\nEpoch 18200, Loss: 14716.5595703125\nEpoch 18300, Loss: 14709.123046875\nEpoch 18400, Loss: 14701.890625\nEpoch 18500, Loss: 14694.841796875\nEpoch 18600, Loss: 14687.94140625\nEpoch 18700, Loss: 14681.193359375\nEpoch 18800, Loss: 14674.580078125\nEpoch 18900, Loss: 14668.1171875\nEpoch 19000, Loss: 14661.75390625\nEpoch 19100, Loss: 14655.5068359375\nEpoch 19200, Loss: 14649.3818359375\nEpoch 19300, Loss: 14643.3583984375\nEpoch 19400, Loss: 14637.44921875\nEpoch 19500, Loss: 14631.65625\nEpoch 19600, Loss: 14625.9501953125\nEpoch 19700, Loss: 14620.3466796875\nEpoch 19800, Loss: 14614.859375\nEpoch 19900, Loss: 14609.46875\nEpoch 20000, Loss: 14604.189453125\nEpoch 20100, Loss: 14599.025390625\nEpoch 20200, Loss: 14593.9638671875\nEpoch 20300, Loss: 14589.017578125\nEpoch 20400, Loss: 14584.1845703125\nEpoch 20500, Loss: 14579.4501953125\nEpoch 20600, Loss: 14574.8310546875\nEpoch 20700, Loss: 14570.3359375\nEpoch 20800, Loss: 14565.9365234375\nEpoch 20900, Loss: 14561.65625\nEpoch 21000, Loss: 14557.4931640625\nEpoch 21100, Loss: 14553.447265625\nEpoch 21200, Loss: 14549.48046875\nEpoch 21300, Loss: 14545.65234375\nEpoch 21400, Loss: 14541.9306640625\nEpoch 21500, Loss: 14538.314453125\nEpoch 21600, Loss: 14534.8037109375\nEpoch 21700, Loss: 14531.4013671875\nEpoch 21800, Loss: 14528.103515625\nEpoch 21900, Loss: 14524.900390625\nEpoch 22000, Loss: 14521.8173828125\nEpoch 22100, Loss: 14518.8212890625\nEpoch 22200, Loss: 14515.9248046875\nEpoch 22300, Loss: 14513.14453125\nEpoch 22400, Loss: 14510.4267578125\nEpoch 22500, Loss: 14507.8037109375\nEpoch 22600, Loss: 14505.29296875\nEpoch 22700, Loss: 14502.859375\nEpoch 22800, Loss: 14500.5205078125\nEpoch 22900, Loss: 14498.248046875\nEpoch 23000, Loss: 14496.0615234375\nEpoch 23100, Loss: 14493.9228515625\nEpoch 23200, Loss: 14491.88671875\nEpoch 23300, Loss: 14489.91796875\nEpoch 23400, Loss: 14488.0107421875\nEpoch 23500, Loss: 14486.1689453125\nEpoch 23600, Loss: 14484.4013671875\nEpoch 23700, Loss: 14482.6904296875\nEpoch 23800, Loss: 14481.052734375\nEpoch 23900, Loss: 14479.45703125\nEpoch 24000, Loss: 14477.9462890625\nEpoch 24100, Loss: 14476.47265625\nEpoch 24200, Loss: 14475.0634765625\nEpoch 24300, Loss: 14473.7060546875\nEpoch 24400, Loss: 14472.4287109375\nEpoch 24500, Loss: 14471.1748046875\nEpoch 24600, Loss: 14469.99609375\nEpoch 24700, Loss: 14468.8623046875\nEpoch 24800, Loss: 14467.779296875\nEpoch 24900, Loss: 14466.7412109375\nEpoch 25000, Loss: 14465.75390625\nEpoch 25100, Loss: 14464.8193359375\nEpoch 25200, Loss: 14463.935546875\nEpoch 25300, Loss: 14463.083984375\nEpoch 25400, Loss: 14462.26171875\nEpoch 25500, Loss: 14461.5087890625\nEpoch 25600, Loss: 14460.77734375\nEpoch 25700, Loss: 14460.10546875\nEpoch 25800, Loss: 14459.4599609375\nEpoch 25900, Loss: 14458.84765625\nEpoch 26000, Loss: 14458.2607421875\nEpoch 26100, Loss: 14457.7177734375\nEpoch 26200, Loss: 14457.2041015625\nEpoch 26300, Loss: 14456.7236328125\nEpoch 26400, Loss: 14456.2763671875\nEpoch 26500, Loss: 14455.833984375\nEpoch 26600, Loss: 14455.4384765625\nEpoch 26700, Loss: 14455.0625\nEpoch 26800, Loss: 14454.724609375\nEpoch 26900, Loss: 14454.3818359375\nEpoch 27000, Loss: 14454.06640625\nEpoch 27100, Loss: 14453.78515625\nEpoch 27200, Loss: 14453.521484375\nEpoch 27300, Loss: 14453.2587890625\nEpoch 27400, Loss: 14453.025390625\nEpoch 27500, Loss: 14452.8037109375\nEpoch 27600, Loss: 14452.5810546875\nEpoch 27700, Loss: 14452.3896484375\nEpoch 27800, Loss: 14452.201171875\nEpoch 27900, Loss: 14452.0166015625\nEpoch 28000, Loss: 14451.845703125\nEpoch 28100, Loss: 14451.681640625\nEpoch 28200, Loss: 14451.529296875\nEpoch 28300, Loss: 14451.3720703125\nEpoch 28400, Loss: 14451.2119140625\nEpoch 28500, Loss: 14451.0791015625\nEpoch 28600, Loss: 14450.91796875\nEpoch 28700, Loss: 14450.794921875\nEpoch 28800, Loss: 14450.62890625\nEpoch 28900, Loss: 14450.4921875\nEpoch 29000, Loss: 14450.3720703125\nEpoch 29100, Loss: 14450.234375\nEpoch 29200, Loss: 14450.1005859375\nEpoch 29300, Loss: 14449.966796875\nEpoch 29400, Loss: 14449.83203125\nEpoch 29500, Loss: 14449.6962890625\nEpoch 29600, Loss: 14449.568359375\nEpoch 29700, Loss: 14449.4521484375\nEpoch 29800, Loss: 14449.3203125\nEpoch 29900, Loss: 14449.193359375\nEpoch 30000, Loss: 14449.05859375\nEpoch 30100, Loss: 14448.9365234375\nEpoch 30200, Loss: 14448.81640625\nEpoch 30300, Loss: 14448.681640625\nEpoch 30400, Loss: 14448.55859375\nEpoch 30500, Loss: 14448.44140625\nEpoch 30600, Loss: 14448.322265625\nEpoch 30700, Loss: 14448.2041015625\nEpoch 30800, Loss: 14448.0703125\nEpoch 30900, Loss: 14447.95703125\nEpoch 31000, Loss: 14447.8408203125\nEpoch 31100, Loss: 14447.7265625\nEpoch 31200, Loss: 14447.6181640625\nEpoch 31300, Loss: 14447.490234375\nEpoch 31400, Loss: 14447.376953125\nEpoch 31500, Loss: 14447.2685546875\nEpoch 31600, Loss: 14447.1484375\nEpoch 31700, Loss: 14447.0498046875\nEpoch 31800, Loss: 14446.9267578125\nEpoch 31900, Loss: 14446.8173828125\nEpoch 32000, Loss: 14446.7177734375\nEpoch 32100, Loss: 14446.59375\nEpoch 32200, Loss: 14446.4931640625\nEpoch 32300, Loss: 14446.376953125\nEpoch 32400, Loss: 14446.27734375\nEpoch 32500, Loss: 14446.177734375\nEpoch 32600, Loss: 14446.0625\nEpoch 32700, Loss: 14445.9736328125\nEpoch 32800, Loss: 14445.8583984375\nEpoch 32900, Loss: 14445.75\nEpoch 33000, Loss: 14445.64453125\nEpoch 33100, Loss: 14445.5546875\nEpoch 33200, Loss: 14445.4736328125\nEpoch 33300, Loss: 14445.3623046875\nEpoch 33400, Loss: 14445.267578125\nEpoch 33500, Loss: 14445.1728515625\nEpoch 33600, Loss: 14445.072265625\nEpoch 33700, Loss: 14444.96484375\nEpoch 33800, Loss: 14444.888671875\nEpoch 33900, Loss: 14444.78125\nEpoch 34000, Loss: 14444.7060546875\nEpoch 34100, Loss: 14444.6025390625\nEpoch 34200, Loss: 14444.50390625\nEpoch 34300, Loss: 14444.423828125\nEpoch 34400, Loss: 14444.33984375\nEpoch 34500, Loss: 14444.2451171875\nEpoch 34600, Loss: 14444.16015625\nEpoch 34700, Loss: 14444.0654296875\nEpoch 34800, Loss: 14443.9912109375\nEpoch 34900, Loss: 14443.900390625\nEpoch 35000, Loss: 14443.8212890625\nEpoch 35100, Loss: 14443.7197265625\nEpoch 35200, Loss: 14443.642578125\nEpoch 35300, Loss: 14443.5712890625\nEpoch 35400, Loss: 14443.494140625\nEpoch 35500, Loss: 14443.404296875\nEpoch 35600, Loss: 14443.3427734375\nEpoch 35700, Loss: 14443.2431640625\nEpoch 35800, Loss: 14443.17578125\nEpoch 35900, Loss: 14443.0927734375\nEpoch 36000, Loss: 14443.033203125\nEpoch 36100, Loss: 14442.94140625\nEpoch 36200, Loss: 14442.8671875\nEpoch 36300, Loss: 14442.7998046875\nEpoch 36400, Loss: 14442.7275390625\nEpoch 36500, Loss: 14442.671875\nEpoch 36600, Loss: 14442.6845703125\nEpoch 36700, Loss: 14442.6640625\nEpoch 36800, Loss: 14442.673828125\nEpoch 36900, Loss: 14442.6572265625\nEpoch 37000, Loss: 14442.6748046875\nEpoch 37100, Loss: 14442.6650390625\nEpoch 37200, Loss: 14442.6748046875\nEpoch 37300, Loss: 14442.6640625\nEpoch 37400, Loss: 14442.669921875\nEpoch 37500, Loss: 14442.6640625\nEpoch 37600, Loss: 14442.6591796875\nEpoch 37700, Loss: 14442.6708984375\nEpoch 37800, Loss: 14442.6669921875\nEpoch 37900, Loss: 14442.666015625\nEpoch 38000, Loss: 14442.66015625\nEpoch 38100, Loss: 14442.6640625\nEpoch 38200, Loss: 14442.6640625\nEpoch 38300, Loss: 14442.65234375\nEpoch 38400, Loss: 14442.6640625\nEpoch 38500, Loss: 14442.6630859375\nEpoch 38600, Loss: 14442.6708984375\nEpoch 38700, Loss: 14442.6650390625\nEpoch 38800, Loss: 14442.6669921875\nEpoch 38900, Loss: 14442.669921875\nEpoch 39000, Loss: 14442.6572265625\nEpoch 39100, Loss: 14442.6708984375\nEpoch 39200, Loss: 14442.662109375\nEpoch 39300, Loss: 14442.6572265625\nEpoch 39400, Loss: 14442.6650390625\nEpoch 39500, Loss: 14442.6591796875\nEpoch 39600, Loss: 14442.658203125\nEpoch 39700, Loss: 14442.6591796875\nEpoch 39800, Loss: 14442.673828125\nEpoch 39900, Loss: 14442.66015625\nEpoch 40000, Loss: 14442.6630859375\nEpoch 40100, Loss: 14442.6640625\nEpoch 40200, Loss: 14442.658203125\nEpoch 40300, Loss: 14442.6708984375\nEpoch 40400, Loss: 14442.669921875\nEpoch 40500, Loss: 14442.6591796875\nEpoch 40600, Loss: 14442.6611328125\nEpoch 40700, Loss: 14442.6650390625\nEpoch 40800, Loss: 14442.66015625\nEpoch 40900, Loss: 14442.6748046875\nEpoch 41000, Loss: 14442.6708984375\nEpoch 41100, Loss: 14442.662109375\nEpoch 41200, Loss: 14442.66015625\nEpoch 41300, Loss: 14442.6630859375\nEpoch 41400, Loss: 14442.6669921875\nEpoch 41500, Loss: 14442.6669921875\nEpoch 41600, Loss: 14442.6669921875\nEpoch 41700, Loss: 14442.658203125\nEpoch 41800, Loss: 14442.662109375\nEpoch 41900, Loss: 14442.6591796875\nEpoch 42000, Loss: 14442.669921875\nEpoch 42100, Loss: 14442.6640625\nEpoch 42200, Loss: 14442.6728515625\nEpoch 42300, Loss: 14442.65625\nEpoch 42400, Loss: 14442.662109375\nEpoch 42500, Loss: 14442.6708984375\nEpoch 42600, Loss: 14442.6669921875\nEpoch 42700, Loss: 14442.6640625\nEpoch 42800, Loss: 14442.671875\nEpoch 42900, Loss: 14442.658203125\nEpoch 43000, Loss: 14442.666015625\nEpoch 43100, Loss: 14442.6669921875\nEpoch 43200, Loss: 14442.6826171875\nEpoch 43300, Loss: 14442.6650390625\nEpoch 43400, Loss: 14442.6708984375\nEpoch 43500, Loss: 14442.671875\nEpoch 43600, Loss: 14442.662109375\nEpoch 43700, Loss: 14442.662109375\nEpoch 43800, Loss: 14442.654296875\nEpoch 43900, Loss: 14442.6728515625\nEpoch 44000, Loss: 14442.6650390625\nEpoch 44100, Loss: 14442.6826171875\nEpoch 44200, Loss: 14442.669921875\nEpoch 44300, Loss: 14442.673828125\nEpoch 44400, Loss: 14442.6669921875\nEpoch 44500, Loss: 14442.67578125\nEpoch 44600, Loss: 14442.6630859375\nEpoch 44700, Loss: 14442.666015625\nEpoch 44800, Loss: 14442.66015625\nEpoch 44900, Loss: 14442.6669921875\nEpoch 45000, Loss: 14442.6640625\nEpoch 45100, Loss: 14442.669921875\nEpoch 45200, Loss: 14442.669921875\nEpoch 45300, Loss: 14442.6708984375\nEpoch 45400, Loss: 14442.666015625\nEpoch 45500, Loss: 14442.6640625\nEpoch 45600, Loss: 14442.6650390625\nEpoch 45700, Loss: 14442.6640625\nEpoch 45800, Loss: 14442.654296875\nEpoch 45900, Loss: 14442.666015625\nEpoch 46000, Loss: 14442.6669921875\nEpoch 46100, Loss: 14442.66015625\nEpoch 46200, Loss: 14442.658203125\nEpoch 46300, Loss: 14442.66015625\nEpoch 46400, Loss: 14442.666015625\nEpoch 46500, Loss: 14442.6669921875\nEpoch 46600, Loss: 14442.6669921875\nEpoch 46700, Loss: 14442.6669921875\nEpoch 46800, Loss: 14442.6640625\nEpoch 46900, Loss: 14442.6689453125\nEpoch 47000, Loss: 14442.6689453125\nEpoch 47100, Loss: 14442.6640625\nEpoch 47200, Loss: 14442.6591796875\nEpoch 47300, Loss: 14442.669921875\nEpoch 47400, Loss: 14442.65625\nEpoch 47500, Loss: 14442.6591796875\nEpoch 47600, Loss: 14442.654296875\nEpoch 47700, Loss: 14442.66796875\nEpoch 47800, Loss: 14442.662109375\nEpoch 47900, Loss: 14442.6630859375\nEpoch 48000, Loss: 14442.6630859375\nEpoch 48100, Loss: 14442.666015625\nEpoch 48200, Loss: 14442.67578125\nEpoch 48300, Loss: 14442.66015625\nEpoch 48400, Loss: 14442.6669921875\nEpoch 48500, Loss: 14442.6611328125\nEpoch 48600, Loss: 14442.662109375\nEpoch 48700, Loss: 14442.6669921875\nEpoch 48800, Loss: 14442.6640625\nEpoch 48900, Loss: 14442.6669921875\nEpoch 49000, Loss: 14442.6630859375\nEpoch 49100, Loss: 14442.669921875\nEpoch 49200, Loss: 14442.673828125\nEpoch 49300, Loss: 14442.6640625\nEpoch 49400, Loss: 14442.6640625\nEpoch 49500, Loss: 14442.6552734375\nEpoch 49600, Loss: 14442.669921875\nEpoch 49700, Loss: 14442.666015625\nEpoch 49800, Loss: 14442.669921875\nEpoch 49900, Loss: 14442.6572265625\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.12: Model selection in LASSO regression on the bikeshare data set. (Left): training and validation MSE as a function of regularization strength. (Right): learned weights as a function of regularization strength.\n\n\n\nWe find that LASSO regression achieves a slightly lower MSE on validation data than ridge regression.\nLet’s train the LASSO model one last time with the best regularization strength and take a look at the learned coefficients:\n\nreg_strength = reg_strengths[val_mses.index(min(val_mses))]\nLR = LinearRegression(n_params=X_train.shape[1])\ntrain_model(LR, X_train, y_train, lr=1e-1, n_epochs=20000, tol = 1e-4, regularization = lambda w: reg_strength*ell_1_regularization(w))\n\nWe can inspect the coefficients to see which features the model found most important:\n\ncoefs = pd.DataFrame({\n    'Feature': X.columns,\n    'Coefficient': LR.w.detach().numpy().flatten()\n}).sort_values(by='Coefficient', key=abs, ascending=False)\ncoefs.head()\n\n\n\n\n\n\n\n\nFeature\nCoefficient\n\n\n\n\n43\nhr_17\n244.998688\n\n\n34\nhr_8\n219.405945\n\n\n2\ntemp\n174.332703\n\n\n44\nhr_18\n144.762268\n\n\n30\nhr_4\n-144.750198\n\n\n\n\n\n\n\nWe see that rush hours and high temperatures are identified by the model as highly predictive of bike rental demand.\nFinally, let’s evaluate our chosen model on the test set to get a final estimate of performance:\n\ny_pred_test = LR.forward(X_test)\ntest_mse = mse(y_pred_test, y_test)\nprint(f\"Test MSE: {test_mse.item():.4f}\")\nprint(f\"Fraction of base rate: {test_mse.item()/base_rate_mse.item():.4f}\")\n\nTest MSE: 11488.4326\nFraction of base rate: 0.3417\n\n\nOur final model achieves a test MSE considerably lower than the base rate, suggesting that it has successfully learned to predict bike rental demand using the available features.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Features and Regularization</span>"
    ]
  },
  {
    "objectID": "chapters/06-regularization.html#references",
    "href": "chapters/06-regularization.html#references",
    "title": "4  Features and Regularization",
    "section": "References",
    "text": "References\n\n\n\n\nFanaee-T, Hadi, and Joao Gama. 2013. “Event Labeling Combining Ensemble Detectors and Background Knowledge.” Progress in Artificial Intelligence, 1–15. https://doi.org/10.1007/s13748-013-0040-3.",
    "crumbs": [
      "Machine Learning Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Features and Regularization</span>"
    ]
  }
]