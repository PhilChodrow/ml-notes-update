{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multinomial Classification\n",
        "\n",
        "Predicting among many categories\n",
        "\n",
        "## Recap\n",
        "\n",
        "[Recently](10-intro-classification.qmd), we developed binary classification (models that learn to predict one of two labels) using the data = signal + noise framework. In this framework, the *signal* was a function $q(\\mathbf{x})$ which, when evaluated at a feature value $\\mathbf{x}$, gives the probability that a data point with features $\\mathbf{x}$ belongs to class 1. The *noise* was a Bernoulli random variable that, when sampled, gives us the observed class label for a data point. We trained our model by letting $q(\\mathbf{x}) = \\sigma(\\mathbf{w}^\\top \\mathbf{x})$, where $\\sigma$ is the logistic sigmoid function and $\\mathbf{w}$ is a trainable weight vector. Using gradient descent, we were able to maximize the likelihood of the observed data under this model, which we found to be equivalent to minimizing the binary cross-entropy loss.\n",
        "\n",
        "In this lecture we’ll consider a simple question: how do we extend this framework to the case where we have more than two classes? The fancy vocabulary word for “more than two classes” is *multinomial*, so we’ll be developing logistic regression for *multinomial classification*.\n",
        "\n",
        "## Data Prep\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://allisonhorst.github.io/palmerpenguins/reference/figures/lter_penguins.png\" alt=\"Image source: @allisonhorst\" />\n",
        "<figcaption aria-hidden=\"true\">Image source: @allisonhorst</figcaption>\n",
        "</figure>\n",
        "\n",
        "Our data set for these notes is Palmer Penguins. This data set contains physiological measurements and species labels for several populations of Adelie, Chinstrap, and Gentoo penguins.\n",
        "\n",
        "<span class=\"column-margin margin-aside\">The Palmer Penguins data was originally collected by Gorman, Williams, and Fraser (2014) and was nicely packaged and released for use in the data science community by Horst, Hill, and Gorman (2020). You can find [a very concise summary](https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html#Supervised-learning-example:-Iris-classification) of the main workflow using a similar data set in Vanderplas (2016).</span>\n",
        "\n",
        "Let’s go ahead and acquire the data."
      ],
      "id": "abb1d45d-55b1-4dca-9f8e-283ca9d4b127"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/palmer-penguins.csv\"\n",
        "\n",
        "df = pd.read_csv(url)"
      ],
      "id": "d5312b61"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span class=\"column-margin margin-aside\">You can learn much more about the capabilities of `pandas.DataFrame` objects in [Chapter 3](https://jakevdp.github.io/PythonDataScienceHandbook/03.00-introduction-to-pandas.html) of Vanderplas (2016)</span> The `df` variable holds a `pandas.DataFrame` object. You can think of a data frame as a table of data with a variety of useful behaviors for data manipulation and visualization.\n",
        "\n",
        "Let’s take a look:"
      ],
      "id": "beca6178-e3ba-45c8-ae6c-35023e252470"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO"
      ],
      "id": "b432724b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each row of this data frame contains measurements for a single penguin, and the columns contain different features of the data. We’ll use the `Species` column as our class labels, and the other columns as features for classification. For illustration, we’ll use only two features: `Culmen Length (mm)` and `Culmen Depth (mm)`. We’ll separate these into a matrix of features $\\mathbf{X}$ and a vector of class labels $\\mathbf{y}$.\n",
        "\n",
        "![](https://allisonhorst.github.io/palmerpenguins/reference/figures/culmen_depth.png)\n",
        "\n",
        "Image credit: [@allisonhorst](https://allisonhorst.github.io/palmerpenguins/)"
      ],
      "id": "4b3b5f4e-c755-4f85-b6be-91d4cc4edf11"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df.dropna(subset=['Culmen Length (mm)', 'Culmen Depth (mm)', 'Species'])\n",
        "df = df[['Culmen Length (mm)', 'Culmen Depth (mm)', 'Species']]\n",
        "df[\"Species\"] = df[\"Species\"].str.split().str[0]  # shorten species names to first letter\n",
        "\n",
        "# features and targets\n",
        "X = df[['Culmen Length (mm)', 'Culmen Depth (mm)']].values\n",
        "labels = df['Species'].astype('category')"
      ],
      "id": "f6192116"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A look at the species labels in feature space suggests that we should be able to do a pretty good job of classifying the species based on these two features. We’ll define a convenience function for visualizing the data points in feature space, colored by species."
      ],
      "id": "d9546827-533c-4bd4-8658-6028e06b87b7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scatter_points(X, labels, cmap, ax): \n",
        "    if isinstance(labels, torch.Tensor):\n",
        "        y = labels\n",
        "        unique_labels = torch.unique(y.detach())\n",
        "    else:\n",
        "        y = labels.cat.codes.values\n",
        "        unique_labels = labels.cat.categories\n",
        "    \n",
        "    for i, label in enumerate(unique_labels):\n",
        "        color = cmap(i)\n",
        "        marker = ['o', 's', 'D'][i]\n",
        "        ax.scatter(X[y == i, 0], \n",
        "                   X[y == i, 1], \n",
        "                   color=color, \n",
        "                   edgecolor='k', \n",
        "                   marker=marker, \n",
        "                   label=f\"{unique_labels[i]}\", \n",
        "                   zorder = 1e3)\n",
        "    \n",
        "    ax.set(xlabel=r\"$x_1$\", ylabel=r\"$x_2$\")\n",
        "    ax.legend().set_zorder(1e7)"
      ],
      "id": "8d5605fa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let’s visualize our penguins:\n",
        "\n",
        "Figure 1: Scatterplot culmen length and culmen depth in the Palmer Penguins data set, with data points colored by species."
      ],
      "id": "95a292f9-bc92-4803-b35b-20cc0316aba8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "cmap = plt.get_cmap('tab20b', 3)\n",
        "scatter_points(X, labels, cmap, ax)\n",
        "ax.set_title(\"Scatterplot of penguin species in feature space\")\n",
        "ax.set_xlabel(\"Culmen Length (mm)\")\n",
        "t = ax.set_ylabel(\"Culmen Depth (mm)\")"
      ],
      "id": "42771638"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have data, we need to extend our logistic regression framework to the multinomial setting.\n",
        "\n",
        "## Implementing Multinomial Logistic Regression\n",
        "\n",
        "### One-Hot Encoding\n",
        "\n",
        "To apply our classification framework to our data, we need to convert the categorical species labels into numerical values. A particularly convenient way to do this is via *one-hot encoding*.\n",
        "\n",
        "<span class=\"theorem-title\">**Definition 1 (One-Hot Encoding)**</span> If $y$ is a categorical variable with $K$ categories, then the one-hot encoding of $\\mathbf{y}$ of $y$ is a vector $\\mathbf{y}\\in \\{0, 1\\}^K$ with entries\n",
        "\n",
        "$$\n",
        "y_i = \\begin{cases}\n",
        "    1 & \\text{if } y \\text{ is in category } i \\\\\n",
        "    0 & \\text{otherwise}\n",
        "\\end{cases}\\;.\n",
        "$$\n",
        "\n",
        "For example, in our case with class labels “Adelie”, “Chinstrap”, and “Gentoo”, a one-hot encoding of a penguin of species “Adelie” would be the vector $\\mathbf{y}= (1, 0, 0)$, a penguin of species “Chinstrap” would be $\\mathbf{y}= (0, 1, 0)$, and a penguin of species “Gentoo” would be $\\mathbf{y}= (0, 0, 1)$.\n",
        "\n",
        "The `pandas` library provides a convenient way to perform one-hot encoding for data frames via the `pd.get_dummies` function."
      ],
      "id": "cecf847a-3d5c-472f-8c4c-7838d86c13b3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.get_dummies(df, columns=['Species'])\n",
        "df.head()"
      ],
      "id": "380d5d19"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we’re ready to split our data into features and targets and to training and test sets."
      ],
      "id": "6a4b2c4e-77fb-4b32-8a0d-d50c69c878a6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_ix = df.sample(frac=0.8, random_state=42).index\n",
        "test_ix = df.drop(train_ix).index\n",
        "\n",
        "feature_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)']\n",
        "target_cols = ['Species_Adelie', 'Species_Chinstrap', 'Species_Gentoo']\n",
        "\n",
        "X_train = torch.tensor(df.loc[train_ix, feature_cols].values, dtype=torch.float32)\n",
        "y_train = torch.tensor(df.loc[train_ix, target_cols].values, dtype=torch.float32)\n",
        "\n",
        "X_test = torch.tensor(df.loc[test_ix, feature_cols].values, dtype=torch.float32)\n",
        "y_test = torch.tensor(df.loc[test_ix, target_cols].values, dtype=torch.float32)"
      ],
      "id": "f1db4e85"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Model\n",
        "\n",
        "Let’s go ahead and write down a model of the data generating process for the multinomial classification problem. We’re going to follow a similar approach as we did in binary classification. Suppose that each data point can be in one of $K$ classes, and suppose that we have a *signal function* $\\mathbf{q}: \\mathbb{R}^d \\rightarrow (0, 1)^K$ that takes in features $\\mathbf{x}_i$ and outputs a vector of probabilities $\\mathbf{q}(\\mathbf{x}_i)$ for each class. To generate a class label for data point $i$, we draw from a categorical distribution with class probabilities given by $\\mathbf{q}(\\mathbf{x}_i)$. So, the likelihood of a single data point $i$ with features $\\mathbf{x}_i$ and one-hot encoded class label $\\mathbf{y}_i$ is then\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    p(\\mathbf{y}_i, \\mathbf{x}_i; \\mathbf{q}) = \\begin{cases}\n",
        "        q_1(\\mathbf{x}_i) & \\text{if } y_{i1} = 1 \\\\  \n",
        "        q_2(\\mathbf{x}_i) & \\text{if } y_{i2} = 1 \\\\\n",
        "        \\vdots \\\\\n",
        "        q_K(\\mathbf{x}_i) & \\text{if } y_{iK} = 1\\;.\n",
        "    \\end{cases}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "It’s convenient to write this expression in the more compact form $$\n",
        "p(\\mathbf{y}_i , \\mathbf{x}_i; \\mathbf{q}) = \\prod_{k=1}^K q_k(\\mathbf{x}_i)^{y_{ik}}\\;.\n",
        "$$\n",
        "\n",
        "We can take the product over all data points to get the likelihood of the full data set. Let $\\mathbf{Y}$ be the $n \\times K$ matrix of one-hot encoded class labels, with $i$th row $\\mathbf{y}_i$ corresponding to the one-hot encoding of the class label for data point $i$:\n",
        "\n",
        "$$\n",
        "\\mathbf{Y}= \\begin{bmatrix}\n",
        "    - & \\mathbf{y}_1^\\top & - \\\\\n",
        "    - & \\mathbf{y}_2^\\top & - \\\\\n",
        "    &\\vdots \\\\\n",
        "    - & \\mathbf{y}_n^\\top & -\n",
        "\\end{bmatrix}\\;,\n",
        "$$\n",
        "\n",
        "Then, the likelihood of the data set under the model is\n",
        "\n",
        "$$\n",
        "p(\\mathbf{Y}, \\mathbf{X}; \\mathbf{q}) = \\prod_{i=1}^n \\prod_{k=1}^K q_k(\\mathbf{x}_i)^{y_{ik}}\\;. \n",
        "$$\n",
        "\n",
        "Taking logs gives us the log-likelihood:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    \\mathcal{L}(\\mathbf{Y}, \\mathbf{X}; \\mathbf{q}) = \\sum_{i = 1}^n \\sum_{k = 1}^K y_{ik} \\log q_k(\\mathbf{x}_i)\\;. \n",
        "\\end{aligned}\n",
        " \\qquad(1)$$\n",
        "\n",
        "<a href=\"#eq-log-likelihood-multinomial\" class=\"quarto-xref\">Equation 1</a> is the generalization of the binary cross-entropy to the case of multiple classes.\n",
        "\n",
        "### Signal Function for Logistic Regression\n",
        "\n",
        "To complete our specification for logistic regression, we need to specify the functional form of the signal function $\\mathbf{q}$. We’ll use the *softmax* function, which takes in a vector of real-valued scores and outputs a vector of probabilities.\n",
        "\n",
        "<span class=\"theorem-title\">**Definition 2 (Softmax Function)**</span> The softmax function $\\boldsymbol{\\sigma}: \\mathbb{R}^K \\rightarrow (0, 1)^K$ is defined as\n",
        "\n",
        "$$\n",
        "\\boldsymbol{\\sigma}(\\mathbf{s}) = \\frac{1}{\\sum_{j = 1}^K e^{s_j}} \\begin{pmatrix}\n",
        "    e^{s_1} \\\\ \n",
        "    e^{s_2} \\\\\n",
        "    \\vdots \\\\\n",
        "    e^{s_K}\n",
        "\\end{pmatrix}\\;,\n",
        "$$\n",
        "\n",
        "for $k = 1, \\ldots, K$. The $k$th entry of $\\boldsymbol{\\sigma}(\\mathbf{s})$ is given by\n",
        "\n",
        "$$\n",
        "\\boldsymbol{\\sigma}(\\mathbf{s})_k = \\frac{e^{s_k}}{\\sum_{j = 1}^K e^{s_j}}\\;.\n",
        "$$\n",
        "\n",
        "Here’s an example of the softmax function in action:"
      ],
      "id": "6d23d68b-349d-4aba-a9e5-9b40d3583eb2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO\n",
        "s = torch.tensor([1.0, 2.0, 3.0])\n",
        "print(softmax(s))"
      ],
      "id": "1643832e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The largest values of $\\mathbf{s}$ correspond to the largest probabilities in $\\boldsymbol{\\sigma}(\\mathbf{s})$; the entries of $\\boldsymbol{\\sigma}(\\mathbf{s})$ are nonnegative, and sum to one, giving a valid probability vector.\n",
        "\n",
        "In order to use the softmax function as our signal function $q$, we need to specify how to get scores $\\mathbf{s}\\in \\mathbb{R}^K$ from our features $\\mathbf{x}\\in \\mathbb{R}^d$. The simplest way to do this is to use a linear function of the features. To do this, we’ll introduce a $d \\times K$ matrix of trainable parameters $\\mathbf{W}$, and we’ll compute scores as $\\mathbf{s}= \\mathbf{x}^\\top \\mathbf{W}$. Our signal function will then be given by\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    \\mathbf{q}(\\mathbf{x}) = \\boldsymbol{\\sigma}(\\mathbf{x}^\\top \\mathbf{W})\\;.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Here, the trainable parameters are contained in the matrix $\\mathbf{W}$. Intuitively, we can think of $w_{jk}$ as the contribution of feature $j$ to the score for class $k$. If $w_{jk}$ is large and positive, then larger values of feature $j$ make it more likely that the data point is in class $k$. If $w_{jk}$ is large and negative, then larger values of feature $j$ make it less likely that the data point is in class $k$.\n",
        "\n",
        "### Vectorized Computation\n",
        "\n",
        "Rather than compute the score vector $\\mathbf{s}_i$ for each data point $i$ separately, we can compute the scores for all data points at once using matrix multiplication. Let $\\mathbf{X}$ be the $n \\times d$ matrix of features, with $i$th row $\\mathbf{x}_i^\\top$ corresponding to the features for data point $i$. Then, we can compute the score matrix $\\mathbf{S}\\in \\mathbb{R}^{n \\times K}$ containing all the scores as\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    \\mathbf{S}= \\begin{bmatrix} \n",
        "    - & \\mathbf{s}_1 & - \\\\\n",
        "    - & \\mathbf{s}_2 & - \\\\\n",
        "    &\\vdots \\\\\n",
        "    - & \\mathbf{s}_n & -\n",
        "    \\end{bmatrix} = \n",
        "    \\begin{bmatrix}\n",
        "    - & \\mathbf{x}_1^\\top \\mathbf{W}& - \\\\\n",
        "    - & \\mathbf{x}_2^\\top \\mathbf{W}& - \\\\\n",
        "    &\\vdots \\\\\n",
        "    - & \\mathbf{x}_n^\\top \\mathbf{W}& -\n",
        "    \\end{bmatrix} \n",
        "    = \\begin{bmatrix}\n",
        "    - & \\mathbf{x}_1^\\top & - \\\\\n",
        "    - & \\mathbf{x}_2^\\top & - \\\\\n",
        "    &\\vdots \\\\  \n",
        "    - & \\mathbf{x}_n^\\top & -\n",
        "    \\end{bmatrix} \\mathbf{W}= \\mathbf{X}\\mathbf{W}\\;\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "To then compute the signals $\\mathbf{q}(\\mathbf{s}_i)$, we need to apply the softmax function to each *row* of the score matrix $\\mathbf{S}$:"
      ],
      "id": "c5d08ce1-fa9e-4b40-8f7b-096b9016518d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO"
      ],
      "id": "13ae7a39"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Torch Implementation\n",
        "\n",
        "We now have everything we need to illustrate the data generation process and perform inference. Our implementation of logistic regression requires only initializing the weight matrix $\\mathbf{W}$ and defining a `forward` method that takes in features $\\mathbf{X}$ and outputs the signal function $\\mathbf{q}(\\mathbf{X})$:"
      ],
      "id": "f79ad1b7-db2b-404d-957a-0768dcfb79b7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO"
      ],
      "id": "156ca586"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, let’s visualize the data generating process using a random weight matrix $\\mathbf{W}$. We’ll compute the signal $\\mathbf{q}(\\mathbf{x})$ for a grid of points in feature space, and we’ll visualize the predicted class probabilities for each class.\n",
        "\n",
        "$\\mathbf{q}$\n",
        "\n",
        "Figure 2: Illustration of the data generating process for multinomial logistic regression. (Top left): The predicted probability of class 1 (color intensity) across feature space. (Top right and bottom left): The predicted probabilities of classes 2 and 3. (Bottom right): The predicted class (color) and confidence (opacity) across feature space. We’ve included several randomly scattered points, with their assigned class given by asmpling from the predicted class probabilities."
      ],
      "id": "d6746b1d-52c0-4f47-aee9-b059a9dc7a05"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set the random number generator for reproducibility\n",
        "torch.random.manual_seed(1234)\n",
        "\n",
        "# initialize colors\n",
        "cmap = plt.get_cmap('tab20b', 3)\n",
        "\n",
        "# create a grid of points in feature space and flatten it into a list of points\n",
        "# for prediction\n",
        "x_grid = torch.linspace(-1, 1, 100)\n",
        "y_grid = torch.linspace(-1, 1, 100)\n",
        "X_grid, Y_grid = torch.meshgrid(x_grid, y_grid, indexing='xy')\n",
        "grid = torch.stack([X_grid.flatten(), Y_grid.flatten()], dim=1)\n",
        "\n",
        "# initialize model with random weights\n",
        "model = LogisticRegression(d_features=2, k_classes=3)\n",
        "model.W = model.W * 4.0\n",
        "\n",
        "# compute predicted class probabilities for each point in the grid\n",
        "Q = model.forward(grid)\n",
        "preds = Q.argmax(dim=1)\n",
        "\n",
        "# compute opacity as the difference between the max probability and uniform probability, for visualization purposes\n",
        "opacities = Q.max(dim=1).values - 1/3\n",
        "\n",
        "# colors for each data point determined by the predicted class, with opacity determined by confidence\n",
        "colors = cmap(preds) \n",
        "colors[:, -1] = opacities.detach().numpy()\n",
        "\n",
        "# compute predicted class for a random set of points in feature space\n",
        "X_points = 2*torch.rand(50, 2) - 1\n",
        "Q_points = model.forward(X_points)\n",
        "point_class = torch.multinomial(Q_points, num_samples=1).squeeze()\n",
        "\n",
        "# construct the visualization\n",
        "fig, axarr = plt.subplots(2, 2, figsize=(7, 7))\n",
        "\n",
        "for k in range(3):\n",
        "\n",
        "    # color for the chosen class\n",
        "    color = cmap(k)\n",
        "    \n",
        "    # colormap for individual panel\n",
        "    class_cmap_grad = plt.cm.colors.LinearSegmentedColormap.from_list('white_to_color', ['white', color])\n",
        "\n",
        "    # plot the predicted class probabilities for class k\n",
        "    axarr.ravel()[k].imshow(Q[:, k].reshape(100, 100).detach(), extent=(x_grid.min(), x_grid.max(), y_grid.min(), y_grid.max()), origin='lower', cmap=class_cmap_grad, vmin=0, vmax=1, zorder = 100)\n",
        "\n",
        "    axarr.ravel()[k].set_title(fr\"$q_{k+1}(\\mathbf{{x}})$ - probability of class {k}\")\n",
        "\n",
        "    # labels\n",
        "    if k == 2: \n",
        "        axarr.ravel()[k].set(xlabel=r\"$x_1$\", ylabel=r\"$x_2$\")\n",
        "    if k == 0: \n",
        "        axarr.ravel()[k].set(ylabel=r\"$x_2$\")\n",
        "    \n",
        "\n",
        "# plot all the predicted class probabilities together, with opacity determined by confidence\n",
        "axarr.ravel()[3].imshow(colors.reshape(100, 100, 4), extent=(x_grid.min(), x_grid.max(), y_grid.min(), y_grid.max()), origin='lower', zorder = 100)\n",
        "\n",
        "# for k in range(3):\n",
        "#     color = cmap(k)\n",
        "#     marker = ['o', 's', 'D'][k]\n",
        "#     axarr.ravel()[3].scatter(X_points[point_class == k, 1], X_points[point_class == k, 0], color=color, edgecolor='k', marker=marker, zorder = 200, label=f\"Class {k+1}\")\n",
        "\n",
        "scatter_points(X_points, point_class, cmap, axarr.ravel()[3])\n",
        "\n",
        "# scatter_points(X_points, point_class, cmap, axarr.ravel()[3])\n",
        "\n",
        "\n",
        "axarr.ravel()[3].set_title(\"Predicted class (color) \\nand confidence (opacity)\")\n",
        "axarr.ravel()[3].set(xlabel=r\"$x_1$\")\n",
        "\n",
        "plt.tight_layout()"
      ],
      "id": "6b8bf885"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Training\n",
        "\n",
        "Now it’s time to train a model. Our approach is again gradient descent. The addition of multiple classes, reflected in the weight *matrix* $\\mathbf{W}\\in \\mathbb{R}^{d \\times k}$ (rather than a weight *vector* $\\mathbf{w}\\in \\mathbb{R}^d$ as in binary classification), doesn’t really change the training procedure but it does make it awkward to compute gradients by hand. We’ll rely on PyTorch’s automatic differentiation to compute the gradients for us. In order to compute the gradients, we first need to implement the cross-entropy loss function from equation {eq-log-likelihood-multinomial}:"
      ],
      "id": "4a634512-3e5e-4444-942f-fe014bce4a6a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO"
      ],
      "id": "8cd93f62"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO"
      ],
      "id": "b6933ecd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Figure 3: Training our multinomial logistic regression model for classification on the Palmer Penguins data set. (Left): The multinomial cross-entropy loss during training. (Right): The predicted class (color) and confidence (opacity) across feature space after training, with the training data points overlaid."
      ],
      "id": "9e6106bd-c02c-47b1-8e1b-63115136e941"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axarr = plt.subplots(1, 2, figsize=(8, 4))\n",
        "\n",
        "x_grid = torch.linspace(df['Culmen Length (mm)'].min()-2, df['Culmen Length (mm)'].max()+2, 100)\n",
        "y_grid = torch.linspace(df['Culmen Depth (mm)'].min()-2, df['Culmen Depth (mm)'].max()+2, 100)\n",
        "X_grid, Y_grid = torch.meshgrid(x_grid, y_grid)\n",
        "grid = torch.stack([X_grid.flatten(), Y_grid.flatten()], dim=1)\n",
        "\n",
        "Q = model.forward(grid)\n",
        "preds = Q.argmax(dim=1)\n",
        "\n",
        "colors = cmap(preds)\n",
        "colors[:, -1] = Q.max(dim=1).values.detach().numpy()\n",
        "\n",
        "axarr[0].plot(losses)\n",
        "axarr[0].set_title(\"Loss during training\")\n",
        "axarr[0].set_xlabel(\"Epoch\")\n",
        "axarr[0].set_ylabel(\"Multinomial Cross-Entropy Loss\")\n",
        "axarr[0].loglog()\n",
        "\n",
        "\n",
        "axarr[1].imshow(colors.reshape(100, 100, 4).transpose(1, 0, 2), extent=(x_grid.min(), x_grid.max(), y_grid.min(), y_grid.max()), origin='lower', zorder = 100, aspect='auto')\n",
        "# sns.scatterplot(data=df, x='Culmen Length (mm)', y='Culmen Depth (mm)', hue='Species', style='Species', edgecolor='k', alpha=0.7, zorder = 200)\n",
        "axarr[1].set_title(\"Predicted class (color) \\nand confidence (opacity)\")\n",
        "\n",
        "y_train_labels = y_train.argmax(dim=1)\n",
        "penguin_names = ['Adelie', 'Chinstrap', 'Gentoo']\n",
        "\n",
        "y_train_labels_named = [penguin_names[i] for i in y_train_labels.numpy()]\n",
        "y_train_labels_named = pd.Series(y_train_labels_named, dtype='category')\n",
        "\n",
        "scatter_points(X_train, y_train_labels_named, cmap, axarr[1])\n",
        "axarr[1].set_xlabel(\"Culmen Length (mm)\")\n",
        "t = axarr[1].set_ylabel(\"Culmen Depth (mm)\")"
      ],
      "id": "65225aa3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s finally evaluate the performance of our model on the test set."
      ],
      "id": "68b3fa77-a9a3-4c34-8d83-32675cc4d8ee"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO"
      ],
      "id": "ea289631"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Figure 4: Evaluating model performance on the test set, as in <a href=\"#fig-multinomial-classification-training\" class=\"quarto-xref\">Figure 3</a> (right panel), using the test set overlaid."
      ],
      "id": "a7476d91-05ad-4cdd-9b8c-86218dbf6b1a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "\n",
        "ax.imshow(colors.reshape(100, 100, 4).transpose(1, 0, 2), extent=(x_grid.min(), x_grid.max(), y_grid.min(), y_grid.max()), origin='lower', zorder = 100, aspect='auto')\n",
        "\n",
        "y_test_labels = y_test.argmax(dim=1)\n",
        "penguin_names = ['Adelie', 'Chinstrap', 'Gentoo']\n",
        "\n",
        "y_test_labels_named = [penguin_names[i] for i in y_test_labels.numpy()]\n",
        "y_test_labels_named = pd.Series(y_test_labels_named, dtype='category')\n",
        "\n",
        "scatter_points(X_test, y_test_labels_named, cmap, ax)\n",
        "\n",
        "ax.set_title(f\"Predicted class (color) \\nand confidence (opacity)\\nTest set accuracy: {acc:.3f}\")\n",
        "\n",
        "ax.set_xlabel(\"Culmen Length (mm)\")\n",
        "t = ax.set_ylabel(\"Culmen Depth (mm)\")"
      ],
      "id": "e9d9cc0a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As in the case of binary classification, it may be useful to construct confusion matrices or other measures of error by class when assessing overall model performance.\n",
        "\n",
        "## References\n",
        "\n",
        "Gorman, Kristen B., Tony D. Williams, and William R. Fraser. 2014. “Ecological Sexual Dimorphism and Environmental Variability Within a Community of Antarctic Penguins (Genus Pygoscelis).” Edited by André Chiaradia. *PLoS ONE* 9 (3): e90081. <https://doi.org/10.1371/journal.pone.0090081>.\n",
        "\n",
        "Horst, Allison M, Alison Presmanes Hill, and Kristen B Gorman. 2020. “Allisonhorst/Palmerpenguins: V0.1.0.” Zenodo. <https://doi.org/10.5281/ZENODO.3960218>.\n",
        "\n",
        "Vanderplas, Jacob T. 2016. *Python Data Science Handbook: Essential Tools for Working with Data*. First edition. Sebastopol, CA: O’Reilly Media, Inc."
      ],
      "id": "c3187756-3561-4e0c-b23e-b747468936e9"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "cs451",
      "display_name": "Python (cs451)",
      "language": "python",
      "path": "/Users/philchodrow/Library/Jupyter/kernels/cs451"
    }
  }
}