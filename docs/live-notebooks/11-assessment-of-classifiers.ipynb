{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Assessment of Classifiers\n",
        "\n",
        "Evaluating performance and making decisions\n",
        "\n",
        "## Introduction: Is Your Classifier Working?\n",
        "\n",
        "[Last time](10-intro-classification.qmd) we introduced the problem of classification, and we implemented logistic regression as a simple model for binary classification. We considered a loss function based on the log-likelihood of the data under a model which included a signal function $q(\\mathbf{x})$ as well as Bernoulli-distributed noise at each data point. We considered a data case study of precipitation prediction, where we trained the model using gradient descent and made a final assessment of the model on a test set by measuring the model’s accuracy.\n",
        "\n",
        "However, accuracy is rarely the most useful way to evaluate a classifier. In this set of notes, we’ll study several of the most important considerations that go into measuring the success of a binary classification model in learning from data. In the following chapter we’ll study some of the considerations in play when using classifiers to not only make predictions but also to inform decisions.\n",
        "\n",
        "## Evaluation Metrics for Classification\n",
        "\n",
        "To begin our discussion of evaluation metrics in classification, let’s return to the weather prediction problem. The code block below imports and prepares our data, and trains a logistic regression model on the training data using the same code as in the previous chapter."
      ],
      "id": "42e68978-be9e-42f7-88eb-3b22e79833a9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# acquire the dat\n",
        "url = \"https://raw.githubusercontent.com/middcs/data-science-notes/refs/heads/main/data/australia-weather/weatherAUS.csv\"\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# drop rows with missing data in the relevant columns\n",
        "df.dropna(subset=['RainTomorrow', 'Humidity3pm', 'Pressure3pm'], inplace=True)\n",
        "\n",
        "# # column\n",
        "# df[\"Pressure9am\"] = df[\"Pressure9am\"]/1e3\n",
        "# df[\"Humidity3pm\"] = df[\"Humidity3pm\"]/100\n",
        "\n",
        "# separate into features we'll use and the target vector\n",
        "X = df[['Humidity3pm', 'Pressure3pm']].to_numpy()\n",
        "\n",
        "# pad with a constant column\n",
        "X_aug = torch.ones((X.shape[0], 3), dtype=torch.float32)\n",
        "X_aug[:, 1:] = torch.tensor(X, dtype=torch.float32)\n",
        "X = X_aug.numpy()\n",
        "\n",
        "y = (df['RainTomorrow'] == 'Yes').to_numpy().astype(float).reshape(-1, 1)\n",
        "\n",
        "# train-test split\n",
        "train_frac = 0.8\n",
        "n = X.shape[0]\n",
        "n_train = int(train_frac * n)\n",
        "\n",
        "X_train = torch.tensor(X[:n_train, :], dtype=torch.float32)\n",
        "y_train = torch.tensor(y[:n_train, :], dtype=torch.float32)\n",
        "\n",
        "X_test = torch.tensor(X[n_train:, :], dtype=torch.float32)\n",
        "y_test = torch.tensor(y[n_train:, :], dtype=torch.float32)"
      ],
      "id": "5f45f7c9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model training\n",
        "\n",
        "# used for the loss function\n",
        "def binary_cross_entropy(q, y): \n",
        "    return -(y * torch.log(q) + (1 - y) * torch.log(1 - q)).mean()\n",
        "\n",
        "def sigmoid(z): \n",
        "    return 1 / (1 + torch.exp(-z))\n",
        "\n",
        "# model class\n",
        "class BinaryLogisticRegression: \n",
        "    def __init__(self, n_features): \n",
        "        self.w = torch.zeros(n_features, 1, requires_grad=True)\n",
        "\n",
        "    def forward(self, X): \n",
        "        return sigmoid(X @ self.w)    \n",
        "\n",
        "# optimizer class\n",
        "class GradientDescentOptimizer: \n",
        "    def __init__(self, model, lr=0.1): \n",
        "        self.model = model\n",
        "        self.lr = lr\n",
        "\n",
        "    def grad_func(self, X, y): \n",
        "        q = self.model.forward(X)\n",
        "        return 1/X.shape[0] * ((q - y).T @ X).T\n",
        "        \n",
        "    def step(self, X, y): \n",
        "        grad = self.grad_func(X, y)\n",
        "        with torch.no_grad(): \n",
        "            self.model.w -= self.lr * grad"
      ],
      "id": "a9379d1c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we’ll run a training loop:"
      ],
      "id": "0a64afd0-3b01-475d-a8b9-67f06119edde"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# training loop\n",
        "model = BinaryLogisticRegression(n_features=3)\n",
        "opt = GradientDescentOptimizer(model, lr=0.00001)\n",
        "\n",
        "losses = []\n",
        "\n",
        "for epoch in range(2000): \n",
        "    q = model.forward(X_train)\n",
        "    loss = binary_cross_entropy(q, y_train)\n",
        "    losses.append(loss.item())\n",
        "    opt.step(X_train, y_train)"
      ],
      "id": "22126818"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Thresholding Scores\n",
        "\n",
        "Having trained the model, we’re ready to evaluate its performance on the training data. As usual, the simplest way to do this is via the model’s *accuracy* on training data. However, the model doesn’t make binary yes/no predictions yet; the output is the signal function $q(\\mathbf{x}) = \\sigma(\\mathbf{w}^\\top \\mathbf{x})$, a predicted probability of the positive class. To get a binary prediction, we need to choose a *threshold* $\\tau$ such that we predict “yes” if $q(\\mathbf{x}) > \\tau$ and “no” otherwise. Syntactically, the simplest way to do this is:"
      ],
      "id": "66010ec7-12b3-40e0-957f-45b81541d90d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO\n",
        "print(f\"Accuracy on training data: {acc.item():.2f}\")"
      ],
      "id": "b5e74aae"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s see how the choice of threshold affects the model’s accuracy on the training data:\n",
        "\n",
        "Figure 1: Accuracy-based optimal thresholding for rain prediction in logistic regression."
      ],
      "id": "5b7be283-fb97-4604-af36-0c065ab9b8bd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accs = []\n",
        "\n",
        "TAU = torch.linspace(0, 1, 100)\n",
        "\n",
        "for tau in TAU: \n",
        "    y_pred = (model.forward(X_train) > tau).float()\n",
        "    acc = (y_pred == y_train).float().mean()\n",
        "    accs.append(acc.item())\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "ax.plot(TAU, accs)\n",
        "ax.set_xlabel(r\"Threshold $\\tau$\")\n",
        "ax.set_ylabel(\"Accuracy\")\n",
        "ax.set_title(\"Accuracy as a function of threshold\")\n",
        "\n",
        "best_thresh = TAU[torch.tensor(accs).argmax()]\n",
        "ax.axvline(best_thresh, color=\"grey\", linestyle=\"--\", label=f\"Best threshold: {best_thresh:.2f} with accuracy {max(accs):.2f}\")\n",
        "ax.legend()"
      ],
      "id": "7ba0d614"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interestingly, the best threshold on the training data is not the usual default choice of 0.5.\n",
        "\n",
        "### Types of Error\n",
        "\n",
        "Accuracy is an answer to the question: *how often does the model prediction match the true label?* However, in many applications, different *kinds* of errors matter.\n",
        "\n",
        "#### Confusion Matrix\n",
        "\n",
        "Given that we have a vector $\\hat{\\mathbf{y}}$ of binary predictions and a vector $\\mathbf{y}$ of true labels, many common ways to assess model performance begin with the *confusion matrix*, a table which compares predicted and true labels.\n",
        "\n",
        "<span class=\"theorem-title\">**Definition 1 (Confusion Matrix)**</span> Given a vector of binary labels $\\mathbf{y}$ and a vector of binary predictions $\\hat{\\mathbf{y}}$, the *confusion matrix* is a 2x2 table which counts the number of instances in each of the following categories:\n",
        "\n",
        "|            | Predicted 0 | Predicted 1 |\n",
        "|------------|-------------|-------------|\n",
        "| **True 0** | TN          | FP          |\n",
        "| **True 1** | FN          | TP          |\n",
        "\n",
        "The entries of the confusion matrix are counts of the number of instances in each category:\n",
        "\n",
        "-   **True Negatives (TN)**: The number of instances where the true label is 0 and the model correctly predicted 0.\n",
        "-   **False Positives (FP)**: The number of instances where the true label is 0 but the model incorrectly predicted 1.\n",
        "-   **False Negatives (FN)**: The number of instances where the true label is 1 but the model incorrectly predicted 0.\n",
        "-   **True Positives (TP)**: The number of instances where the true label is 1 and the model correctly predicted 1.\n",
        "\n",
        "We can compute a 2x2 confusion matrix for our model’s predictions (on the training data) with specified threshold as follows:"
      ],
      "id": "c629511d-2ddd-4766-b2f7-851a29e02594"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO\n",
        "\n",
        "tau = 0.42\n",
        "y_pred = (model.forward(X_train) > tau).float()\n",
        "cm = confusion_matrix(y_train, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)"
      ],
      "id": "306ce962"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Entries on the diagonal of the confusion matrix correspond to correct predictions, while entries off the diagonal correspond to errors.\n",
        "\n",
        "From the confusion matrix, we can compute a variety of different performance metrics. For example, the accuracy can be computed as the sum of the diagonal entries divided by the total number of instances:"
      ],
      "id": "5a1e4f40-aae6-4e98-b21a-b2857c32350e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Accuracy:\", (cm[0, 0] + cm[1, 1]) / cm.sum())"
      ],
      "id": "70b6f399"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Error Rates\n",
        "\n",
        "There are several *error rates* which are commonly used to evaluate classifiers and which can be computed from the confusion matrix:\n",
        "\n",
        "<span class=\"theorem-title\">**Definition 2 (Error Rates in Binary Classification)**</span>  \n",
        "\n",
        "-   The **true positive rate** (TPR, also known as *sensitivity* or *recall*) is the proportion of true positives out of all actual positives: $$\\text{TPR} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$$\n",
        "\n",
        "-   The **false positive rate** (FPR) is the proportion of false positives out of all actual negatives: $$\\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}$$\n",
        "\n",
        "-   The **true negative rate** (TNR, also known as *specificity*) is the proportion of true negatives out of all actual negatives: $$\\text{TNR} = \\frac{\\text{TN}}{\\text{FP} + \\text{TN}}$$\n",
        "\n",
        "-   The **false negative rate** (FNR) is the proportion of false negatives out of all actual positives: $$\\text{FNR} = \\frac{\\text{FN}}{\\text{TP} + \\text{FN}}$$\n",
        "\n",
        "For example:"
      ],
      "id": "77320ab7-6f8d-47b6-986a-94d8ea65be17"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"False positive rate:\", cm[0, 1] / (cm[0, 0] + cm[0, 1]))\n",
        "print(\"True positive rate:\", cm[1, 1] / (cm[1, 0] + cm[1, 1]))"
      ],
      "id": "1d01b764"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In general it’s not usually required to calculate all four error rates due to the mathematical relationships\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    \\mathrm{TPR}= 1 - \\mathrm{FNR}\\\\\n",
        "    \\mathrm{FPR}= 1 - \\mathrm{TNR}\\;.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "#### Precision, Recall, F1 Score\n",
        "\n",
        "Another common set of performance metrics are *precision* and *recall*, which can be combined into the *F1 score*. These metrics are particularly useful in cases where the classes are imbalanced, meaning that one class is much more common than the other.\n",
        "\n",
        "<span class=\"theorem-title\">**Definition 3 (Precision, Recall, and F1 Score)**</span>  \n",
        "\n",
        "-   **Precision** is the proportion of true positives out of all predicted positives: $$\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}$$\n",
        "\n",
        "-   **Recall** is the same as the true positive rate: $$\\text{Recall} = \\text{TPR} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$$\n",
        "\n",
        "Precision and recall are often combined into a single metric called the **F1 score**, which is the harmonic mean of precision and recall: $$\\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
        "\n",
        "Here’s the F1 score for our model:"
      ],
      "id": "c94fb6a4-7bb5-4205-9526-0f053c775c22"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)"
      ],
      "id": "fe8b0f9f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Precision and recall are often more informative than accuracy in cases where the classes are imbalanced, meaning that one class is much more common than the other. In such cases, a model could achieve high accuracy by simply predicting the majority class, but it would perform poorly in terms of precision and recall for the minority class. The F1 score is often used as a single metric to evaluate the performance of a classifier in the presence of imbalanced classes.\n",
        "\n",
        "### ROC curve and AUC\n",
        "\n",
        "Error rates and metrics like precision/recall/F1 are useful for evaluating a *specific* set of predictions, e.g. ones that we would obtain from a model like logistic regression with a specific threshold $\\tau$. However, we might want to evaluate the model’s performance across *all* possible thresholds. One way to do this is via the **Receiver Operating Characteristic (ROC) curve**, which plots the true positive rate (TPR) against the false positive rate (FPR) as the threshold $\\tau$ varies. The area under the ROC curve (AUC) is a common summary statistic for evaluating the overall performance of a classifier across all thresholds. Computing the ROC simply requires sweeping across thresholds and computing the TPR and FPR at each threshold. Note that the input here is not a vector of binary predictions $\\hat{\\mathbf{y}}$ but rather the vector of predicted probabilities $q(\\mathbf{x})$, which we can threshold at different levels to get different binary predictions.\n",
        "\n",
        "Figure 2: ROC curve (on training data) for logistic regression, evaluated on both training and test data."
      ],
      "id": "100ad18e-a348-4361-8e92-35f169a9445c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def roc_curve(y_true, q): \n",
        "    TPR = []\n",
        "    FPR = []\n",
        "    for tau in torch.linspace(0, 1, 100): \n",
        "        y_pred = (q > tau).float()\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        tpr = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
        "        fpr = cm[0, 1] / (cm[0, 0] + cm[0, 1])\n",
        "        TPR.append(tpr)\n",
        "        FPR.append(fpr)\n",
        "    return FPR, TPR\n",
        "\n",
        "q_train = model.forward(X_train)\n",
        "fpr_train, tpr_train = roc_curve(y_train, q_train)\n",
        "\n",
        "fpr_test, tpr_test = roc_curve(y_test, model.forward(X_test))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "ax.plot(fpr_train, tpr_train, label=\"ROC Curve (Training)\")\n",
        "ax.plot(fpr_test, tpr_test, label=\"ROC Curve (Test)\")\n",
        "ax.plot([0, 1], [0, 1], color=\"grey\", linestyle=\"--\", label=\"Random Classifier\")\n",
        "ax.set_xlim(0, 1)\n",
        "ax.set_ylim(0, 1)\n",
        "ax.set_xlabel(\"False Positive Rate\")\n",
        "ax.set_ylabel(\"True Positive Rate\")\n",
        "ax.set_title(\"ROC Curve for Logistic Regression\")\n",
        "ax.legend()"
      ],
      "id": "89cc4e02"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Area Under the Curve (AUC) is a measure of the overall performance of the classifier across all thresholds. An ideal classifier which makes no errors has an AUC of 1, while a random classifier has an AUC of 0.5. The AUC can be computed using numerical integration, for example via the trapezoidal rule:\n",
        "\n",
        "Figure 3: Area under the ROC curve (AUC) for logistic regression on test data."
      ],
      "id": "45c944d9-0df7-4f76-b196-1be02717aee9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "ax.plot(fpr_test, tpr_test, label=\"ROC Curve (Test)\")\n",
        "ax.plot([0, 1], [0, 1], color=\"grey\", linestyle=\"--\", label=\"Random Classifier\")\n",
        "ax.set_xlim(0, 1)\n",
        "ax.set_ylim(0, 1)\n",
        "ax.set_xlabel(\"False Positive Rate\")\n",
        "ax.set_ylabel(\"True Positive Rate\")\n",
        "ax.set_title(\"ROC Curve for Logistic Regression\")\n",
        "ax.fill_between(fpr_test, tpr_test, alpha=0.2,  label=f\"AUC (Test) = {torch.abs(torch.trapz(torch.tensor(tpr_test), torch.tensor(fpr_test))):.2f}\")\n",
        "ax.legend()"
      ],
      "id": "f58792aa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clarifying the Task\n",
        "\n",
        "<span class=\"column-margin margin-aside\">This is a brief discussion of a published academic article. The authors of the article were very careful in describing their findings and put up a very helpful [FAQ website](https://life2vec.dk/) clarifying some important points. The brief discussion below is not a dunk on the authors, but a brief lesson in the importance of very carefully parsing hype in media.</span>\n",
        "\n",
        "A few years ago, an international group of authors published a model aimed at predicting live outcomes for patients based on sequences of life events Savcisens et al. (2024). One of the key functionalities of the model was *death prediction*; the researchers scored the model on a prediction task in which the model attempted to predict whether a given individual was alive four years after the end of the training data. A [media writeup](https://www.lex18.com/ai-death-calculator-can-predict-when-you-ll-die-with-eery-accuracy) of the model said:\n",
        "\n",
        "> Researchers analyzed aspects of a person’s life story between 2008 and 2016, with the model seeking patterns in the data. Then, they used the algorithm to determine whether someone had died by 2020. The Life2vec model made predictions with 78% accuracy.\n",
        "\n",
        "Wow! 78% accuracy sounds impressive (and also scary!!).\n",
        "\n",
        "Writeups like these pose interesting questions about how we want to live our lives and what kinds of knowledge are healthy for us to have. If a model had a 78% accuracy in predicting whether you were going to die in the next four years, would you want to know?\n",
        "\n",
        "Before we get carried away by deep questions like that, however, it’s important to ask some technical questions to help us understand how the model is *actually* scored and what that 78% accuracy really means.\n",
        "\n",
        "### What’s the population?\n",
        "\n",
        "Does the model work for everybody? Not necessarily: as the researchers write in their [FAQ](https://life2vec.dk/) about the paper, the model is trained on Danish individuals ages 35-65. The model’s performance on older, younger, or non-Danish individuals is unknown.\n",
        "\n",
        "### What’s the base rate?\n",
        "\n",
        "One important piece of context is the *base rate* in the data. In a country in which the average length of a human life is 80 years (representative of many developed countries), the base rate of death in a single year is roughly $1/80 = 1.25\\%$. In a four-year period, the base rate of death is roughly $4/80 = 5\\%$. Of course, this is an average across a population, with considerably variability by age:"
      ],
      "id": "c7d27efd-6691-4cd0-9148-f16287b0eff2"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<iframe src=\"https://ourworldindata.org/grapher/annual-death-rate-by-age-group?tab=chart\" loading=\"lazy\" style=\"width: 100%; height: 600px; border: 0px none;\" allow=\"web-share; clipboard-write\">"
      ],
      "id": "ec67c3c0-dff0-41a1-b2a7-5951abe63f68"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</iframe>"
      ],
      "id": "983def72-4eb2-4c3d-a957-7ad9961170ba"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That said, a model which always predicted “not dead in the next four years” would be correct on average about 95% of the time. So, the researchers must have done something more subtle.\n",
        "\n",
        "### What’s the task?\n",
        "\n",
        "Indeed, the researchers *balanced* their testing data set between the two classes “alive in 2020” and “dead in 2020”. This means that they selected instances for the test data to overrepresent cases of death, resulting in a test set in which the base rate of death was 50% instead of 5%. This makes it *harder* for the model to achieve high accuracy, since just predicting “alive” doesn’t work anymore. However, it also means that the 78% accuracy isn’t directly relevant to a random individual in the population: it’s a specific, narrowly-interpretable measure of performance describing the model’s ability to make predictions in an artificially balanced dataset.\n",
        "\n",
        "## Model Comparison\n",
        "\n",
        "Maybe just use something simpler? Simpler model, fewer features, etc? Difficult Lessons for Prediction\n",
        "\n",
        "https://dl.acm.org/doi/pdf/10.1145/3715275.3732175\n",
        "\n",
        "## Assessment of Data Sets\n",
        "\n",
        "Case study from Calling Bull on criminal face detection.\n",
        "\n",
        "Savcisens, Germans, Tina Eliassi-Rad, Lars Kai Hansen, Laust Hvas Mortensen, Lau Lilleholt, Anna Rogers, Ingo Zettler, and Sune Lehmann. 2024. “Using Sequences of Life-Events to Predict Human Lives.” *Nature Computational Science* 4 (1): 43–56."
      ],
      "id": "1bed8e48-2022-4666-b4df-cb064d8b8ede"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "cs451",
      "display_name": "Python (cs451)",
      "language": "python",
      "path": "/Users/philchodrow/Library/Jupyter/kernels/cs451"
    }
  }
}