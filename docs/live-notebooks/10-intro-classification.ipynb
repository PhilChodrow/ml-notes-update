{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction: Binary Labels\n",
        "\n",
        "Predicting categories and informing decisions\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this set of notes, we’ll begin our investigation into *classification*. Whereas in regression we aimed to predict a *number* (like an amount of rainfall, or the price of a house), in classification we aim to predict a *category* (like whether an email is spam or not, or whether a tumor is malignant or benign).\n",
        "\n",
        "### Classification and Decision-Making\n",
        "\n",
        "Classification is in many contexts a more practically-relevant task than regression, because classification relates directly to decision-making. For example, consider a spam filter. The goal of a spam filter is to classify incoming emails as either “spam” or “not spam”. This classification directly informs the decision of whether to deliver the email to the user’s inbox or to the spam folder.\n",
        "\n",
        "### Data = Signal + Noise: Classification Edition\n",
        "\n",
        "When studying regression, we considered a framework in which the data $y_i$ was generated according to a process of the form\n",
        "\n",
        "$$\n",
        "y_i = f(\\mathbf{x}_i) + \\epsilon_i\\;,\n",
        "$$\n",
        "\n",
        "where $f$ was a deterministic function of the input $\\mathbf{x}_i$, and $\\epsilon_i$ was a random noise term. Our goal was to learn the signal $f$ rather than the noise $\\epsilon_i$. For classification we still want to use the “signal + noise” paradigm, but the presence of categorical data means that we need to make some adjustments to the framework. In particular, since $y_i$ is now a category rather than number, we can’t write it as the “sum” of anything, so our idea of “signal + noise” will be a bit metaphorical.\n",
        "\n",
        "## Binary Classification\n",
        "\n",
        "In binary classification, we have two classes (e.g. “spam” vs “not spam”). We can represent the class labels as $y_i \\in \\{0, 1\\}$, where $0$ represents one class and $1$ represents the other class. Here’s an example of the kind of data we have in mind with a binary classification problem:\n",
        "\n",
        "Figure 1: Example data for a binary classification problem with two features. The data points are colored and shaped according to their class label."
      ],
      "id": "da4eb256-15f7-4a1c-9d83-549641c4ec49"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "w = torch.tensor([[0.0], [1.5], [1.5]])\n",
        "X = torch.randn(100, 3)\n",
        "X[:, 0] = 1.0\n",
        "\n",
        "q = torch.sigmoid(X @ w)\n",
        "y = torch.bernoulli(q)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "\n",
        "markers = ['o', 's']\n",
        "for i in range(2): \n",
        "    idx = (y.flatten() == i)\n",
        "    ax.scatter(X[idx, 1], X[idx, 2], c = y.flatten()[idx], label=f'Class {i}', edgecolor='k', marker=markers[i],   cmap = 'BrBG', vmin = -0.5, vmax = 1.5, alpha = 0.7)\n",
        "\n",
        "ax.legend()\n",
        "t = ax.set(title='Example Classification Data', xlabel=r'$x_1$', ylabel=r'$x_2$')"
      ],
      "id": "b0b51a8e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are going to conceptualize this data as arising according to the following process:\n",
        "\n",
        "> **Data Generating Process for Classification**\n",
        ">\n",
        "> 1.  There is some true underlying *signal function* $q$ that takes in features $\\mathbf{x}_i$ and outputs a *probability* of being in class 1.\n",
        "> 2.  For each data point $i$, we first compute the signal function $q(\\mathbf{x}_i)$, which gives us a probability of being in class 1.\n",
        "> 3.  Then, we flip a biased coin with bias $q(\\mathbf{x}_i)$ to determine the class label $y_i$. The uncertainty of this coin flip is the *noise* in the data generating process.\n",
        "\n",
        "Here’s a visualization of this process:\n",
        "\n",
        "Figure 2: (Left): example of a signal function $q$ that maps features $\\mathbf{x}$ to a probability of being in class 1. (Middle): the data points in feature space. (Right): the data points with their observed class label after having been assigned categories according to the value of the signal function."
      ],
      "id": "0777cfca-ff9f-4337-9a39-48c0b3d3119e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axarr = plt.subplots(1, 3, figsize=(8, 3))\n",
        "x_1_grid = torch.linspace(X.min(), X.max(), 100)\n",
        "x_2_grid = torch.linspace(X.min(), X.max(), 100)\n",
        "xx, yy = torch.meshgrid(x_1_grid, x_2_grid, indexing='ij')\n",
        "grid = torch.cat([torch.ones_like(xx).reshape(-1, 1), xx.reshape(-1, 1), yy.reshape(-1, 1)], dim=1)\n",
        "with torch.no_grad():\n",
        "    q_grid = torch.sigmoid(grid @ w).reshape(xx.shape)\n",
        "\n",
        "axarr[0].contourf(xx, yy, q_grid, levels=torch.linspace(0, 1, steps=10), alpha=0.3, cmap='BrBG', extent = (x_1_grid.min(), x_1_grid.max(), x_2_grid.min(), x_2_grid.max()))\n",
        "\n",
        "axarr[1].contourf(xx, yy, q_grid, levels=torch.linspace(0, 1, steps=10), alpha=0.3, cmap='BrBG')\n",
        "\n",
        "sns.scatterplot(x=X[:, 1], y=X[:, 2], color = \"black\", ax=axarr[1], legend=False, edgecolor='k', facecolor = \"none\")\n",
        "\n",
        "\n",
        "for i in range(2): \n",
        "    idx = (y.flatten() == i)\n",
        "    axarr[2].scatter(X[idx, 1], X[idx, 2], c = y.flatten()[idx], label=f'Class {i}', edgecolor='k', marker=markers[i],   cmap = 'BrBG', vmin = -0.5, vmax = 1.5, alpha = 0.7)\n",
        "\n",
        "for i, ax in enumerate(axarr): \n",
        "    ax.set(xlim=(x_1_grid.min(), x_1_grid.max()), ylim=(x_2_grid.min(), x_2_grid.max()))\n",
        "    ax.set(xlabel=r'$x_1$')\n",
        "    if i == 0: \n",
        "        ax.set(ylabel=r'$x_2$')\n",
        "    else: \n",
        "        ax.set(ylabel='')\n",
        "\n",
        "# plt.colorbar(axarr[1].collections[0], ax=axarr[0], label='Probability of label 1 $q(\\mathbf{x})$')\n",
        "axarr[0].set(title='Signal $q$')\n",
        "axarr[1].set(title='Data points in\\nfeature space')\n",
        "axarr[2].set(title='Data points\\nassigned categories')\n",
        "    \n",
        "plt.tight_layout()"
      ],
      "id": "e50e2999"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Likelihood Function for Binary Classification\n",
        "\n",
        "Suppose that we evaluate the signal function $q$ at some point $\\mathbf{x}$, giving us a probability $q(\\mathbf{x})$ that a data point with features $\\mathbf{x}$ belongs to class 1. We can then write the probability of that data point belonging to class $y\\in \\{0,1\\}$ as\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "    p_Y(y; q(\\mathbf{x})) = \\begin{cases}\n",
        "        q(\\mathbf{x}) & \\text{if } y = 1 \\\\\n",
        "        1 - q(\\mathbf{x}) & \\text{if } y = 0\n",
        "    \\end{cases}  \n",
        "\\end{align}\n",
        " \\qquad(1)$$\n",
        "\n",
        "Although it might seem a bit unnecessarily complicated at first, it turns out that a very useful way to write this $p_Y(y; q(\\mathbf{x})) = q(\\mathbf{x})^y (1-q(\\mathbf{x}))^{1-y}$\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    p_Y(y; q(\\mathbf{x})) &= q(\\mathbf{x})^y (1 - q(\\mathbf{x}))^{1 - y}\\;.\n",
        "\\end{aligned}\n",
        " \\qquad(2)$$\n",
        "\n",
        "This is an instance of the Bernoulli distribution with parameter $q(\\mathbf{x})$:\n",
        "\n",
        "<span class=\"theorem-title\">**Definition 1 (Bernoulli Distribution)**</span> Random variable $Y$ is said to be Bernoulli distributed with parameter $q$ if it takes value $1$ with probability $q$ and value $0$ with probability $1-q$.\n",
        "\n",
        "The *probability mass function* of a Bernoulli distribution is given by\n",
        "\n",
        "$$\n",
        "p_Y(y; q) = \\mathbb{P}(Y = y;q) = q^y (1-q)^{1-y}\\;.\n",
        "$$\n",
        "\n",
        "We can now write down the likelihood function for some data consisting of a feature matrix $\\mathbf{X}$ and a vector of class labels $\\mathbf{y}$ given a signal function $q$ by multiplying together the probabilities for each data point:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    L(\\mathbf{X}, \\mathbf{y}; q) &= \\prod_{i = 1}^n p_Y(y_i; q(\\mathbf{x})) \\\\ \n",
        "                   &= \\prod_{i = 1}^n q(\\mathbf{x}_i)^{y_i} (1 - q(\\mathbf{x}_i))^{1 - y_i}\\;.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Just like with regression, it’s usually more convenient to work with the log-likelihood:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    \\mathcal{L}(\\mathbf{X}, \\mathbf{y}; q) &= \\log L(\\mathbf{X}, \\mathbf{y}; q) \\\\ \n",
        "                   &= \\sum_{i = 1}^n \\left[y_i \\log q(\\mathbf{x}_i) + (1 - y_i) \\log (1 - q(\\mathbf{x}_i))\\right]\\;.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Since we customarily minimize when working with optimization problems, we aim to minimize the negative log-likelihood, which is given by\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    -\\mathcal{L}(\\mathbf{X}, \\mathbf{y}; q)  \n",
        "                   &= -\\sum_{i = 1}^n \\left[y_i \\log q(\\mathbf{x}_i) + (1 - y_i) \\log (1 - q(\\mathbf{x}_i))\\right]\\;.\n",
        "\\end{aligned}\n",
        " \\qquad(3)$$\n",
        "\n",
        "<span class=\"theorem-title\">**Definition 2 (Binary Cross-Entropy Loss)**</span> The *binary cross entropy* between a collection of predicted probabilities $\\mathbf{q}= (q_1,q_2,\\ldots,q_n) \\in [0,1]^n$ and true labels $\\mathbf{y}= (y_1,y_2,\\ldots,y_n) \\in \\{0,1\\}^n$ is given by the formula\n",
        "\n",
        "$$\n",
        "\\mathrm{CE}(\\mathbf{y}, \\mathbf{q}) = -\\sum_{i = 1}^n \\left[y_i \\log q_i + (1 - y_i) \\log (1 - q_i)\\right]\\;.\n",
        " \\qquad(4)$$\n",
        "\n",
        "While `torch` implements some bespoke cross-entropy loss functions optimized for various cases, it’s also a quick formula to implement by hand:"
      ],
      "id": "3f4b2914-c858-4105-ba27-52222f6b1323"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO"
      ],
      "id": "963327bc"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So, using the binary cross entropy, our loss function for classification in this model is given by\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    -\\mathcal{L}(\\mathbf{X}, \\mathbf{y}; q) = \\mathrm{CE}(\\mathbf{y}, \\mathbf{q}(\\mathbf{X}))\\;,\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where here we are letting $\\mathbf{q}(\\mathbf{X}) = (q(\\mathbf{x}_1), q(\\mathbf{x}_2), \\ldots, q(\\mathbf{x}_n))$ be the vector of predicted probabilities for each data point. We’d like to find a choice of the signal function $q$ that makes this loss as small as possible, which as usual is equivalent to maximizing the log-likelihood.\n",
        "\n",
        "### Binary Logistic Regression\n",
        "\n",
        "To complete an algorithm for binary classification, we need to specify the set of possible signal functions $q$ that we are going to search over. In *logistic regression*, we consider signal functions which consist of applying the *logistic sigmoid* to a linear function of the features.\n",
        "\n",
        "<span class=\"theorem-title\">**Definition 3 (Logistic Sigmoid)**</span> The logistic sigmoid $\\sigma: \\mathbb{R}\\rightarrow (0, 1)$ is the function with formula\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\\;.\n",
        " \\qquad(5)$$\n",
        "\n",
        "Plot of the logistic sigmoid $\\sigma$.\n",
        "\n",
        "Here’s a quick implementation:"
      ],
      "id": "54e38730-206c-4327-9cd5-369847f03a37"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO"
      ],
      "id": "65b0af7d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The logistic sigmoid sends any numerical input to a value between 0 and 1, which makes it a natural choice for modeling probabilities. In logistic regression, we apply the logistic sigmoid to a linear function of the features, which gives us the following form for the signal function $q$:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    q(\\mathbf{x}_i) = \\sigma(\\mathbf{x}_i^\\top \\mathbf{w})\\;. \n",
        "\\end{aligned}\n",
        " \\qquad(6)$$\n",
        "\n",
        "If we insert <a href=\"#eq-logistic-regression\" class=\"quarto-xref\">Equation 6</a> into <a href=\"#eq-cross-entropy\" class=\"quarto-xref\">Equation 4</a>, we get the following expression for the cross-entropy of the predictions and data for a given parameter vector $\\mathbf{w}$:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    -\\mathcal{L}(\\mathbf{X}, \\mathbf{y}; \\mathbf{w}) &= \\mathrm{CE}(\\mathbf{y}, \\mathbf{q}(\\mathbf{X})) \\\\ \n",
        "    &= \\mathrm{CE}(\\mathbf{y}, \\sigma(\\mathbf{X}\\mathbf{w})) &\\quad \\text{(sigmoid applied entrywise to $\\mathbf{X}\\mathbf{w}$)} \\\\\n",
        "    &=  -\\sum_{i = 1}^n \\left[y_i \\log \\sigma(\\mathbf{x}_i^\\top \\mathbf{w}) + (1 - y_i) \\log (1 - \\sigma(\\mathbf{x}_i^\\top \\mathbf{w}))\\right]\\;.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Much like with linear regression, it’s typical to normalize by the number of data points $n$ to get an average log-likelihood per data point, which gives our final formula for the loss function in binary logistic regression: <span class=\"column-margin margin-aside\">This normalization is why we used `.mean()` instead of `.sum()` in the `binary_cross_entropy` function above.</span>\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    \\mathrm{Loss} &= \\frac{1}{n} \\sum_{i = 1}^n \\left[y_i \\log \\sigma(\\mathbf{x}_i^\\top \\mathbf{w}) + (1 - y_i) \\log (1 - \\sigma(\\mathbf{x}_i^\\top \\mathbf{w}))\\right]\\;.\n",
        "\\end{aligned}\n",
        " \\qquad(7)$$\n",
        "\n",
        "### Implementation of Binary Logistic Regression\n",
        "\n",
        "To implement binary logistic regression, we can use largely the same machinery that we used for linear regression. The `forward` method will compute the value of the signal function $q(\\mathbf{x}_i) = \\sigma(\\mathbf{x}_i^\\top \\mathbf{w})$ for each data point."
      ],
      "id": "3fa732a6-2b34-4413-88a8-fce736ad8d81"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO"
      ],
      "id": "d832b707"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we’re ready to train the model using gradient descent on the parameters $\\mathbf{w}$. The gradient of a single term in the binary cross-entropy loss is given by the formula\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    \\nabla \\mathcal{L}(\\mathbf{x}_i, y_i; \\mathbf{w}) = (\\sigma(\\mathbf{x}_i^\\top \\mathbf{w}) - y_i) \\mathbf{x}_i\\;,\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "which means that the gradient of the full loss is given by\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    \\nabla \\mathcal{L}(\\mathbf{X}, \\mathbf{y}; \\mathbf{w}) = \\frac{1}{n} \\sum_{i = 1}^n (\\sigma(\\mathbf{x}_i^\\top \\mathbf{w}) - y_i) \\mathbf{x}_i\\;.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Let’s build in these calculations to a class for performing gradient descent optimization."
      ],
      "id": "084cc177-c08c-49c7-b700-10f1a17ef3fe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO"
      ],
      "id": "596a87f5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO"
      ],
      "id": "ec6da9d8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s visualize the learned signal function:"
      ],
      "id": "bfc2b83b-d45b-49d1-a62a-44fa6d7de79c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_1_grid = torch.linspace(X.min(), X.max(), 100)\n",
        "x_2_grid = torch.linspace(X.min(), X.max(), 100)\n",
        "xx, yy = torch.meshgrid(x_1_grid, x_2_grid, indexing='ij')\n",
        "grid = torch.cat([torch.ones_like(xx).reshape(-1, 1), xx.reshape(-1, 1), yy.reshape(-1, 1)], dim=1)\n",
        "with torch.no_grad():\n",
        "    q_grid = model.forward(grid).reshape(xx.shape)\n",
        "\n",
        "fig, axarr = plt.subplots(1, 2, figsize=(8, 4))\n",
        "\n",
        "axarr[0].plot(losses, color = \"grey\")\n",
        "axarr[0].set(title='Loss over training', xlabel='Epoch', ylabel='Binary cross-entropy loss')\n",
        "axarr[0].set_ylim(0, 1)\n",
        "axarr[1].set(title='Learned signal\\nfunction $q$', xlabel=r'$x_1$', ylabel=r'$x_2$')\n",
        "\n",
        "\n",
        "axarr[1].contourf(xx, yy, q_grid, levels=torch.linspace(0, 1, steps=10), alpha=0.3, cmap='BrBG', extent = (x_1_grid.min(), x_1_grid.max(), x_2_grid.min(), x_2_grid.max()))\n",
        "axarr[1].contour(xx, yy, q_grid, levels=[0.5], colors='k', linewidths=1, extent = (x_1_grid.min(), x_1_grid.max(), x_2_grid.min(), x_2_grid.max()), linestyles='--')\n",
        "\n",
        "for i in range(2): \n",
        "    idx = (y.flatten() == i)\n",
        "    axarr[1].scatter(X[idx, 1], X[idx, 2], c = y.flatten()[idx], label=f'Class {i}', edgecolor='k', marker=markers[i],   cmap = 'BrBG', vmin = -0.5, vmax = 1.5, alpha = 0.7)\n",
        "\n",
        "plt.tight_layout()"
      ],
      "id": "0554f6aa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can obtain predictions from the model by thresholding the signal function, for example at $q^* = 0.5$ as shown in the contour plot."
      ],
      "id": "9576c63b-e2fd-41d8-844c-d1166909144a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO"
      ],
      "id": "54f6409b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As usual, to fully assess the classifier we would evaluate the accuracy on a held-out test set.\n",
        "\n",
        "### Data Case Study\n",
        "\n",
        "As an empirical test of our logistic regression classifier, let’s train a model to predict rainfall based on the previous day’s weather."
      ],
      "id": "34d0f76a-9d9d-4212-a313-19f30d119ff0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "url = \"https://raw.githubusercontent.com/middcs/data-science-notes/refs/heads/main/data/australia-weather/weatherAUS.csv\"\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "df.head()\n",
        "# df[\"Pressure9am\"] = df[\"Pressure9am\"]/1e3\n",
        "# df[\"Humidity3pm\"] = df[\"Humidity3pm\"]/100\n",
        "\n",
        "df.dropna(subset=['RainTomorrow', 'Humidity3pm', 'Pressure3pm'], inplace=True)"
      ],
      "id": "4e945d6b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the purposes of this example, we’ll use just two features: the humidity at 3pm and the pressure at 3pm, and use these to predict whether or not rain occurs the next day."
      ],
      "id": "57e176c4-d824-4a8a-9d10-9ec9d00c1143"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "\n",
        "subset = df.head(1000)\n",
        "y = subset['RainTomorrow'].apply(lambda x: 1 if x == 'Yes' else 0).to_numpy().reshape(-1, 1)\n",
        "X = subset[['Humidity3pm', 'Pressure3pm']].to_numpy()\n",
        "\n",
        "\n",
        "for i, label in enumerate([\"No rain tomorrow\", \"Rain tomorrow\"]): \n",
        "    idx = y.flatten() == i\n",
        "    ax.scatter(X[idx, 0], X[idx, 1], c = y.flatten()[idx], label=f'{label}', edgecolor='k', marker=markers[i],   cmap = 'BrBG', vmin = -0.5, vmax = 1.5, alpha = 0.7)\n",
        "\n",
        "ax.set(xlabel='Humidity3pm', ylabel='Pressure3pm')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "t = plt.title(\"Tomorrow's rain as a function of today's humidity and pressure\")"
      ],
      "id": "6871695c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We’ll start our modeling pipeline by producing a feature matrix and label vector from the data."
      ],
      "id": "09660ae4-275d-491d-8dfa-306773d066fe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO"
      ],
      "id": "dc18033f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we’ll perform a train-test split to evaluate the model’s performance on held-out data."
      ],
      "id": "9e35d9e8-d919-4127-830a-08a912bf1ae2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO"
      ],
      "id": "75ef6d1c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we’re ready to train the logistic regression model."
      ],
      "id": "971a3c51-9cc6-4803-9e4d-39ef48b5d085"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO"
      ],
      "id": "3c3fb15d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Figure 3: Visualization of the binary cross-entropy loss over training epochs."
      ],
      "id": "b46f78b6-107d-4a5b-8133-c433f05405d6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(losses, color = \"grey\")\n",
        "t = plt.title('Loss over training')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Binary cross-entropy loss')\n",
        "plt.ylim(0, 1)\n",
        "plt.semilogx()"
      ],
      "id": "9832e51c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s plot the learned signal function $q$ in feature space, along with a selection from the test data:\n",
        "\n",
        "Figure 4: Learned decision boundary (dashed line) in feature space, along with 1,000 of the test data points."
      ],
      "id": "dd426ecb-2ba4-491b-8fd7-c314a8320ca7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x1_grid = torch.linspace(X[:,0].min(), X[:,0].max(), 100)\n",
        "x2_grid = torch.linspace(X[:,1].min(), X[:,1].max(), 100)\n",
        "xx1, xx2 = torch.meshgrid(x1_grid, x2_grid, indexing='ij')\n",
        "grid = torch.cat([torch.ones_like(xx1).reshape(-1, 1), xx1.reshape(-1, 1), xx2.reshape(-1, 1)], dim=1)\n",
        "with torch.no_grad():\n",
        "    q_grid = model.forward(grid).reshape(xx1.shape)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "\n",
        "ax.contour(xx1, xx2, q_grid, levels=[0.5], colors='k', linewidths=1, extent = (x1_grid.min(), x1_grid.max(), x2_grid.min(), x2_grid.max()), linestyles='--')\n",
        "ax.contourf(xx1, xx2, q_grid, levels=torch.linspace(0, 1, steps=10), alpha=0.3, cmap='BrBG', extent = (x1_grid.min(), x1_grid.max(), x2_grid.min(), x2_grid.max()))\n",
        "\n",
        "\n",
        "ax.set(title='Learned decision boundary', xlabel='Humidity3pm', ylabel='Pressure3pm')\n",
        "\n",
        "for i, label in enumerate([\"No rain tomorrow\", \"Rain tomorrow\"]): \n",
        "    idx = (y_test[:1000, 0] == i)\n",
        "    ax.scatter(X_test[:1000, 0][idx], X_test[:1000, 1][idx], c = y_test[:1000, 0][idx], label=f'{label}', edgecolor='k', marker=markers[i],   cmap = 'BrBG', vmin = -0.5, vmax = 1.5, alpha = 0.7)\n",
        "\n",
        "plt.legend()"
      ],
      "id": "5e60db7b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To evaluate this model on the test set, we need to evaluate the model on the test features and threshold the results to obtain class predictions."
      ],
      "id": "634a1d71-a1ed-438f-995e-cbcdf4299403"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# obtain the probabilities q_test\n",
        "X_test_aug = torch.ones((X_test.shape[0], n_features))\n",
        "X_test_aug[:, 1:] = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "q_test = model.forward(X_test_aug)\n",
        "\n",
        "# obtain class predictions via thresholding and compute accuracy\n",
        "y_test_pred = (q_test >= 0.5).float()\n",
        "accuracy_test = (y_test_pred == y_test_tensor).float().mean()\n",
        "print(f'Test accuracy: {accuracy_test:.3f}')"
      ],
      "id": "90e307a5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This accuracy is slightly better than the result we would obtain by always guessing that it won’t rain:"
      ],
      "id": "80cc5bee-8b06-4609-ba17-cdb93afbc731"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO"
      ],
      "id": "2dd8fd39"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our learned classifier does slightly better than baseline in predicting whether or not will rain tomorrow, as measured by accuracy on the test set."
      ],
      "id": "41149a8f-93b1-4f17-af46-588d3ea2f50b"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "cs451",
      "display_name": "Python (cs451)",
      "language": "python",
      "path": "/Users/philchodrow/Library/Jupyter/kernels/cs451"
    }
  }
}